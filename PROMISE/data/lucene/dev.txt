package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Query ; import org . apache . lucene . search . Weight ; import org . apache . lucene . search . Searcher ; public abstract class SpanQuery extends Query { public abstract Spans getSpans ( IndexReader reader ) throws IOException ; public abstract String getField ( ) ; public abstract Collection getTerms ( ) ; protected Weight createWeight ( Searcher searcher ) throws IOException { return new SpanWeight ( this , searcher ) ; } } 	0
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . util . PriorityQueue ; import org . apache . lucene . util . ToStringUtils ; import java . io . IOException ; public class FuzzyQuery extends MultiTermQuery { public final static float defaultMinSimilarity = 0.5f ; public final static int defaultPrefixLength = 0 ; private float minimumSimilarity ; private int prefixLength ; public FuzzyQuery ( Term term , float minimumSimilarity , int prefixLength ) throws IllegalArgumentException { super ( term ) ; if ( minimumSimilarity >= 1.0f ) throw new IllegalArgumentException ( "minimumSimilarity >= 1" ) ; else if ( minimumSimilarity < 0.0f ) throw new IllegalArgumentException ( "minimumSimilarity < 0" ) ; if ( prefixLength < 0 ) throw new IllegalArgumentException ( "prefixLength < 0" ) ; this . minimumSimilarity = minimumSimilarity ; this . prefixLength = prefixLength ; } public FuzzyQuery ( Term term , float minimumSimilarity ) throws IllegalArgumentException { this ( term , minimumSimilarity , defaultPrefixLength ) ; } public FuzzyQuery ( Term term ) { this ( term , defaultMinSimilarity , defaultPrefixLength ) ; } public float getMinSimilarity ( ) { return minimumSimilarity ; } public int getPrefixLength ( ) { return prefixLength ; } protected FilteredTermEnum getEnum ( IndexReader reader ) throws IOException { return new FuzzyTermEnum ( reader , getTerm ( ) , minimumSimilarity , prefixLength ) ; } public Query rewrite ( IndexReader reader ) throws IOException { FilteredTermEnum enumerator = getEnum ( reader ) ; int maxClauseCount = BooleanQuery . getMaxClauseCount ( ) ; ScoreTermQueue stQueue = new ScoreTermQueue ( maxClauseCount ) ; try { do { float minScore = 0.0f ; float score = 0.0f ; Term t = enumerator . term ( ) ; if ( t != null ) { score = enumerator . difference ( ) ; if ( stQueue . size ( ) < maxClauseCount || score > minScore ) { stQueue . insert ( new ScoreTerm ( t , score ) ) ; minScore = ( ( ScoreTerm ) stQueue . top ( ) ) . score ; } } } while ( enumerator . next ( ) ) ; } finally { enumerator . close ( ) ; } BooleanQuery query = new BooleanQuery ( true ) ; int size = stQueue . size ( ) ; for ( int i = 0 ; i < size ; i ++ ) { ScoreTerm st = ( ScoreTerm ) stQueue . pop ( ) ; TermQuery tq = new TermQuery ( st . term ) ; tq . setBoost ( getBoost ( ) * st . score ) ; query . add ( tq , BooleanClause . Occur . SHOULD ) ; } return query ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; Term term = getTerm ( ) ; if ( ! term . field ( ) . equals ( field ) ) { buffer . append ( term . field ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( term . text ( ) ) ; buffer . append ( '~' ) ; buffer . append ( Float . toString ( minimumSimilarity ) ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } protected static class ScoreTerm { public Term term ; public float score ; public ScoreTerm ( Term term , float score ) { this . term = term ; this . score = score ; } } protected static class ScoreTermQueue extends PriorityQueue { public ScoreTermQueue ( int size ) { initialize ( size ) ; } protected boolean lessThan ( Object a , Object b ) { ScoreTerm termA = ( ScoreTerm ) a ; ScoreTerm termB = ( ScoreTerm ) b ; if ( termA . score == termB . score ) return termA . term . compareTo ( termB . term ) > 0 ; else return termA . score < termB . score ; } } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof FuzzyQuery ) ) return false ; if ( ! super . equals ( o ) ) return false ; final FuzzyQuery fuzzyQuery = ( FuzzyQuery ) o ; if ( minimumSimilarity != fuzzyQuery . minimumSimilarity ) return false ; if ( prefixLength != fuzzyQuery . prefixLength ) return false ; return true ; } public int hashCode ( ) { int result = super . hashCode ( ) ; result = 29 * result + minimumSimilarity != + 0.0f ? Float . floatToIntBits ( minimumSimilarity ) : 0 ; result = 29 * result + prefixLength ; return result ; } } 	1
package org . apache . lucene . analysis . standard ; public interface CharStream { char readChar ( ) throws java . io . IOException ; int getColumn ( ) ; int getLine ( ) ; int getEndColumn ( ) ; int getEndLine ( ) ; int getBeginColumn ( ) ; int getBeginLine ( ) ; void backup ( int amount ) ; char BeginToken ( ) throws java . io . IOException ; String GetImage ( ) ; char [ ] GetSuffix ( int len ) ; void Done ( ) ; } 	0
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import org . apache . lucene . store . BufferedIndexInput ; import org . apache . lucene . store . IndexInput ; abstract class MultiLevelSkipListReader { private int maxNumberOfSkipLevels ; private int numberOfSkipLevels ; private int numberOfLevelsToBuffer = 1 ; private int docCount ; private boolean haveSkipped ; private IndexInput [ ] skipStream ; private long skipPointer [ ] ; private int skipInterval [ ] ; private int [ ] numSkipped ; private int [ ] skipDoc ; private int lastDoc ; private long [ ] childPointer ; private long lastChildPointer ; private boolean inputIsBuffered ; public MultiLevelSkipListReader ( IndexInput skipStream , int maxSkipLevels , int skipInterval ) { this . skipStream = new IndexInput [ maxSkipLevels ] ; this . skipPointer = new long [ maxSkipLevels ] ; this . childPointer = new long [ maxSkipLevels ] ; this . numSkipped = new int [ maxSkipLevels ] ; this . maxNumberOfSkipLevels = maxSkipLevels ; this . skipInterval = new int [ maxSkipLevels ] ; this . skipStream [ 0 ] = skipStream ; this . inputIsBuffered = ( skipStream instanceof BufferedIndexInput ) ; this . skipInterval [ 0 ] = skipInterval ; for ( int i = 1 ; i < maxSkipLevels ; i ++ ) { this . skipInterval [ i ] = this . skipInterval [ i - 1 ] * skipInterval ; } skipDoc = new int [ maxSkipLevels ] ; } int getDoc ( ) { return lastDoc ; } int skipTo ( int target ) throws IOException { if ( ! haveSkipped ) { loadSkipLevels ( ) ; haveSkipped = true ; } int level = 0 ; while ( level < numberOfSkipLevels - 1 && target > skipDoc [ level + 1 ] ) { level ++ ; } while ( level >= 0 ) { if ( target > skipDoc [ level ] ) { if ( ! loadNextSkip ( level ) ) { continue ; } } else { if ( level > 0 && lastChildPointer > skipStream [ level - 1 ] . getFilePointer ( ) ) { seekChild ( level - 1 ) ; } level -- ; } } return numSkipped [ 0 ] - skipInterval [ 0 ] - 1 ; } private boolean loadNextSkip ( int level ) throws IOException { setLastSkipData ( level ) ; numSkipped [ level ] += skipInterval [ level ] ; if ( numSkipped [ level ] > docCount ) { skipDoc [ level ] = Integer . MAX_VALUE ; if ( numberOfSkipLevels > level ) numberOfSkipLevels = level ; return false ; } skipDoc [ level ] += readSkipData ( level , skipStream [ level ] ) ; if ( level != 0 ) { childPointer [ level ] = skipStream [ level ] . readVLong ( ) + skipPointer [ level - 1 ] ; } return true ; } protected void seekChild ( int level ) throws IOException { skipStream [ level ] . seek ( lastChildPointer ) ; numSkipped [ level ] = numSkipped [ level + 1 ] - skipInterval [ level + 1 ] ; skipDoc [ level ] = lastDoc ; if ( level > 0 ) { childPointer [ level ] = skipStream [ level ] . readVLong ( ) + skipPointer [ level - 1 ] ; } } void close ( ) throws IOException { for ( int i = 1 ; i < skipStream . length ; i ++ ) { if ( skipStream [ i ] != null ) { skipStream [ i ] . close ( ) ; } } } void init ( long skipPointer , int df ) { this . skipPointer [ 0 ] = skipPointer ; this . docCount = df ; Arrays . fill ( skipDoc , 0 ) ; Arrays . fill ( numSkipped , 0 ) ; haveSkipped = false ; for ( int i = 1 ; i < numberOfSkipLevels ; i ++ ) { skipStream [ 0 ] = null ; } } private void loadSkipLevels ( ) throws IOException { numberOfSkipLevels = docCount == 0 ? 0 : ( int ) Math . floor ( Math . log ( docCount ) / Math . log ( skipInterval [ 0 ] ) ) ; if ( numberOfSkipLevels > maxNumberOfSkipLevels ) { numberOfSkipLevels = maxNumberOfSkipLevels ; } skipStream [ 0 ] . seek ( skipPointer [ 0 ] ) ; int toBuffer = numberOfLevelsToBuffer ; for ( int i = numberOfSkipLevels - 1 ; i > 0 ; i -- ) { long length = skipStream [ 0 ] . readVLong ( ) ; skipPointer [ i ] = skipStream [ 0 ] . getFilePointer ( ) ; if ( toBuffer > 0 ) { skipStream [ i ] = new SkipBuffer ( skipStream [ 0 ] , ( int ) length ) ; toBuffer -- ; } else { skipStream [ i ] = ( IndexInput ) skipStream [ 0 ] . clone ( ) ; if ( inputIsBuffered && length < BufferedIndexInput . BUFFER_SIZE ) { ( ( BufferedIndexInput ) skipStream [ i ] ) . setBufferSize ( ( int ) length ) ; } skipStream [ 0 ] . seek ( skipStream [ 0 ] . getFilePointer ( ) + length ) ; } } skipPointer [ 0 ] = skipStream [ 0 ] . getFilePointer ( ) ; } protected abstract int readSkipData ( int level , IndexInput skipStream ) throws IOException ; protected void setLastSkipData ( int level ) { lastDoc = skipDoc [ level ] ; lastChildPointer = childPointer [ level ] ; } private final static class SkipBuffer extends IndexInput { private byte [ ] data ; private long pointer ; private int pos ; SkipBuffer ( IndexInput input , int length ) throws IOException { data = new byte [ length ] ; pointer = input . getFilePointer ( ) ; input . readBytes ( data , 0 , length ) ; } public void close ( ) throws IOException { data = null ; } public long getFilePointer ( ) { return pointer + pos ; } public long length ( ) { return data . length ; } public byte readByte ( ) throws IOException { return data [ pos ++ ] ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { System . arraycopy ( data , pos , b , offset , len ) ; pos += len ; } public void seek ( long pos ) throws IOException { this . pos = ( int ) ( pos - pointer ) ; } } } 	1
package org . apache . lucene . analysis . standard ; import java . io . * ; public final class FastCharStream implements CharStream { char [ ] buffer = null ; int bufferLength = 0 ; int bufferPosition = 0 ; int tokenStart = 0 ; int bufferStart = 0 ; Reader input ; public FastCharStream ( Reader r ) { input = r ; } public final char readChar ( ) throws IOException { if ( bufferPosition >= bufferLength ) refill ( ) ; return buffer [ bufferPosition ++ ] ; } private final void refill ( ) throws IOException { int newPosition = bufferLength - tokenStart ; if ( tokenStart == 0 ) { if ( buffer == null ) { buffer = new char [ 2048 ] ; } else if ( bufferLength == buffer . length ) { char [ ] newBuffer = new char [ buffer . length * 2 ] ; System . arraycopy ( buffer , 0 , newBuffer , 0 , bufferLength ) ; buffer = newBuffer ; } } else { System . arraycopy ( buffer , tokenStart , buffer , 0 , newPosition ) ; } bufferLength = newPosition ; bufferPosition = newPosition ; bufferStart += tokenStart ; tokenStart = 0 ; int charsRead = input . read ( buffer , newPosition , buffer . length - newPosition ) ; if ( charsRead == - 1 ) throw new IOException ( "read past eof" ) ; else bufferLength += charsRead ; } public final char BeginToken ( ) throws IOException { tokenStart = bufferPosition ; return readChar ( ) ; } public final void backup ( int amount ) { bufferPosition -= amount ; } public final String GetImage ( ) { return new String ( buffer , tokenStart , bufferPosition - tokenStart ) ; } public final char [ ] GetSuffix ( int len ) { char [ ] value = new char [ len ] ; System . arraycopy ( buffer , bufferPosition - len , value , 0 , len ) ; return value ; } public final void Done ( ) { try { input . close ( ) ; } catch ( IOException e ) { System . err . println ( "Caught: " + e + "; ignoring." ) ; } } public final int getColumn ( ) { return bufferStart + bufferPosition ; } public final int getLine ( ) { return 1 ; } public final int getEndColumn ( ) { return bufferStart + bufferPosition ; } public final int getEndLine ( ) { return 1 ; } public final int getBeginColumn ( ) { return bufferStart + tokenStart ; } public final int getBeginLine ( ) { return 1 ; } } 	0
package org . apache . lucene . store ; import java . io . IOException ; import java . io . FileNotFoundException ; import java . io . File ; import java . io . Serializable ; import java . util . HashMap ; import java . util . Iterator ; import java . util . Set ; public class RAMDirectory extends Directory implements Serializable { private static final long serialVersionUID = 1l ; HashMap fileMap = new HashMap ( ) ; long sizeInBytes = 0 ; public RAMDirectory ( ) { setLockFactory ( new SingleInstanceLockFactory ( ) ) ; } public RAMDirectory ( Directory dir ) throws IOException { this ( dir , false ) ; } private RAMDirectory ( Directory dir , boolean closeDir ) throws IOException { this ( ) ; Directory . copy ( dir , this , closeDir ) ; } public RAMDirectory ( File dir ) throws IOException { this ( FSDirectory . getDirectory ( dir ) , true ) ; } public RAMDirectory ( String dir ) throws IOException { this ( FSDirectory . getDirectory ( dir ) , true ) ; } public synchronized final String [ ] list ( ) { ensureOpen ( ) ; Set fileNames = fileMap . keySet ( ) ; String [ ] result = new String [ fileNames . size ( ) ] ; int i = 0 ; Iterator it = fileNames . iterator ( ) ; while ( it . hasNext ( ) ) result [ i ++ ] = ( String ) it . next ( ) ; return result ; } public final boolean fileExists ( String name ) { ensureOpen ( ) ; RAMFile file ; synchronized ( this ) { file = ( RAMFile ) fileMap . get ( name ) ; } return file != null ; } public final long fileModified ( String name ) throws IOException { ensureOpen ( ) ; RAMFile file ; synchronized ( this ) { file = ( RAMFile ) fileMap . get ( name ) ; } if ( file == null ) throw new FileNotFoundException ( name ) ; return file . getLastModified ( ) ; } public void touchFile ( String name ) throws IOException { ensureOpen ( ) ; RAMFile file ; synchronized ( this ) { file = ( RAMFile ) fileMap . get ( name ) ; } if ( file == null ) throw new FileNotFoundException ( name ) ; long ts2 , ts1 = System . currentTimeMillis ( ) ; do { try { Thread . sleep ( 0 , 1 ) ; } catch ( InterruptedException e ) { } ts2 = System . currentTimeMillis ( ) ; } while ( ts1 == ts2 ) ; file . setLastModified ( ts2 ) ; } public final long fileLength ( String name ) throws IOException { ensureOpen ( ) ; RAMFile file ; synchronized ( this ) { file = ( RAMFile ) fileMap . get ( name ) ; } if ( file == null ) throw new FileNotFoundException ( name ) ; return file . getLength ( ) ; } public synchronized final long sizeInBytes ( ) { ensureOpen ( ) ; return sizeInBytes ; } public synchronized void deleteFile ( String name ) throws IOException { ensureOpen ( ) ; RAMFile file = ( RAMFile ) fileMap . get ( name ) ; if ( file != null ) { fileMap . remove ( name ) ; file . directory = null ; sizeInBytes -= file . sizeInBytes ; } else throw new FileNotFoundException ( name ) ; } public synchronized final void renameFile ( String from , String to ) throws IOException { ensureOpen ( ) ; RAMFile fromFile = ( RAMFile ) fileMap . get ( from ) ; if ( fromFile == null ) throw new FileNotFoundException ( from ) ; RAMFile toFile = ( RAMFile ) fileMap . get ( to ) ; if ( toFile != null ) { sizeInBytes -= toFile . sizeInBytes ; toFile . directory = null ; } fileMap . remove ( from ) ; fileMap . put ( to , fromFile ) ; } public IndexOutput createOutput ( String name ) { ensureOpen ( ) ; RAMFile file = new RAMFile ( this ) ; synchronized ( this ) { RAMFile existing = ( RAMFile ) fileMap . get ( name ) ; if ( existing != null ) { sizeInBytes -= existing . sizeInBytes ; existing . directory = null ; } fileMap . put ( name , file ) ; } return new RAMOutputStream ( file ) ; } public IndexInput openInput ( String name ) throws IOException { ensureOpen ( ) ; RAMFile file ; synchronized ( this ) { file = ( RAMFile ) fileMap . get ( name ) ; } if ( file == null ) throw new FileNotFoundException ( name ) ; return new RAMInputStream ( file ) ; } public void close ( ) { fileMap = null ; } protected final void ensureOpen ( ) throws AlreadyClosedException { if ( fileMap == null ) { throw new AlreadyClosedException ( "this RAMDirectory is closed" ) ; } } } 	1
package org . apache . lucene . document ; public class LoadFirstFieldSelector implements FieldSelector { public FieldSelectorResult accept ( String fieldName ) { return FieldSelectorResult . LOAD_AND_BREAK ; } } 	0
package org . apache . lucene . index ; import java . io . IOException ; public interface TermPositions extends TermDocs { int nextPosition ( ) throws IOException ; int getPayloadLength ( ) ; byte [ ] getPayload ( byte [ ] data , int offset ) throws IOException ; public boolean isPayloadAvailable ( ) ; } 	1
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import org . apache . lucene . store . IndexInput ; class DefaultSkipListReader extends MultiLevelSkipListReader { private boolean currentFieldStoresPayloads ; private long freqPointer [ ] ; private long proxPointer [ ] ; private int payloadLength [ ] ; private long lastFreqPointer ; private long lastProxPointer ; private int lastPayloadLength ; DefaultSkipListReader ( IndexInput skipStream , int maxSkipLevels , int skipInterval ) { super ( skipStream , maxSkipLevels , skipInterval ) ; freqPointer = new long [ maxSkipLevels ] ; proxPointer = new long [ maxSkipLevels ] ; payloadLength = new int [ maxSkipLevels ] ; } void init ( long skipPointer , long freqBasePointer , long proxBasePointer , int df , boolean storesPayloads ) { super . init ( skipPointer , df ) ; this . currentFieldStoresPayloads = storesPayloads ; lastFreqPointer = freqBasePointer ; lastProxPointer = proxBasePointer ; Arrays . fill ( freqPointer , freqBasePointer ) ; Arrays . fill ( proxPointer , proxBasePointer ) ; Arrays . fill ( payloadLength , 0 ) ; } long getFreqPointer ( ) { return lastFreqPointer ; } long getProxPointer ( ) { return lastProxPointer ; } int getPayloadLength ( ) { return lastPayloadLength ; } protected void seekChild ( int level ) throws IOException { super . seekChild ( level ) ; freqPointer [ level ] = lastFreqPointer ; proxPointer [ level ] = lastProxPointer ; payloadLength [ level ] = lastPayloadLength ; } protected void setLastSkipData ( int level ) { super . setLastSkipData ( level ) ; lastFreqPointer = freqPointer [ level ] ; lastProxPointer = proxPointer [ level ] ; lastPayloadLength = payloadLength [ level ] ; } protected int readSkipData ( int level , IndexInput skipStream ) throws IOException { int delta ; if ( currentFieldStoresPayloads ) { delta = skipStream . readVInt ( ) ; if ( ( delta & 1 ) != 0 ) { payloadLength [ level ] = skipStream . readVInt ( ) ; } delta >>>= 1 ; } else { delta = skipStream . readVInt ( ) ; } freqPointer [ level ] += skipStream . readVInt ( ) ; proxPointer [ level ] += skipStream . readVInt ( ) ; return delta ; } } 	0
package org . apache . lucene . search . function ; import org . apache . lucene . search . Explanation ; public abstract class DocValues { private int nVals ; public DocValues ( int nVals ) { this . nVals = nVals ; } private DocValues ( ) { } public abstract float floatVal ( int doc ) ; public int intVal ( int doc ) { return ( int ) floatVal ( doc ) ; } public long longVal ( int doc ) { return ( long ) floatVal ( doc ) ; } public double doubleVal ( int doc ) { return ( double ) floatVal ( doc ) ; } public String strVal ( int doc ) { return Float . toString ( floatVal ( doc ) ) ; } public abstract String toString ( int doc ) ; public Explanation explain ( int doc ) { return new Explanation ( floatVal ( doc ) , toString ( doc ) ) ; } Object getInnerArray ( ) { return new Object [ 0 ] ; } private float minVal ; private float maxVal ; private float avgVal ; private boolean computed = false ; private void compute ( ) { if ( computed ) { return ; } minVal = Float . MAX_VALUE ; maxVal = 0 ; float sum = 0 ; for ( int i = 0 ; i < nVals ; i ++ ) { float val = floatVal ( i ) ; sum += val ; minVal = Math . min ( minVal , val ) ; maxVal = Math . max ( maxVal , val ) ; } avgVal = sum / nVals ; computed = true ; } public float getMinValue ( ) { compute ( ) ; return minVal ; } public float getMaxValue ( ) { compute ( ) ; return maxVal ; } public float getAverageValue ( ) { compute ( ) ; return avgVal ; } } 	1
package org . apache . lucene . search ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . FieldSelector ; import org . apache . lucene . index . CorruptIndexException ; import org . apache . lucene . index . Term ; import java . io . IOException ; import java . util . HashMap ; import java . util . HashSet ; import java . util . Map ; import java . util . Set ; public class MultiSearcher extends Searcher { private static class CachedDfSource extends Searcher { private Map dfMap ; private int maxDoc ; public CachedDfSource ( Map dfMap , int maxDoc , Similarity similarity ) { this . dfMap = dfMap ; this . maxDoc = maxDoc ; setSimilarity ( similarity ) ; } public int docFreq ( Term term ) { int df ; try { df = ( ( Integer ) dfMap . get ( term ) ) . intValue ( ) ; } catch ( NullPointerException e ) { throw new IllegalArgumentException ( "df for term " + term . text ( ) + " not available" ) ; } return df ; } public int [ ] docFreqs ( Term [ ] terms ) { int [ ] result = new int [ terms . length ] ; for ( int i = 0 ; i < terms . length ; i ++ ) { result [ i ] = docFreq ( terms [ i ] ) ; } return result ; } public int maxDoc ( ) { return maxDoc ; } public Query rewrite ( Query query ) { return query ; } public void close ( ) { throw new UnsupportedOperationException ( ) ; } public Document doc ( int i ) { throw new UnsupportedOperationException ( ) ; } public Document doc ( int i , FieldSelector fieldSelector ) { throw new UnsupportedOperationException ( ) ; } public Explanation explain ( Weight weight , int doc ) { throw new UnsupportedOperationException ( ) ; } public void search ( Weight weight , Filter filter , HitCollector results ) { throw new UnsupportedOperationException ( ) ; } public TopDocs search ( Weight weight , Filter filter , int n ) { throw new UnsupportedOperationException ( ) ; } public TopFieldDocs search ( Weight weight , Filter filter , int n , Sort sort ) { throw new UnsupportedOperationException ( ) ; } } private Searchable [ ] searchables ; private int [ ] starts ; private int maxDoc = 0 ; public MultiSearcher ( Searchable [ ] searchables ) throws IOException { this . searchables = searchables ; starts = new int [ searchables . length + 1 ] ; for ( int i = 0 ; i < searchables . length ; i ++ ) { starts [ i ] = maxDoc ; maxDoc += searchables [ i ] . maxDoc ( ) ; } starts [ searchables . length ] = maxDoc ; } public Searchable [ ] getSearchables ( ) { return searchables ; } protected int [ ] getStarts ( ) { return starts ; } public void close ( ) throws IOException { for ( int i = 0 ; i < searchables . length ; i ++ ) searchables [ i ] . close ( ) ; } public int docFreq ( Term term ) throws IOException { int docFreq = 0 ; for ( int i = 0 ; i < searchables . length ; i ++ ) docFreq += searchables [ i ] . docFreq ( term ) ; return docFreq ; } public Document doc ( int n ) throws CorruptIndexException , IOException { int i = subSearcher ( n ) ; return searchables [ i ] . doc ( n - starts [ i ] ) ; } public Document doc ( int n , FieldSelector fieldSelector ) throws CorruptIndexException , IOException { int i = subSearcher ( n ) ; return searchables [ i ] . doc ( n - starts [ i ] , fieldSelector ) ; } public int subSearcher ( int n ) { int lo = 0 ; int hi = searchables . length - 1 ; while ( hi >= lo ) { int mid = ( lo + hi ) > > 1 ; int midValue = starts [ mid ] ; if ( n < midValue ) hi = mid - 1 ; else if ( n > midValue ) lo = mid + 1 ; else { while ( mid + 1 < searchables . length && starts [ mid + 1 ] == midValue ) { mid ++ ; } return mid ; } } return hi ; } public int subDoc ( int n ) { return n - starts [ subSearcher ( n ) ] ; } public int maxDoc ( ) throws IOException { return maxDoc ; } public TopDocs search ( Weight weight , Filter filter , int nDocs ) throws IOException { HitQueue hq = new HitQueue ( nDocs ) ; int totalHits = 0 ; for ( int i = 0 ; i < searchables . length ; i ++ ) { TopDocs docs = searchables [ i ] . search ( weight , filter , nDocs ) ; totalHits += docs . totalHits ; ScoreDoc [ ] scoreDocs = docs . scoreDocs ; for ( int j = 0 ; j < scoreDocs . length ; j ++ ) { ScoreDoc scoreDoc = scoreDocs [ j ] ; scoreDoc . doc += starts [ i ] ; if ( ! hq . insert ( scoreDoc ) ) break ; } } ScoreDoc [ ] scoreDocs = new ScoreDoc [ hq . size ( ) ] ; for ( int i = hq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = ( ScoreDoc ) hq . pop ( ) ; float maxScore = ( totalHits == 0 ) ? Float . NEGATIVE_INFINITY : scoreDocs [ 0 ] . score ; return new TopDocs ( totalHits , scoreDocs , maxScore ) ; } public TopFieldDocs search ( Weight weight , Filter filter , int n , Sort sort ) throws IOException { FieldDocSortedHitQueue hq = null ; int totalHits = 0 ; float maxScore = Float . NEGATIVE_INFINITY ; for ( int i = 0 ; i < searchables . length ; i ++ ) { TopFieldDocs docs = searchables [ i ] . search ( weight , filter , n , sort ) ; if ( hq == null ) hq = new FieldDocSortedHitQueue ( docs . fields , n ) ; totalHits += docs . totalHits ; maxScore = Math . max ( maxScore , docs . getMaxScore ( ) ) ; ScoreDoc [ ] scoreDocs = docs . scoreDocs ; for ( int j = 0 ; j < scoreDocs . length ; j ++ ) { ScoreDoc scoreDoc = scoreDocs [ j ] ; scoreDoc . doc += starts [ i ] ; if ( ! hq . insert ( scoreDoc ) ) break ; } } ScoreDoc [ ] scoreDocs = new ScoreDoc [ hq . size ( ) ] ; for ( int i = hq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = ( ScoreDoc ) hq . pop ( ) ; return new TopFieldDocs ( totalHits , scoreDocs , hq . getFields ( ) , maxScore ) ; } public void search ( Weight weight , Filter filter , final HitCollector results ) throws IOException { for ( int i = 0 ; i < searchables . length ; i ++ ) { final int start = starts [ i ] ; searchables [ i ] . search ( weight , filter , new HitCollector ( ) { public void collect ( int doc , float score ) { results . collect ( doc + start , score ) ; } } ) ; } } public Query rewrite ( Query original ) throws IOException { Query [ ] queries = new Query [ searchables . length ] ; for ( int i = 0 ; i < searchables . length ; i ++ ) { queries [ i ] = searchables [ i ] . rewrite ( original ) ; } return queries [ 0 ] . combine ( queries ) ; } public Explanation explain ( Weight weight , int doc ) throws IOException { int i = subSearcher ( doc ) ; return searchables [ i ] . explain ( weight , doc - starts [ i ] ) ; } protected Weight createWeight ( Query original ) throws IOException { Query rewrittenQuery = rewrite ( original ) ; Set terms = new HashSet ( ) ; rewrittenQuery . extractTerms ( terms ) ; Term [ ] allTermsArray = new Term [ terms . size ( ) ] ; terms . toArray ( allTermsArray ) ; int [ ] aggregatedDfs = new int [ terms . size ( ) ] ; for ( int i = 0 ; i < searchables . length ; i ++ ) { int [ ] dfs = searchables [ i ] . docFreqs ( allTermsArray ) ; for ( int j = 0 ; j < aggregatedDfs . length ; j ++ ) { aggregatedDfs [ j ] += dfs [ j ] ; } } HashMap dfMap = new HashMap ( ) ; for ( int i = 0 ; i < allTermsArray . length ; i ++ ) { dfMap . put ( allTermsArray [ i ] , new Integer ( aggregatedDfs [ i ] ) ) ; } int numDocs = maxDoc ( ) ; CachedDfSource cacheSim = new CachedDfSource ( dfMap , numDocs , getSimilarity ( ) ) ; return rewrittenQuery . weight ( cacheSim ) ; } } 	0
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermDocs ; import org . apache . lucene . index . TermEnum ; import java . io . IOException ; import java . util . Locale ; import java . util . Map ; import java . util . WeakHashMap ; import java . util . HashMap ; class FieldCacheImpl implements FieldCache { abstract static class Cache { private final Map readerCache = new WeakHashMap ( ) ; protected abstract Object createValue ( IndexReader reader , Object key ) throws IOException ; public Object get ( IndexReader reader , Object key ) throws IOException { Map innerCache ; Object value ; synchronized ( readerCache ) { innerCache = ( Map ) readerCache . get ( reader ) ; if ( innerCache == null ) { innerCache = new HashMap ( ) ; readerCache . put ( reader , innerCache ) ; value = null ; } else { value = innerCache . get ( key ) ; } if ( value == null ) { value = new CreationPlaceholder ( ) ; innerCache . put ( key , value ) ; } } if ( value instanceof CreationPlaceholder ) { synchronized ( value ) { CreationPlaceholder progress = ( CreationPlaceholder ) value ; if ( progress . value == null ) { progress . value = createValue ( reader , key ) ; synchronized ( readerCache ) { innerCache . put ( key , progress . value ) ; } } return progress . value ; } } return value ; } } static final class CreationPlaceholder { Object value ; } static class Entry { final String field ; final int type ; final Object custom ; final Locale locale ; Entry ( String field , int type , Locale locale ) { this . field = field . intern ( ) ; this . type = type ; this . custom = null ; this . locale = locale ; } Entry ( String field , Object custom ) { this . field = field . intern ( ) ; this . type = SortField . CUSTOM ; this . custom = custom ; this . locale = null ; } public boolean equals ( Object o ) { if ( o instanceof Entry ) { Entry other = ( Entry ) o ; if ( other . field == field && other . type == type ) { if ( other . locale == null ? locale == null : other . locale . equals ( locale ) ) { if ( other . custom == null ) { if ( custom == null ) return true ; } else if ( other . custom . equals ( custom ) ) { return true ; } } } } return false ; } public int hashCode ( ) { return field . hashCode ( ) ^ type ^ ( custom == null ? 0 : custom . hashCode ( ) ) ^ ( locale == null ? 0 : locale . hashCode ( ) ) ; } } private static final ByteParser BYTE_PARSER = new ByteParser ( ) { public byte parseByte ( String value ) { return Byte . parseByte ( value ) ; } } ; private static final ShortParser SHORT_PARSER = new ShortParser ( ) { public short parseShort ( String value ) { return Short . parseShort ( value ) ; } } ; private static final IntParser INT_PARSER = new IntParser ( ) { public int parseInt ( String value ) { return Integer . parseInt ( value ) ; } } ; private static final FloatParser FLOAT_PARSER = new FloatParser ( ) { public float parseFloat ( String value ) { return Float . parseFloat ( value ) ; } } ; public byte [ ] getBytes ( IndexReader reader , String field ) throws IOException { return getBytes ( reader , field , BYTE_PARSER ) ; } public byte [ ] getBytes ( IndexReader reader , String field , ByteParser parser ) throws IOException { return ( byte [ ] ) bytesCache . get ( reader , new Entry ( field , parser ) ) ; } Cache bytesCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object entryKey ) throws IOException { Entry entry = ( Entry ) entryKey ; String field = entry . field ; ByteParser parser = ( ByteParser ) entry . custom ; final byte [ ] retArray = new byte [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field , "" ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; byte termval = parser . parseByte ( term . text ( ) ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } return retArray ; } } ; public short [ ] getShorts ( IndexReader reader , String field ) throws IOException { return getShorts ( reader , field , SHORT_PARSER ) ; } public short [ ] getShorts ( IndexReader reader , String field , ShortParser parser ) throws IOException { return ( short [ ] ) shortsCache . get ( reader , new Entry ( field , parser ) ) ; } Cache shortsCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object entryKey ) throws IOException { Entry entry = ( Entry ) entryKey ; String field = entry . field ; ShortParser parser = ( ShortParser ) entry . custom ; final short [ ] retArray = new short [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field , "" ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; short termval = parser . parseShort ( term . text ( ) ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } return retArray ; } } ; public int [ ] getInts ( IndexReader reader , String field ) throws IOException { return getInts ( reader , field , INT_PARSER ) ; } public int [ ] getInts ( IndexReader reader , String field , IntParser parser ) throws IOException { return ( int [ ] ) intsCache . get ( reader , new Entry ( field , parser ) ) ; } Cache intsCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object entryKey ) throws IOException { Entry entry = ( Entry ) entryKey ; String field = entry . field ; IntParser parser = ( IntParser ) entry . custom ; final int [ ] retArray = new int [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field , "" ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; int termval = parser . parseInt ( term . text ( ) ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } return retArray ; } } ; public float [ ] getFloats ( IndexReader reader , String field ) throws IOException { return getFloats ( reader , field , FLOAT_PARSER ) ; } public float [ ] getFloats ( IndexReader reader , String field , FloatParser parser ) throws IOException { return ( float [ ] ) floatsCache . get ( reader , new Entry ( field , parser ) ) ; } Cache floatsCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object entryKey ) throws IOException { Entry entry = ( Entry ) entryKey ; String field = entry . field ; FloatParser parser = ( FloatParser ) entry . custom ; final float [ ] retArray = new float [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field , "" ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; float termval = parser . parseFloat ( term . text ( ) ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } return retArray ; } } ; public String [ ] getStrings ( IndexReader reader , String field ) throws IOException { return ( String [ ] ) stringsCache . get ( reader , field ) ; } Cache stringsCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object fieldKey ) throws IOException { String field = ( ( String ) fieldKey ) . intern ( ) ; final String [ ] retArray = new String [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field , "" ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; String termval = term . text ( ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } return retArray ; } } ; public StringIndex getStringIndex ( IndexReader reader , String field ) throws IOException { return ( StringIndex ) stringsIndexCache . get ( reader , field ) ; } Cache stringsIndexCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object fieldKey ) throws IOException { String field = ( ( String ) fieldKey ) . intern ( ) ; final int [ ] retArray = new int [ reader . maxDoc ( ) ] ; String [ ] mterms = new String [ reader . maxDoc ( ) + 1 ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field , "" ) ) ; int t = 0 ; mterms [ t ++ ] = null ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; if ( t >= mterms . length ) throw new RuntimeException ( "there are more terms than " + "documents in field \"" + field + "\", but it's impossible to sort on " + "tokenized fields" ) ; mterms [ t ] = term . text ( ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = t ; } t ++ ; } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } if ( t == 0 ) { mterms = new String [ 1 ] ; } else if ( t < mterms . length ) { String [ ] terms = new String [ t ] ; System . arraycopy ( mterms , 0 , terms , 0 , t ) ; mterms = terms ; } StringIndex value = new StringIndex ( retArray , mterms ) ; return value ; } } ; public Object getAuto ( IndexReader reader , String field ) throws IOException { return autoCache . get ( reader , field ) ; } Cache autoCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object fieldKey ) throws IOException { String field = ( ( String ) fieldKey ) . intern ( ) ; TermEnum enumerator = reader . terms ( new Term ( field , "" ) ) ; try { Term term = enumerator . term ( ) ; if ( term == null ) { throw new RuntimeException ( "no terms in field " + field + " - cannot determine sort type" ) ; } Object ret = null ; if ( term . field ( ) == field ) { String termtext = term . text ( ) . trim ( ) ; try { Integer . parseInt ( termtext ) ; ret = getInts ( reader , field ) ; } catch ( NumberFormatException nfe1 ) { try { Float . parseFloat ( termtext ) ; ret = getFloats ( reader , field ) ; } catch ( NumberFormatException nfe2 ) { ret = getStringIndex ( reader , field ) ; } } } else { throw new RuntimeException ( "field \"" + field + "\" does not appear to be indexed" ) ; } return ret ; } finally { enumerator . close ( ) ; } } } ; public Comparable [ ] getCustom ( IndexReader reader , String field , SortComparator comparator ) throws IOException { return ( Comparable [ ] ) customCache . get ( reader , new Entry ( field , comparator ) ) ; } Cache customCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object entryKey ) throws IOException { Entry entry = ( Entry ) entryKey ; String field = entry . field ; SortComparator comparator = ( SortComparator ) entry . custom ; final Comparable [ ] retArray = new Comparable [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field , "" ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; Comparable termval = comparator . getComparable ( term . text ( ) ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } return retArray ; } } ; } 	1
package org . apache . lucene . analysis ; import java . io . BufferedReader ; import java . io . File ; import java . io . FileReader ; import java . io . IOException ; import java . io . Reader ; import java . util . HashMap ; import java . util . HashSet ; public class WordlistLoader { public static HashSet getWordSet ( File wordfile ) throws IOException { HashSet result = new HashSet ( ) ; FileReader reader = null ; try { reader = new FileReader ( wordfile ) ; result = getWordSet ( reader ) ; } finally { if ( reader != null ) reader . close ( ) ; } return result ; } public static HashSet getWordSet ( Reader reader ) throws IOException { HashSet result = new HashSet ( ) ; BufferedReader br = null ; try { if ( reader instanceof BufferedReader ) { br = ( BufferedReader ) reader ; } else { br = new BufferedReader ( reader ) ; } String word = null ; while ( ( word = br . readLine ( ) ) != null ) { result . add ( word . trim ( ) ) ; } } finally { if ( br != null ) br . close ( ) ; } return result ; } public static HashMap getStemDict ( File wordstemfile ) throws IOException { if ( wordstemfile == null ) throw new NullPointerException ( "wordstemfile may not be null" ) ; HashMap result = new HashMap ( ) ; BufferedReader br = null ; FileReader fr = null ; try { fr = new FileReader ( wordstemfile ) ; br = new BufferedReader ( fr ) ; String line ; while ( ( line = br . readLine ( ) ) != null ) { String [ ] wordstem = line . split ( "\t" , 2 ) ; result . put ( wordstem [ 0 ] , wordstem [ 1 ] ) ; } } finally { if ( fr != null ) fr . close ( ) ; if ( br != null ) br . close ( ) ; } return result ; } } 	0
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . IndexInput ; import java . io . IOException ; import java . util . List ; import java . util . ArrayList ; final class SegmentInfo { static final int NO = - 1 ; static final int YES = 1 ; static final int CHECK_DIR = 0 ; static final int WITHOUT_GEN = 0 ; public String name ; public int docCount ; public Directory dir ; private boolean preLockless ; private long delGen ; private long [ ] normGen ; private byte isCompoundFile ; private boolean hasSingleNormFile ; private List files ; public SegmentInfo ( String name , int docCount , Directory dir ) { this . name = name ; this . docCount = docCount ; this . dir = dir ; delGen = NO ; isCompoundFile = CHECK_DIR ; preLockless = true ; hasSingleNormFile = false ; } public SegmentInfo ( String name , int docCount , Directory dir , boolean isCompoundFile , boolean hasSingleNormFile ) { this ( name , docCount , dir ) ; this . isCompoundFile = ( byte ) ( isCompoundFile ? YES : NO ) ; this . hasSingleNormFile = hasSingleNormFile ; preLockless = false ; } void reset ( SegmentInfo src ) { files = null ; name = src . name ; docCount = src . docCount ; dir = src . dir ; preLockless = src . preLockless ; delGen = src . delGen ; if ( src . normGen == null ) { normGen = null ; } else { normGen = new long [ src . normGen . length ] ; System . arraycopy ( src . normGen , 0 , normGen , 0 , src . normGen . length ) ; } isCompoundFile = src . isCompoundFile ; hasSingleNormFile = src . hasSingleNormFile ; } SegmentInfo ( Directory dir , int format , IndexInput input ) throws IOException { this . dir = dir ; name = input . readString ( ) ; docCount = input . readInt ( ) ; if ( format <= SegmentInfos . FORMAT_LOCKLESS ) { delGen = input . readLong ( ) ; if ( format <= SegmentInfos . FORMAT_SINGLE_NORM_FILE ) { hasSingleNormFile = ( 1 == input . readByte ( ) ) ; } else { hasSingleNormFile = false ; } int numNormGen = input . readInt ( ) ; if ( numNormGen == NO ) { normGen = null ; } else { normGen = new long [ numNormGen ] ; for ( int j = 0 ; j < numNormGen ; j ++ ) { normGen [ j ] = input . readLong ( ) ; } } isCompoundFile = input . readByte ( ) ; preLockless = ( isCompoundFile == CHECK_DIR ) ; } else { delGen = CHECK_DIR ; normGen = null ; isCompoundFile = CHECK_DIR ; preLockless = true ; hasSingleNormFile = false ; } } void setNumFields ( int numFields ) { if ( normGen == null ) { normGen = new long [ numFields ] ; if ( preLockless ) { } else { for ( int i = 0 ; i < numFields ; i ++ ) { normGen [ i ] = NO ; } } } } boolean hasDeletions ( ) throws IOException { if ( delGen == NO ) { return false ; } else if ( delGen >= YES ) { return true ; } else { return dir . fileExists ( getDelFileName ( ) ) ; } } void advanceDelGen ( ) { if ( delGen == NO ) { delGen = YES ; } else { delGen ++ ; } files = null ; } void clearDelGen ( ) { delGen = NO ; files = null ; } public Object clone ( ) { SegmentInfo si = new SegmentInfo ( name , docCount , dir ) ; si . isCompoundFile = isCompoundFile ; si . delGen = delGen ; si . preLockless = preLockless ; si . hasSingleNormFile = hasSingleNormFile ; if ( normGen != null ) { si . normGen = ( long [ ] ) normGen . clone ( ) ; } return si ; } String getDelFileName ( ) { if ( delGen == NO ) { return null ; } else { return IndexFileNames . fileNameFromGeneration ( name , "." + IndexFileNames . DELETES_EXTENSION , delGen ) ; } } boolean hasSeparateNorms ( int fieldNumber ) throws IOException { if ( ( normGen == null && preLockless ) || ( normGen != null && normGen [ fieldNumber ] == CHECK_DIR ) ) { String fileName = name + ".s" + fieldNumber ; return dir . fileExists ( fileName ) ; } else if ( normGen == null || normGen [ fieldNumber ] == NO ) { return false ; } else { return true ; } } boolean hasSeparateNorms ( ) throws IOException { if ( normGen == null ) { if ( ! preLockless ) { return false ; } else { String [ ] result = dir . list ( ) ; if ( result == null ) throw new IOException ( "cannot read directory " + dir + ": list() returned null" ) ; String pattern ; pattern = name + ".s" ; int patternLength = pattern . length ( ) ; for ( int i = 0 ; i < result . length ; i ++ ) { if ( result [ i ] . startsWith ( pattern ) && Character . isDigit ( result [ i ] . charAt ( patternLength ) ) ) return true ; } return false ; } } else { for ( int i = 0 ; i < normGen . length ; i ++ ) { if ( normGen [ i ] >= YES ) { return true ; } } for ( int i = 0 ; i < normGen . length ; i ++ ) { if ( normGen [ i ] == CHECK_DIR ) { if ( hasSeparateNorms ( i ) ) { return true ; } } } } return false ; } void advanceNormGen ( int fieldIndex ) { if ( normGen [ fieldIndex ] == NO ) { normGen [ fieldIndex ] = YES ; } else { normGen [ fieldIndex ] ++ ; } files = null ; } String getNormFileName ( int number ) throws IOException { String prefix ; long gen ; if ( normGen == null ) { gen = CHECK_DIR ; } else { gen = normGen [ number ] ; } if ( hasSeparateNorms ( number ) ) { prefix = ".s" ; return IndexFileNames . fileNameFromGeneration ( name , prefix + number , gen ) ; } if ( hasSingleNormFile ) { prefix = "." + IndexFileNames . NORMS_EXTENSION ; return IndexFileNames . fileNameFromGeneration ( name , prefix , WITHOUT_GEN ) ; } prefix = ".f" ; return IndexFileNames . fileNameFromGeneration ( name , prefix + number , WITHOUT_GEN ) ; } void setUseCompoundFile ( boolean isCompoundFile ) { if ( isCompoundFile ) { this . isCompoundFile = YES ; } else { this . isCompoundFile = NO ; } files = null ; } boolean getUseCompoundFile ( ) throws IOException { if ( isCompoundFile == NO ) { return false ; } else if ( isCompoundFile == YES ) { return true ; } else { return dir . fileExists ( name + "." + IndexFileNames . COMPOUND_FILE_EXTENSION ) ; } } void write ( IndexOutput output ) throws IOException { output . writeString ( name ) ; output . writeInt ( docCount ) ; output . writeLong ( delGen ) ; output . writeByte ( ( byte ) ( hasSingleNormFile ? 1 : 0 ) ) ; if ( normGen == null ) { output . writeInt ( NO ) ; } else { output . writeInt ( normGen . length ) ; for ( int j = 0 ; j < normGen . length ; j ++ ) { output . writeLong ( normGen [ j ] ) ; } } output . writeByte ( isCompoundFile ) ; } public List files ( ) throws IOException { if ( files != null ) { return files ; } files = new ArrayList ( ) ; boolean useCompoundFile = getUseCompoundFile ( ) ; if ( useCompoundFile ) { files . add ( name + "." + IndexFileNames . COMPOUND_FILE_EXTENSION ) ; } else { for ( int i = 0 ; i < IndexFileNames . INDEX_EXTENSIONS_IN_COMPOUND_FILE . length ; i ++ ) { String ext = IndexFileNames . INDEX_EXTENSIONS_IN_COMPOUND_FILE [ i ] ; String fileName = name + "." + ext ; if ( dir . fileExists ( fileName ) ) { files . add ( fileName ) ; } } } String delFileName = IndexFileNames . fileNameFromGeneration ( name , "." + IndexFileNames . DELETES_EXTENSION , delGen ) ; if ( delFileName != null && ( delGen >= YES || dir . fileExists ( delFileName ) ) ) { files . add ( delFileName ) ; } if ( normGen != null ) { for ( int i = 0 ; i < normGen . length ; i ++ ) { long gen = normGen [ i ] ; if ( gen >= YES ) { files . add ( IndexFileNames . fileNameFromGeneration ( name , "." + IndexFileNames . SEPARATE_NORMS_EXTENSION + i , gen ) ) ; } else if ( NO == gen ) { if ( ! hasSingleNormFile && ! useCompoundFile ) { String fileName = name + "." + IndexFileNames . PLAIN_NORMS_EXTENSION + i ; if ( dir . fileExists ( fileName ) ) { files . add ( fileName ) ; } } } else if ( CHECK_DIR == gen ) { String fileName = null ; if ( useCompoundFile ) { fileName = name + "." + IndexFileNames . SEPARATE_NORMS_EXTENSION + i ; } else if ( ! hasSingleNormFile ) { fileName = name + "." + IndexFileNames . PLAIN_NORMS_EXTENSION + i ; } if ( fileName != null && dir . fileExists ( fileName ) ) { files . add ( fileName ) ; } } } } else if ( preLockless || ( ! hasSingleNormFile && ! useCompoundFile ) ) { String prefix ; if ( useCompoundFile ) prefix = name + "." + IndexFileNames . SEPARATE_NORMS_EXTENSION ; else prefix = name + "." + IndexFileNames . PLAIN_NORMS_EXTENSION ; int prefixLength = prefix . length ( ) ; String [ ] allFiles = dir . list ( ) ; if ( allFiles == null ) throw new IOException ( "cannot read directory " + dir + ": list() returned null" ) ; for ( int i = 0 ; i < allFiles . length ; i ++ ) { String fileName = allFiles [ i ] ; if ( fileName . length ( ) > prefixLength && Character . isDigit ( fileName . charAt ( prefixLength ) ) && fileName . startsWith ( prefix ) ) { files . add ( fileName ) ; } } } return files ; } } 	1
package org . apache . lucene . search . spans ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermPositions ; import java . io . IOException ; public class TermSpans implements Spans { protected TermPositions positions ; protected Term term ; protected int doc ; protected int freq ; protected int count ; protected int position ; public TermSpans ( TermPositions positions , Term term ) throws IOException { this . positions = positions ; this . term = term ; doc = - 1 ; } public boolean next ( ) throws IOException { if ( count == freq ) { if ( ! positions . next ( ) ) { doc = Integer . MAX_VALUE ; return false ; } doc = positions . doc ( ) ; freq = positions . freq ( ) ; count = 0 ; } position = positions . nextPosition ( ) ; count ++ ; return true ; } public boolean skipTo ( int target ) throws IOException { if ( doc >= target ) { return true ; } if ( ! positions . skipTo ( target ) ) { doc = Integer . MAX_VALUE ; return false ; } doc = positions . doc ( ) ; freq = positions . freq ( ) ; count = 0 ; position = positions . nextPosition ( ) ; count ++ ; return true ; } public int doc ( ) { return doc ; } public int start ( ) { return position ; } public int end ( ) { return position + 1 ; } public String toString ( ) { return "spans(" + term . toString ( ) + ")@" + ( doc == - 1 ? "START" : ( doc == Integer . MAX_VALUE ) ? "END" : doc + "-" + position ) ; } public TermPositions getPositions ( ) { return positions ; } } 	0
package org . apache . lucene . store ; import java . io . IOException ; public abstract class BufferedIndexInput extends IndexInput { public static final int BUFFER_SIZE = 1024 ; private int bufferSize = BUFFER_SIZE ; private byte [ ] buffer ; private long bufferStart = 0 ; private int bufferLength = 0 ; private int bufferPosition = 0 ; public byte readByte ( ) throws IOException { if ( bufferPosition >= bufferLength ) refill ( ) ; return buffer [ bufferPosition ++ ] ; } public BufferedIndexInput ( ) { } public BufferedIndexInput ( int bufferSize ) { checkBufferSize ( bufferSize ) ; this . bufferSize = bufferSize ; } public void setBufferSize ( int newSize ) { assert buffer == null || bufferSize == buffer . length ; if ( newSize != bufferSize ) { checkBufferSize ( newSize ) ; bufferSize = newSize ; if ( buffer != null ) { byte [ ] newBuffer = new byte [ newSize ] ; final int leftInBuffer = bufferLength - bufferPosition ; final int numToCopy ; if ( leftInBuffer > newSize ) numToCopy = newSize ; else numToCopy = leftInBuffer ; System . arraycopy ( buffer , bufferPosition , newBuffer , 0 , numToCopy ) ; bufferStart += bufferPosition ; bufferPosition = 0 ; bufferLength = numToCopy ; buffer = newBuffer ; } } } public int getBufferSize ( ) { return bufferSize ; } private void checkBufferSize ( int bufferSize ) { if ( bufferSize <= 0 ) throw new IllegalArgumentException ( "bufferSize must be greater than 0 (got " + bufferSize + ")" ) ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { if ( len <= ( bufferLength - bufferPosition ) ) { if ( len > 0 ) System . arraycopy ( buffer , bufferPosition , b , offset , len ) ; bufferPosition += len ; } else { int available = bufferLength - bufferPosition ; if ( available > 0 ) { System . arraycopy ( buffer , bufferPosition , b , offset , available ) ; offset += available ; len -= available ; bufferPosition += available ; } if ( len < bufferSize ) { refill ( ) ; if ( bufferLength < len ) { System . arraycopy ( buffer , 0 , b , offset , bufferLength ) ; throw new IOException ( "read past EOF" ) ; } else { System . arraycopy ( buffer , 0 , b , offset , len ) ; bufferPosition = len ; } } else { long after = bufferStart + bufferPosition + len ; if ( after > length ( ) ) throw new IOException ( "read past EOF" ) ; readInternal ( b , offset , len ) ; bufferStart = after ; bufferPosition = 0 ; bufferLength = 0 ; } } } private void refill ( ) throws IOException { long start = bufferStart + bufferPosition ; long end = start + bufferSize ; if ( end > length ( ) ) end = length ( ) ; bufferLength = ( int ) ( end - start ) ; if ( bufferLength <= 0 ) throw new IOException ( "read past EOF" ) ; if ( buffer == null ) { buffer = new byte [ bufferSize ] ; seekInternal ( bufferStart ) ; } readInternal ( buffer , 0 , bufferLength ) ; bufferStart = start ; bufferPosition = 0 ; } protected abstract void readInternal ( byte [ ] b , int offset , int length ) throws IOException ; public long getFilePointer ( ) { return bufferStart + bufferPosition ; } public void seek ( long pos ) throws IOException { if ( pos >= bufferStart && pos < ( bufferStart + bufferLength ) ) bufferPosition = ( int ) ( pos - bufferStart ) ; else { bufferStart = pos ; bufferPosition = 0 ; bufferLength = 0 ; seekInternal ( pos ) ; } } protected abstract void seekInternal ( long pos ) throws IOException ; public Object clone ( ) { BufferedIndexInput clone = ( BufferedIndexInput ) super . clone ( ) ; clone . buffer = null ; clone . bufferLength = 0 ; clone . bufferPosition = 0 ; clone . bufferStart = getFilePointer ( ) ; return clone ; } } 	1
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . List ; import java . util . ArrayList ; import java . util . Iterator ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Query ; import org . apache . lucene . util . ToStringUtils ; public class SpanNearQuery extends SpanQuery { private List clauses ; private int slop ; private boolean inOrder ; private String field ; public SpanNearQuery ( SpanQuery [ ] clauses , int slop , boolean inOrder ) { this . clauses = new ArrayList ( clauses . length ) ; for ( int i = 0 ; i < clauses . length ; i ++ ) { SpanQuery clause = clauses [ i ] ; if ( i == 0 ) { field = clause . getField ( ) ; } else if ( ! clause . getField ( ) . equals ( field ) ) { throw new IllegalArgumentException ( "Clauses must have same field." ) ; } this . clauses . add ( clause ) ; } this . slop = slop ; this . inOrder = inOrder ; } public SpanQuery [ ] getClauses ( ) { return ( SpanQuery [ ] ) clauses . toArray ( new SpanQuery [ clauses . size ( ) ] ) ; } public int getSlop ( ) { return slop ; } public boolean isInOrder ( ) { return inOrder ; } public String getField ( ) { return field ; } public Collection getTerms ( ) { Collection terms = new ArrayList ( ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; terms . addAll ( clause . getTerms ( ) ) ; } return terms ; } public void extractTerms ( Set terms ) { Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; clause . extractTerms ( terms ) ; } } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "spanNear([" ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; buffer . append ( clause . toString ( field ) ) ; if ( i . hasNext ( ) ) { buffer . append ( ", " ) ; } } buffer . append ( "], " ) ; buffer . append ( slop ) ; buffer . append ( ", " ) ; buffer . append ( inOrder ) ; buffer . append ( ")" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public Spans getSpans ( final IndexReader reader ) throws IOException { if ( clauses . size ( ) == 0 ) return new SpanOrQuery ( getClauses ( ) ) . getSpans ( reader ) ; if ( clauses . size ( ) == 1 ) return ( ( SpanQuery ) clauses . get ( 0 ) ) . getSpans ( reader ) ; return inOrder ? ( Spans ) new NearSpansOrdered ( this , reader ) : ( Spans ) new NearSpansUnordered ( this , reader ) ; } public Query rewrite ( IndexReader reader ) throws IOException { SpanNearQuery clone = null ; for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { SpanQuery c = ( SpanQuery ) clauses . get ( i ) ; SpanQuery query = ( SpanQuery ) c . rewrite ( reader ) ; if ( query != c ) { if ( clone == null ) clone = ( SpanNearQuery ) this . clone ( ) ; clone . clauses . set ( i , query ) ; } } if ( clone != null ) { return clone ; } else { return this ; } } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof SpanNearQuery ) ) return false ; final SpanNearQuery spanNearQuery = ( SpanNearQuery ) o ; if ( inOrder != spanNearQuery . inOrder ) return false ; if ( slop != spanNearQuery . slop ) return false ; if ( ! clauses . equals ( spanNearQuery . clauses ) ) return false ; return getBoost ( ) == spanNearQuery . getBoost ( ) ; } public int hashCode ( ) { int result ; result = clauses . hashCode ( ) ; result ^= ( result << 14 ) | ( result > > > 19 ) ; result += Float . floatToRawIntBits ( getBoost ( ) ) ; result += slop ; result ^= ( inOrder ? 0x99AFD3BD : 0 ) ; return result ; } } 	0
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . Directory ; import org . apache . lucene . util . StringHelper ; final class TermInfosWriter { public static final int FORMAT = - 3 ; private FieldInfos fieldInfos ; private IndexOutput output ; private Term lastTerm = new Term ( "" , "" ) ; private TermInfo lastTi = new TermInfo ( ) ; private long size = 0 ; int indexInterval = 128 ; int skipInterval = 16 ; int maxSkipLevels = 10 ; private long lastIndexPointer = 0 ; private boolean isIndex = false ; private TermInfosWriter other = null ; TermInfosWriter ( Directory directory , String segment , FieldInfos fis , int interval ) throws IOException { initialize ( directory , segment , fis , interval , false ) ; other = new TermInfosWriter ( directory , segment , fis , interval , true ) ; other . other = this ; } private TermInfosWriter ( Directory directory , String segment , FieldInfos fis , int interval , boolean isIndex ) throws IOException { initialize ( directory , segment , fis , interval , isIndex ) ; } private void initialize ( Directory directory , String segment , FieldInfos fis , int interval , boolean isi ) throws IOException { indexInterval = interval ; fieldInfos = fis ; isIndex = isi ; output = directory . createOutput ( segment + ( isIndex ? ".tii" : ".tis" ) ) ; output . writeInt ( FORMAT ) ; output . writeLong ( 0 ) ; output . writeInt ( indexInterval ) ; output . writeInt ( skipInterval ) ; output . writeInt ( maxSkipLevels ) ; } final void add ( Term term , TermInfo ti ) throws CorruptIndexException , IOException { if ( ! isIndex && term . compareTo ( lastTerm ) <= 0 ) throw new CorruptIndexException ( "term out of order (\"" + term + "\".compareTo(\"" + lastTerm + "\") <= 0)" ) ; if ( ti . freqPointer < lastTi . freqPointer ) throw new CorruptIndexException ( "freqPointer out of order (" + ti . freqPointer + " < " + lastTi . freqPointer + ")" ) ; if ( ti . proxPointer < lastTi . proxPointer ) throw new CorruptIndexException ( "proxPointer out of order (" + ti . proxPointer + " < " + lastTi . proxPointer + ")" ) ; if ( ! isIndex && size % indexInterval == 0 ) other . add ( lastTerm , lastTi ) ; writeTerm ( term ) ; output . writeVInt ( ti . docFreq ) ; output . writeVLong ( ti . freqPointer - lastTi . freqPointer ) ; output . writeVLong ( ti . proxPointer - lastTi . proxPointer ) ; if ( ti . docFreq >= skipInterval ) { output . writeVInt ( ti . skipOffset ) ; } if ( isIndex ) { output . writeVLong ( other . output . getFilePointer ( ) - lastIndexPointer ) ; lastIndexPointer = other . output . getFilePointer ( ) ; } lastTi . set ( ti ) ; size ++ ; } private final void writeTerm ( Term term ) throws IOException { int start = StringHelper . stringDifference ( lastTerm . text , term . text ) ; int length = term . text . length ( ) - start ; output . writeVInt ( start ) ; output . writeVInt ( length ) ; output . writeChars ( term . text , start , length ) ; output . writeVInt ( fieldInfos . fieldNumber ( term . field ) ) ; lastTerm = term ; } final void close ( ) throws IOException { output . seek ( 4 ) ; output . writeLong ( size ) ; output . close ( ) ; if ( ! isIndex ) other . close ( ) ; } } 	1
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . * ; final class PhrasePositions { int doc ; int position ; int count ; int offset ; TermPositions tp ; PhrasePositions next ; boolean repeats ; PhrasePositions ( TermPositions t , int o ) { tp = t ; offset = o ; } final boolean next ( ) throws IOException { if ( ! tp . next ( ) ) { tp . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } doc = tp . doc ( ) ; position = 0 ; return true ; } final boolean skipTo ( int target ) throws IOException { if ( ! tp . skipTo ( target ) ) { tp . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } doc = tp . doc ( ) ; position = 0 ; return true ; } final void firstPosition ( ) throws IOException { count = tp . freq ( ) ; nextPosition ( ) ; } final boolean nextPosition ( ) throws IOException { if ( count -- > 0 ) { position = tp . nextPosition ( ) - offset ; return true ; } else return false ; } } 	0
package org . apache . lucene . search ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . FieldSelector ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . CorruptIndexException ; import java . io . IOException ; public interface Searchable extends java . rmi . Remote { void search ( Weight weight , Filter filter , HitCollector results ) throws IOException ; void close ( ) throws IOException ; int docFreq ( Term term ) throws IOException ; int [ ] docFreqs ( Term [ ] terms ) throws IOException ; int maxDoc ( ) throws IOException ; TopDocs search ( Weight weight , Filter filter , int n ) throws IOException ; Document doc ( int i ) throws CorruptIndexException , IOException ; Document doc ( int n , FieldSelector fieldSelector ) throws CorruptIndexException , IOException ; Query rewrite ( Query query ) throws IOException ; Explanation explain ( Weight weight , int doc ) throws IOException ; TopFieldDocs search ( Weight weight , Filter filter , int n , Sort sort ) throws IOException ; } 	1
package org . apache . lucene . document ; import java . io . Serializable ; public interface FieldSelector extends Serializable { FieldSelectorResult accept ( String fieldName ) ; } 	0
package org . apache . lucene . search ; import java . io . IOException ; import java . util . * ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . MultipleTermPositions ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermPositions ; import org . apache . lucene . search . Query ; import org . apache . lucene . util . ToStringUtils ; public class MultiPhraseQuery extends Query { private String field ; private ArrayList termArrays = new ArrayList ( ) ; private Vector positions = new Vector ( ) ; private int slop = 0 ; public void setSlop ( int s ) { slop = s ; } public int getSlop ( ) { return slop ; } public void add ( Term term ) { add ( new Term [ ] { term } ) ; } public void add ( Term [ ] terms ) { int position = 0 ; if ( positions . size ( ) > 0 ) position = ( ( Integer ) positions . lastElement ( ) ) . intValue ( ) + 1 ; add ( terms , position ) ; } public void add ( Term [ ] terms , int position ) { if ( termArrays . size ( ) == 0 ) field = terms [ 0 ] . field ( ) ; for ( int i = 0 ; i < terms . length ; i ++ ) { if ( terms [ i ] . field ( ) != field ) { throw new IllegalArgumentException ( "All phrase terms must be in the same field (" + field + "): " + terms [ i ] ) ; } } termArrays . add ( terms ) ; positions . addElement ( new Integer ( position ) ) ; } public List getTermArrays ( ) { return Collections . unmodifiableList ( termArrays ) ; } public int [ ] getPositions ( ) { int [ ] result = new int [ positions . size ( ) ] ; for ( int i = 0 ; i < positions . size ( ) ; i ++ ) result [ i ] = ( ( Integer ) positions . elementAt ( i ) ) . intValue ( ) ; return result ; } public void extractTerms ( Set terms ) { for ( Iterator iter = termArrays . iterator ( ) ; iter . hasNext ( ) ; ) { Term [ ] arr = ( Term [ ] ) iter . next ( ) ; for ( int i = 0 ; i < arr . length ; i ++ ) { terms . add ( arr [ i ] ) ; } } } private class MultiPhraseWeight implements Weight { private Similarity similarity ; private float value ; private float idf ; private float queryNorm ; private float queryWeight ; public MultiPhraseWeight ( Searcher searcher ) throws IOException { this . similarity = getSimilarity ( searcher ) ; Iterator i = termArrays . iterator ( ) ; while ( i . hasNext ( ) ) { Term [ ] terms = ( Term [ ] ) i . next ( ) ; for ( int j = 0 ; j < terms . length ; j ++ ) { idf += getSimilarity ( searcher ) . idf ( terms [ j ] , searcher ) ; } } } public Query getQuery ( ) { return MultiPhraseQuery . this ; } public float getValue ( ) { return value ; } public float sumOfSquaredWeights ( ) { queryWeight = idf * getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float queryNorm ) { this . queryNorm = queryNorm ; queryWeight *= queryNorm ; value = queryWeight * idf ; } public Scorer scorer ( IndexReader reader ) throws IOException { if ( termArrays . size ( ) == 0 ) return null ; TermPositions [ ] tps = new TermPositions [ termArrays . size ( ) ] ; for ( int i = 0 ; i < tps . length ; i ++ ) { Term [ ] terms = ( Term [ ] ) termArrays . get ( i ) ; TermPositions p ; if ( terms . length > 1 ) p = new MultipleTermPositions ( reader , terms ) ; else p = reader . termPositions ( terms [ 0 ] ) ; if ( p == null ) return null ; tps [ i ] = p ; } if ( slop == 0 ) return new ExactPhraseScorer ( this , tps , getPositions ( ) , similarity , reader . norms ( field ) ) ; else return new SloppyPhraseScorer ( this , tps , getPositions ( ) , similarity , slop , reader . norms ( field ) ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { ComplexExplanation result = new ComplexExplanation ( ) ; result . setDescription ( "weight(" + getQuery ( ) + " in " + doc + "), product of:" ) ; Explanation idfExpl = new Explanation ( idf , "idf(" + getQuery ( ) + ")" ) ; Explanation queryExpl = new Explanation ( ) ; queryExpl . setDescription ( "queryWeight(" + getQuery ( ) + "), product of:" ) ; Explanation boostExpl = new Explanation ( getBoost ( ) , "boost" ) ; if ( getBoost ( ) != 1.0f ) queryExpl . addDetail ( boostExpl ) ; queryExpl . addDetail ( idfExpl ) ; Explanation queryNormExpl = new Explanation ( queryNorm , "queryNorm" ) ; queryExpl . addDetail ( queryNormExpl ) ; queryExpl . setValue ( boostExpl . getValue ( ) * idfExpl . getValue ( ) * queryNormExpl . getValue ( ) ) ; result . addDetail ( queryExpl ) ; ComplexExplanation fieldExpl = new ComplexExplanation ( ) ; fieldExpl . setDescription ( "fieldWeight(" + getQuery ( ) + " in " + doc + "), product of:" ) ; Explanation tfExpl = scorer ( reader ) . explain ( doc ) ; fieldExpl . addDetail ( tfExpl ) ; fieldExpl . addDetail ( idfExpl ) ; Explanation fieldNormExpl = new Explanation ( ) ; byte [ ] fieldNorms = reader . norms ( field ) ; float fieldNorm = fieldNorms != null ? Similarity . decodeNorm ( fieldNorms [ doc ] ) : 0.0f ; fieldNormExpl . setValue ( fieldNorm ) ; fieldNormExpl . setDescription ( "fieldNorm(field=" + field + ", doc=" + doc + ")" ) ; fieldExpl . addDetail ( fieldNormExpl ) ; fieldExpl . setMatch ( Boolean . valueOf ( tfExpl . isMatch ( ) ) ) ; fieldExpl . setValue ( tfExpl . getValue ( ) * idfExpl . getValue ( ) * fieldNormExpl . getValue ( ) ) ; result . addDetail ( fieldExpl ) ; result . setMatch ( fieldExpl . getMatch ( ) ) ; result . setValue ( queryExpl . getValue ( ) * fieldExpl . getValue ( ) ) ; if ( queryExpl . getValue ( ) == 1.0f ) return fieldExpl ; return result ; } } public Query rewrite ( IndexReader reader ) { if ( termArrays . size ( ) == 1 ) { Term [ ] terms = ( Term [ ] ) termArrays . get ( 0 ) ; BooleanQuery boq = new BooleanQuery ( true ) ; for ( int i = 0 ; i < terms . length ; i ++ ) { boq . add ( new TermQuery ( terms [ i ] ) , BooleanClause . Occur . SHOULD ) ; } boq . setBoost ( getBoost ( ) ) ; return boq ; } else { return this ; } } protected Weight createWeight ( Searcher searcher ) throws IOException { return new MultiPhraseWeight ( searcher ) ; } public final String toString ( String f ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! field . equals ( f ) ) { buffer . append ( field ) ; buffer . append ( ":" ) ; } buffer . append ( "\"" ) ; Iterator i = termArrays . iterator ( ) ; while ( i . hasNext ( ) ) { Term [ ] terms = ( Term [ ] ) i . next ( ) ; if ( terms . length > 1 ) { buffer . append ( "(" ) ; for ( int j = 0 ; j < terms . length ; j ++ ) { buffer . append ( terms [ j ] . text ( ) ) ; if ( j < terms . length - 1 ) buffer . append ( " " ) ; } buffer . append ( ")" ) ; } else { buffer . append ( terms [ 0 ] . text ( ) ) ; } if ( i . hasNext ( ) ) buffer . append ( " " ) ; } buffer . append ( "\"" ) ; if ( slop != 0 ) { buffer . append ( "~" ) ; buffer . append ( slop ) ; } buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof MultiPhraseQuery ) ) return false ; MultiPhraseQuery other = ( MultiPhraseQuery ) o ; return this . getBoost ( ) == other . getBoost ( ) && this . slop == other . slop && this . termArrays . equals ( other . termArrays ) && this . positions . equals ( other . positions ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ slop ^ termArrays . hashCode ( ) ^ positions . hashCode ( ) ^ 0x4AC65113 ; } } 	1
package org . apache . lucene . search . function ; public class FieldScoreQuery extends ValueSourceQuery { public static class Type { public static final Type BYTE = new Type ( "byte" ) ; public static final Type SHORT = new Type ( "short" ) ; public static final Type INT = new Type ( "int" ) ; public static final Type FLOAT = new Type ( "float" ) ; private String typeName ; private Type ( String name ) { this . typeName = name ; } public String toString ( ) { return getClass ( ) . getName ( ) + "::" + typeName ; } } public FieldScoreQuery ( String field , Type type ) { super ( getValueSource ( field , type ) ) ; } private static ValueSource getValueSource ( String field , Type type ) { if ( type == Type . BYTE ) { return new ByteFieldSource ( field ) ; } if ( type == Type . SHORT ) { return new ShortFieldSource ( field ) ; } if ( type == Type . INT ) { return new IntFieldSource ( field ) ; } if ( type == Type . FLOAT ) { return new FloatFieldSource ( field ) ; } throw new IllegalArgumentException ( type + " is not a known Field Score Query Type!" ) ; } } 	0
package org . apache . lucene . store ; import java . io . IOException ; public abstract class IndexOutput { public abstract void writeByte ( byte b ) throws IOException ; public void writeBytes ( byte [ ] b , int length ) throws IOException { writeBytes ( b , 0 , length ) ; } public abstract void writeBytes ( byte [ ] b , int offset , int length ) throws IOException ; public void writeInt ( int i ) throws IOException { writeByte ( ( byte ) ( i > > 24 ) ) ; writeByte ( ( byte ) ( i > > 16 ) ) ; writeByte ( ( byte ) ( i > > 8 ) ) ; writeByte ( ( byte ) i ) ; } public void writeVInt ( int i ) throws IOException { while ( ( i & ~ 0x7F ) != 0 ) { writeByte ( ( byte ) ( ( i & 0x7f ) | 0x80 ) ) ; i >>>= 7 ; } writeByte ( ( byte ) i ) ; } public void writeLong ( long i ) throws IOException { writeInt ( ( int ) ( i > > 32 ) ) ; writeInt ( ( int ) i ) ; } public void writeVLong ( long i ) throws IOException { while ( ( i & ~ 0x7F ) != 0 ) { writeByte ( ( byte ) ( ( i & 0x7f ) | 0x80 ) ) ; i >>>= 7 ; } writeByte ( ( byte ) i ) ; } public void writeString ( String s ) throws IOException { int length = s . length ( ) ; writeVInt ( length ) ; writeChars ( s , 0 , length ) ; } public void writeChars ( String s , int start , int length ) throws IOException { final int end = start + length ; for ( int i = start ; i < end ; i ++ ) { final int code = ( int ) s . charAt ( i ) ; if ( code >= 0x01 && code <= 0x7F ) writeByte ( ( byte ) code ) ; else if ( ( ( code >= 0x80 ) && ( code <= 0x7FF ) ) || code == 0 ) { writeByte ( ( byte ) ( 0xC0 | ( code > > 6 ) ) ) ; writeByte ( ( byte ) ( 0x80 | ( code & 0x3F ) ) ) ; } else { writeByte ( ( byte ) ( 0xE0 | ( code > > > 12 ) ) ) ; writeByte ( ( byte ) ( 0x80 | ( ( code > > 6 ) & 0x3F ) ) ) ; writeByte ( ( byte ) ( 0x80 | ( code & 0x3F ) ) ) ; } } } public abstract void flush ( ) throws IOException ; public abstract void close ( ) throws IOException ; public abstract long getFilePointer ( ) ; public abstract void seek ( long pos ) throws IOException ; public abstract long length ( ) throws IOException ; } 	1
package org . apache . lucene . search ; import java . io . IOException ; import java . util . HashSet ; import java . util . Iterator ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; public abstract class Query implements java . io . Serializable , Cloneable { private float boost = 1.0f ; public void setBoost ( float b ) { boost = b ; } public float getBoost ( ) { return boost ; } public abstract String toString ( String field ) ; public String toString ( ) { return toString ( "" ) ; } protected Weight createWeight ( Searcher searcher ) throws IOException { throw new UnsupportedOperationException ( ) ; } public Weight weight ( Searcher searcher ) throws IOException { Query query = searcher . rewrite ( this ) ; Weight weight = query . createWeight ( searcher ) ; float sum = weight . sumOfSquaredWeights ( ) ; float norm = getSimilarity ( searcher ) . queryNorm ( sum ) ; weight . normalize ( norm ) ; return weight ; } public Query rewrite ( IndexReader reader ) throws IOException { return this ; } public Query combine ( Query [ ] queries ) { HashSet uniques = new HashSet ( ) ; for ( int i = 0 ; i < queries . length ; i ++ ) { Query query = queries [ i ] ; BooleanClause [ ] clauses = null ; boolean splittable = ( query instanceof BooleanQuery ) ; if ( splittable ) { BooleanQuery bq = ( BooleanQuery ) query ; splittable = bq . isCoordDisabled ( ) ; clauses = bq . getClauses ( ) ; for ( int j = 0 ; splittable && j < clauses . length ; j ++ ) { splittable = ( clauses [ j ] . getOccur ( ) == BooleanClause . Occur . SHOULD ) ; } } if ( splittable ) { for ( int j = 0 ; j < clauses . length ; j ++ ) { uniques . add ( clauses [ j ] . getQuery ( ) ) ; } } else { uniques . add ( query ) ; } } if ( uniques . size ( ) == 1 ) { return ( Query ) uniques . iterator ( ) . next ( ) ; } Iterator it = uniques . iterator ( ) ; BooleanQuery result = new BooleanQuery ( true ) ; while ( it . hasNext ( ) ) result . add ( ( Query ) it . next ( ) , BooleanClause . Occur . SHOULD ) ; return result ; } public void extractTerms ( Set terms ) { throw new UnsupportedOperationException ( ) ; } public static Query mergeBooleanQueries ( Query [ ] queries ) { HashSet allClauses = new HashSet ( ) ; for ( int i = 0 ; i < queries . length ; i ++ ) { BooleanClause [ ] clauses = ( ( BooleanQuery ) queries [ i ] ) . getClauses ( ) ; for ( int j = 0 ; j < clauses . length ; j ++ ) { allClauses . add ( clauses [ j ] ) ; } } boolean coordDisabled = queries . length == 0 ? false : ( ( BooleanQuery ) queries [ 0 ] ) . isCoordDisabled ( ) ; BooleanQuery result = new BooleanQuery ( coordDisabled ) ; Iterator i = allClauses . iterator ( ) ; while ( i . hasNext ( ) ) { result . add ( ( BooleanClause ) i . next ( ) ) ; } return result ; } public Similarity getSimilarity ( Searcher searcher ) { return searcher . getSimilarity ( ) ; } public Object clone ( ) { try { return ( Query ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { throw new RuntimeException ( "Clone not supported: " + e . getMessage ( ) ) ; } } } 	0
package org . apache . lucene . analysis ; import java . io . IOException ; public final class LowerCaseFilter extends TokenFilter { public LowerCaseFilter ( TokenStream in ) { super ( in ) ; } public final Token next ( ) throws IOException { Token t = input . next ( ) ; if ( t == null ) return null ; t . termText = t . termText . toLowerCase ( ) ; return t ; } } 	1
package org . apache . lucene . analysis ; import java . io . Reader ; public final class SimpleAnalyzer extends Analyzer { public TokenStream tokenStream ( String fieldName , Reader reader ) { return new LowerCaseTokenizer ( reader ) ; } } 	0
package org . apache . lucene . index ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . FieldSelector ; import org . apache . lucene . store . Directory ; import java . io . IOException ; import java . util . Collection ; import java . util . HashSet ; import java . util . Hashtable ; import java . util . Set ; public class MultiReader extends IndexReader { private IndexReader [ ] subReaders ; private int [ ] starts ; private Hashtable normsCache = new Hashtable ( ) ; private int maxDoc = 0 ; private int numDocs = - 1 ; private boolean hasDeletions = false ; public MultiReader ( IndexReader [ ] subReaders ) throws IOException { super ( subReaders . length == 0 ? null : subReaders [ 0 ] . directory ( ) ) ; initialize ( subReaders ) ; } MultiReader ( Directory directory , SegmentInfos sis , boolean closeDirectory , IndexReader [ ] subReaders ) { super ( directory , sis , closeDirectory ) ; initialize ( subReaders ) ; } private void initialize ( IndexReader [ ] subReaders ) { this . subReaders = subReaders ; starts = new int [ subReaders . length + 1 ] ; for ( int i = 0 ; i < subReaders . length ; i ++ ) { starts [ i ] = maxDoc ; maxDoc += subReaders [ i ] . maxDoc ( ) ; if ( subReaders [ i ] . hasDeletions ( ) ) hasDeletions = true ; } starts [ subReaders . length ] = maxDoc ; } public TermFreqVector [ ] getTermFreqVectors ( int n ) throws IOException { ensureOpen ( ) ; int i = readerIndex ( n ) ; return subReaders [ i ] . getTermFreqVectors ( n - starts [ i ] ) ; } public TermFreqVector getTermFreqVector ( int n , String field ) throws IOException { ensureOpen ( ) ; int i = readerIndex ( n ) ; return subReaders [ i ] . getTermFreqVector ( n - starts [ i ] , field ) ; } public synchronized int numDocs ( ) { if ( numDocs == - 1 ) { int n = 0 ; for ( int i = 0 ; i < subReaders . length ; i ++ ) n += subReaders [ i ] . numDocs ( ) ; numDocs = n ; } return numDocs ; } public int maxDoc ( ) { return maxDoc ; } public Document document ( int n , FieldSelector fieldSelector ) throws CorruptIndexException , IOException { ensureOpen ( ) ; int i = readerIndex ( n ) ; return subReaders [ i ] . document ( n - starts [ i ] , fieldSelector ) ; } public boolean isDeleted ( int n ) { int i = readerIndex ( n ) ; return subReaders [ i ] . isDeleted ( n - starts [ i ] ) ; } public boolean hasDeletions ( ) { return hasDeletions ; } protected void doDelete ( int n ) throws CorruptIndexException , IOException { numDocs = - 1 ; int i = readerIndex ( n ) ; subReaders [ i ] . deleteDocument ( n - starts [ i ] ) ; hasDeletions = true ; } protected void doUndeleteAll ( ) throws CorruptIndexException , IOException { for ( int i = 0 ; i < subReaders . length ; i ++ ) subReaders [ i ] . undeleteAll ( ) ; hasDeletions = false ; numDocs = - 1 ; } private int readerIndex ( int n ) { int lo = 0 ; int hi = subReaders . length - 1 ; while ( hi >= lo ) { int mid = ( lo + hi ) > > 1 ; int midValue = starts [ mid ] ; if ( n < midValue ) hi = mid - 1 ; else if ( n > midValue ) lo = mid + 1 ; else { while ( mid + 1 < subReaders . length && starts [ mid + 1 ] == midValue ) { mid ++ ; } return mid ; } } return hi ; } public boolean hasNorms ( String field ) throws IOException { ensureOpen ( ) ; for ( int i = 0 ; i < subReaders . length ; i ++ ) { if ( subReaders [ i ] . hasNorms ( field ) ) return true ; } return false ; } private byte [ ] ones ; private byte [ ] fakeNorms ( ) { if ( ones == null ) ones = SegmentReader . createFakeNorms ( maxDoc ( ) ) ; return ones ; } public synchronized byte [ ] norms ( String field ) throws IOException { ensureOpen ( ) ; byte [ ] bytes = ( byte [ ] ) normsCache . get ( field ) ; if ( bytes != null ) return bytes ; if ( ! hasNorms ( field ) ) return fakeNorms ( ) ; bytes = new byte [ maxDoc ( ) ] ; for ( int i = 0 ; i < subReaders . length ; i ++ ) subReaders [ i ] . norms ( field , bytes , starts [ i ] ) ; normsCache . put ( field , bytes ) ; return bytes ; } public synchronized void norms ( String field , byte [ ] result , int offset ) throws IOException { ensureOpen ( ) ; byte [ ] bytes = ( byte [ ] ) normsCache . get ( field ) ; if ( bytes == null && ! hasNorms ( field ) ) bytes = fakeNorms ( ) ; if ( bytes != null ) System . arraycopy ( bytes , 0 , result , offset , maxDoc ( ) ) ; for ( int i = 0 ; i < subReaders . length ; i ++ ) subReaders [ i ] . norms ( field , result , offset + starts [ i ] ) ; } protected void doSetNorm ( int n , String field , byte value ) throws CorruptIndexException , IOException { normsCache . remove ( field ) ; int i = readerIndex ( n ) ; subReaders [ i ] . setNorm ( n - starts [ i ] , field , value ) ; } public TermEnum terms ( ) throws IOException { ensureOpen ( ) ; return new MultiTermEnum ( subReaders , starts , null ) ; } public TermEnum terms ( Term term ) throws IOException { ensureOpen ( ) ; return new MultiTermEnum ( subReaders , starts , term ) ; } public int docFreq ( Term t ) throws IOException { ensureOpen ( ) ; int total = 0 ; for ( int i = 0 ; i < subReaders . length ; i ++ ) total += subReaders [ i ] . docFreq ( t ) ; return total ; } public TermDocs termDocs ( ) throws IOException { ensureOpen ( ) ; return new MultiTermDocs ( subReaders , starts ) ; } public TermPositions termPositions ( ) throws IOException { ensureOpen ( ) ; return new MultiTermPositions ( subReaders , starts ) ; } protected void doCommit ( ) throws IOException { for ( int i = 0 ; i < subReaders . length ; i ++ ) subReaders [ i ] . commit ( ) ; } void startCommit ( ) { super . startCommit ( ) ; for ( int i = 0 ; i < subReaders . length ; i ++ ) { subReaders [ i ] . startCommit ( ) ; } } void rollbackCommit ( ) { super . rollbackCommit ( ) ; for ( int i = 0 ; i < subReaders . length ; i ++ ) { subReaders [ i ] . rollbackCommit ( ) ; } } protected synchronized void doClose ( ) throws IOException { for ( int i = 0 ; i < subReaders . length ; i ++ ) subReaders [ i ] . close ( ) ; } public Collection getFieldNames ( IndexReader . FieldOption fieldNames ) { ensureOpen ( ) ; Set fieldSet = new HashSet ( ) ; for ( int i = 0 ; i < subReaders . length ; i ++ ) { IndexReader reader = subReaders [ i ] ; Collection names = reader . getFieldNames ( fieldNames ) ; fieldSet . addAll ( names ) ; } return fieldSet ; } } class MultiTermEnum extends TermEnum { private SegmentMergeQueue queue ; private Term term ; private int docFreq ; public MultiTermEnum ( IndexReader [ ] readers , int [ ] starts , Term t ) throws IOException { queue = new SegmentMergeQueue ( readers . length ) ; for ( int i = 0 ; i < readers . length ; i ++ ) { IndexReader reader = readers [ i ] ; TermEnum termEnum ; if ( t != null ) { termEnum = reader . terms ( t ) ; } else termEnum = reader . terms ( ) ; SegmentMergeInfo smi = new SegmentMergeInfo ( starts [ i ] , termEnum , reader ) ; if ( t == null ? smi . next ( ) : termEnum . term ( ) != null ) queue . put ( smi ) ; else smi . close ( ) ; } if ( t != null && queue . size ( ) > 0 ) { next ( ) ; } } public boolean next ( ) throws IOException { SegmentMergeInfo top = ( SegmentMergeInfo ) queue . top ( ) ; if ( top == null ) { term = null ; return false ; } term = top . term ; docFreq = 0 ; while ( top != null && term . compareTo ( top . term ) == 0 ) { queue . pop ( ) ; docFreq += top . termEnum . docFreq ( ) ; if ( top . next ( ) ) queue . put ( top ) ; else top . close ( ) ; top = ( SegmentMergeInfo ) queue . top ( ) ; } return true ; } public Term term ( ) { return term ; } public int docFreq ( ) { return docFreq ; } public void close ( ) throws IOException { queue . close ( ) ; } } class MultiTermDocs implements TermDocs { protected IndexReader [ ] readers ; protected int [ ] starts ; protected Term term ; protected int base = 0 ; protected int pointer = 0 ; private TermDocs [ ] readerTermDocs ; protected TermDocs current ; public MultiTermDocs ( IndexReader [ ] r , int [ ] s ) { readers = r ; starts = s ; readerTermDocs = new TermDocs [ r . length ] ; } public int doc ( ) { return base + current . doc ( ) ; } public int freq ( ) { return current . freq ( ) ; } public void seek ( Term term ) { this . term = term ; this . base = 0 ; this . pointer = 0 ; this . current = null ; } public void seek ( TermEnum termEnum ) throws IOException { seek ( termEnum . term ( ) ) ; } public boolean next ( ) throws IOException { for ( ; ; ) { if ( current != null && current . next ( ) ) { return true ; } else if ( pointer < readers . length ) { base = starts [ pointer ] ; current = termDocs ( pointer ++ ) ; } else { return false ; } } } public int read ( final int [ ] docs , final int [ ] freqs ) throws IOException { while ( true ) { while ( current == null ) { if ( pointer < readers . length ) { base = starts [ pointer ] ; current = termDocs ( pointer ++ ) ; } else { return 0 ; } } int end = current . read ( docs , freqs ) ; if ( end == 0 ) { current = null ; } else { final int b = base ; for ( int i = 0 ; i < end ; i ++ ) docs [ i ] += b ; return end ; } } } public boolean skipTo ( int target ) throws IOException { for ( ; ; ) { if ( current != null && current . skipTo ( target - base ) ) { return true ; } else if ( pointer < readers . length ) { base = starts [ pointer ] ; current = termDocs ( pointer ++ ) ; } else return false ; } } private TermDocs termDocs ( int i ) throws IOException { if ( term == null ) return null ; TermDocs result = readerTermDocs [ i ] ; if ( result == null ) result = readerTermDocs [ i ] = termDocs ( readers [ i ] ) ; result . seek ( term ) ; return result ; } protected TermDocs termDocs ( IndexReader reader ) throws IOException { return reader . termDocs ( ) ; } public void close ( ) throws IOException { for ( int i = 0 ; i < readerTermDocs . length ; i ++ ) { if ( readerTermDocs [ i ] != null ) readerTermDocs [ i ] . close ( ) ; } } } class MultiTermPositions extends MultiTermDocs implements TermPositions { public MultiTermPositions ( IndexReader [ ] r , int [ ] s ) { super ( r , s ) ; } protected TermDocs termDocs ( IndexReader reader ) throws IOException { return ( TermDocs ) reader . termPositions ( ) ; } public int nextPosition ( ) throws IOException { return ( ( TermPositions ) current ) . nextPosition ( ) ; } public int getPayloadLength ( ) { return ( ( TermPositions ) current ) . getPayloadLength ( ) ; } public byte [ ] getPayload ( byte [ ] data , int offset ) throws IOException { return ( ( TermPositions ) current ) . getPayload ( data , offset ) ; } public boolean isPayloadAvailable ( ) { return ( ( TermPositions ) current ) . isPayloadAvailable ( ) ; } } 	1
package org . apache . lucene . index ; final class TermInfo { int docFreq = 0 ; long freqPointer = 0 ; long proxPointer = 0 ; int skipOffset ; TermInfo ( ) { } TermInfo ( int df , long fp , long pp ) { docFreq = df ; freqPointer = fp ; proxPointer = pp ; } TermInfo ( TermInfo ti ) { docFreq = ti . docFreq ; freqPointer = ti . freqPointer ; proxPointer = ti . proxPointer ; skipOffset = ti . skipOffset ; } final void set ( int docFreq , long freqPointer , long proxPointer , int skipOffset ) { this . docFreq = docFreq ; this . freqPointer = freqPointer ; this . proxPointer = proxPointer ; this . skipOffset = skipOffset ; } final void set ( TermInfo ti ) { docFreq = ti . docFreq ; freqPointer = ti . freqPointer ; proxPointer = ti . proxPointer ; skipOffset = ti . skipOffset ; } } 	0
package org . apache . lucene . search . spans ; import org . apache . lucene . search . Explanation ; import org . apache . lucene . search . Scorer ; import org . apache . lucene . search . Similarity ; import org . apache . lucene . search . Weight ; import java . io . IOException ; public class SpanScorer extends Scorer { protected Spans spans ; protected Weight weight ; protected byte [ ] norms ; protected float value ; protected boolean firstTime = true ; protected boolean more = true ; protected int doc ; protected float freq ; protected SpanScorer ( Spans spans , Weight weight , Similarity similarity , byte [ ] norms ) throws IOException { super ( similarity ) ; this . spans = spans ; this . norms = norms ; this . weight = weight ; this . value = weight . getValue ( ) ; doc = - 1 ; } public boolean next ( ) throws IOException { if ( firstTime ) { more = spans . next ( ) ; firstTime = false ; } return setFreqCurrentDoc ( ) ; } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { more = spans . skipTo ( target ) ; firstTime = false ; } if ( ! more ) { return false ; } if ( spans . doc ( ) < target ) { more = spans . skipTo ( target ) ; } return setFreqCurrentDoc ( ) ; } protected boolean setFreqCurrentDoc ( ) throws IOException { if ( ! more ) { return false ; } doc = spans . doc ( ) ; freq = 0.0f ; while ( more && doc == spans . doc ( ) ) { int matchLength = spans . end ( ) - spans . start ( ) ; freq += getSimilarity ( ) . sloppyFreq ( matchLength ) ; more = spans . next ( ) ; } return more || ( freq != 0 ) ; } public int doc ( ) { return doc ; } public float score ( ) throws IOException { float raw = getSimilarity ( ) . tf ( freq ) * value ; return raw * Similarity . decodeNorm ( norms [ doc ] ) ; } public Explanation explain ( final int doc ) throws IOException { Explanation tfExplanation = new Explanation ( ) ; skipTo ( doc ) ; float phraseFreq = ( doc ( ) == doc ) ? freq : 0.0f ; tfExplanation . setValue ( getSimilarity ( ) . tf ( phraseFreq ) ) ; tfExplanation . setDescription ( "tf(phraseFreq=" + phraseFreq + ")" ) ; return tfExplanation ; } } 	1
package org . apache . lucene . index ; import java . io . IOException ; public class CorruptIndexException extends IOException { public CorruptIndexException ( String message ) { super ( message ) ; } } 	0
package org . apache . lucene . document ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Hits ; import org . apache . lucene . search . Searcher ; import java . util . * ; public final class Document implements java . io . Serializable { List fields = new Vector ( ) ; private float boost = 1.0f ; public Document ( ) { } public void setBoost ( float boost ) { this . boost = boost ; } public float getBoost ( ) { return boost ; } public final void add ( Fieldable field ) { fields . add ( field ) ; } public final void removeField ( String name ) { Iterator it = fields . iterator ( ) ; while ( it . hasNext ( ) ) { Fieldable field = ( Fieldable ) it . next ( ) ; if ( field . name ( ) . equals ( name ) ) { it . remove ( ) ; return ; } } } public final void removeFields ( String name ) { Iterator it = fields . iterator ( ) ; while ( it . hasNext ( ) ) { Fieldable field = ( Fieldable ) it . next ( ) ; if ( field . name ( ) . equals ( name ) ) { it . remove ( ) ; } } } public final Field getField ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Field field = ( Field ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) return field ; } return null ; } public Fieldable getFieldable ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) return field ; } return null ; } public final String get ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( ! field . isBinary ( ) ) ) return field . stringValue ( ) ; } return null ; } public final Enumeration fields ( ) { return ( ( Vector ) fields ) . elements ( ) ; } public final List getFields ( ) { return fields ; } public final Field [ ] getFields ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Field field = ( Field ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) { result . add ( field ) ; } } if ( result . size ( ) == 0 ) return null ; return ( Field [ ] ) result . toArray ( new Field [ result . size ( ) ] ) ; } public Fieldable [ ] getFieldables ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) { result . add ( field ) ; } } if ( result . size ( ) == 0 ) return null ; return ( Fieldable [ ] ) result . toArray ( new Fieldable [ result . size ( ) ] ) ; } public final String [ ] getValues ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( ! field . isBinary ( ) ) ) result . add ( field . stringValue ( ) ) ; } if ( result . size ( ) == 0 ) return null ; return ( String [ ] ) result . toArray ( new String [ result . size ( ) ] ) ; } public final byte [ ] [ ] getBinaryValues ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( field . isBinary ( ) ) ) result . add ( field . binaryValue ( ) ) ; } if ( result . size ( ) == 0 ) return null ; return ( byte [ ] [ ] ) result . toArray ( new byte [ result . size ( ) ] [ ] ) ; } public final byte [ ] getBinaryValue ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( field . isBinary ( ) ) ) return field . binaryValue ( ) ; } return null ; } public final String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "Document<" ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; buffer . append ( field . toString ( ) ) ; if ( i != fields . size ( ) - 1 ) buffer . append ( " " ) ; } buffer . append ( ">" ) ; return buffer . toString ( ) ; } } 	1
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . util . PriorityQueue ; final class SegmentMergeQueue extends PriorityQueue { SegmentMergeQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object a , Object b ) { SegmentMergeInfo stiA = ( SegmentMergeInfo ) a ; SegmentMergeInfo stiB = ( SegmentMergeInfo ) b ; int comparison = stiA . term . compareTo ( stiB . term ) ; if ( comparison == 0 ) return stiA . base < stiB . base ; else return comparison < 0 ; } final void close ( ) throws IOException { while ( top ( ) != null ) ( ( SegmentMergeInfo ) pop ( ) ) . close ( ) ; } } 	0
package org . apache . lucene . queryParser ; public interface CharStream { char readChar ( ) throws java . io . IOException ; int getColumn ( ) ; int getLine ( ) ; int getEndColumn ( ) ; int getEndLine ( ) ; int getBeginColumn ( ) ; int getBeginLine ( ) ; void backup ( int amount ) ; char BeginToken ( ) throws java . io . IOException ; String GetImage ( ) ; char [ ] GetSuffix ( int len ) ; void Done ( ) ; } 	1
package org . apache . lucene . search ; import java . util . List ; import java . util . Iterator ; import java . io . IOException ; import org . apache . lucene . util . ScorerDocQueue ; class DisjunctionSumScorer extends Scorer { private final int nrScorers ; protected final List subScorers ; private final int minimumNrMatchers ; private ScorerDocQueue scorerDocQueue = null ; private int queueSize = - 1 ; private int currentDoc = - 1 ; protected int nrMatchers = - 1 ; private float currentScore = Float . NaN ; public DisjunctionSumScorer ( List subScorers , int minimumNrMatchers ) { super ( null ) ; nrScorers = subScorers . size ( ) ; if ( minimumNrMatchers <= 0 ) { throw new IllegalArgumentException ( "Minimum nr of matchers must be positive" ) ; } if ( nrScorers <= 1 ) { throw new IllegalArgumentException ( "There must be at least 2 subScorers" ) ; } this . minimumNrMatchers = minimumNrMatchers ; this . subScorers = subScorers ; } public DisjunctionSumScorer ( List subScorers ) { this ( subScorers , 1 ) ; } private void initScorerDocQueue ( ) throws IOException { Iterator si = subScorers . iterator ( ) ; scorerDocQueue = new ScorerDocQueue ( nrScorers ) ; queueSize = 0 ; while ( si . hasNext ( ) ) { Scorer se = ( Scorer ) si . next ( ) ; if ( se . next ( ) ) { if ( scorerDocQueue . insert ( se ) ) { queueSize ++ ; } } } } public void score ( HitCollector hc ) throws IOException { while ( next ( ) ) { hc . collect ( currentDoc , currentScore ) ; } } protected boolean score ( HitCollector hc , int max ) throws IOException { while ( currentDoc < max ) { hc . collect ( currentDoc , currentScore ) ; if ( ! next ( ) ) { return false ; } } return true ; } public boolean next ( ) throws IOException { if ( scorerDocQueue == null ) { initScorerDocQueue ( ) ; } return ( scorerDocQueue . size ( ) >= minimumNrMatchers ) && advanceAfterCurrent ( ) ; } protected boolean advanceAfterCurrent ( ) throws IOException { do { currentDoc = scorerDocQueue . topDoc ( ) ; currentScore = scorerDocQueue . topScore ( ) ; nrMatchers = 1 ; do { if ( ! scorerDocQueue . topNextAndAdjustElsePop ( ) ) { if ( -- queueSize == 0 ) { break ; } } if ( scorerDocQueue . topDoc ( ) != currentDoc ) { break ; } currentScore += scorerDocQueue . topScore ( ) ; nrMatchers ++ ; } while ( true ) ; if ( nrMatchers >= minimumNrMatchers ) { return true ; } else if ( queueSize < minimumNrMatchers ) { return false ; } } while ( true ) ; } public float score ( ) throws IOException { return currentScore ; } public int doc ( ) { return currentDoc ; } public int nrMatchers ( ) { return nrMatchers ; } public boolean skipTo ( int target ) throws IOException { if ( scorerDocQueue == null ) { initScorerDocQueue ( ) ; } if ( queueSize < minimumNrMatchers ) { return false ; } if ( target <= currentDoc ) { return true ; } do { if ( scorerDocQueue . topDoc ( ) >= target ) { return advanceAfterCurrent ( ) ; } else if ( ! scorerDocQueue . topSkipToAndAdjustElsePop ( target ) ) { if ( -- queueSize < minimumNrMatchers ) { return false ; } } } while ( true ) ; } public Explanation explain ( int doc ) throws IOException { Explanation res = new Explanation ( ) ; Iterator ssi = subScorers . iterator ( ) ; float sumScore = 0.0f ; int nrMatches = 0 ; while ( ssi . hasNext ( ) ) { Explanation es = ( ( Scorer ) ssi . next ( ) ) . explain ( doc ) ; if ( es . getValue ( ) > 0.0f ) { sumScore += es . getValue ( ) ; nrMatches ++ ; } res . addDetail ( es ) ; } if ( nrMatchers >= minimumNrMatchers ) { res . setValue ( sumScore ) ; res . setDescription ( "sum over at least " + minimumNrMatchers + " of " + subScorers . size ( ) + ":" ) ; } else { res . setValue ( 0.0f ) ; res . setDescription ( nrMatches + " match(es) but at least " + minimumNrMatchers + " of " + subScorers . size ( ) + " needed" ) ; } return res ; } } 	0
package org . apache . lucene . search ; import org . apache . lucene . index . TermPositions ; import java . io . IOException ; import java . util . Arrays ; import java . util . Comparator ; import java . util . HashMap ; final class SloppyPhraseScorer extends PhraseScorer { private int slop ; private PhrasePositions repeats [ ] ; private boolean checkedRepeats ; SloppyPhraseScorer ( Weight weight , TermPositions [ ] tps , int [ ] offsets , Similarity similarity , int slop , byte [ ] norms ) { super ( weight , tps , offsets , similarity , norms ) ; this . slop = slop ; } protected final float phraseFreq ( ) throws IOException { int end = initPhrasePositions ( ) ; float freq = 0.0f ; boolean done = ( end < 0 ) ; while ( ! done ) { PhrasePositions pp = ( PhrasePositions ) pq . pop ( ) ; int start = pp . position ; int next = ( ( PhrasePositions ) pq . top ( ) ) . position ; boolean tpsDiffer = true ; for ( int pos = start ; pos <= next || ! tpsDiffer ; pos = pp . position ) { if ( pos <= next && tpsDiffer ) start = pos ; if ( ! pp . nextPosition ( ) ) { done = true ; break ; } tpsDiffer = ! pp . repeats || termPositionsDiffer ( pp ) ; } int matchLength = end - start ; if ( matchLength <= slop ) freq += getSimilarity ( ) . sloppyFreq ( matchLength ) ; if ( pp . position > end ) end = pp . position ; pq . put ( pp ) ; } return freq ; } private int initPhrasePositions ( ) throws IOException { int end = 0 ; if ( checkedRepeats && repeats == null ) { pq . clear ( ) ; for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) { pp . firstPosition ( ) ; if ( pp . position > end ) end = pp . position ; pq . put ( pp ) ; } return end ; } for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) pp . firstPosition ( ) ; if ( ! checkedRepeats ) { checkedRepeats = true ; HashMap m = null ; for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) { int tpPos = pp . position + pp . offset ; for ( PhrasePositions pp2 = pp . next ; pp2 != null ; pp2 = pp2 . next ) { int tpPos2 = pp2 . position + pp2 . offset ; if ( tpPos2 == tpPos ) { if ( m == null ) m = new HashMap ( ) ; pp . repeats = true ; pp2 . repeats = true ; m . put ( pp , null ) ; m . put ( pp2 , null ) ; } } } if ( m != null ) repeats = ( PhrasePositions [ ] ) m . keySet ( ) . toArray ( new PhrasePositions [ 0 ] ) ; } if ( repeats != null ) { Arrays . sort ( repeats , new Comparator ( ) { public int compare ( Object x , Object y ) { return ( ( PhrasePositions ) y ) . offset - ( ( PhrasePositions ) x ) . offset ; } } ) ; for ( int i = 0 ; i < repeats . length ; i ++ ) { PhrasePositions pp = repeats [ i ] ; while ( ! termPositionsDiffer ( pp ) ) { if ( ! pp . nextPosition ( ) ) return - 1 ; } } } pq . clear ( ) ; for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) { if ( pp . position > end ) end = pp . position ; pq . put ( pp ) ; } return end ; } private boolean termPositionsDiffer ( PhrasePositions pp ) { int tpPos = pp . position + pp . offset ; for ( int i = 0 ; i < repeats . length ; i ++ ) { PhrasePositions pp2 = repeats [ i ] ; if ( pp2 == pp ) continue ; int tpPos2 = pp2 . position + pp2 . offset ; if ( tpPos2 == tpPos ) return false ; } return true ; } } 	1
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexInput ; final class TermBuffer implements Cloneable { private static final char [ ] NO_CHARS = new char [ 0 ] ; private String field ; private char [ ] text = NO_CHARS ; private int textLength ; private Term term ; public final int compareTo ( TermBuffer other ) { if ( field == other . field ) return compareChars ( text , textLength , other . text , other . textLength ) ; else return field . compareTo ( other . field ) ; } private static final int compareChars ( char [ ] v1 , int len1 , char [ ] v2 , int len2 ) { int end = Math . min ( len1 , len2 ) ; for ( int k = 0 ; k < end ; k ++ ) { char c1 = v1 [ k ] ; char c2 = v2 [ k ] ; if ( c1 != c2 ) { return c1 - c2 ; } } return len1 - len2 ; } private final void setTextLength ( int newLength ) { if ( text . length < newLength ) { char [ ] newText = new char [ newLength ] ; System . arraycopy ( text , 0 , newText , 0 , textLength ) ; text = newText ; } textLength = newLength ; } public final void read ( IndexInput input , FieldInfos fieldInfos ) throws IOException { this . term = null ; int start = input . readVInt ( ) ; int length = input . readVInt ( ) ; int totalLength = start + length ; setTextLength ( totalLength ) ; input . readChars ( this . text , start , length ) ; this . field = fieldInfos . fieldName ( input . readVInt ( ) ) ; } public final void set ( Term term ) { if ( term == null ) { reset ( ) ; return ; } setTextLength ( term . text ( ) . length ( ) ) ; term . text ( ) . getChars ( 0 , term . text ( ) . length ( ) , text , 0 ) ; this . field = term . field ( ) ; this . term = term ; } public final void set ( TermBuffer other ) { setTextLength ( other . textLength ) ; System . arraycopy ( other . text , 0 , text , 0 , textLength ) ; this . field = other . field ; this . term = other . term ; } public void reset ( ) { this . field = null ; this . textLength = 0 ; this . term = null ; } public Term toTerm ( ) { if ( field == null ) return null ; if ( term == null ) term = new Term ( field , new String ( text , 0 , textLength ) , false ) ; return term ; } protected Object clone ( ) { TermBuffer clone = null ; try { clone = ( TermBuffer ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . text = new char [ text . length ] ; System . arraycopy ( text , 0 , clone . text , 0 , textLength ) ; return clone ; } } 	0
package org . apache . lucene . index ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . IndexOutput ; import java . io . IOException ; import java . util . * ; final class FieldInfos { static final byte IS_INDEXED = 0x1 ; static final byte STORE_TERMVECTOR = 0x2 ; static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x4 ; static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x8 ; static final byte OMIT_NORMS = 0x10 ; static final byte STORE_PAYLOADS = 0x20 ; private ArrayList byNumber = new ArrayList ( ) ; private HashMap byName = new HashMap ( ) ; FieldInfos ( ) { } FieldInfos ( Directory d , String name ) throws IOException { IndexInput input = d . openInput ( name ) ; try { read ( input ) ; } finally { input . close ( ) ; } } public void add ( Document doc ) { List fields = doc . getFields ( ) ; Iterator fieldIterator = fields . iterator ( ) ; while ( fieldIterator . hasNext ( ) ) { Fieldable field = ( Fieldable ) fieldIterator . next ( ) ; add ( field . name ( ) , field . isIndexed ( ) , field . isTermVectorStored ( ) , field . isStorePositionWithTermVector ( ) , field . isStoreOffsetWithTermVector ( ) , field . getOmitNorms ( ) ) ; } } public void addIndexed ( Collection names , boolean storeTermVectors , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector ) { Iterator i = names . iterator ( ) ; while ( i . hasNext ( ) ) { add ( ( String ) i . next ( ) , true , storeTermVectors , storePositionWithTermVector , storeOffsetWithTermVector ) ; } } public void add ( Collection names , boolean isIndexed ) { Iterator i = names . iterator ( ) ; while ( i . hasNext ( ) ) { add ( ( String ) i . next ( ) , isIndexed ) ; } } public void add ( String name , boolean isIndexed ) { add ( name , isIndexed , false , false , false , false ) ; } public void add ( String name , boolean isIndexed , boolean storeTermVector ) { add ( name , isIndexed , storeTermVector , false , false , false ) ; } public void add ( String name , boolean isIndexed , boolean storeTermVector , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector ) { add ( name , isIndexed , storeTermVector , storePositionWithTermVector , storeOffsetWithTermVector , false ) ; } public void add ( String name , boolean isIndexed , boolean storeTermVector , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector , boolean omitNorms ) { add ( name , isIndexed , storeTermVector , storePositionWithTermVector , storeOffsetWithTermVector , omitNorms , false ) ; } public FieldInfo add ( String name , boolean isIndexed , boolean storeTermVector , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector , boolean omitNorms , boolean storePayloads ) { FieldInfo fi = fieldInfo ( name ) ; if ( fi == null ) { return addInternal ( name , isIndexed , storeTermVector , storePositionWithTermVector , storeOffsetWithTermVector , omitNorms , storePayloads ) ; } else { if ( fi . isIndexed != isIndexed ) { fi . isIndexed = true ; } if ( fi . storeTermVector != storeTermVector ) { fi . storeTermVector = true ; } if ( fi . storePositionWithTermVector != storePositionWithTermVector ) { fi . storePositionWithTermVector = true ; } if ( fi . storeOffsetWithTermVector != storeOffsetWithTermVector ) { fi . storeOffsetWithTermVector = true ; } if ( fi . omitNorms != omitNorms ) { fi . omitNorms = false ; } if ( fi . storePayloads != storePayloads ) { fi . storePayloads = true ; } } return fi ; } private FieldInfo addInternal ( String name , boolean isIndexed , boolean storeTermVector , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector , boolean omitNorms , boolean storePayloads ) { FieldInfo fi = new FieldInfo ( name , isIndexed , byNumber . size ( ) , storeTermVector , storePositionWithTermVector , storeOffsetWithTermVector , omitNorms , storePayloads ) ; byNumber . add ( fi ) ; byName . put ( name , fi ) ; return fi ; } public int fieldNumber ( String fieldName ) { try { FieldInfo fi = fieldInfo ( fieldName ) ; if ( fi != null ) return fi . number ; } catch ( IndexOutOfBoundsException ioobe ) { return - 1 ; } return - 1 ; } public FieldInfo fieldInfo ( String fieldName ) { return ( FieldInfo ) byName . get ( fieldName ) ; } public String fieldName ( int fieldNumber ) { try { return fieldInfo ( fieldNumber ) . name ; } catch ( NullPointerException npe ) { return "" ; } } public FieldInfo fieldInfo ( int fieldNumber ) { try { return ( FieldInfo ) byNumber . get ( fieldNumber ) ; } catch ( IndexOutOfBoundsException ioobe ) { return null ; } } public int size ( ) { return byNumber . size ( ) ; } public boolean hasVectors ( ) { boolean hasVectors = false ; for ( int i = 0 ; i < size ( ) ; i ++ ) { if ( fieldInfo ( i ) . storeTermVector ) { hasVectors = true ; break ; } } return hasVectors ; } public void write ( Directory d , String name ) throws IOException { IndexOutput output = d . createOutput ( name ) ; try { write ( output ) ; } finally { output . close ( ) ; } } public void write ( IndexOutput output ) throws IOException { output . writeVInt ( size ( ) ) ; for ( int i = 0 ; i < size ( ) ; i ++ ) { FieldInfo fi = fieldInfo ( i ) ; byte bits = 0x0 ; if ( fi . isIndexed ) bits |= IS_INDEXED ; if ( fi . storeTermVector ) bits |= STORE_TERMVECTOR ; if ( fi . storePositionWithTermVector ) bits |= STORE_POSITIONS_WITH_TERMVECTOR ; if ( fi . storeOffsetWithTermVector ) bits |= STORE_OFFSET_WITH_TERMVECTOR ; if ( fi . omitNorms ) bits |= OMIT_NORMS ; if ( fi . storePayloads ) bits |= STORE_PAYLOADS ; output . writeString ( fi . name ) ; output . writeByte ( bits ) ; } } private void read ( IndexInput input ) throws IOException { int size = input . readVInt ( ) ; for ( int i = 0 ; i < size ; i ++ ) { String name = input . readString ( ) . intern ( ) ; byte bits = input . readByte ( ) ; boolean isIndexed = ( bits & IS_INDEXED ) != 0 ; boolean storeTermVector = ( bits & STORE_TERMVECTOR ) != 0 ; boolean storePositionsWithTermVector = ( bits & STORE_POSITIONS_WITH_TERMVECTOR ) != 0 ; boolean storeOffsetWithTermVector = ( bits & STORE_OFFSET_WITH_TERMVECTOR ) != 0 ; boolean omitNorms = ( bits & OMIT_NORMS ) != 0 ; boolean storePayloads = ( bits & STORE_PAYLOADS ) != 0 ; addInternal ( name , isIndexed , storeTermVector , storePositionsWithTermVector , storeOffsetWithTermVector , omitNorms , storePayloads ) ; } } } 	1
package org . apache . lucene . store ; import java . io . IOException ; public abstract class BufferedIndexOutput extends IndexOutput { static final int BUFFER_SIZE = 16384 ; private final byte [ ] buffer = new byte [ BUFFER_SIZE ] ; private long bufferStart = 0 ; private int bufferPosition = 0 ; public void writeByte ( byte b ) throws IOException { if ( bufferPosition >= BUFFER_SIZE ) flush ( ) ; buffer [ bufferPosition ++ ] = b ; } public void writeBytes ( byte [ ] b , int offset , int length ) throws IOException { int bytesLeft = BUFFER_SIZE - bufferPosition ; if ( bytesLeft >= length ) { System . arraycopy ( b , offset , buffer , bufferPosition , length ) ; bufferPosition += length ; if ( BUFFER_SIZE - bufferPosition == 0 ) flush ( ) ; } else { if ( length > BUFFER_SIZE ) { if ( bufferPosition > 0 ) flush ( ) ; flushBuffer ( b , offset , length ) ; bufferStart += length ; } else { int pos = 0 ; int pieceLength ; while ( pos < length ) { pieceLength = ( length - pos < bytesLeft ) ? length - pos : bytesLeft ; System . arraycopy ( b , pos + offset , buffer , bufferPosition , pieceLength ) ; pos += pieceLength ; bufferPosition += pieceLength ; bytesLeft = BUFFER_SIZE - bufferPosition ; if ( bytesLeft == 0 ) { flush ( ) ; bytesLeft = BUFFER_SIZE ; } } } } } public void flush ( ) throws IOException { flushBuffer ( buffer , bufferPosition ) ; bufferStart += bufferPosition ; bufferPosition = 0 ; } private void flushBuffer ( byte [ ] b , int len ) throws IOException { flushBuffer ( b , 0 , len ) ; } protected abstract void flushBuffer ( byte [ ] b , int offset , int len ) throws IOException ; public void close ( ) throws IOException { flush ( ) ; } public long getFilePointer ( ) { return bufferStart + bufferPosition ; } public void seek ( long pos ) throws IOException { flush ( ) ; bufferStart = pos ; } public abstract long length ( ) throws IOException ; } 	0
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . IndexInput ; import java . util . LinkedList ; import java . util . HashSet ; import java . util . Iterator ; import java . io . IOException ; final class CompoundFileWriter { private static final class FileEntry { String file ; long directoryOffset ; long dataOffset ; } private Directory directory ; private String fileName ; private HashSet ids ; private LinkedList entries ; private boolean merged = false ; public CompoundFileWriter ( Directory dir , String name ) { if ( dir == null ) throw new NullPointerException ( "directory cannot be null" ) ; if ( name == null ) throw new NullPointerException ( "name cannot be null" ) ; directory = dir ; fileName = name ; ids = new HashSet ( ) ; entries = new LinkedList ( ) ; } public Directory getDirectory ( ) { return directory ; } public String getName ( ) { return fileName ; } public void addFile ( String file ) { if ( merged ) throw new IllegalStateException ( "Can't add extensions after merge has been called" ) ; if ( file == null ) throw new NullPointerException ( "file cannot be null" ) ; if ( ! ids . add ( file ) ) throw new IllegalArgumentException ( "File " + file + " already added" ) ; FileEntry entry = new FileEntry ( ) ; entry . file = file ; entries . add ( entry ) ; } public void close ( ) throws IOException { if ( merged ) throw new IllegalStateException ( "Merge already performed" ) ; if ( entries . isEmpty ( ) ) throw new IllegalStateException ( "No entries to merge have been defined" ) ; merged = true ; IndexOutput os = null ; try { os = directory . createOutput ( fileName ) ; os . writeVInt ( entries . size ( ) ) ; Iterator it = entries . iterator ( ) ; while ( it . hasNext ( ) ) { FileEntry fe = ( FileEntry ) it . next ( ) ; fe . directoryOffset = os . getFilePointer ( ) ; os . writeLong ( 0 ) ; os . writeString ( fe . file ) ; } byte buffer [ ] = new byte [ 16384 ] ; it = entries . iterator ( ) ; while ( it . hasNext ( ) ) { FileEntry fe = ( FileEntry ) it . next ( ) ; fe . dataOffset = os . getFilePointer ( ) ; copyFile ( fe , os , buffer ) ; } it = entries . iterator ( ) ; while ( it . hasNext ( ) ) { FileEntry fe = ( FileEntry ) it . next ( ) ; os . seek ( fe . directoryOffset ) ; os . writeLong ( fe . dataOffset ) ; } IndexOutput tmp = os ; os = null ; tmp . close ( ) ; } finally { if ( os != null ) try { os . close ( ) ; } catch ( IOException e ) { } } } private void copyFile ( FileEntry source , IndexOutput os , byte buffer [ ] ) throws IOException { IndexInput is = null ; try { long startPtr = os . getFilePointer ( ) ; is = directory . openInput ( source . file ) ; long length = is . length ( ) ; long remainder = length ; int chunk = buffer . length ; while ( remainder > 0 ) { int len = ( int ) Math . min ( chunk , remainder ) ; is . readBytes ( buffer , 0 , len ) ; os . writeBytes ( buffer , len ) ; remainder -= len ; } if ( remainder != 0 ) throw new IOException ( "Non-zero remainder length after copying: " + remainder + " (id: " + source . file + ", length: " + length + ", buffer size: " + chunk + ")" ) ; long endPtr = os . getFilePointer ( ) ; long diff = endPtr - startPtr ; if ( diff != length ) throw new IOException ( "Difference in the output file offsets " + diff + " does not match the original file length " + length ) ; } finally { if ( is != null ) is . close ( ) ; } } } 	1
package org . apache . lucene . index ; public class TermVectorOffsetInfo { public static final TermVectorOffsetInfo [ ] EMPTY_OFFSET_INFO = new TermVectorOffsetInfo [ 0 ] ; private int startOffset ; private int endOffset ; public TermVectorOffsetInfo ( ) { } public TermVectorOffsetInfo ( int startOffset , int endOffset ) { this . endOffset = endOffset ; this . startOffset = startOffset ; } public int getEndOffset ( ) { return endOffset ; } public void setEndOffset ( int endOffset ) { this . endOffset = endOffset ; } public int getStartOffset ( ) { return startOffset ; } public void setStartOffset ( int startOffset ) { this . startOffset = startOffset ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof TermVectorOffsetInfo ) ) return false ; final TermVectorOffsetInfo termVectorOffsetInfo = ( TermVectorOffsetInfo ) o ; if ( endOffset != termVectorOffsetInfo . endOffset ) return false ; if ( startOffset != termVectorOffsetInfo . startOffset ) return false ; return true ; } public int hashCode ( ) { int result ; result = startOffset ; result = 29 * result + endOffset ; return result ; } } 	0
package org . apache . lucene . search ; import org . apache . lucene . search . Filter ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . TermEnum ; import org . apache . lucene . index . TermDocs ; import java . util . BitSet ; import java . io . IOException ; public class PrefixFilter extends Filter { protected final Term prefix ; public PrefixFilter ( Term prefix ) { this . prefix = prefix ; } public Term getPrefix ( ) { return prefix ; } public BitSet bits ( IndexReader reader ) throws IOException { final BitSet bitSet = new BitSet ( reader . maxDoc ( ) ) ; new PrefixGenerator ( prefix ) { public void handleDoc ( int doc ) { bitSet . set ( doc ) ; } } . generate ( reader ) ; return bitSet ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "PrefixFilter(" ) ; buffer . append ( prefix . toString ( ) ) ; buffer . append ( ")" ) ; return buffer . toString ( ) ; } } interface IdGenerator { public void generate ( IndexReader reader ) throws IOException ; public void handleDoc ( int doc ) ; } abstract class PrefixGenerator implements IdGenerator { protected final Term prefix ; PrefixGenerator ( Term prefix ) { this . prefix = prefix ; } public void generate ( IndexReader reader ) throws IOException { TermEnum enumerator = reader . terms ( prefix ) ; TermDocs termDocs = reader . termDocs ( ) ; try { String prefixText = prefix . text ( ) ; String prefixField = prefix . field ( ) ; do { Term term = enumerator . term ( ) ; if ( term != null && term . text ( ) . startsWith ( prefixText ) && term . field ( ) == prefixField ) { termDocs . seek ( term ) ; while ( termDocs . next ( ) ) { handleDoc ( termDocs . doc ( ) ) ; } } else { break ; } } while ( enumerator . next ( ) ) ; } finally { termDocs . close ( ) ; enumerator . close ( ) ; } } } 	1
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . RAMOutputStream ; abstract class MultiLevelSkipListWriter { private int numberOfSkipLevels ; private int skipInterval ; private RAMOutputStream [ ] skipBuffer ; protected MultiLevelSkipListWriter ( int skipInterval , int maxSkipLevels , int df ) { this . skipInterval = skipInterval ; numberOfSkipLevels = df == 0 ? 0 : ( int ) Math . floor ( Math . log ( df ) / Math . log ( skipInterval ) ) ; if ( numberOfSkipLevels > maxSkipLevels ) { numberOfSkipLevels = maxSkipLevels ; } } protected void init ( ) { skipBuffer = new RAMOutputStream [ numberOfSkipLevels ] ; for ( int i = 0 ; i < numberOfSkipLevels ; i ++ ) { skipBuffer [ i ] = new RAMOutputStream ( ) ; } } protected void resetSkip ( ) { if ( skipBuffer == null ) { init ( ) ; } else { for ( int i = 0 ; i < skipBuffer . length ; i ++ ) { skipBuffer [ i ] . reset ( ) ; } } } protected abstract void writeSkipData ( int level , IndexOutput skipBuffer ) throws IOException ; void bufferSkip ( int df ) throws IOException { int numLevels ; for ( numLevels = 0 ; ( df % skipInterval ) == 0 && numLevels < numberOfSkipLevels ; df /= skipInterval ) { numLevels ++ ; } long childPointer = 0 ; for ( int level = 0 ; level < numLevels ; level ++ ) { writeSkipData ( level , skipBuffer [ level ] ) ; long newChildPointer = skipBuffer [ level ] . getFilePointer ( ) ; if ( level != 0 ) { skipBuffer [ level ] . writeVLong ( childPointer ) ; } childPointer = newChildPointer ; } } long writeSkip ( IndexOutput output ) throws IOException { long skipPointer = output . getFilePointer ( ) ; if ( skipBuffer == null || skipBuffer . length == 0 ) return skipPointer ; for ( int level = numberOfSkipLevels - 1 ; level > 0 ; level -- ) { long length = skipBuffer [ level ] . getFilePointer ( ) ; if ( length > 0 ) { output . writeVLong ( length ) ; skipBuffer [ level ] . writeTo ( output ) ; } } skipBuffer [ 0 ] . writeTo ( output ) ; return skipPointer ; } } 	0
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . util . StringHelper ; import java . io . IOException ; import java . util . Vector ; final class TermVectorsWriter { static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x1 ; static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x2 ; static final int FORMAT_VERSION = 2 ; static final int FORMAT_SIZE = 4 ; static final String TVX_EXTENSION = ".tvx" ; static final String TVD_EXTENSION = ".tvd" ; static final String TVF_EXTENSION = ".tvf" ; private IndexOutput tvx = null , tvd = null , tvf = null ; private Vector fields = null ; private Vector terms = null ; private FieldInfos fieldInfos ; private TVField currentField = null ; private long currentDocPointer = - 1 ; public TermVectorsWriter ( Directory directory , String segment , FieldInfos fieldInfos ) throws IOException { tvx = directory . createOutput ( segment + TVX_EXTENSION ) ; tvx . writeInt ( FORMAT_VERSION ) ; tvd = directory . createOutput ( segment + TVD_EXTENSION ) ; tvd . writeInt ( FORMAT_VERSION ) ; tvf = directory . createOutput ( segment + TVF_EXTENSION ) ; tvf . writeInt ( FORMAT_VERSION ) ; this . fieldInfos = fieldInfos ; fields = new Vector ( fieldInfos . size ( ) ) ; terms = new Vector ( ) ; } public final void openDocument ( ) throws IOException { closeDocument ( ) ; currentDocPointer = tvd . getFilePointer ( ) ; } public final void closeDocument ( ) throws IOException { if ( isDocumentOpen ( ) ) { closeField ( ) ; writeDoc ( ) ; fields . clear ( ) ; currentDocPointer = - 1 ; } } public final boolean isDocumentOpen ( ) { return currentDocPointer != - 1 ; } public final void openField ( String field ) throws IOException { FieldInfo fieldInfo = fieldInfos . fieldInfo ( field ) ; openField ( fieldInfo . number , fieldInfo . storePositionWithTermVector , fieldInfo . storeOffsetWithTermVector ) ; } private void openField ( int fieldNumber , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector ) throws IOException { if ( ! isDocumentOpen ( ) ) throw new IllegalStateException ( "Cannot open field when no document is open." ) ; closeField ( ) ; currentField = new TVField ( fieldNumber , storePositionWithTermVector , storeOffsetWithTermVector ) ; } public final void closeField ( ) throws IOException { if ( isFieldOpen ( ) ) { writeField ( ) ; fields . add ( currentField ) ; terms . clear ( ) ; currentField = null ; } } public final boolean isFieldOpen ( ) { return currentField != null ; } public final void addTerm ( String termText , int freq ) { addTerm ( termText , freq , null , null ) ; } public final void addTerm ( String termText , int freq , int [ ] positions , TermVectorOffsetInfo [ ] offsets ) { if ( ! isDocumentOpen ( ) ) throw new IllegalStateException ( "Cannot add terms when document is not open" ) ; if ( ! isFieldOpen ( ) ) throw new IllegalStateException ( "Cannot add terms when field is not open" ) ; addTermInternal ( termText , freq , positions , offsets ) ; } private final void addTermInternal ( String termText , int freq , int [ ] positions , TermVectorOffsetInfo [ ] offsets ) { TVTerm term = new TVTerm ( ) ; term . termText = termText ; term . freq = freq ; term . positions = positions ; term . offsets = offsets ; terms . add ( term ) ; } public final void addAllDocVectors ( TermFreqVector [ ] vectors ) throws IOException { openDocument ( ) ; if ( vectors != null ) { for ( int i = 0 ; i < vectors . length ; i ++ ) { boolean storePositionWithTermVector = false ; boolean storeOffsetWithTermVector = false ; try { TermPositionVector tpVector = ( TermPositionVector ) vectors [ i ] ; if ( tpVector . size ( ) > 0 && tpVector . getTermPositions ( 0 ) != null ) storePositionWithTermVector = true ; if ( tpVector . size ( ) > 0 && tpVector . getOffsets ( 0 ) != null ) storeOffsetWithTermVector = true ; FieldInfo fieldInfo = fieldInfos . fieldInfo ( tpVector . getField ( ) ) ; openField ( fieldInfo . number , storePositionWithTermVector , storeOffsetWithTermVector ) ; for ( int j = 0 ; j < tpVector . size ( ) ; j ++ ) addTermInternal ( tpVector . getTerms ( ) [ j ] , tpVector . getTermFrequencies ( ) [ j ] , tpVector . getTermPositions ( j ) , tpVector . getOffsets ( j ) ) ; closeField ( ) ; } catch ( ClassCastException ignore ) { TermFreqVector tfVector = vectors [ i ] ; FieldInfo fieldInfo = fieldInfos . fieldInfo ( tfVector . getField ( ) ) ; openField ( fieldInfo . number , storePositionWithTermVector , storeOffsetWithTermVector ) ; for ( int j = 0 ; j < tfVector . size ( ) ; j ++ ) addTermInternal ( tfVector . getTerms ( ) [ j ] , tfVector . getTermFrequencies ( ) [ j ] , null , null ) ; closeField ( ) ; } } } closeDocument ( ) ; } final void close ( ) throws IOException { try { closeDocument ( ) ; } finally { IOException keep = null ; if ( tvx != null ) try { tvx . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tvd != null ) try { tvd . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tvf != null ) try { tvf . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( keep != null ) throw ( IOException ) keep . fillInStackTrace ( ) ; } } private void writeField ( ) throws IOException { currentField . tvfPointer = tvf . getFilePointer ( ) ; final int size = terms . size ( ) ; tvf . writeVInt ( size ) ; boolean storePositions = currentField . storePositions ; boolean storeOffsets = currentField . storeOffsets ; byte bits = 0x0 ; if ( storePositions ) bits |= STORE_POSITIONS_WITH_TERMVECTOR ; if ( storeOffsets ) bits |= STORE_OFFSET_WITH_TERMVECTOR ; tvf . writeByte ( bits ) ; String lastTermText = "" ; for ( int i = 0 ; i < size ; i ++ ) { TVTerm term = ( TVTerm ) terms . elementAt ( i ) ; int start = StringHelper . stringDifference ( lastTermText , term . termText ) ; int length = term . termText . length ( ) - start ; tvf . writeVInt ( start ) ; tvf . writeVInt ( length ) ; tvf . writeChars ( term . termText , start , length ) ; tvf . writeVInt ( term . freq ) ; lastTermText = term . termText ; if ( storePositions ) { if ( term . positions == null ) throw new IllegalStateException ( "Trying to write positions that are null!" ) ; int position = 0 ; for ( int j = 0 ; j < term . freq ; j ++ ) { tvf . writeVInt ( term . positions [ j ] - position ) ; position = term . positions [ j ] ; } } if ( storeOffsets ) { if ( term . offsets == null ) throw new IllegalStateException ( "Trying to write offsets that are null!" ) ; int position = 0 ; for ( int j = 0 ; j < term . freq ; j ++ ) { tvf . writeVInt ( term . offsets [ j ] . getStartOffset ( ) - position ) ; tvf . writeVInt ( term . offsets [ j ] . getEndOffset ( ) - term . offsets [ j ] . getStartOffset ( ) ) ; position = term . offsets [ j ] . getEndOffset ( ) ; } } } } private void writeDoc ( ) throws IOException { if ( isFieldOpen ( ) ) throw new IllegalStateException ( "Field is still open while writing document" ) ; tvx . writeLong ( currentDocPointer ) ; final int size = fields . size ( ) ; tvd . writeVInt ( size ) ; for ( int i = 0 ; i < size ; i ++ ) { TVField field = ( TVField ) fields . elementAt ( i ) ; tvd . writeVInt ( field . number ) ; } long lastFieldPointer = 0 ; for ( int i = 0 ; i < size ; i ++ ) { TVField field = ( TVField ) fields . elementAt ( i ) ; tvd . writeVLong ( field . tvfPointer - lastFieldPointer ) ; lastFieldPointer = field . tvfPointer ; } } private static class TVField { int number ; long tvfPointer = 0 ; boolean storePositions = false ; boolean storeOffsets = false ; TVField ( int number , boolean storePos , boolean storeOff ) { this . number = number ; storePositions = storePos ; storeOffsets = storeOff ; } } private static class TVTerm { String termText ; int freq = 0 ; int positions [ ] = null ; TermVectorOffsetInfo [ ] offsets = null ; } } 	1
package org . apache . lucene . analysis ; import java . io . Reader ; public class LetterTokenizer extends CharTokenizer { public LetterTokenizer ( Reader in ) { super ( in ) ; } protected boolean isTokenChar ( char c ) { return Character . isLetter ( c ) ; } } 	0
package org . apache . lucene . index ; import org . apache . lucene . analysis . TokenStream ; import org . apache . lucene . document . * ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . AlreadyClosedException ; import org . apache . lucene . store . BufferedIndexInput ; import java . io . ByteArrayOutputStream ; import java . io . IOException ; import java . io . Reader ; import java . util . zip . DataFormatException ; import java . util . zip . Inflater ; final class FieldsReader { private final FieldInfos fieldInfos ; private final IndexInput cloneableFieldsStream ; private final IndexInput fieldsStream ; private final IndexInput indexStream ; private int size ; private boolean closed ; private ThreadLocal fieldsStreamTL = new ThreadLocal ( ) ; FieldsReader ( Directory d , String segment , FieldInfos fn ) throws IOException { this ( d , segment , fn , BufferedIndexInput . BUFFER_SIZE ) ; } FieldsReader ( Directory d , String segment , FieldInfos fn , int readBufferSize ) throws IOException { fieldInfos = fn ; cloneableFieldsStream = d . openInput ( segment + ".fdt" , readBufferSize ) ; fieldsStream = ( IndexInput ) cloneableFieldsStream . clone ( ) ; indexStream = d . openInput ( segment + ".fdx" , readBufferSize ) ; size = ( int ) ( indexStream . length ( ) / 8 ) ; } protected final void ensureOpen ( ) throws AlreadyClosedException { if ( closed ) { throw new AlreadyClosedException ( "this FieldsReader is closed" ) ; } } final void close ( ) throws IOException { if ( ! closed ) { fieldsStream . close ( ) ; cloneableFieldsStream . close ( ) ; indexStream . close ( ) ; IndexInput localFieldsStream = ( IndexInput ) fieldsStreamTL . get ( ) ; if ( localFieldsStream != null ) { localFieldsStream . close ( ) ; fieldsStreamTL . set ( null ) ; } closed = true ; } } final int size ( ) { return size ; } final Document doc ( int n , FieldSelector fieldSelector ) throws CorruptIndexException , IOException { indexStream . seek ( n * 8L ) ; long position = indexStream . readLong ( ) ; fieldsStream . seek ( position ) ; Document doc = new Document ( ) ; int numFields = fieldsStream . readVInt ( ) ; for ( int i = 0 ; i < numFields ; i ++ ) { int fieldNumber = fieldsStream . readVInt ( ) ; FieldInfo fi = fieldInfos . fieldInfo ( fieldNumber ) ; FieldSelectorResult acceptField = fieldSelector == null ? FieldSelectorResult . LOAD : fieldSelector . accept ( fi . name ) ; byte bits = fieldsStream . readByte ( ) ; boolean compressed = ( bits & FieldsWriter . FIELD_IS_COMPRESSED ) != 0 ; boolean tokenize = ( bits & FieldsWriter . FIELD_IS_TOKENIZED ) != 0 ; boolean binary = ( bits & FieldsWriter . FIELD_IS_BINARY ) != 0 ; if ( acceptField . equals ( FieldSelectorResult . LOAD ) ) { addField ( doc , fi , binary , compressed , tokenize ) ; } else if ( acceptField . equals ( FieldSelectorResult . LOAD_FOR_MERGE ) ) { addFieldForMerge ( doc , fi , binary , compressed , tokenize ) ; } else if ( acceptField . equals ( FieldSelectorResult . LOAD_AND_BREAK ) ) { addField ( doc , fi , binary , compressed , tokenize ) ; break ; } else if ( acceptField . equals ( FieldSelectorResult . LAZY_LOAD ) ) { addFieldLazy ( doc , fi , binary , compressed , tokenize ) ; } else if ( acceptField . equals ( FieldSelectorResult . SIZE ) ) { skipField ( binary , compressed , addFieldSize ( doc , fi , binary , compressed ) ) ; } else if ( acceptField . equals ( FieldSelectorResult . SIZE_AND_BREAK ) ) { addFieldSize ( doc , fi , binary , compressed ) ; break ; } else { skipField ( binary , compressed ) ; } } return doc ; } private void skipField ( boolean binary , boolean compressed ) throws IOException { skipField ( binary , compressed , fieldsStream . readVInt ( ) ) ; } private void skipField ( boolean binary , boolean compressed , int toRead ) throws IOException { if ( binary || compressed ) { long pointer = fieldsStream . getFilePointer ( ) ; fieldsStream . seek ( pointer + toRead ) ; } else { fieldsStream . skipChars ( toRead ) ; } } private void addFieldLazy ( Document doc , FieldInfo fi , boolean binary , boolean compressed , boolean tokenize ) throws IOException { if ( binary == true ) { int toRead = fieldsStream . readVInt ( ) ; long pointer = fieldsStream . getFilePointer ( ) ; if ( compressed ) { doc . add ( new LazyField ( fi . name , Field . Store . COMPRESS , toRead , pointer ) ) ; } else { doc . add ( new LazyField ( fi . name , Field . Store . YES , toRead , pointer ) ) ; } fieldsStream . seek ( pointer + toRead ) ; } else { Field . Store store = Field . Store . YES ; Field . Index index = getIndexType ( fi , tokenize ) ; Field . TermVector termVector = getTermVectorType ( fi ) ; Fieldable f ; if ( compressed ) { store = Field . Store . COMPRESS ; int toRead = fieldsStream . readVInt ( ) ; long pointer = fieldsStream . getFilePointer ( ) ; f = new LazyField ( fi . name , store , toRead , pointer ) ; fieldsStream . seek ( pointer + toRead ) ; f . setOmitNorms ( fi . omitNorms ) ; } else { int length = fieldsStream . readVInt ( ) ; long pointer = fieldsStream . getFilePointer ( ) ; fieldsStream . skipChars ( length ) ; f = new LazyField ( fi . name , store , index , termVector , length , pointer ) ; f . setOmitNorms ( fi . omitNorms ) ; } doc . add ( f ) ; } } private void addFieldForMerge ( Document doc , FieldInfo fi , boolean binary , boolean compressed , boolean tokenize ) throws IOException { Object data ; if ( binary || compressed ) { int toRead = fieldsStream . readVInt ( ) ; final byte [ ] b = new byte [ toRead ] ; fieldsStream . readBytes ( b , 0 , b . length ) ; data = b ; } else { data = fieldsStream . readString ( ) ; } doc . add ( new FieldForMerge ( data , fi , binary , compressed , tokenize ) ) ; } private void addField ( Document doc , FieldInfo fi , boolean binary , boolean compressed , boolean tokenize ) throws CorruptIndexException , IOException { if ( binary ) { int toRead = fieldsStream . readVInt ( ) ; final byte [ ] b = new byte [ toRead ] ; fieldsStream . readBytes ( b , 0 , b . length ) ; if ( compressed ) doc . add ( new Field ( fi . name , uncompress ( b ) , Field . Store . COMPRESS ) ) ; else doc . add ( new Field ( fi . name , b , Field . Store . YES ) ) ; } else { Field . Store store = Field . Store . YES ; Field . Index index = getIndexType ( fi , tokenize ) ; Field . TermVector termVector = getTermVectorType ( fi ) ; Fieldable f ; if ( compressed ) { store = Field . Store . COMPRESS ; int toRead = fieldsStream . readVInt ( ) ; final byte [ ] b = new byte [ toRead ] ; fieldsStream . readBytes ( b , 0 , b . length ) ; f = new Field ( fi . name , new String ( uncompress ( b ) , "UTF-8" ) , store , index , termVector ) ; f . setOmitNorms ( fi . omitNorms ) ; } else { f = new Field ( fi . name , fieldsStream . readString ( ) , store , index , termVector ) ; f . setOmitNorms ( fi . omitNorms ) ; } doc . add ( f ) ; } } private int addFieldSize ( Document doc , FieldInfo fi , boolean binary , boolean compressed ) throws IOException { int size = fieldsStream . readVInt ( ) , bytesize = binary || compressed ? size : 2 * size ; byte [ ] sizebytes = new byte [ 4 ] ; sizebytes [ 0 ] = ( byte ) ( bytesize > > > 24 ) ; sizebytes [ 1 ] = ( byte ) ( bytesize > > > 16 ) ; sizebytes [ 2 ] = ( byte ) ( bytesize > > > 8 ) ; sizebytes [ 3 ] = ( byte ) bytesize ; doc . add ( new Field ( fi . name , sizebytes , Field . Store . YES ) ) ; return size ; } private Field . TermVector getTermVectorType ( FieldInfo fi ) { Field . TermVector termVector = null ; if ( fi . storeTermVector ) { if ( fi . storeOffsetWithTermVector ) { if ( fi . storePositionWithTermVector ) { termVector = Field . TermVector . WITH_POSITIONS_OFFSETS ; } else { termVector = Field . TermVector . WITH_OFFSETS ; } } else if ( fi . storePositionWithTermVector ) { termVector = Field . TermVector . WITH_POSITIONS ; } else { termVector = Field . TermVector . YES ; } } else { termVector = Field . TermVector . NO ; } return termVector ; } private Field . Index getIndexType ( FieldInfo fi , boolean tokenize ) { Field . Index index ; if ( fi . isIndexed && tokenize ) index = Field . Index . TOKENIZED ; else if ( fi . isIndexed && ! tokenize ) index = Field . Index . UN_TOKENIZED ; else index = Field . Index . NO ; return index ; } private class LazyField extends AbstractField implements Fieldable { private int toRead ; private long pointer ; public LazyField ( String name , Field . Store store , int toRead , long pointer ) { super ( name , store , Field . Index . NO , Field . TermVector . NO ) ; this . toRead = toRead ; this . pointer = pointer ; lazy = true ; } public LazyField ( String name , Field . Store store , Field . Index index , Field . TermVector termVector , int toRead , long pointer ) { super ( name , store , index , termVector ) ; this . toRead = toRead ; this . pointer = pointer ; lazy = true ; } private IndexInput getFieldStream ( ) { IndexInput localFieldsStream = ( IndexInput ) fieldsStreamTL . get ( ) ; if ( localFieldsStream == null ) { localFieldsStream = ( IndexInput ) cloneableFieldsStream . clone ( ) ; fieldsStreamTL . set ( localFieldsStream ) ; } return localFieldsStream ; } public byte [ ] binaryValue ( ) { ensureOpen ( ) ; if ( fieldsData == null ) { final byte [ ] b = new byte [ toRead ] ; IndexInput localFieldsStream = getFieldStream ( ) ; try { localFieldsStream . seek ( pointer ) ; localFieldsStream . readBytes ( b , 0 , b . length ) ; if ( isCompressed == true ) { fieldsData = uncompress ( b ) ; } else { fieldsData = b ; } } catch ( IOException e ) { throw new FieldReaderException ( e ) ; } } return fieldsData instanceof byte [ ] ? ( byte [ ] ) fieldsData : null ; } public Reader readerValue ( ) { ensureOpen ( ) ; return fieldsData instanceof Reader ? ( Reader ) fieldsData : null ; } public TokenStream tokenStreamValue ( ) { ensureOpen ( ) ; return fieldsData instanceof TokenStream ? ( TokenStream ) fieldsData : null ; } public String stringValue ( ) { ensureOpen ( ) ; if ( fieldsData == null ) { IndexInput localFieldsStream = getFieldStream ( ) ; try { localFieldsStream . seek ( pointer ) ; if ( isCompressed ) { final byte [ ] b = new byte [ toRead ] ; localFieldsStream . readBytes ( b , 0 , b . length ) ; fieldsData = new String ( uncompress ( b ) , "UTF-8" ) ; } else { char [ ] chars = new char [ toRead ] ; localFieldsStream . readChars ( chars , 0 , toRead ) ; fieldsData = new String ( chars ) ; } } catch ( IOException e ) { throw new FieldReaderException ( e ) ; } } return fieldsData instanceof String ? ( String ) fieldsData : null ; } public long getPointer ( ) { ensureOpen ( ) ; return pointer ; } public void setPointer ( long pointer ) { ensureOpen ( ) ; this . pointer = pointer ; } public int getToRead ( ) { ensureOpen ( ) ; return toRead ; } public void setToRead ( int toRead ) { ensureOpen ( ) ; this . toRead = toRead ; } } private final byte [ ] uncompress ( final byte [ ] input ) throws CorruptIndexException , IOException { Inflater decompressor = new Inflater ( ) ; decompressor . setInput ( input ) ; ByteArrayOutputStream bos = new ByteArrayOutputStream ( input . length ) ; byte [ ] buf = new byte [ 1024 ] ; while ( ! decompressor . finished ( ) ) { try { int count = decompressor . inflate ( buf ) ; bos . write ( buf , 0 , count ) ; } catch ( DataFormatException e ) { CorruptIndexException newException = new CorruptIndexException ( "field data are in wrong format: " + e . toString ( ) ) ; newException . initCause ( e ) ; throw newException ; } } decompressor . end ( ) ; return bos . toByteArray ( ) ; } final static class FieldForMerge extends AbstractField { public String stringValue ( ) { return ( String ) this . fieldsData ; } public Reader readerValue ( ) { return null ; } public byte [ ] binaryValue ( ) { return ( byte [ ] ) this . fieldsData ; } public TokenStream tokenStreamValue ( ) { return null ; } public FieldForMerge ( Object value , FieldInfo fi , boolean binary , boolean compressed , boolean tokenize ) { this . isStored = true ; this . fieldsData = value ; this . isCompressed = compressed ; this . isBinary = binary ; this . isTokenized = tokenize ; this . name = fi . name . intern ( ) ; this . isIndexed = fi . isIndexed ; this . omitNorms = fi . omitNorms ; this . storeOffsetWithTermVector = fi . storeOffsetWithTermVector ; this . storePositionWithTermVector = fi . storePositionWithTermVector ; this . storeTermVector = fi . storeTermVector ; } } } 	1
package org . apache . lucene . search ; import org . apache . lucene . util . Parameter ; public class BooleanClause implements java . io . Serializable { public static final class Occur extends Parameter implements java . io . Serializable { private Occur ( String name ) { super ( name ) ; } public String toString ( ) { if ( this == MUST ) return "+" ; if ( this == MUST_NOT ) return "-" ; return "" ; } public static final Occur MUST = new Occur ( "MUST" ) ; public static final Occur SHOULD = new Occur ( "SHOULD" ) ; public static final Occur MUST_NOT = new Occur ( "MUST_NOT" ) ; } private Query query ; private Occur occur ; public BooleanClause ( Query query , Occur occur ) { this . query = query ; this . occur = occur ; } public Occur getOccur ( ) { return occur ; } public void setOccur ( Occur occur ) { this . occur = occur ; } public Query getQuery ( ) { return query ; } public void setQuery ( Query query ) { this . query = query ; } public boolean isProhibited ( ) { return Occur . MUST_NOT . equals ( occur ) ; } public boolean isRequired ( ) { return Occur . MUST . equals ( occur ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof BooleanClause ) ) return false ; BooleanClause other = ( BooleanClause ) o ; return this . query . equals ( other . query ) && this . occur . equals ( other . occur ) ; } public int hashCode ( ) { return query . hashCode ( ) ^ ( Occur . MUST . equals ( occur ) ? 1 : 0 ) ^ ( Occur . MUST_NOT . equals ( occur ) ? 2 : 0 ) ; } public String toString ( ) { return occur . toString ( ) + query . toString ( ) ; } } 	0
package org . apache . lucene . util ; public abstract class PriorityQueue { private Object [ ] heap ; private int size ; private int maxSize ; protected abstract boolean lessThan ( Object a , Object b ) ; protected final void initialize ( int maxSize ) { size = 0 ; int heapSize = maxSize + 1 ; heap = new Object [ heapSize ] ; this . maxSize = maxSize ; } public final void put ( Object element ) { size ++ ; heap [ size ] = element ; upHeap ( ) ; } public boolean insert ( Object element ) { if ( size < maxSize ) { put ( element ) ; return true ; } else if ( size > 0 && ! lessThan ( element , top ( ) ) ) { heap [ 1 ] = element ; adjustTop ( ) ; return true ; } else return false ; } public final Object top ( ) { if ( size > 0 ) return heap [ 1 ] ; else return null ; } public final Object pop ( ) { if ( size > 0 ) { Object result = heap [ 1 ] ; heap [ 1 ] = heap [ size ] ; heap [ size ] = null ; size -- ; downHeap ( ) ; return result ; } else return null ; } public final void adjustTop ( ) { downHeap ( ) ; } public final int size ( ) { return size ; } public final void clear ( ) { for ( int i = 0 ; i <= size ; i ++ ) heap [ i ] = null ; size = 0 ; } private final void upHeap ( ) { int i = size ; Object node = heap [ i ] ; int j = i > > > 1 ; while ( j > 0 && lessThan ( node , heap [ j ] ) ) { heap [ i ] = heap [ j ] ; i = j ; j = j > > > 1 ; } heap [ i ] = node ; } private final void downHeap ( ) { int i = 1 ; Object node = heap [ i ] ; int j = i << 1 ; int k = j + 1 ; if ( k <= size && lessThan ( heap [ k ] , heap [ j ] ) ) { j = k ; } while ( j <= size && lessThan ( heap [ j ] , node ) ) { heap [ i ] = heap [ j ] ; i = j ; j = i << 1 ; k = j + 1 ; if ( k <= size && lessThan ( heap [ k ] , heap [ j ] ) ) { j = k ; } } heap [ i ] = node ; } } 	1
package org . apache . lucene . util ; public class ToStringUtils { public static String boost ( float boost ) { if ( boost != 1.0f ) { return "^" + Float . toString ( boost ) ; } else return "" ; } } 	0
package org . apache . lucene . index ; import org . apache . lucene . util . PriorityQueue ; import java . io . IOException ; import java . util . Arrays ; import java . util . Iterator ; import java . util . LinkedList ; import java . util . List ; public class MultipleTermPositions implements TermPositions { private static final class TermPositionsQueue extends PriorityQueue { TermPositionsQueue ( List termPositions ) throws IOException { initialize ( termPositions . size ( ) ) ; Iterator i = termPositions . iterator ( ) ; while ( i . hasNext ( ) ) { TermPositions tp = ( TermPositions ) i . next ( ) ; if ( tp . next ( ) ) put ( tp ) ; } } final TermPositions peek ( ) { return ( TermPositions ) top ( ) ; } public final boolean lessThan ( Object a , Object b ) { return ( ( TermPositions ) a ) . doc ( ) < ( ( TermPositions ) b ) . doc ( ) ; } } private static final class IntQueue { private int _arraySize = 16 ; private int _index = 0 ; private int _lastIndex = 0 ; private int [ ] _array = new int [ _arraySize ] ; final void add ( int i ) { if ( _lastIndex == _arraySize ) growArray ( ) ; _array [ _lastIndex ++ ] = i ; } final int next ( ) { return _array [ _index ++ ] ; } final void sort ( ) { Arrays . sort ( _array , _index , _lastIndex ) ; } final void clear ( ) { _index = 0 ; _lastIndex = 0 ; } final int size ( ) { return ( _lastIndex - _index ) ; } private void growArray ( ) { int [ ] newArray = new int [ _arraySize * 2 ] ; System . arraycopy ( _array , 0 , newArray , 0 , _arraySize ) ; _array = newArray ; _arraySize *= 2 ; } } private int _doc ; private int _freq ; private TermPositionsQueue _termPositionsQueue ; private IntQueue _posList ; public MultipleTermPositions ( IndexReader indexReader , Term [ ] terms ) throws IOException { List termPositions = new LinkedList ( ) ; for ( int i = 0 ; i < terms . length ; i ++ ) termPositions . add ( indexReader . termPositions ( terms [ i ] ) ) ; _termPositionsQueue = new TermPositionsQueue ( termPositions ) ; _posList = new IntQueue ( ) ; } public final boolean next ( ) throws IOException { if ( _termPositionsQueue . size ( ) == 0 ) return false ; _posList . clear ( ) ; _doc = _termPositionsQueue . peek ( ) . doc ( ) ; TermPositions tp ; do { tp = _termPositionsQueue . peek ( ) ; for ( int i = 0 ; i < tp . freq ( ) ; i ++ ) _posList . add ( tp . nextPosition ( ) ) ; if ( tp . next ( ) ) _termPositionsQueue . adjustTop ( ) ; else { _termPositionsQueue . pop ( ) ; tp . close ( ) ; } } while ( _termPositionsQueue . size ( ) > 0 && _termPositionsQueue . peek ( ) . doc ( ) == _doc ) ; _posList . sort ( ) ; _freq = _posList . size ( ) ; return true ; } public final int nextPosition ( ) { return _posList . next ( ) ; } public final boolean skipTo ( int target ) throws IOException { while ( _termPositionsQueue . peek ( ) != null && target > _termPositionsQueue . peek ( ) . doc ( ) ) { TermPositions tp = ( TermPositions ) _termPositionsQueue . pop ( ) ; if ( tp . skipTo ( target ) ) _termPositionsQueue . put ( tp ) ; else tp . close ( ) ; } return next ( ) ; } public final int doc ( ) { return _doc ; } public final int freq ( ) { return _freq ; } public final void close ( ) throws IOException { while ( _termPositionsQueue . size ( ) > 0 ) ( ( TermPositions ) _termPositionsQueue . pop ( ) ) . close ( ) ; } public void seek ( Term arg0 ) throws IOException { throw new UnsupportedOperationException ( ) ; } public void seek ( TermEnum termEnum ) throws IOException { throw new UnsupportedOperationException ( ) ; } public int read ( int [ ] arg0 , int [ ] arg1 ) throws IOException { throw new UnsupportedOperationException ( ) ; } public int getPayloadLength ( ) { throw new UnsupportedOperationException ( ) ; } public byte [ ] getPayload ( byte [ ] data , int offset ) throws IOException { throw new UnsupportedOperationException ( ) ; } public boolean isPayloadAvailable ( ) { return false ; } } 	1
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import java . io . IOException ; public class WildcardQuery extends MultiTermQuery { private boolean termContainsWildcard ; public WildcardQuery ( Term term ) { super ( term ) ; this . termContainsWildcard = ( term . text ( ) . indexOf ( '*' ) != - 1 ) || ( term . text ( ) . indexOf ( '?' ) != - 1 ) ; } protected FilteredTermEnum getEnum ( IndexReader reader ) throws IOException { return new WildcardTermEnum ( reader , getTerm ( ) ) ; } public boolean equals ( Object o ) { if ( o instanceof WildcardQuery ) return super . equals ( o ) ; return false ; } public Query rewrite ( IndexReader reader ) throws IOException { if ( this . termContainsWildcard ) { return super . rewrite ( reader ) ; } return new TermQuery ( getTerm ( ) ) ; } } 	0
package org . apache . lucene . queryParser ; import java . util . Vector ; import java . io . * ; import java . text . * ; import java . util . * ; import org . apache . lucene . index . Term ; import org . apache . lucene . analysis . * ; import org . apache . lucene . document . * ; import org . apache . lucene . search . * ; import org . apache . lucene . util . Parameter ; public class QueryParser implements QueryParserConstants { private static final int CONJ_NONE = 0 ; private static final int CONJ_AND = 1 ; private static final int CONJ_OR = 2 ; private static final int MOD_NONE = 0 ; private static final int MOD_NOT = 10 ; private static final int MOD_REQ = 11 ; public static final Operator AND_OPERATOR = Operator . AND ; public static final Operator OR_OPERATOR = Operator . OR ; private Operator operator = OR_OPERATOR ; boolean lowercaseExpandedTerms = true ; boolean useOldRangeQuery = false ; boolean allowLeadingWildcard = false ; Analyzer analyzer ; String field ; int phraseSlop = 0 ; float fuzzyMinSim = FuzzyQuery . defaultMinSimilarity ; int fuzzyPrefixLength = FuzzyQuery . defaultPrefixLength ; Locale locale = Locale . getDefault ( ) ; DateTools . Resolution dateResolution = null ; Map fieldToDateResolution = null ; static public final class Operator extends Parameter { private Operator ( String name ) { super ( name ) ; } static public final Operator OR = new Operator ( "OR" ) ; static public final Operator AND = new Operator ( "AND" ) ; } public QueryParser ( String f , Analyzer a ) { this ( new FastCharStream ( new StringReader ( "" ) ) ) ; analyzer = a ; field = f ; } public Query parse ( String query ) throws ParseException { ReInit ( new FastCharStream ( new StringReader ( query ) ) ) ; try { return TopLevelQuery ( field ) ; } catch ( ParseException tme ) { throw new ParseException ( "Cannot parse '" + query + "': " + tme . getMessage ( ) ) ; } catch ( TokenMgrError tme ) { throw new ParseException ( "Cannot parse '" + query + "': " + tme . getMessage ( ) ) ; } catch ( BooleanQuery . TooManyClauses tmc ) { throw new ParseException ( "Cannot parse '" + query + "': too many boolean clauses" ) ; } } public Analyzer getAnalyzer ( ) { return analyzer ; } public String getField ( ) { return field ; } public float getFuzzyMinSim ( ) { return fuzzyMinSim ; } public void setFuzzyMinSim ( float fuzzyMinSim ) { this . fuzzyMinSim = fuzzyMinSim ; } public int getFuzzyPrefixLength ( ) { return fuzzyPrefixLength ; } public void setFuzzyPrefixLength ( int fuzzyPrefixLength ) { this . fuzzyPrefixLength = fuzzyPrefixLength ; } public void setPhraseSlop ( int phraseSlop ) { this . phraseSlop = phraseSlop ; } public int getPhraseSlop ( ) { return phraseSlop ; } public void setAllowLeadingWildcard ( boolean allowLeadingWildcard ) { this . allowLeadingWildcard = allowLeadingWildcard ; } public boolean getAllowLeadingWildcard ( ) { return allowLeadingWildcard ; } public void setDefaultOperator ( Operator op ) { this . operator = op ; } public Operator getDefaultOperator ( ) { return operator ; } public void setLowercaseExpandedTerms ( boolean lowercaseExpandedTerms ) { this . lowercaseExpandedTerms = lowercaseExpandedTerms ; } public boolean getLowercaseExpandedTerms ( ) { return lowercaseExpandedTerms ; } public void setUseOldRangeQuery ( boolean useOldRangeQuery ) { this . useOldRangeQuery = useOldRangeQuery ; } public boolean getUseOldRangeQuery ( ) { return useOldRangeQuery ; } public void setLocale ( Locale locale ) { this . locale = locale ; } public Locale getLocale ( ) { return locale ; } public void setDateResolution ( DateTools . Resolution dateResolution ) { this . dateResolution = dateResolution ; } public void setDateResolution ( String fieldName , DateTools . Resolution dateResolution ) { if ( fieldName == null ) { throw new IllegalArgumentException ( "Field cannot be null." ) ; } if ( fieldToDateResolution == null ) { fieldToDateResolution = new HashMap ( ) ; } fieldToDateResolution . put ( fieldName , dateResolution ) ; } public DateTools . Resolution getDateResolution ( String fieldName ) { if ( fieldName == null ) { throw new IllegalArgumentException ( "Field cannot be null." ) ; } if ( fieldToDateResolution == null ) { return this . dateResolution ; } DateTools . Resolution resolution = ( DateTools . Resolution ) fieldToDateResolution . get ( fieldName ) ; if ( resolution == null ) { resolution = this . dateResolution ; } return resolution ; } protected void addClause ( Vector clauses , int conj , int mods , Query q ) { boolean required , prohibited ; if ( clauses . size ( ) > 0 && conj == CONJ_AND ) { BooleanClause c = ( BooleanClause ) clauses . elementAt ( clauses . size ( ) - 1 ) ; if ( ! c . isProhibited ( ) ) c . setOccur ( BooleanClause . Occur . MUST ) ; } if ( clauses . size ( ) > 0 && operator == AND_OPERATOR && conj == CONJ_OR ) { BooleanClause c = ( BooleanClause ) clauses . elementAt ( clauses . size ( ) - 1 ) ; if ( ! c . isProhibited ( ) ) c . setOccur ( BooleanClause . Occur . SHOULD ) ; } if ( q == null ) return ; if ( operator == OR_OPERATOR ) { prohibited = ( mods == MOD_NOT ) ; required = ( mods == MOD_REQ ) ; if ( conj == CONJ_AND && ! prohibited ) { required = true ; } } else { prohibited = ( mods == MOD_NOT ) ; required = ( ! prohibited && conj != CONJ_OR ) ; } if ( required && ! prohibited ) clauses . addElement ( new BooleanClause ( q , BooleanClause . Occur . MUST ) ) ; else if ( ! required && ! prohibited ) clauses . addElement ( new BooleanClause ( q , BooleanClause . Occur . SHOULD ) ) ; else if ( ! required && prohibited ) clauses . addElement ( new BooleanClause ( q , BooleanClause . Occur . MUST_NOT ) ) ; else throw new RuntimeException ( "Clause cannot be both required and prohibited" ) ; } protected Query getFieldQuery ( String field , String queryText ) throws ParseException { TokenStream source = analyzer . tokenStream ( field , new StringReader ( queryText ) ) ; Vector v = new Vector ( ) ; org . apache . lucene . analysis . Token t ; int positionCount = 0 ; boolean severalTokensAtSamePosition = false ; while ( true ) { try { t = source . next ( ) ; } catch ( IOException e ) { t = null ; } if ( t == null ) break ; v . addElement ( t ) ; if ( t . getPositionIncrement ( ) != 0 ) positionCount += t . getPositionIncrement ( ) ; else severalTokensAtSamePosition = true ; } try { source . close ( ) ; } catch ( IOException e ) { } if ( v . size ( ) == 0 ) return null ; else if ( v . size ( ) == 1 ) { t = ( org . apache . lucene . analysis . Token ) v . elementAt ( 0 ) ; return new TermQuery ( new Term ( field , t . termText ( ) ) ) ; } else { if ( severalTokensAtSamePosition ) { if ( positionCount == 1 ) { BooleanQuery q = new BooleanQuery ( true ) ; for ( int i = 0 ; i < v . size ( ) ; i ++ ) { t = ( org . apache . lucene . analysis . Token ) v . elementAt ( i ) ; TermQuery currentQuery = new TermQuery ( new Term ( field , t . termText ( ) ) ) ; q . add ( currentQuery , BooleanClause . Occur . SHOULD ) ; } return q ; } else { MultiPhraseQuery mpq = new MultiPhraseQuery ( ) ; mpq . setSlop ( phraseSlop ) ; List multiTerms = new ArrayList ( ) ; for ( int i = 0 ; i < v . size ( ) ; i ++ ) { t = ( org . apache . lucene . analysis . Token ) v . elementAt ( i ) ; if ( t . getPositionIncrement ( ) == 1 && multiTerms . size ( ) > 0 ) { mpq . add ( ( Term [ ] ) multiTerms . toArray ( new Term [ 0 ] ) ) ; multiTerms . clear ( ) ; } multiTerms . add ( new Term ( field , t . termText ( ) ) ) ; } mpq . add ( ( Term [ ] ) multiTerms . toArray ( new Term [ 0 ] ) ) ; return mpq ; } } else { PhraseQuery q = new PhraseQuery ( ) ; q . setSlop ( phraseSlop ) ; for ( int i = 0 ; i < v . size ( ) ; i ++ ) { q . add ( new Term ( field , ( ( org . apache . lucene . analysis . Token ) v . elementAt ( i ) ) . termText ( ) ) ) ; } return q ; } } } protected Query getFieldQuery ( String field , String queryText , int slop ) throws ParseException { Query query = getFieldQuery ( field , queryText ) ; if ( query instanceof PhraseQuery ) { ( ( PhraseQuery ) query ) . setSlop ( slop ) ; } if ( query instanceof MultiPhraseQuery ) { ( ( MultiPhraseQuery ) query ) . setSlop ( slop ) ; } return query ; } protected Query getRangeQuery ( String field , String part1 , String part2 , boolean inclusive ) throws ParseException { if ( lowercaseExpandedTerms ) { part1 = part1 . toLowerCase ( ) ; part2 = part2 . toLowerCase ( ) ; } try { DateFormat df = DateFormat . getDateInstance ( DateFormat . SHORT , locale ) ; df . setLenient ( true ) ; Date d1 = df . parse ( part1 ) ; Date d2 = df . parse ( part2 ) ; if ( inclusive ) { Calendar cal = Calendar . getInstance ( locale ) ; cal . setTime ( d2 ) ; cal . set ( Calendar . HOUR_OF_DAY , 23 ) ; cal . set ( Calendar . MINUTE , 59 ) ; cal . set ( Calendar . SECOND , 59 ) ; cal . set ( Calendar . MILLISECOND , 999 ) ; d2 = cal . getTime ( ) ; } DateTools . Resolution resolution = getDateResolution ( field ) ; if ( resolution == null ) { part1 = DateField . dateToString ( d1 ) ; part2 = DateField . dateToString ( d2 ) ; } else { part1 = DateTools . dateToString ( d1 , resolution ) ; part2 = DateTools . dateToString ( d2 , resolution ) ; } } catch ( Exception e ) { } if ( useOldRangeQuery ) { return new RangeQuery ( new Term ( field , part1 ) , new Term ( field , part2 ) , inclusive ) ; } else { return new ConstantScoreRangeQuery ( field , part1 , part2 , inclusive , inclusive ) ; } } protected Query getBooleanQuery ( Vector clauses ) throws ParseException { return getBooleanQuery ( clauses , false ) ; } protected Query getBooleanQuery ( Vector clauses , boolean disableCoord ) throws ParseException { BooleanQuery query = new BooleanQuery ( disableCoord ) ; for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { query . add ( ( BooleanClause ) clauses . elementAt ( i ) ) ; } return query ; } protected Query getWildcardQuery ( String field , String termStr ) throws ParseException { if ( "*" . equals ( field ) ) { if ( "*" . equals ( termStr ) ) return new MatchAllDocsQuery ( ) ; } if ( ! allowLeadingWildcard && ( termStr . startsWith ( "*" ) || termStr . startsWith ( "?" ) ) ) throw new ParseException ( "'*' or '?' not allowed as first character in WildcardQuery" ) ; if ( lowercaseExpandedTerms ) { termStr = termStr . toLowerCase ( ) ; } Term t = new Term ( field , termStr ) ; return new WildcardQuery ( t ) ; } protected Query getPrefixQuery ( String field , String termStr ) throws ParseException { if ( ! allowLeadingWildcard && termStr . startsWith ( "*" ) ) throw new ParseException ( "'*' not allowed as first character in PrefixQuery" ) ; if ( lowercaseExpandedTerms ) { termStr = termStr . toLowerCase ( ) ; } Term t = new Term ( field , termStr ) ; return new PrefixQuery ( t ) ; } protected Query getFuzzyQuery ( String field , String termStr , float minSimilarity ) throws ParseException { if ( lowercaseExpandedTerms ) { termStr = termStr . toLowerCase ( ) ; } Term t = new Term ( field , termStr ) ; return new FuzzyQuery ( t , minSimilarity , fuzzyPrefixLength ) ; } private String discardEscapeChar ( String input ) throws ParseException { char [ ] output = new char [ input . length ( ) ] ; int length = 0 ; boolean lastCharWasEscapeChar = false ; int codePointMultiplier = 0 ; int codePoint = 0 ; for ( int i = 0 ; i < input . length ( ) ; i ++ ) { char curChar = input . charAt ( i ) ; if ( codePointMultiplier > 0 ) { codePoint += hexToInt ( curChar ) * codePointMultiplier ; codePointMultiplier >>>= 4 ; if ( codePointMultiplier == 0 ) { output [ length ++ ] = ( char ) codePoint ; codePoint = 0 ; } } else if ( lastCharWasEscapeChar ) { if ( curChar == 'u' ) { codePointMultiplier = 16 * 16 * 16 ; } else { output [ length ] = curChar ; length ++ ; } lastCharWasEscapeChar = false ; } else { if ( curChar == '\\' ) { lastCharWasEscapeChar = true ; } else { output [ length ] = curChar ; length ++ ; } } } if ( codePointMultiplier > 0 ) { throw new ParseException ( "Truncated unicode escape sequence." ) ; } if ( lastCharWasEscapeChar ) { throw new ParseException ( "Term can not end with escape character." ) ; } return new String ( output , 0 , length ) ; } private static final int hexToInt ( char c ) throws ParseException { if ( '0' <= c && c <= '9' ) { return c - '0' ; } else if ( 'a' <= c && c <= 'f' ) { return c - 'a' + 10 ; } else if ( 'A' <= c && c <= 'F' ) { return c - 'A' + 10 ; } else { throw new ParseException ( "None-hex character in unicode escape sequence: " + c ) ; } } public static String escape ( String s ) { StringBuffer sb = new StringBuffer ( ) ; for ( int i = 0 ; i < s . length ( ) ; i ++ ) { char c = s . charAt ( i ) ; if ( c == '\\' || c == '+' || c == '-' || c == '!' || c == '(' || c == ')' || c == ':' || c == '^' || c == '[' || c == ']' || c == '\"' || c == '{' || c == '}' || c == '~' || c == '*' || c == '?' || c == '|' || c == '&' ) { sb . append ( '\\' ) ; } sb . append ( c ) ; } return sb . toString ( ) ; } public static void main ( String [ ] args ) throws Exception { if ( args . length == 0 ) { System . out . println ( "Usage: java org.apache.lucene.queryParser.QueryParser <input>" ) ; System . exit ( 0 ) ; } QueryParser qp = new QueryParser ( "field" , new org . apache . lucene . analysis . SimpleAnalyzer ( ) ) ; Query q = qp . parse ( args [ 0 ] ) ; System . out . println ( q . toString ( "field" ) ) ; } final public int Conjunction ( ) throws ParseException { int ret = CONJ_NONE ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case AND : case OR : switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case AND : jj_consume_token ( AND ) ; ret = CONJ_AND ; break ; case OR : jj_consume_token ( OR ) ; ret = CONJ_OR ; break ; default : jj_la1 [ 0 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } break ; default : jj_la1 [ 1 ] = jj_gen ; ; } { if ( true ) return ret ; } throw new Error ( "Missing return statement in function" ) ; } final public int Modifiers ( ) throws ParseException { int ret = MOD_NONE ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case NOT : case PLUS : case MINUS : switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case PLUS : jj_consume_token ( PLUS ) ; ret = MOD_REQ ; break ; case MINUS : jj_consume_token ( MINUS ) ; ret = MOD_NOT ; break ; case NOT : jj_consume_token ( NOT ) ; ret = MOD_NOT ; break ; default : jj_la1 [ 2 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } break ; default : jj_la1 [ 3 ] = jj_gen ; ; } { if ( true ) return ret ; } throw new Error ( "Missing return statement in function" ) ; } final public Query TopLevelQuery ( String field ) throws ParseException { Query q ; q = Query ( field ) ; jj_consume_token ( 0 ) ; { if ( true ) return q ; } throw new Error ( "Missing return statement in function" ) ; } final public Query Query ( String field ) throws ParseException { Vector clauses = new Vector ( ) ; Query q , firstQuery = null ; int conj , mods ; mods = Modifiers ( ) ; q = Clause ( field ) ; addClause ( clauses , CONJ_NONE , mods , q ) ; if ( mods == MOD_NONE ) firstQuery = q ; label_1 : while ( true ) { switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case AND : case OR : case NOT : case PLUS : case MINUS : case LPAREN : case STAR : case QUOTED : case TERM : case PREFIXTERM : case WILDTERM : case RANGEIN_START : case RANGEEX_START : case NUMBER : ; break ; default : jj_la1 [ 4 ] = jj_gen ; break label_1 ; } conj = Conjunction ( ) ; mods = Modifiers ( ) ; q = Clause ( field ) ; addClause ( clauses , conj , mods , q ) ; } if ( clauses . size ( ) == 1 && firstQuery != null ) { if ( true ) return firstQuery ; } else { { if ( true ) return getBooleanQuery ( clauses ) ; } } throw new Error ( "Missing return statement in function" ) ; } final public Query Clause ( String field ) throws ParseException { Query q ; Token fieldToken = null , boost = null ; if ( jj_2_1 ( 2 ) ) { switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case TERM : fieldToken = jj_consume_token ( TERM ) ; jj_consume_token ( COLON ) ; field = discardEscapeChar ( fieldToken . image ) ; break ; case STAR : jj_consume_token ( STAR ) ; jj_consume_token ( COLON ) ; field = "*" ; break ; default : jj_la1 [ 5 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } } else { ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case STAR : case QUOTED : case TERM : case PREFIXTERM : case WILDTERM : case RANGEIN_START : case RANGEEX_START : case NUMBER : q = Term ( field ) ; break ; case LPAREN : jj_consume_token ( LPAREN ) ; q = Query ( field ) ; jj_consume_token ( RPAREN ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case CARAT : jj_consume_token ( CARAT ) ; boost = jj_consume_token ( NUMBER ) ; break ; default : jj_la1 [ 6 ] = jj_gen ; ; } break ; default : jj_la1 [ 7 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } if ( boost != null ) { float f = ( float ) 1.0 ; try { f = Float . valueOf ( boost . image ) . floatValue ( ) ; q . setBoost ( f ) ; } catch ( Exception ignored ) { } } { if ( true ) return q ; } throw new Error ( "Missing return statement in function" ) ; } final public Query Term ( String field ) throws ParseException { Token term , boost = null , fuzzySlop = null , goop1 , goop2 ; boolean prefix = false ; boolean wildcard = false ; boolean fuzzy = false ; boolean rangein = false ; Query q ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case STAR : case TERM : case PREFIXTERM : case WILDTERM : case NUMBER : switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case TERM : term = jj_consume_token ( TERM ) ; break ; case STAR : term = jj_consume_token ( STAR ) ; wildcard = true ; break ; case PREFIXTERM : term = jj_consume_token ( PREFIXTERM ) ; prefix = true ; break ; case WILDTERM : term = jj_consume_token ( WILDTERM ) ; wildcard = true ; break ; case NUMBER : term = jj_consume_token ( NUMBER ) ; break ; default : jj_la1 [ 8 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case FUZZY_SLOP : fuzzySlop = jj_consume_token ( FUZZY_SLOP ) ; fuzzy = true ; break ; default : jj_la1 [ 9 ] = jj_gen ; ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case CARAT : jj_consume_token ( CARAT ) ; boost = jj_consume_token ( NUMBER ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case FUZZY_SLOP : fuzzySlop = jj_consume_token ( FUZZY_SLOP ) ; fuzzy = true ; break ; default : jj_la1 [ 10 ] = jj_gen ; ; } break ; default : jj_la1 [ 11 ] = jj_gen ; ; } String termImage = discardEscapeChar ( term . image ) ; if ( wildcard ) { q = getWildcardQuery ( field , termImage ) ; } else if ( prefix ) { q = getPrefixQuery ( field , discardEscapeChar ( term . image . substring ( 0 , term . image . length ( ) - 1 ) ) ) ; } else if ( fuzzy ) { float fms = fuzzyMinSim ; try { fms = Float . valueOf ( fuzzySlop . image . substring ( 1 ) ) . floatValue ( ) ; } catch ( Exception ignored ) { } if ( fms < 0.0f || fms > 1.0f ) { { if ( true ) throw new ParseException ( "Minimum similarity for a FuzzyQuery has to be between 0.0f and 1.0f !" ) ; } } q = getFuzzyQuery ( field , termImage , fms ) ; } else { q = getFieldQuery ( field , termImage ) ; } break ; case RANGEIN_START : jj_consume_token ( RANGEIN_START ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case RANGEIN_GOOP : goop1 = jj_consume_token ( RANGEIN_GOOP ) ; break ; case RANGEIN_QUOTED : goop1 = jj_consume_token ( RANGEIN_QUOTED ) ; break ; default : jj_la1 [ 12 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case RANGEIN_TO : jj_consume_token ( RANGEIN_TO ) ; break ; default : jj_la1 [ 13 ] = jj_gen ; ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case RANGEIN_GOOP : goop2 = jj_consume_token ( RANGEIN_GOOP ) ; break ; case RANGEIN_QUOTED : goop2 = jj_consume_token ( RANGEIN_QUOTED ) ; break ; default : jj_la1 [ 14 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } jj_consume_token ( RANGEIN_END ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case CARAT : jj_consume_token ( CARAT ) ; boost = jj_consume_token ( NUMBER ) ; break ; default : jj_la1 [ 15 ] = jj_gen ; ; } if ( goop1 . kind == RANGEIN_QUOTED ) { goop1 . image = goop1 . image . substring ( 1 , goop1 . image . length ( ) - 1 ) ; } if ( goop2 . kind == RANGEIN_QUOTED ) { goop2 . image = goop2 . image . substring ( 1 , goop2 . image . length ( ) - 1 ) ; } q = getRangeQuery ( field , discardEscapeChar ( goop1 . image ) , discardEscapeChar ( goop2 . image ) , true ) ; break ; case RANGEEX_START : jj_consume_token ( RANGEEX_START ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case RANGEEX_GOOP : goop1 = jj_consume_token ( RANGEEX_GOOP ) ; break ; case RANGEEX_QUOTED : goop1 = jj_consume_token ( RANGEEX_QUOTED ) ; break ; default : jj_la1 [ 16 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case RANGEEX_TO : jj_consume_token ( RANGEEX_TO ) ; break ; default : jj_la1 [ 17 ] = jj_gen ; ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case RANGEEX_GOOP : goop2 = jj_consume_token ( RANGEEX_GOOP ) ; break ; case RANGEEX_QUOTED : goop2 = jj_consume_token ( RANGEEX_QUOTED ) ; break ; default : jj_la1 [ 18 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } jj_consume_token ( RANGEEX_END ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case CARAT : jj_consume_token ( CARAT ) ; boost = jj_consume_token ( NUMBER ) ; break ; default : jj_la1 [ 19 ] = jj_gen ; ; } if ( goop1 . kind == RANGEEX_QUOTED ) { goop1 . image = goop1 . image . substring ( 1 , goop1 . image . length ( ) - 1 ) ; } if ( goop2 . kind == RANGEEX_QUOTED ) { goop2 . image = goop2 . image . substring ( 1 , goop2 . image . length ( ) - 1 ) ; } q = getRangeQuery ( field , discardEscapeChar ( goop1 . image ) , discardEscapeChar ( goop2 . image ) , false ) ; break ; case QUOTED : term = jj_consume_token ( QUOTED ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case FUZZY_SLOP : fuzzySlop = jj_consume_token ( FUZZY_SLOP ) ; break ; default : jj_la1 [ 20 ] = jj_gen ; ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case CARAT : jj_consume_token ( CARAT ) ; boost = jj_consume_token ( NUMBER ) ; break ; default : jj_la1 [ 21 ] = jj_gen ; ; } int s = phraseSlop ; if ( fuzzySlop != null ) { try { s = Float . valueOf ( fuzzySlop . image . substring ( 1 ) ) . intValue ( ) ; } catch ( Exception ignored ) { } } q = getFieldQuery ( field , discardEscapeChar ( term . image . substring ( 1 , term . image . length ( ) - 1 ) ) , s ) ; break ; default : jj_la1 [ 22 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } if ( boost != null ) { float f = ( float ) 1.0 ; try { f = Float . valueOf ( boost . image ) . floatValue ( ) ; } catch ( Exception ignored ) { } if ( q != null ) { q . setBoost ( f ) ; } } { if ( true ) return q ; } throw new Error ( "Missing return statement in function" ) ; } final private boolean jj_2_1 ( int xla ) { jj_la = xla ; jj_lastpos = jj_scanpos = token ; try { return ! jj_3_1 ( ) ; } catch ( LookaheadSuccess ls ) { return true ; } finally { jj_save ( 0 , xla ) ; } } final private boolean jj_3_1 ( ) { Token xsp ; xsp = jj_scanpos ; if ( jj_3R_2 ( ) ) { jj_scanpos = xsp ; if ( jj_3R_3 ( ) ) return true ; } return false ; } final private boolean jj_3R_3 ( ) { if ( jj_scan_token ( STAR ) ) return true ; if ( jj_scan_token ( COLON ) ) return true ; return false ; } final private boolean jj_3R_2 ( ) { if ( jj_scan_token ( TERM ) ) return true ; if ( jj_scan_token ( COLON ) ) return true ; return false ; } public QueryParserTokenManager token_source ; public Token token , jj_nt ; private int jj_ntk ; private Token jj_scanpos , jj_lastpos ; private int jj_la ; public boolean lookingAhead = false ; private boolean jj_semLA ; private int jj_gen ; final private int [ ] jj_la1 = new int [ 23 ] ; static private int [ ] jj_la1_0 ; static private int [ ] jj_la1_1 ; static { jj_la1_0 ( ) ; jj_la1_1 ( ) ; } private static void jj_la1_0 ( ) { jj_la1_0 = new int [ ] { 0x180 , 0x180 , 0xe00 , 0xe00 , 0x1f69f80 , 0x48000 , 0x10000 , 0x1f69000 , 0x1348000 , 0x80000 , 0x80000 , 0x10000 , 0x18000000 , 0x2000000 , 0x18000000 , 0x10000 , 0x80000000 , 0x20000000 , 0x80000000 , 0x10000 , 0x80000 , 0x10000 , 0x1f68000 , } ; } private static void jj_la1_1 ( ) { jj_la1_1 = new int [ ] { 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x1 , 0x0 , 0x1 , 0x0 , 0x0 , 0x0 , 0x0 , } ; } final private JJCalls [ ] jj_2_rtns = new JJCalls [ 1 ] ; private boolean jj_rescan = false ; private int jj_gc = 0 ; public QueryParser ( CharStream stream ) { token_source = new QueryParserTokenManager ( stream ) ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 23 ; i ++ ) jj_la1 [ i ] = - 1 ; for ( int i = 0 ; i < jj_2_rtns . length ; i ++ ) jj_2_rtns [ i ] = new JJCalls ( ) ; } public void ReInit ( CharStream stream ) { token_source . ReInit ( stream ) ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 23 ; i ++ ) jj_la1 [ i ] = - 1 ; for ( int i = 0 ; i < jj_2_rtns . length ; i ++ ) jj_2_rtns [ i ] = new JJCalls ( ) ; } public QueryParser ( QueryParserTokenManager tm ) { token_source = tm ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 23 ; i ++ ) jj_la1 [ i ] = - 1 ; for ( int i = 0 ; i < jj_2_rtns . length ; i ++ ) jj_2_rtns [ i ] = new JJCalls ( ) ; } public void ReInit ( QueryParserTokenManager tm ) { token_source = tm ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 23 ; i ++ ) jj_la1 [ i ] = - 1 ; for ( int i = 0 ; i < jj_2_rtns . length ; i ++ ) jj_2_rtns [ i ] = new JJCalls ( ) ; } final private Token jj_consume_token ( int kind ) throws ParseException { Token oldToken ; if ( ( oldToken = token ) . next != null ) token = token . next ; else token = token . next = token_source . getNextToken ( ) ; jj_ntk = - 1 ; if ( token . kind == kind ) { jj_gen ++ ; if ( ++ jj_gc > 100 ) { jj_gc = 0 ; for ( int i = 0 ; i < jj_2_rtns . length ; i ++ ) { JJCalls c = jj_2_rtns [ i ] ; while ( c != null ) { if ( c . gen < jj_gen ) c . first = null ; c = c . next ; } } } return token ; } token = oldToken ; jj_kind = kind ; throw generateParseException ( ) ; } static private final class LookaheadSuccess extends java . lang . Error { } final private LookaheadSuccess jj_ls = new LookaheadSuccess ( ) ; final private boolean jj_scan_token ( int kind ) { if ( jj_scanpos == jj_lastpos ) { jj_la -- ; if ( jj_scanpos . next == null ) { jj_lastpos = jj_scanpos = jj_scanpos . next = token_source . getNextToken ( ) ; } else { jj_lastpos = jj_scanpos = jj_scanpos . next ; } } else { jj_scanpos = jj_scanpos . next ; } if ( jj_rescan ) { int i = 0 ; Token tok = token ; while ( tok != null && tok != jj_scanpos ) { i ++ ; tok = tok . next ; } if ( tok != null ) jj_add_error_token ( kind , i ) ; } if ( jj_scanpos . kind != kind ) return true ; if ( jj_la == 0 && jj_scanpos == jj_lastpos ) throw jj_ls ; return false ; } final public Token getNextToken ( ) { if ( token . next != null ) token = token . next ; else token = token . next = token_source . getNextToken ( ) ; jj_ntk = - 1 ; jj_gen ++ ; return token ; } final public Token getToken ( int index ) { Token t = lookingAhead ? jj_scanpos : token ; for ( int i = 0 ; i < index ; i ++ ) { if ( t . next != null ) t = t . next ; else t = t . next = token_source . getNextToken ( ) ; } return t ; } final private int jj_ntk ( ) { if ( ( jj_nt = token . next ) == null ) return ( jj_ntk = ( token . next = token_source . getNextToken ( ) ) . kind ) ; else return ( jj_ntk = jj_nt . kind ) ; } private java . util . Vector jj_expentries = new java . util . Vector ( ) ; private int [ ] jj_expentry ; private int jj_kind = - 1 ; private int [ ] jj_lasttokens = new int [ 100 ] ; private int jj_endpos ; private void jj_add_error_token ( int kind , int pos ) { if ( pos >= 100 ) return ; if ( pos == jj_endpos + 1 ) { jj_lasttokens [ jj_endpos ++ ] = kind ; } else if ( jj_endpos != 0 ) { jj_expentry = new int [ jj_endpos ] ; for ( int i = 0 ; i < jj_endpos ; i ++ ) { jj_expentry [ i ] = jj_lasttokens [ i ] ; } boolean exists = false ; for ( java . util . Enumeration e = jj_expentries . elements ( ) ; e . hasMoreElements ( ) ; ) { int [ ] oldentry = ( int [ ] ) ( e . nextElement ( ) ) ; if ( oldentry . length == jj_expentry . length ) { exists = true ; for ( int i = 0 ; i < jj_expentry . length ; i ++ ) { if ( oldentry [ i ] != jj_expentry [ i ] ) { exists = false ; break ; } } if ( exists ) break ; } } if ( ! exists ) jj_expentries . addElement ( jj_expentry ) ; if ( pos != 0 ) jj_lasttokens [ ( jj_endpos = pos ) - 1 ] = kind ; } } public ParseException generateParseException ( ) { jj_expentries . removeAllElements ( ) ; boolean [ ] la1tokens = new boolean [ 33 ] ; for ( int i = 0 ; i < 33 ; i ++ ) { la1tokens [ i ] = false ; } if ( jj_kind >= 0 ) { la1tokens [ jj_kind ] = true ; jj_kind = - 1 ; } for ( int i = 0 ; i < 23 ; i ++ ) { if ( jj_la1 [ i ] == jj_gen ) { for ( int j = 0 ; j < 32 ; j ++ ) { if ( ( jj_la1_0 [ i ] & ( 1 << j ) ) != 0 ) { la1tokens [ j ] = true ; } if ( ( jj_la1_1 [ i ] & ( 1 << j ) ) != 0 ) { la1tokens [ 32 + j ] = true ; } } } } for ( int i = 0 ; i < 33 ; i ++ ) { if ( la1tokens [ i ] ) { jj_expentry = new int [ 1 ] ; jj_expentry [ 0 ] = i ; jj_expentries . addElement ( jj_expentry ) ; } } jj_endpos = 0 ; jj_rescan_token ( ) ; jj_add_error_token ( 0 , 0 ) ; int [ ] [ ] exptokseq = new int [ jj_expentries . size ( ) ] [ ] ; for ( int i = 0 ; i < jj_expentries . size ( ) ; i ++ ) { exptokseq [ i ] = ( int [ ] ) jj_expentries . elementAt ( i ) ; } return new ParseException ( token , exptokseq , tokenImage ) ; } final public void enable_tracing ( ) { } final public void disable_tracing ( ) { } final private void jj_rescan_token ( ) { jj_rescan = true ; for ( int i = 0 ; i < 1 ; i ++ ) { JJCalls p = jj_2_rtns [ i ] ; do { if ( p . gen > jj_gen ) { jj_la = p . arg ; jj_lastpos = jj_scanpos = p . first ; switch ( i ) { case 0 : jj_3_1 ( ) ; break ; } } p = p . next ; } while ( p != null ) ; } jj_rescan = false ; } final private void jj_save ( int index , int xla ) { JJCalls p = jj_2_rtns [ index ] ; while ( p . gen > jj_gen ) { if ( p . next == null ) { p = p . next = new JJCalls ( ) ; break ; } p = p . next ; } p . gen = jj_gen + xla - jj_la ; p . first = token ; p . arg = xla ; } static final class JJCalls { int gen ; Token first ; int arg ; JJCalls next ; } } 	1
package org . apache . lucene . document ; import org . apache . lucene . search . PrefixQuery ; import org . apache . lucene . search . RangeQuery ; import java . util . Date ; public class DateField { private DateField ( ) { } private static int DATE_LEN = Long . toString ( 1000L * 365 * 24 * 60 * 60 * 1000 , Character . MAX_RADIX ) . length ( ) ; public static String MIN_DATE_STRING ( ) { return timeToString ( 0 ) ; } public static String MAX_DATE_STRING ( ) { char [ ] buffer = new char [ DATE_LEN ] ; char c = Character . forDigit ( Character . MAX_RADIX - 1 , Character . MAX_RADIX ) ; for ( int i = 0 ; i < DATE_LEN ; i ++ ) buffer [ i ] = c ; return new String ( buffer ) ; } public static String dateToString ( Date date ) { return timeToString ( date . getTime ( ) ) ; } public static String timeToString ( long time ) { if ( time < 0 ) throw new RuntimeException ( "time '" + time + "' is too early, must be >= 0" ) ; String s = Long . toString ( time , Character . MAX_RADIX ) ; if ( s . length ( ) > DATE_LEN ) throw new RuntimeException ( "time '" + time + "' is too late, length of string " + "representation must be <= " + DATE_LEN ) ; if ( s . length ( ) < DATE_LEN ) { StringBuffer sb = new StringBuffer ( s ) ; while ( sb . length ( ) < DATE_LEN ) sb . insert ( 0 , 0 ) ; s = sb . toString ( ) ; } return s ; } public static long stringToTime ( String s ) { return Long . parseLong ( s , Character . MAX_RADIX ) ; } public static Date stringToDate ( String s ) { return new Date ( stringToTime ( s ) ) ; } } 	0
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; import java . text . Collator ; import java . util . Locale ; class FieldDocSortedHitQueue extends PriorityQueue { volatile SortField [ ] fields ; volatile Collator [ ] collators ; FieldDocSortedHitQueue ( SortField [ ] fields , int size ) { this . fields = fields ; this . collators = hasCollators ( fields ) ; initialize ( size ) ; } synchronized void setFields ( SortField [ ] fields ) { if ( this . fields == null ) { this . fields = fields ; this . collators = hasCollators ( fields ) ; } } SortField [ ] getFields ( ) { return fields ; } private Collator [ ] hasCollators ( final SortField [ ] fields ) { if ( fields == null ) return null ; Collator [ ] ret = new Collator [ fields . length ] ; for ( int i = 0 ; i < fields . length ; ++ i ) { Locale locale = fields [ i ] . getLocale ( ) ; if ( locale != null ) ret [ i ] = Collator . getInstance ( locale ) ; } return ret ; } protected final boolean lessThan ( final Object a , final Object b ) { final FieldDoc docA = ( FieldDoc ) a ; final FieldDoc docB = ( FieldDoc ) b ; final int n = fields . length ; int c = 0 ; for ( int i = 0 ; i < n && c == 0 ; ++ i ) { final int type = fields [ i ] . getType ( ) ; switch ( type ) { case SortField . SCORE : float r1 = ( ( Float ) docA . fields [ i ] ) . floatValue ( ) ; float r2 = ( ( Float ) docB . fields [ i ] ) . floatValue ( ) ; if ( r1 > r2 ) c = - 1 ; if ( r1 < r2 ) c = 1 ; break ; case SortField . DOC : case SortField . INT : int i1 = ( ( Integer ) docA . fields [ i ] ) . intValue ( ) ; int i2 = ( ( Integer ) docB . fields [ i ] ) . intValue ( ) ; if ( i1 < i2 ) c = - 1 ; if ( i1 > i2 ) c = 1 ; break ; case SortField . STRING : String s1 = ( String ) docA . fields [ i ] ; String s2 = ( String ) docB . fields [ i ] ; if ( s1 == null ) c = ( s2 == null ) ? 0 : - 1 ; else if ( s2 == null ) c = 1 ; else if ( fields [ i ] . getLocale ( ) == null ) { c = s1 . compareTo ( s2 ) ; } else { c = collators [ i ] . compare ( s1 , s2 ) ; } break ; case SortField . FLOAT : float f1 = ( ( Float ) docA . fields [ i ] ) . floatValue ( ) ; float f2 = ( ( Float ) docB . fields [ i ] ) . floatValue ( ) ; if ( f1 < f2 ) c = - 1 ; if ( f1 > f2 ) c = 1 ; break ; case SortField . CUSTOM : c = docA . fields [ i ] . compareTo ( docB . fields [ i ] ) ; break ; case SortField . AUTO : throw new RuntimeException ( "FieldDocSortedHitQueue cannot use an AUTO SortField" ) ; default : throw new RuntimeException ( "invalid SortField type: " + type ) ; } if ( fields [ i ] . getReverse ( ) ) { c = - c ; } } if ( c == 0 ) return docA . doc > docB . doc ; return c > 0 ; } } 	1
package org . apache . lucene . document ; import java . io . Serializable ; public final class FieldSelectorResult implements Serializable { public transient static final FieldSelectorResult LOAD = new FieldSelectorResult ( 0 ) ; public transient static final FieldSelectorResult LAZY_LOAD = new FieldSelectorResult ( 1 ) ; public transient static final FieldSelectorResult NO_LOAD = new FieldSelectorResult ( 2 ) ; public transient static final FieldSelectorResult LOAD_AND_BREAK = new FieldSelectorResult ( 3 ) ; public transient static final FieldSelectorResult LOAD_FOR_MERGE = new FieldSelectorResult ( 4 ) ; public transient static final FieldSelectorResult SIZE = new FieldSelectorResult ( 5 ) ; public transient static final FieldSelectorResult SIZE_AND_BREAK = new FieldSelectorResult ( 6 ) ; private int id ; private FieldSelectorResult ( int id ) { this . id = id ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( o == null || getClass ( ) != o . getClass ( ) ) return false ; final FieldSelectorResult that = ( FieldSelectorResult ) o ; if ( id != that . id ) return false ; return true ; } public int hashCode ( ) { return id ; } } 	0
package org . apache . lucene . search . function ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . FieldCache ; public abstract class FieldCacheSource extends ValueSource { private String field ; private FieldCache cache = FieldCache . DEFAULT ; public FieldCacheSource ( String field ) { this . field = field ; } public final DocValues getValues ( IndexReader reader ) throws IOException { return getCachedFieldValues ( cache , field , reader ) ; } public String description ( ) { return field ; } public abstract DocValues getCachedFieldValues ( FieldCache cache , String field , IndexReader reader ) throws IOException ; public final boolean equals ( Object o ) { if ( ! ( o instanceof FieldCacheSource ) ) { return false ; } FieldCacheSource other = ( FieldCacheSource ) o ; return this . cache == other . cache && this . field . equals ( other . field ) && cachedFieldSourceEquals ( other ) ; } public final int hashCode ( ) { return cache . hashCode ( ) + field . hashCode ( ) + cachedFieldSourceHashCode ( ) ; } public abstract boolean cachedFieldSourceEquals ( FieldCacheSource other ) ; public abstract int cachedFieldSourceHashCode ( ) ; } 	1
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; import java . io . Serializable ; public interface SortComparatorSource extends Serializable { ScoreDocComparator newComparator ( IndexReader reader , String fieldname ) throws IOException ; } 	0
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; import org . apache . lucene . search . BooleanClause . Occur ; import java . io . IOException ; import java . util . * ; public class BooleanQuery extends Query { private static int maxClauseCount = 1024 ; public static class TooManyClauses extends RuntimeException { public TooManyClauses ( ) { } public String getMessage ( ) { return "maxClauseCount is set to " + maxClauseCount ; } } public static int getMaxClauseCount ( ) { return maxClauseCount ; } public static void setMaxClauseCount ( int maxClauseCount ) { if ( maxClauseCount < 1 ) throw new IllegalArgumentException ( "maxClauseCount must be >= 1" ) ; BooleanQuery . maxClauseCount = maxClauseCount ; } private ArrayList clauses = new ArrayList ( ) ; private boolean disableCoord ; public BooleanQuery ( ) { } public BooleanQuery ( boolean disableCoord ) { this . disableCoord = disableCoord ; } public boolean isCoordDisabled ( ) { return disableCoord ; } public Similarity getSimilarity ( Searcher searcher ) { Similarity result = super . getSimilarity ( searcher ) ; if ( disableCoord ) { result = new SimilarityDelegator ( result ) { public float coord ( int overlap , int maxOverlap ) { return 1.0f ; } } ; } return result ; } public void setMinimumNumberShouldMatch ( int min ) { this . minNrShouldMatch = min ; } protected int minNrShouldMatch = 0 ; public int getMinimumNumberShouldMatch ( ) { return minNrShouldMatch ; } public void add ( Query query , BooleanClause . Occur occur ) { add ( new BooleanClause ( query , occur ) ) ; } public void add ( BooleanClause clause ) { if ( clauses . size ( ) >= maxClauseCount ) throw new TooManyClauses ( ) ; clauses . add ( clause ) ; } public BooleanClause [ ] getClauses ( ) { return ( BooleanClause [ ] ) clauses . toArray ( new BooleanClause [ clauses . size ( ) ] ) ; } public List clauses ( ) { return clauses ; } private class BooleanWeight implements Weight { protected Similarity similarity ; protected Vector weights = new Vector ( ) ; public BooleanWeight ( Searcher searcher ) throws IOException { this . similarity = getSimilarity ( searcher ) ; for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . get ( i ) ; weights . add ( c . getQuery ( ) . createWeight ( searcher ) ) ; } } public Query getQuery ( ) { return BooleanQuery . this ; } public float getValue ( ) { return getBoost ( ) ; } public float sumOfSquaredWeights ( ) throws IOException { float sum = 0.0f ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . get ( i ) ; Weight w = ( Weight ) weights . elementAt ( i ) ; float s = w . sumOfSquaredWeights ( ) ; if ( ! c . isProhibited ( ) ) sum += s ; } sum *= getBoost ( ) * getBoost ( ) ; return sum ; } public void normalize ( float norm ) { norm *= getBoost ( ) ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . get ( i ) ; Weight w = ( Weight ) weights . elementAt ( i ) ; w . normalize ( norm ) ; } } public Scorer scorer ( IndexReader reader ) throws IOException { BooleanScorer2 result = new BooleanScorer2 ( similarity , minNrShouldMatch , allowDocsOutOfOrder ) ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . get ( i ) ; Weight w = ( Weight ) weights . elementAt ( i ) ; Scorer subScorer = w . scorer ( reader ) ; if ( subScorer != null ) result . add ( subScorer , c . isRequired ( ) , c . isProhibited ( ) ) ; else if ( c . isRequired ( ) ) return null ; } return result ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { final int minShouldMatch = BooleanQuery . this . getMinimumNumberShouldMatch ( ) ; ComplexExplanation sumExpl = new ComplexExplanation ( ) ; sumExpl . setDescription ( "sum of:" ) ; int coord = 0 ; int maxCoord = 0 ; float sum = 0.0f ; boolean fail = false ; int shouldMatchCount = 0 ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . get ( i ) ; Weight w = ( Weight ) weights . elementAt ( i ) ; Explanation e = w . explain ( reader , doc ) ; if ( ! c . isProhibited ( ) ) maxCoord ++ ; if ( e . isMatch ( ) ) { if ( ! c . isProhibited ( ) ) { sumExpl . addDetail ( e ) ; sum += e . getValue ( ) ; coord ++ ; } else { Explanation r = new Explanation ( 0.0f , "match on prohibited clause (" + c . getQuery ( ) . toString ( ) + ")" ) ; r . addDetail ( e ) ; sumExpl . addDetail ( r ) ; fail = true ; } if ( c . getOccur ( ) . equals ( Occur . SHOULD ) ) shouldMatchCount ++ ; } else if ( c . isRequired ( ) ) { Explanation r = new Explanation ( 0.0f , "no match on required clause (" + c . getQuery ( ) . toString ( ) + ")" ) ; r . addDetail ( e ) ; sumExpl . addDetail ( r ) ; fail = true ; } } if ( fail ) { sumExpl . setMatch ( Boolean . FALSE ) ; sumExpl . setValue ( 0.0f ) ; sumExpl . setDescription ( "Failure to meet condition(s) of required/prohibited clause(s)" ) ; return sumExpl ; } else if ( shouldMatchCount < minShouldMatch ) { sumExpl . setMatch ( Boolean . FALSE ) ; sumExpl . setValue ( 0.0f ) ; sumExpl . setDescription ( "Failure to match minimum number " + "of optional clauses: " + minShouldMatch ) ; return sumExpl ; } sumExpl . setMatch ( 0 < coord ? Boolean . TRUE : Boolean . FALSE ) ; sumExpl . setValue ( sum ) ; float coordFactor = similarity . coord ( coord , maxCoord ) ; if ( coordFactor == 1.0f ) return sumExpl ; else { ComplexExplanation result = new ComplexExplanation ( sumExpl . isMatch ( ) , sum * coordFactor , "product of:" ) ; result . addDetail ( sumExpl ) ; result . addDetail ( new Explanation ( coordFactor , "coord(" + coord + "/" + maxCoord + ")" ) ) ; return result ; } } } private static boolean allowDocsOutOfOrder = false ; public static void setAllowDocsOutOfOrder ( boolean allow ) { allowDocsOutOfOrder = allow ; } public static boolean getAllowDocsOutOfOrder ( ) { return allowDocsOutOfOrder ; } public static void setUseScorer14 ( boolean use14 ) { setAllowDocsOutOfOrder ( use14 ) ; } public static boolean getUseScorer14 ( ) { return getAllowDocsOutOfOrder ( ) ; } protected Weight createWeight ( Searcher searcher ) throws IOException { return new BooleanWeight ( searcher ) ; } public Query rewrite ( IndexReader reader ) throws IOException { if ( clauses . size ( ) == 1 ) { BooleanClause c = ( BooleanClause ) clauses . get ( 0 ) ; if ( ! c . isProhibited ( ) ) { Query query = c . getQuery ( ) . rewrite ( reader ) ; if ( getBoost ( ) != 1.0f ) { if ( query == c . getQuery ( ) ) query = ( Query ) query . clone ( ) ; query . setBoost ( getBoost ( ) * query . getBoost ( ) ) ; } return query ; } } BooleanQuery clone = null ; for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . get ( i ) ; Query query = c . getQuery ( ) . rewrite ( reader ) ; if ( query != c . getQuery ( ) ) { if ( clone == null ) clone = ( BooleanQuery ) this . clone ( ) ; clone . clauses . set ( i , new BooleanClause ( query , c . getOccur ( ) ) ) ; } } if ( clone != null ) { return clone ; } else return this ; } public void extractTerms ( Set terms ) { for ( Iterator i = clauses . iterator ( ) ; i . hasNext ( ) ; ) { BooleanClause clause = ( BooleanClause ) i . next ( ) ; clause . getQuery ( ) . extractTerms ( terms ) ; } } public Object clone ( ) { BooleanQuery clone = ( BooleanQuery ) super . clone ( ) ; clone . clauses = ( ArrayList ) this . clauses . clone ( ) ; return clone ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; boolean needParens = ( getBoost ( ) != 1.0 ) || ( getMinimumNumberShouldMatch ( ) > 0 ) ; if ( needParens ) { buffer . append ( "(" ) ; } for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . get ( i ) ; if ( c . isProhibited ( ) ) buffer . append ( "-" ) ; else if ( c . isRequired ( ) ) buffer . append ( "+" ) ; Query subQuery = c . getQuery ( ) ; if ( subQuery instanceof BooleanQuery ) { buffer . append ( "(" ) ; buffer . append ( c . getQuery ( ) . toString ( field ) ) ; buffer . append ( ")" ) ; } else buffer . append ( c . getQuery ( ) . toString ( field ) ) ; if ( i != clauses . size ( ) - 1 ) buffer . append ( " " ) ; } if ( needParens ) { buffer . append ( ")" ) ; } if ( getMinimumNumberShouldMatch ( ) > 0 ) { buffer . append ( '~' ) ; buffer . append ( getMinimumNumberShouldMatch ( ) ) ; } if ( getBoost ( ) != 1.0f ) { buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; } return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof BooleanQuery ) ) return false ; BooleanQuery other = ( BooleanQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && this . clauses . equals ( other . clauses ) && this . getMinimumNumberShouldMatch ( ) == other . getMinimumNumberShouldMatch ( ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ clauses . hashCode ( ) + getMinimumNumberShouldMatch ( ) ; } } 	1
package org . apache . lucene . search ; import java . io . IOException ; public class ReqOptSumScorer extends Scorer { private Scorer reqScorer ; private Scorer optScorer ; public ReqOptSumScorer ( Scorer reqScorer , Scorer optScorer ) { super ( null ) ; this . reqScorer = reqScorer ; this . optScorer = optScorer ; } private boolean firstTimeOptScorer = true ; public boolean next ( ) throws IOException { return reqScorer . next ( ) ; } public boolean skipTo ( int target ) throws IOException { return reqScorer . skipTo ( target ) ; } public int doc ( ) { return reqScorer . doc ( ) ; } public float score ( ) throws IOException { int curDoc = reqScorer . doc ( ) ; float reqScore = reqScorer . score ( ) ; if ( firstTimeOptScorer ) { firstTimeOptScorer = false ; if ( ! optScorer . skipTo ( curDoc ) ) { optScorer = null ; return reqScore ; } } else if ( optScorer == null ) { return reqScore ; } else if ( ( optScorer . doc ( ) < curDoc ) && ( ! optScorer . skipTo ( curDoc ) ) ) { optScorer = null ; return reqScore ; } return ( optScorer . doc ( ) == curDoc ) ? reqScore + optScorer . score ( ) : reqScore ; } public Explanation explain ( int doc ) throws IOException { Explanation res = new Explanation ( ) ; res . setDescription ( "required, optional" ) ; res . addDetail ( reqScorer . explain ( doc ) ) ; res . addDetail ( optScorer . explain ( doc ) ) ; return res ; } } 	0
package org . apache . lucene . index ; public final class Term implements Comparable , java . io . Serializable { String field ; String text ; public Term ( String fld , String txt ) { this ( fld , txt , true ) ; } Term ( String fld , String txt , boolean intern ) { field = intern ? fld . intern ( ) : fld ; text = txt ; } public final String field ( ) { return field ; } public final String text ( ) { return text ; } public Term createTerm ( String text ) { return new Term ( field , text , false ) ; } public final boolean equals ( Object o ) { if ( o == this ) return true ; if ( o == null ) return false ; if ( ! ( o instanceof Term ) ) return false ; Term other = ( Term ) o ; return field == other . field && text . equals ( other . text ) ; } public final int hashCode ( ) { return field . hashCode ( ) + text . hashCode ( ) ; } public int compareTo ( Object other ) { return compareTo ( ( Term ) other ) ; } public final int compareTo ( Term other ) { if ( field == other . field ) return text . compareTo ( other . text ) ; else return field . compareTo ( other . field ) ; } final void set ( String fld , String txt ) { field = fld ; text = txt ; } public final String toString ( ) { return field + ":" + text ; } private void readObject ( java . io . ObjectInputStream in ) throws java . io . IOException , ClassNotFoundException { in . defaultReadObject ( ) ; field = field . intern ( ) ; } } 	1
package org . apache . lucene . document ; public class NumberTools { private static final int RADIX = 36 ; private static final char NEGATIVE_PREFIX = '-' ; private static final char POSITIVE_PREFIX = '0' ; public static final String MIN_STRING_VALUE = NEGATIVE_PREFIX + "0000000000000" ; public static final String MAX_STRING_VALUE = POSITIVE_PREFIX + "1y2p0ij32e8e7" ; public static final int STR_SIZE = MIN_STRING_VALUE . length ( ) ; public static String longToString ( long l ) { if ( l == Long . MIN_VALUE ) { return MIN_STRING_VALUE ; } StringBuffer buf = new StringBuffer ( STR_SIZE ) ; if ( l < 0 ) { buf . append ( NEGATIVE_PREFIX ) ; l = Long . MAX_VALUE + l + 1 ; } else { buf . append ( POSITIVE_PREFIX ) ; } String num = Long . toString ( l , RADIX ) ; int padLen = STR_SIZE - num . length ( ) - buf . length ( ) ; while ( padLen -- > 0 ) { buf . append ( '0' ) ; } buf . append ( num ) ; return buf . toString ( ) ; } public static long stringToLong ( String str ) { if ( str == null ) { throw new NullPointerException ( "string cannot be null" ) ; } if ( str . length ( ) != STR_SIZE ) { throw new NumberFormatException ( "string is the wrong size" ) ; } if ( str . equals ( MIN_STRING_VALUE ) ) { return Long . MIN_VALUE ; } char prefix = str . charAt ( 0 ) ; long l = Long . parseLong ( str . substring ( 1 ) , RADIX ) ; if ( prefix == POSITIVE_PREFIX ) { } else if ( prefix == NEGATIVE_PREFIX ) { l = l - Long . MAX_VALUE - 1 ; } else { throw new NumberFormatException ( "string does not begin with the correct prefix" ) ; } return l ; } } 	0
package org . apache . lucene . search ; import java . io . IOException ; import java . util . Set ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermDocs ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; public class TermQuery extends Query { private Term term ; private class TermWeight implements Weight { private Similarity similarity ; private float value ; private float idf ; private float queryNorm ; private float queryWeight ; public TermWeight ( Searcher searcher ) throws IOException { this . similarity = getSimilarity ( searcher ) ; idf = similarity . idf ( term , searcher ) ; } public String toString ( ) { return "weight(" + TermQuery . this + ")" ; } public Query getQuery ( ) { return TermQuery . this ; } public float getValue ( ) { return value ; } public float sumOfSquaredWeights ( ) { queryWeight = idf * getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float queryNorm ) { this . queryNorm = queryNorm ; queryWeight *= queryNorm ; value = queryWeight * idf ; } public Scorer scorer ( IndexReader reader ) throws IOException { TermDocs termDocs = reader . termDocs ( term ) ; if ( termDocs == null ) return null ; return new TermScorer ( this , termDocs , similarity , reader . norms ( term . field ( ) ) ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { ComplexExplanation result = new ComplexExplanation ( ) ; result . setDescription ( "weight(" + getQuery ( ) + " in " + doc + "), product of:" ) ; Explanation idfExpl = new Explanation ( idf , "idf(docFreq=" + reader . docFreq ( term ) + ")" ) ; Explanation queryExpl = new Explanation ( ) ; queryExpl . setDescription ( "queryWeight(" + getQuery ( ) + "), product of:" ) ; Explanation boostExpl = new Explanation ( getBoost ( ) , "boost" ) ; if ( getBoost ( ) != 1.0f ) queryExpl . addDetail ( boostExpl ) ; queryExpl . addDetail ( idfExpl ) ; Explanation queryNormExpl = new Explanation ( queryNorm , "queryNorm" ) ; queryExpl . addDetail ( queryNormExpl ) ; queryExpl . setValue ( boostExpl . getValue ( ) * idfExpl . getValue ( ) * queryNormExpl . getValue ( ) ) ; result . addDetail ( queryExpl ) ; String field = term . field ( ) ; ComplexExplanation fieldExpl = new ComplexExplanation ( ) ; fieldExpl . setDescription ( "fieldWeight(" + term + " in " + doc + "), product of:" ) ; Explanation tfExpl = scorer ( reader ) . explain ( doc ) ; fieldExpl . addDetail ( tfExpl ) ; fieldExpl . addDetail ( idfExpl ) ; Explanation fieldNormExpl = new Explanation ( ) ; byte [ ] fieldNorms = reader . norms ( field ) ; float fieldNorm = fieldNorms != null ? Similarity . decodeNorm ( fieldNorms [ doc ] ) : 0.0f ; fieldNormExpl . setValue ( fieldNorm ) ; fieldNormExpl . setDescription ( "fieldNorm(field=" + field + ", doc=" + doc + ")" ) ; fieldExpl . addDetail ( fieldNormExpl ) ; fieldExpl . setMatch ( Boolean . valueOf ( tfExpl . isMatch ( ) ) ) ; fieldExpl . setValue ( tfExpl . getValue ( ) * idfExpl . getValue ( ) * fieldNormExpl . getValue ( ) ) ; result . addDetail ( fieldExpl ) ; result . setMatch ( fieldExpl . getMatch ( ) ) ; result . setValue ( queryExpl . getValue ( ) * fieldExpl . getValue ( ) ) ; if ( queryExpl . getValue ( ) == 1.0f ) return fieldExpl ; return result ; } } public TermQuery ( Term t ) { term = t ; } public Term getTerm ( ) { return term ; } protected Weight createWeight ( Searcher searcher ) throws IOException { return new TermWeight ( searcher ) ; } public void extractTerms ( Set terms ) { terms . add ( getTerm ( ) ) ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! term . field ( ) . equals ( field ) ) { buffer . append ( term . field ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( term . text ( ) ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof TermQuery ) ) return false ; TermQuery other = ( TermQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && this . term . equals ( other . term ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ term . hashCode ( ) ; } } 	1
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import java . io . IOException ; public final class FuzzyTermEnum extends FilteredTermEnum { private static final int TYPICAL_LONGEST_WORD_IN_INDEX = 19 ; private int [ ] [ ] d ; private float similarity ; private boolean endEnum = false ; private Term searchTerm = null ; private final String field ; private final String text ; private final String prefix ; private final float minimumSimilarity ; private final float scale_factor ; private final int [ ] maxDistances = new int [ TYPICAL_LONGEST_WORD_IN_INDEX ] ; public FuzzyTermEnum ( IndexReader reader , Term term ) throws IOException { this ( reader , term , FuzzyQuery . defaultMinSimilarity , FuzzyQuery . defaultPrefixLength ) ; } public FuzzyTermEnum ( IndexReader reader , Term term , float minSimilarity ) throws IOException { this ( reader , term , minSimilarity , FuzzyQuery . defaultPrefixLength ) ; } public FuzzyTermEnum ( IndexReader reader , Term term , final float minSimilarity , final int prefixLength ) throws IOException { super ( ) ; if ( minSimilarity >= 1.0f ) throw new IllegalArgumentException ( "minimumSimilarity cannot be greater than or equal to 1" ) ; else if ( minSimilarity < 0.0f ) throw new IllegalArgumentException ( "minimumSimilarity cannot be less than 0" ) ; if ( prefixLength < 0 ) throw new IllegalArgumentException ( "prefixLength cannot be less than 0" ) ; this . minimumSimilarity = minSimilarity ; this . scale_factor = 1.0f / ( 1.0f - minimumSimilarity ) ; this . searchTerm = term ; this . field = searchTerm . field ( ) ; final int fullSearchTermLength = searchTerm . text ( ) . length ( ) ; final int realPrefixLength = prefixLength > fullSearchTermLength ? fullSearchTermLength : prefixLength ; this . text = searchTerm . text ( ) . substring ( realPrefixLength ) ; this . prefix = searchTerm . text ( ) . substring ( 0 , realPrefixLength ) ; initializeMaxDistances ( ) ; this . d = initDistanceArray ( ) ; setEnum ( reader . terms ( new Term ( searchTerm . field ( ) , prefix ) ) ) ; } protected final boolean termCompare ( Term term ) { if ( field == term . field ( ) && term . text ( ) . startsWith ( prefix ) ) { final String target = term . text ( ) . substring ( prefix . length ( ) ) ; this . similarity = similarity ( target ) ; return ( similarity > minimumSimilarity ) ; } endEnum = true ; return false ; } public final float difference ( ) { return ( float ) ( ( similarity - minimumSimilarity ) * scale_factor ) ; } public final boolean endEnum ( ) { return endEnum ; } private static final int min ( int a , int b , int c ) { final int t = ( a < b ) ? a : b ; return ( t < c ) ? t : c ; } private final int [ ] [ ] initDistanceArray ( ) { return new int [ this . text . length ( ) + 1 ] [ TYPICAL_LONGEST_WORD_IN_INDEX ] ; } private synchronized final float similarity ( final String target ) { final int m = target . length ( ) ; final int n = text . length ( ) ; if ( n == 0 ) { return prefix . length ( ) == 0 ? 0.0f : 1.0f - ( ( float ) m / prefix . length ( ) ) ; } if ( m == 0 ) { return prefix . length ( ) == 0 ? 0.0f : 1.0f - ( ( float ) n / prefix . length ( ) ) ; } final int maxDistance = getMaxDistance ( m ) ; if ( maxDistance < Math . abs ( m - n ) ) { return 0.0f ; } if ( d [ 0 ] . length <= m ) { growDistanceArray ( m ) ; } for ( int i = 0 ; i <= n ; i ++ ) d [ i ] [ 0 ] = i ; for ( int j = 0 ; j <= m ; j ++ ) d [ 0 ] [ j ] = j ; for ( int i = 1 ; i <= n ; i ++ ) { int bestPossibleEditDistance = m ; final char s_i = text . charAt ( i - 1 ) ; for ( int j = 1 ; j <= m ; j ++ ) { if ( s_i != target . charAt ( j - 1 ) ) { d [ i ] [ j ] = min ( d [ i - 1 ] [ j ] , d [ i ] [ j - 1 ] , d [ i - 1 ] [ j - 1 ] ) + 1 ; } else { d [ i ] [ j ] = min ( d [ i - 1 ] [ j ] + 1 , d [ i ] [ j - 1 ] + 1 , d [ i - 1 ] [ j - 1 ] ) ; } bestPossibleEditDistance = Math . min ( bestPossibleEditDistance , d [ i ] [ j ] ) ; } if ( i > maxDistance && bestPossibleEditDistance > maxDistance ) { return 0.0f ; } } return 1.0f - ( ( float ) d [ n ] [ m ] / ( float ) ( prefix . length ( ) + Math . min ( n , m ) ) ) ; } private void growDistanceArray ( int m ) { for ( int i = 0 ; i < d . length ; i ++ ) { d [ i ] = new int [ m + 1 ] ; } } private final int getMaxDistance ( int m ) { return ( m < maxDistances . length ) ? maxDistances [ m ] : calculateMaxDistance ( m ) ; } private void initializeMaxDistances ( ) { for ( int i = 0 ; i < maxDistances . length ; i ++ ) { maxDistances [ i ] = calculateMaxDistance ( i ) ; } } private int calculateMaxDistance ( int m ) { return ( int ) ( ( 1 - minimumSimilarity ) * ( Math . min ( text . length ( ) , m ) + prefix . length ( ) ) ) ; } public void close ( ) throws IOException { super . close ( ) ; } } 	0
package org . apache . lucene . analysis ; import java . io . IOException ; import java . io . Reader ; public abstract class CharTokenizer extends Tokenizer { public CharTokenizer ( Reader input ) { super ( input ) ; } private int offset = 0 , bufferIndex = 0 , dataLen = 0 ; private static final int MAX_WORD_LEN = 255 ; private static final int IO_BUFFER_SIZE = 1024 ; private final char [ ] buffer = new char [ MAX_WORD_LEN ] ; private final char [ ] ioBuffer = new char [ IO_BUFFER_SIZE ] ; protected abstract boolean isTokenChar ( char c ) ; protected char normalize ( char c ) { return c ; } public final Token next ( ) throws IOException { int length = 0 ; int start = offset ; while ( true ) { final char c ; offset ++ ; if ( bufferIndex >= dataLen ) { dataLen = input . read ( ioBuffer ) ; bufferIndex = 0 ; } ; if ( dataLen == - 1 ) { if ( length > 0 ) break ; else return null ; } else c = ioBuffer [ bufferIndex ++ ] ; if ( isTokenChar ( c ) ) { if ( length == 0 ) start = offset - 1 ; buffer [ length ++ ] = normalize ( c ) ; if ( length == MAX_WORD_LEN ) break ; } else if ( length > 0 ) break ; } return new Token ( new String ( buffer , 0 , length ) , start , start + length ) ; } } 	1
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Query ; import org . apache . lucene . util . ToStringUtils ; public class SpanFirstQuery extends SpanQuery { private SpanQuery match ; private int end ; public SpanFirstQuery ( SpanQuery match , int end ) { this . match = match ; this . end = end ; } public SpanQuery getMatch ( ) { return match ; } public int getEnd ( ) { return end ; } public String getField ( ) { return match . getField ( ) ; } public Collection getTerms ( ) { return match . getTerms ( ) ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "spanFirst(" ) ; buffer . append ( match . toString ( field ) ) ; buffer . append ( ", " ) ; buffer . append ( end ) ; buffer . append ( ")" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public void extractTerms ( Set terms ) { match . extractTerms ( terms ) ; } public Spans getSpans ( final IndexReader reader ) throws IOException { return new Spans ( ) { private Spans spans = match . getSpans ( reader ) ; public boolean next ( ) throws IOException { while ( spans . next ( ) ) { if ( end ( ) <= end ) return true ; } return false ; } public boolean skipTo ( int target ) throws IOException { if ( ! spans . skipTo ( target ) ) return false ; if ( spans . end ( ) <= end ) return true ; return next ( ) ; } public int doc ( ) { return spans . doc ( ) ; } public int start ( ) { return spans . start ( ) ; } public int end ( ) { return spans . end ( ) ; } public String toString ( ) { return "spans(" + SpanFirstQuery . this . toString ( ) + ")" ; } } ; } public Query rewrite ( IndexReader reader ) throws IOException { SpanFirstQuery clone = null ; SpanQuery rewritten = ( SpanQuery ) match . rewrite ( reader ) ; if ( rewritten != match ) { clone = ( SpanFirstQuery ) this . clone ( ) ; clone . match = rewritten ; } if ( clone != null ) { return clone ; } else { return this ; } } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof SpanFirstQuery ) ) return false ; SpanFirstQuery other = ( SpanFirstQuery ) o ; return this . end == other . end && this . match . equals ( other . match ) && this . getBoost ( ) == other . getBoost ( ) ; } public int hashCode ( ) { int h = match . hashCode ( ) ; h ^= ( h << 8 ) | ( h > > > 25 ) ; h ^= Float . floatToRawIntBits ( getBoost ( ) ) ^ end ; return h ; } } 	0
package org . apache . lucene . queryParser ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . search . BooleanClause ; import org . apache . lucene . search . BooleanQuery ; import org . apache . lucene . search . MultiPhraseQuery ; import org . apache . lucene . search . PhraseQuery ; import org . apache . lucene . search . Query ; import java . util . Vector ; import java . util . Map ; public class MultiFieldQueryParser extends QueryParser { protected String [ ] fields ; protected Map boosts ; public MultiFieldQueryParser ( String [ ] fields , Analyzer analyzer , Map boosts ) { this ( fields , analyzer ) ; this . boosts = boosts ; } public MultiFieldQueryParser ( String [ ] fields , Analyzer analyzer ) { super ( null , analyzer ) ; this . fields = fields ; } protected Query getFieldQuery ( String field , String queryText , int slop ) throws ParseException { if ( field == null ) { Vector clauses = new Vector ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { Query q = getFieldQuery ( fields [ i ] , queryText ) ; if ( q != null ) { if ( boosts != null ) { Float boost = ( Float ) boosts . get ( fields [ i ] ) ; if ( boost != null ) { q . setBoost ( boost . floatValue ( ) ) ; } } if ( q instanceof PhraseQuery ) { ( ( PhraseQuery ) q ) . setSlop ( slop ) ; } if ( q instanceof MultiPhraseQuery ) { ( ( MultiPhraseQuery ) q ) . setSlop ( slop ) ; } clauses . add ( new BooleanClause ( q , BooleanClause . Occur . SHOULD ) ) ; } } if ( clauses . size ( ) == 0 ) return null ; return getBooleanQuery ( clauses , true ) ; } return super . getFieldQuery ( field , queryText ) ; } protected Query getFieldQuery ( String field , String queryText ) throws ParseException { return getFieldQuery ( field , queryText , 0 ) ; } protected Query getFuzzyQuery ( String field , String termStr , float minSimilarity ) throws ParseException { if ( field == null ) { Vector clauses = new Vector ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( getFuzzyQuery ( fields [ i ] , termStr , minSimilarity ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getFuzzyQuery ( field , termStr , minSimilarity ) ; } protected Query getPrefixQuery ( String field , String termStr ) throws ParseException { if ( field == null ) { Vector clauses = new Vector ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( getPrefixQuery ( fields [ i ] , termStr ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getPrefixQuery ( field , termStr ) ; } protected Query getWildcardQuery ( String field , String termStr ) throws ParseException { if ( field == null ) { Vector clauses = new Vector ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( getWildcardQuery ( fields [ i ] , termStr ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getWildcardQuery ( field , termStr ) ; } protected Query getRangeQuery ( String field , String part1 , String part2 , boolean inclusive ) throws ParseException { if ( field == null ) { Vector clauses = new Vector ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( getRangeQuery ( fields [ i ] , part1 , part2 , inclusive ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getRangeQuery ( field , part1 , part2 , inclusive ) ; } public static Query parse ( String [ ] queries , String [ ] fields , Analyzer analyzer ) throws ParseException { if ( queries . length != fields . length ) throw new IllegalArgumentException ( "queries.length != fields.length" ) ; BooleanQuery bQuery = new BooleanQuery ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { QueryParser qp = new QueryParser ( fields [ i ] , analyzer ) ; Query q = qp . parse ( queries [ i ] ) ; bQuery . add ( q , BooleanClause . Occur . SHOULD ) ; } return bQuery ; } public static Query parse ( String query , String [ ] fields , BooleanClause . Occur [ ] flags , Analyzer analyzer ) throws ParseException { if ( fields . length != flags . length ) throw new IllegalArgumentException ( "fields.length != flags.length" ) ; BooleanQuery bQuery = new BooleanQuery ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { QueryParser qp = new QueryParser ( fields [ i ] , analyzer ) ; Query q = qp . parse ( query ) ; bQuery . add ( q , flags [ i ] ) ; } return bQuery ; } public static Query parse ( String [ ] queries , String [ ] fields , BooleanClause . Occur [ ] flags , Analyzer analyzer ) throws ParseException { if ( ! ( queries . length == fields . length && queries . length == flags . length ) ) throw new IllegalArgumentException ( "queries, fields, and flags array have have different length" ) ; BooleanQuery bQuery = new BooleanQuery ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { QueryParser qp = new QueryParser ( fields [ i ] , analyzer ) ; Query q = qp . parse ( queries [ i ] ) ; bQuery . add ( q , flags [ i ] ) ; } return bQuery ; } } 	1
package org . apache . lucene ; public final class LucenePackage { private LucenePackage ( ) { } public static Package get ( ) { return LucenePackage . class . getPackage ( ) ; } } 	0
package org . apache . lucene . store ; import java . io . IOException ; public abstract class Directory { protected LockFactory lockFactory ; public abstract String [ ] list ( ) throws IOException ; public abstract boolean fileExists ( String name ) throws IOException ; public abstract long fileModified ( String name ) throws IOException ; public abstract void touchFile ( String name ) throws IOException ; public abstract void deleteFile ( String name ) throws IOException ; public abstract void renameFile ( String from , String to ) throws IOException ; public abstract long fileLength ( String name ) throws IOException ; public abstract IndexOutput createOutput ( String name ) throws IOException ; public abstract IndexInput openInput ( String name ) throws IOException ; public IndexInput openInput ( String name , int bufferSize ) throws IOException { return openInput ( name ) ; } public Lock makeLock ( String name ) { return lockFactory . makeLock ( name ) ; } public void clearLock ( String name ) throws IOException { if ( lockFactory != null ) { lockFactory . clearLock ( name ) ; } } public abstract void close ( ) throws IOException ; public void setLockFactory ( LockFactory lockFactory ) { this . lockFactory = lockFactory ; lockFactory . setLockPrefix ( this . getLockID ( ) ) ; } public LockFactory getLockFactory ( ) { return this . lockFactory ; } public String getLockID ( ) { return this . toString ( ) ; } public static void copy ( Directory src , Directory dest , boolean closeDirSrc ) throws IOException { final String [ ] files = src . list ( ) ; if ( files == null ) throw new IOException ( "cannot read directory " + src + ": list() returned null" ) ; byte [ ] buf = new byte [ BufferedIndexOutput . BUFFER_SIZE ] ; for ( int i = 0 ; i < files . length ; i ++ ) { IndexOutput os = null ; IndexInput is = null ; try { os = dest . createOutput ( files [ i ] ) ; is = src . openInput ( files [ i ] ) ; long len = is . length ( ) ; long readCount = 0 ; while ( readCount < len ) { int toRead = readCount + BufferedIndexOutput . BUFFER_SIZE > len ? ( int ) ( len - readCount ) : BufferedIndexOutput . BUFFER_SIZE ; is . readBytes ( buf , 0 , toRead ) ; os . writeBytes ( buf , toRead ) ; readCount += toRead ; } } finally { try { if ( os != null ) os . close ( ) ; } finally { if ( is != null ) is . close ( ) ; } } } if ( closeDirSrc ) src . close ( ) ; } } 	1
package org . apache . lucene . util ; import java . io . ObjectStreamException ; import java . io . Serializable ; import java . io . StreamCorruptedException ; import java . util . HashMap ; import java . util . Map ; public abstract class Parameter implements Serializable { static Map allParameters = new HashMap ( ) ; private String name ; private Parameter ( ) { } protected Parameter ( String name ) { this . name = name ; String key = makeKey ( name ) ; if ( allParameters . containsKey ( key ) ) throw new IllegalArgumentException ( "Parameter name " + key + " already used!" ) ; allParameters . put ( key , this ) ; } private String makeKey ( String name ) { return getClass ( ) + " " + name ; } public String toString ( ) { return name ; } protected Object readResolve ( ) throws ObjectStreamException { Object par = allParameters . get ( makeKey ( name ) ) ; if ( par == null ) throw new StreamCorruptedException ( "Unknown parameter value: " + name ) ; return par ; } } 	0
package org . apache . lucene . analysis ; import java . io . * ; class PorterStemmer { private char [ ] b ; private int i , j , k , k0 ; private boolean dirty = false ; private static final int INC = 50 ; private static final int EXTRA = 1 ; public PorterStemmer ( ) { b = new char [ INC ] ; i = 0 ; } public void reset ( ) { i = 0 ; dirty = false ; } public void add ( char ch ) { if ( b . length <= i + EXTRA ) { char [ ] new_b = new char [ b . length + INC ] ; for ( int c = 0 ; c < b . length ; c ++ ) new_b [ c ] = b [ c ] ; b = new_b ; } b [ i ++ ] = ch ; } public String toString ( ) { return new String ( b , 0 , i ) ; } public int getResultLength ( ) { return i ; } public char [ ] getResultBuffer ( ) { return b ; } private final boolean cons ( int i ) { switch ( b [ i ] ) { case 'a' : case 'e' : case 'i' : case 'o' : case 'u' : return false ; case 'y' : return ( i == k0 ) ? true : ! cons ( i - 1 ) ; default : return true ; } } private final int m ( ) { int n = 0 ; int i = k0 ; while ( true ) { if ( i > j ) return n ; if ( ! cons ( i ) ) break ; i ++ ; } i ++ ; while ( true ) { while ( true ) { if ( i > j ) return n ; if ( cons ( i ) ) break ; i ++ ; } i ++ ; n ++ ; while ( true ) { if ( i > j ) return n ; if ( ! cons ( i ) ) break ; i ++ ; } i ++ ; } } private final boolean vowelinstem ( ) { int i ; for ( i = k0 ; i <= j ; i ++ ) if ( ! cons ( i ) ) return true ; return false ; } private final boolean doublec ( int j ) { if ( j < k0 + 1 ) return false ; if ( b [ j ] != b [ j - 1 ] ) return false ; return cons ( j ) ; } private final boolean cvc ( int i ) { if ( i < k0 + 2 || ! cons ( i ) || cons ( i - 1 ) || ! cons ( i - 2 ) ) return false ; else { int ch = b [ i ] ; if ( ch == 'w' || ch == 'x' || ch == 'y' ) return false ; } return true ; } private final boolean ends ( String s ) { int l = s . length ( ) ; int o = k - l + 1 ; if ( o < k0 ) return false ; for ( int i = 0 ; i < l ; i ++ ) if ( b [ o + i ] != s . charAt ( i ) ) return false ; j = k - l ; return true ; } void setto ( String s ) { int l = s . length ( ) ; int o = j + 1 ; for ( int i = 0 ; i < l ; i ++ ) b [ o + i ] = s . charAt ( i ) ; k = j + l ; dirty = true ; } void r ( String s ) { if ( m ( ) > 0 ) setto ( s ) ; } private final void step1 ( ) { if ( b [ k ] == 's' ) { if ( ends ( "sses" ) ) k -= 2 ; else if ( ends ( "ies" ) ) setto ( "i" ) ; else if ( b [ k - 1 ] != 's' ) k -- ; } if ( ends ( "eed" ) ) { if ( m ( ) > 0 ) k -- ; } else if ( ( ends ( "ed" ) || ends ( "ing" ) ) && vowelinstem ( ) ) { k = j ; if ( ends ( "at" ) ) setto ( "ate" ) ; else if ( ends ( "bl" ) ) setto ( "ble" ) ; else if ( ends ( "iz" ) ) setto ( "ize" ) ; else if ( doublec ( k ) ) { int ch = b [ k -- ] ; if ( ch == 'l' || ch == 's' || ch == 'z' ) k ++ ; } else if ( m ( ) == 1 && cvc ( k ) ) setto ( "e" ) ; } } private final void step2 ( ) { if ( ends ( "y" ) && vowelinstem ( ) ) { b [ k ] = 'i' ; dirty = true ; } } private final void step3 ( ) { if ( k == k0 ) return ; switch ( b [ k - 1 ] ) { case 'a' : if ( ends ( "ational" ) ) { r ( "ate" ) ; break ; } if ( ends ( "tional" ) ) { r ( "tion" ) ; break ; } break ; case 'c' : if ( ends ( "enci" ) ) { r ( "ence" ) ; break ; } if ( ends ( "anci" ) ) { r ( "ance" ) ; break ; } break ; case 'e' : if ( ends ( "izer" ) ) { r ( "ize" ) ; break ; } break ; case 'l' : if ( ends ( "bli" ) ) { r ( "ble" ) ; break ; } if ( ends ( "alli" ) ) { r ( "al" ) ; break ; } if ( ends ( "entli" ) ) { r ( "ent" ) ; break ; } if ( ends ( "eli" ) ) { r ( "e" ) ; break ; } if ( ends ( "ousli" ) ) { r ( "ous" ) ; break ; } break ; case 'o' : if ( ends ( "ization" ) ) { r ( "ize" ) ; break ; } if ( ends ( "ation" ) ) { r ( "ate" ) ; break ; } if ( ends ( "ator" ) ) { r ( "ate" ) ; break ; } break ; case 's' : if ( ends ( "alism" ) ) { r ( "al" ) ; break ; } if ( ends ( "iveness" ) ) { r ( "ive" ) ; break ; } if ( ends ( "fulness" ) ) { r ( "ful" ) ; break ; } if ( ends ( "ousness" ) ) { r ( "ous" ) ; break ; } break ; case 't' : if ( ends ( "aliti" ) ) { r ( "al" ) ; break ; } if ( ends ( "iviti" ) ) { r ( "ive" ) ; break ; } if ( ends ( "biliti" ) ) { r ( "ble" ) ; break ; } break ; case 'g' : if ( ends ( "logi" ) ) { r ( "log" ) ; break ; } } } private final void step4 ( ) { switch ( b [ k ] ) { case 'e' : if ( ends ( "icate" ) ) { r ( "ic" ) ; break ; } if ( ends ( "ative" ) ) { r ( "" ) ; break ; } if ( ends ( "alize" ) ) { r ( "al" ) ; break ; } break ; case 'i' : if ( ends ( "iciti" ) ) { r ( "ic" ) ; break ; } break ; case 'l' : if ( ends ( "ical" ) ) { r ( "ic" ) ; break ; } if ( ends ( "ful" ) ) { r ( "" ) ; break ; } break ; case 's' : if ( ends ( "ness" ) ) { r ( "" ) ; break ; } break ; } } private final void step5 ( ) { if ( k == k0 ) return ; switch ( b [ k - 1 ] ) { case 'a' : if ( ends ( "al" ) ) break ; return ; case 'c' : if ( ends ( "ance" ) ) break ; if ( ends ( "ence" ) ) break ; return ; case 'e' : if ( ends ( "er" ) ) break ; return ; case 'i' : if ( ends ( "ic" ) ) break ; return ; case 'l' : if ( ends ( "able" ) ) break ; if ( ends ( "ible" ) ) break ; return ; case 'n' : if ( ends ( "ant" ) ) break ; if ( ends ( "ement" ) ) break ; if ( ends ( "ment" ) ) break ; if ( ends ( "ent" ) ) break ; return ; case 'o' : if ( ends ( "ion" ) && j >= 0 && ( b [ j ] == 's' || b [ j ] == 't' ) ) break ; if ( ends ( "ou" ) ) break ; return ; case 's' : if ( ends ( "ism" ) ) break ; return ; case 't' : if ( ends ( "ate" ) ) break ; if ( ends ( "iti" ) ) break ; return ; case 'u' : if ( ends ( "ous" ) ) break ; return ; case 'v' : if ( ends ( "ive" ) ) break ; return ; case 'z' : if ( ends ( "ize" ) ) break ; return ; default : return ; } if ( m ( ) > 1 ) k = j ; } private final void step6 ( ) { j = k ; if ( b [ k ] == 'e' ) { int a = m ( ) ; if ( a > 1 || a == 1 && ! cvc ( k - 1 ) ) k -- ; } if ( b [ k ] == 'l' && doublec ( k ) && m ( ) > 1 ) k -- ; } public String stem ( String s ) { if ( stem ( s . toCharArray ( ) , s . length ( ) ) ) return toString ( ) ; else return s ; } public boolean stem ( char [ ] word ) { return stem ( word , word . length ) ; } public boolean stem ( char [ ] wordBuffer , int offset , int wordLen ) { reset ( ) ; if ( b . length < wordLen ) { char [ ] new_b = new char [ wordLen + EXTRA ] ; b = new_b ; } for ( int j = 0 ; j < wordLen ; j ++ ) b [ j ] = wordBuffer [ offset + j ] ; i = wordLen ; return stem ( 0 ) ; } public boolean stem ( char [ ] word , int wordLen ) { return stem ( word , 0 , wordLen ) ; } public boolean stem ( ) { return stem ( 0 ) ; } public boolean stem ( int i0 ) { k = i - 1 ; k0 = i0 ; if ( k > k0 + 1 ) { step1 ( ) ; step2 ( ) ; step3 ( ) ; step4 ( ) ; step5 ( ) ; step6 ( ) ; } if ( i != k + 1 ) dirty = true ; i = k + 1 ; return dirty ; } public static void main ( String [ ] args ) { PorterStemmer s = new PorterStemmer ( ) ; for ( int i = 0 ; i < args . length ; i ++ ) { try { InputStream in = new FileInputStream ( args [ i ] ) ; byte [ ] buffer = new byte [ 1024 ] ; int bufferLen , offset , ch ; bufferLen = in . read ( buffer ) ; offset = 0 ; s . reset ( ) ; while ( true ) { if ( offset < bufferLen ) ch = buffer [ offset ++ ] ; else { bufferLen = in . read ( buffer ) ; offset = 0 ; if ( bufferLen < 0 ) ch = - 1 ; else ch = buffer [ offset ++ ] ; } if ( Character . isLetter ( ( char ) ch ) ) { s . add ( Character . toLowerCase ( ( char ) ch ) ) ; } else { s . stem ( ) ; System . out . print ( s . toString ( ) ) ; s . reset ( ) ; if ( ch < 0 ) break ; else { System . out . print ( ( char ) ch ) ; } } } in . close ( ) ; } catch ( IOException e ) { System . out . println ( "error reading " + args [ i ] ) ; } } } } 	1
package org . apache . lucene . search . spans ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . util . ToStringUtils ; import java . io . IOException ; import java . util . ArrayList ; import java . util . Collection ; import java . util . Set ; public class SpanTermQuery extends SpanQuery { protected Term term ; public SpanTermQuery ( Term term ) { this . term = term ; } public Term getTerm ( ) { return term ; } public String getField ( ) { return term . field ( ) ; } public Collection getTerms ( ) { Collection terms = new ArrayList ( ) ; terms . add ( term ) ; return terms ; } public void extractTerms ( Set terms ) { terms . add ( term ) ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( term . field ( ) . equals ( field ) ) buffer . append ( term . text ( ) ) ; else buffer . append ( term . toString ( ) ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof SpanTermQuery ) ) return false ; SpanTermQuery other = ( SpanTermQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && this . term . equals ( other . term ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ term . hashCode ( ) ^ 0xD23FE494 ; } public Spans getSpans ( final IndexReader reader ) throws IOException { return new TermSpans ( reader . termPositions ( term ) , term ) ; } } 	0
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . CorruptIndexException ; import org . apache . lucene . index . Term ; import org . apache . lucene . document . Document ; public abstract class Searcher implements Searchable { public final Hits search ( Query query ) throws IOException { return search ( query , ( Filter ) null ) ; } public Hits search ( Query query , Filter filter ) throws IOException { return new Hits ( this , query , filter ) ; } public Hits search ( Query query , Sort sort ) throws IOException { return new Hits ( this , query , null , sort ) ; } public Hits search ( Query query , Filter filter , Sort sort ) throws IOException { return new Hits ( this , query , filter , sort ) ; } public TopFieldDocs search ( Query query , Filter filter , int n , Sort sort ) throws IOException { return search ( createWeight ( query ) , filter , n , sort ) ; } public void search ( Query query , HitCollector results ) throws IOException { search ( query , ( Filter ) null , results ) ; } public void search ( Query query , Filter filter , HitCollector results ) throws IOException { search ( createWeight ( query ) , filter , results ) ; } public TopDocs search ( Query query , Filter filter , int n ) throws IOException { return search ( createWeight ( query ) , filter , n ) ; } public Explanation explain ( Query query , int doc ) throws IOException { return explain ( createWeight ( query ) , doc ) ; } private Similarity similarity = Similarity . getDefault ( ) ; public void setSimilarity ( Similarity similarity ) { this . similarity = similarity ; } public Similarity getSimilarity ( ) { return this . similarity ; } protected Weight createWeight ( Query query ) throws IOException { return query . weight ( this ) ; } public int [ ] docFreqs ( Term [ ] terms ) throws IOException { int [ ] result = new int [ terms . length ] ; for ( int i = 0 ; i < terms . length ; i ++ ) { result [ i ] = docFreq ( terms [ i ] ) ; } return result ; } abstract public void search ( Weight weight , Filter filter , HitCollector results ) throws IOException ; abstract public void close ( ) throws IOException ; abstract public int docFreq ( Term term ) throws IOException ; abstract public int maxDoc ( ) throws IOException ; abstract public TopDocs search ( Weight weight , Filter filter , int n ) throws IOException ; abstract public Document doc ( int i ) throws CorruptIndexException , IOException ; abstract public Query rewrite ( Query query ) throws IOException ; abstract public Explanation explain ( Weight weight , int doc ) throws IOException ; abstract public TopFieldDocs search ( Weight weight , Filter filter , int n , Sort sort ) throws IOException ; } 	1
package org . apache . lucene . util ; public abstract class StringHelper { public static final int stringDifference ( String s1 , String s2 ) { int len1 = s1 . length ( ) ; int len2 = s2 . length ( ) ; int len = len1 < len2 ? len1 : len2 ; for ( int i = 0 ; i < len ; i ++ ) { if ( s1 . charAt ( i ) != s2 . charAt ( i ) ) { return i ; } } return len ; } private StringHelper ( ) { } } 	0
package org . apache . lucene . search . payloads ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermPositions ; import org . apache . lucene . search . * ; import org . apache . lucene . search . spans . SpanScorer ; import org . apache . lucene . search . spans . SpanTermQuery ; import org . apache . lucene . search . spans . SpanWeight ; import org . apache . lucene . search . spans . TermSpans ; import java . io . IOException ; public class BoostingTermQuery extends SpanTermQuery { public BoostingTermQuery ( Term term ) { super ( term ) ; } protected Weight createWeight ( Searcher searcher ) throws IOException { return new BoostingTermWeight ( this , searcher ) ; } protected class BoostingTermWeight extends SpanWeight implements Weight { public BoostingTermWeight ( BoostingTermQuery query , Searcher searcher ) throws IOException { super ( query , searcher ) ; } public Scorer scorer ( IndexReader reader ) throws IOException { return new BoostingSpanScorer ( ( TermSpans ) query . getSpans ( reader ) , this , similarity , reader . norms ( query . getField ( ) ) ) ; } class BoostingSpanScorer extends SpanScorer { byte [ ] payload = new byte [ 256 ] ; private TermPositions positions ; protected float payloadScore ; private int payloadsSeen ; public BoostingSpanScorer ( TermSpans spans , Weight weight , Similarity similarity , byte [ ] norms ) throws IOException { super ( spans , weight , similarity , norms ) ; positions = spans . getPositions ( ) ; } protected boolean setFreqCurrentDoc ( ) throws IOException { if ( ! more ) { return false ; } doc = spans . doc ( ) ; freq = 0.0f ; payloadScore = 0 ; payloadsSeen = 0 ; Similarity similarity1 = getSimilarity ( ) ; while ( more && doc == spans . doc ( ) ) { int matchLength = spans . end ( ) - spans . start ( ) ; freq += similarity1 . sloppyFreq ( matchLength ) ; processPayload ( similarity1 ) ; more = spans . next ( ) ; } return more || ( freq != 0 ) ; } protected void processPayload ( Similarity similarity ) throws IOException { if ( positions . isPayloadAvailable ( ) ) { payload = positions . getPayload ( payload , 0 ) ; payloadScore += similarity . scorePayload ( payload , 0 , positions . getPayloadLength ( ) ) ; payloadsSeen ++ ; } else { } } public float score ( ) throws IOException { return super . score ( ) * ( payloadsSeen > 0 ? ( payloadScore / payloadsSeen ) : 1 ) ; } public Explanation explain ( final int doc ) throws IOException { Explanation result = new Explanation ( ) ; Explanation nonPayloadExpl = super . explain ( doc ) ; result . addDetail ( nonPayloadExpl ) ; Explanation payloadBoost = new Explanation ( ) ; result . addDetail ( payloadBoost ) ; float avgPayloadScore = payloadScore / payloadsSeen ; payloadBoost . setValue ( avgPayloadScore ) ; payloadBoost . setDescription ( "scorePayload(...)" ) ; result . setValue ( nonPayloadExpl . getValue ( ) * avgPayloadScore ) ; result . setDescription ( "btq, product of:" ) ; return result ; } } } public boolean equals ( Object o ) { if ( ! ( o instanceof BoostingTermQuery ) ) return false ; BoostingTermQuery other = ( BoostingTermQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && this . term . equals ( other . term ) ; } } 	1
package org . apache . lucene . index ; public interface TermPositionVector extends TermFreqVector { public int [ ] getTermPositions ( int index ) ; public TermVectorOffsetInfo [ ] getOffsets ( int index ) ; } 	0
package org . apache . lucene . analysis ; import java . io . Reader ; public abstract class Analyzer { public abstract TokenStream tokenStream ( String fieldName , Reader reader ) ; public int getPositionIncrementGap ( String fieldName ) { return 0 ; } } 	1
package org . apache . lucene . util ; import java . io . IOException ; import org . apache . lucene . search . Scorer ; public class ScorerDocQueue { private final HeapedScorerDoc [ ] heap ; private final int maxSize ; private int size ; private class HeapedScorerDoc { Scorer scorer ; int doc ; HeapedScorerDoc ( Scorer s ) { this ( s , s . doc ( ) ) ; } HeapedScorerDoc ( Scorer scorer , int doc ) { this . scorer = scorer ; this . doc = doc ; } void adjust ( ) { doc = scorer . doc ( ) ; } } private HeapedScorerDoc topHSD ; public ScorerDocQueue ( int maxSize ) { size = 0 ; int heapSize = maxSize + 1 ; heap = new HeapedScorerDoc [ heapSize ] ; this . maxSize = maxSize ; topHSD = heap [ 1 ] ; } public final void put ( Scorer scorer ) { size ++ ; heap [ size ] = new HeapedScorerDoc ( scorer ) ; upHeap ( ) ; } public boolean insert ( Scorer scorer ) { if ( size < maxSize ) { put ( scorer ) ; return true ; } else { int docNr = scorer . doc ( ) ; if ( ( size > 0 ) && ( ! ( docNr < topHSD . doc ) ) ) { heap [ 1 ] = new HeapedScorerDoc ( scorer , docNr ) ; downHeap ( ) ; return true ; } else { return false ; } } } public final Scorer top ( ) { return topHSD . scorer ; } public final int topDoc ( ) { return topHSD . doc ; } public final float topScore ( ) throws IOException { return topHSD . scorer . score ( ) ; } public final boolean topNextAndAdjustElsePop ( ) throws IOException { return checkAdjustElsePop ( topHSD . scorer . next ( ) ) ; } public final boolean topSkipToAndAdjustElsePop ( int target ) throws IOException { return checkAdjustElsePop ( topHSD . scorer . skipTo ( target ) ) ; } private boolean checkAdjustElsePop ( boolean cond ) { if ( cond ) { topHSD . doc = topHSD . scorer . doc ( ) ; } else { heap [ 1 ] = heap [ size ] ; heap [ size ] = null ; size -- ; } downHeap ( ) ; return cond ; } public final Scorer pop ( ) { Scorer result = topHSD . scorer ; popNoResult ( ) ; return result ; } private final void popNoResult ( ) { heap [ 1 ] = heap [ size ] ; heap [ size ] = null ; size -- ; downHeap ( ) ; } public final void adjustTop ( ) { topHSD . adjust ( ) ; downHeap ( ) ; } public final int size ( ) { return size ; } public final void clear ( ) { for ( int i = 0 ; i <= size ; i ++ ) { heap [ i ] = null ; } size = 0 ; } private final void upHeap ( ) { int i = size ; HeapedScorerDoc node = heap [ i ] ; int j = i > > > 1 ; while ( ( j > 0 ) && ( node . doc < heap [ j ] . doc ) ) { heap [ i ] = heap [ j ] ; i = j ; j = j > > > 1 ; } heap [ i ] = node ; topHSD = heap [ 1 ] ; } private final void downHeap ( ) { int i = 1 ; HeapedScorerDoc node = heap [ i ] ; int j = i << 1 ; int k = j + 1 ; if ( ( k <= size ) && ( heap [ k ] . doc < heap [ j ] . doc ) ) { j = k ; } while ( ( j <= size ) && ( heap [ j ] . doc < node . doc ) ) { heap [ i ] = heap [ j ] ; i = j ; j = i << 1 ; k = j + 1 ; if ( k <= size && ( heap [ k ] . doc < heap [ j ] . doc ) ) { j = k ; } } heap [ i ] = node ; topHSD = heap [ 1 ] ; } } 	0
package org . apache . lucene . store ; import java . io . File ; import java . io . IOException ; public class SimpleFSLockFactory extends LockFactory { private File lockDir ; SimpleFSLockFactory ( ) throws IOException { this ( ( File ) null ) ; } public SimpleFSLockFactory ( File lockDir ) throws IOException { setLockDir ( lockDir ) ; } public SimpleFSLockFactory ( String lockDirName ) throws IOException { lockDir = new File ( lockDirName ) ; setLockDir ( lockDir ) ; } void setLockDir ( File lockDir ) throws IOException { this . lockDir = lockDir ; } public Lock makeLock ( String lockName ) { if ( lockPrefix != null ) { lockName = lockPrefix + "-" + lockName ; } return new SimpleFSLock ( lockDir , lockName ) ; } public void clearLock ( String lockName ) throws IOException { if ( lockDir . exists ( ) ) { if ( lockPrefix != null ) { lockName = lockPrefix + "-" + lockName ; } File lockFile = new File ( lockDir , lockName ) ; if ( lockFile . exists ( ) && ! lockFile . delete ( ) ) { throw new IOException ( "Cannot delete " + lockFile ) ; } } } } ; class SimpleFSLock extends Lock { File lockFile ; File lockDir ; public SimpleFSLock ( File lockDir , String lockFileName ) { this . lockDir = lockDir ; lockFile = new File ( lockDir , lockFileName ) ; } public boolean obtain ( ) throws IOException { if ( ! lockDir . exists ( ) ) { if ( ! lockDir . mkdirs ( ) ) throw new IOException ( "Cannot create directory: " + lockDir . getAbsolutePath ( ) ) ; } else if ( ! lockDir . isDirectory ( ) ) { throw new IOException ( "Found regular file where directory expected: " + lockDir . getAbsolutePath ( ) ) ; } return lockFile . createNewFile ( ) ; } public void release ( ) { lockFile . delete ( ) ; } public boolean isLocked ( ) { return lockFile . exists ( ) ; } public String toString ( ) { return "SimpleFSLock@" + lockFile ; } } 	1
package org . apache . lucene . search ; public class DefaultSimilarity extends Similarity { public float lengthNorm ( String fieldName , int numTerms ) { return ( float ) ( 1.0 / Math . sqrt ( numTerms ) ) ; } public float queryNorm ( float sumOfSquaredWeights ) { return ( float ) ( 1.0 / Math . sqrt ( sumOfSquaredWeights ) ) ; } public float tf ( float freq ) { return ( float ) Math . sqrt ( freq ) ; } public float sloppyFreq ( int distance ) { return 1.0f / ( distance + 1 ) ; } public float idf ( int docFreq , int numDocs ) { return ( float ) ( Math . log ( numDocs / ( double ) ( docFreq + 1 ) ) + 1.0 ) ; } public float coord ( int overlap , int maxOverlap ) { return overlap / ( float ) maxOverlap ; } } 	0
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . FieldCache ; import org . apache . lucene . search . function . DocValues ; import java . io . IOException ; public class IntFieldSource extends FieldCacheSource { private FieldCache . IntParser parser ; public IntFieldSource ( String field ) { this ( field , null ) ; } public IntFieldSource ( String field , FieldCache . IntParser parser ) { super ( field ) ; this . parser = parser ; } public String description ( ) { return "int(" + super . description ( ) + ')' ; } public DocValues getCachedFieldValues ( FieldCache cache , String field , IndexReader reader ) throws IOException { final int [ ] arr = ( parser == null ) ? cache . getInts ( reader , field ) : cache . getInts ( reader , field , parser ) ; return new DocValues ( reader . maxDoc ( ) ) { public float floatVal ( int doc ) { return ( float ) arr [ doc ] ; } public int intVal ( int doc ) { return arr [ doc ] ; } public String toString ( int doc ) { return description ( ) + '=' + intVal ( doc ) ; } Object getInnerArray ( ) { return arr ; } } ; } public boolean cachedFieldSourceEquals ( FieldCacheSource o ) { if ( o . getClass ( ) != IntFieldSource . class ) { return false ; } IntFieldSource other = ( IntFieldSource ) o ; return this . parser == null ? other . parser == null : this . parser . getClass ( ) == other . parser . getClass ( ) ; } public int cachedFieldSourceHashCode ( ) { return parser == null ? Integer . class . hashCode ( ) : parser . getClass ( ) . hashCode ( ) ; } } 	1
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; final class PhraseQueue extends PriorityQueue { PhraseQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object o1 , Object o2 ) { PhrasePositions pp1 = ( PhrasePositions ) o1 ; PhrasePositions pp2 = ( PhrasePositions ) o2 ; if ( pp1 . doc == pp2 . doc ) if ( pp1 . position == pp2 . position ) return pp1 . offset < pp2 . offset ; else return pp1 . position < pp2 . position ; else return pp1 . doc < pp2 . doc ; } } 	0
package org . apache . lucene . index ; import java . util . List ; import java . io . IOException ; public interface IndexDeletionPolicy { public void onInit ( List commits ) throws IOException ; public void onCommit ( List commits ) throws IOException ; } 	1
package org . apache . lucene . store ; public class AlreadyClosedException extends IllegalStateException { public AlreadyClosedException ( String message ) { super ( message ) ; } } 	0
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Explanation ; import org . apache . lucene . search . Query ; import org . apache . lucene . search . Scorer ; import org . apache . lucene . search . Searcher ; import org . apache . lucene . search . Similarity ; import org . apache . lucene . search . Weight ; import org . apache . lucene . util . ToStringUtils ; import java . util . Set ; public class MatchAllDocsQuery extends Query { public MatchAllDocsQuery ( ) { } private class MatchAllScorer extends Scorer { final IndexReader reader ; int id ; final int maxId ; final float score ; MatchAllScorer ( IndexReader reader , Similarity similarity , Weight w ) { super ( similarity ) ; this . reader = reader ; id = - 1 ; maxId = reader . maxDoc ( ) - 1 ; score = w . getValue ( ) ; } public Explanation explain ( int doc ) { return null ; } public int doc ( ) { return id ; } public boolean next ( ) { while ( id < maxId ) { id ++ ; if ( ! reader . isDeleted ( id ) ) { return true ; } } return false ; } public float score ( ) { return score ; } public boolean skipTo ( int target ) { id = target - 1 ; return next ( ) ; } } private class MatchAllDocsWeight implements Weight { private Similarity similarity ; private float queryWeight ; private float queryNorm ; public MatchAllDocsWeight ( Searcher searcher ) { this . similarity = searcher . getSimilarity ( ) ; } public String toString ( ) { return "weight(" + MatchAllDocsQuery . this + ")" ; } public Query getQuery ( ) { return MatchAllDocsQuery . this ; } public float getValue ( ) { return queryWeight ; } public float sumOfSquaredWeights ( ) { queryWeight = getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float queryNorm ) { this . queryNorm = queryNorm ; queryWeight *= this . queryNorm ; } public Scorer scorer ( IndexReader reader ) { return new MatchAllScorer ( reader , similarity , this ) ; } public Explanation explain ( IndexReader reader , int doc ) { Explanation queryExpl = new ComplexExplanation ( true , getValue ( ) , "MatchAllDocsQuery, product of:" ) ; if ( getBoost ( ) != 1.0f ) { queryExpl . addDetail ( new Explanation ( getBoost ( ) , "boost" ) ) ; } queryExpl . addDetail ( new Explanation ( queryNorm , "queryNorm" ) ) ; return queryExpl ; } } protected Weight createWeight ( Searcher searcher ) { return new MatchAllDocsWeight ( searcher ) ; } public void extractTerms ( Set terms ) { } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "MatchAllDocsQuery" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof MatchAllDocsQuery ) ) return false ; MatchAllDocsQuery other = ( MatchAllDocsQuery ) o ; return this . getBoost ( ) == other . getBoost ( ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ 0x1AA71190 ; } } 	1
package org . apache . lucene . search . spans ; import java . io . IOException ; public interface Spans { boolean next ( ) throws IOException ; boolean skipTo ( int target ) throws IOException ; int doc ( ) ; int start ( ) ; int end ( ) ; } 	0
package org . apache . lucene . index ; public interface IndexCommitPoint { public String getSegmentsFileName ( ) ; public void delete ( ) ; } 	1
package org . apache . lucene . index ; import java . io . IOException ; final class SegmentMergeInfo { Term term ; int base ; TermEnum termEnum ; IndexReader reader ; private TermPositions postings ; private int [ ] docMap ; SegmentMergeInfo ( int b , TermEnum te , IndexReader r ) throws IOException { base = b ; reader = r ; termEnum = te ; term = te . term ( ) ; } int [ ] getDocMap ( ) { if ( docMap == null ) { if ( reader . hasDeletions ( ) ) { int maxDoc = reader . maxDoc ( ) ; docMap = new int [ maxDoc ] ; int j = 0 ; for ( int i = 0 ; i < maxDoc ; i ++ ) { if ( reader . isDeleted ( i ) ) docMap [ i ] = - 1 ; else docMap [ i ] = j ++ ; } } } return docMap ; } TermPositions getPositions ( ) throws IOException { if ( postings == null ) { postings = reader . termPositions ( ) ; } return postings ; } final boolean next ( ) throws IOException { if ( termEnum . next ( ) ) { term = termEnum . term ( ) ; return true ; } else { term = null ; return false ; } } final void close ( ) throws IOException { termEnum . close ( ) ; if ( postings != null ) { postings . close ( ) ; } } } 	0
package org . apache . lucene . index ; import java . util . * ; class SegmentTermVector implements TermFreqVector { private String field ; private String terms [ ] ; private int termFreqs [ ] ; SegmentTermVector ( String field , String terms [ ] , int termFreqs [ ] ) { this . field = field ; this . terms = terms ; this . termFreqs = termFreqs ; } public String getField ( ) { return field ; } public String toString ( ) { StringBuffer sb = new StringBuffer ( ) ; sb . append ( '{' ) ; sb . append ( field ) . append ( ": " ) ; if ( terms != null ) { for ( int i = 0 ; i < terms . length ; i ++ ) { if ( i > 0 ) sb . append ( ", " ) ; sb . append ( terms [ i ] ) . append ( '/' ) . append ( termFreqs [ i ] ) ; } } sb . append ( '}' ) ; return sb . toString ( ) ; } public int size ( ) { return terms == null ? 0 : terms . length ; } public String [ ] getTerms ( ) { return terms ; } public int [ ] getTermFrequencies ( ) { return termFreqs ; } public int indexOf ( String termText ) { if ( terms == null ) return - 1 ; int res = Arrays . binarySearch ( terms , termText ) ; return res >= 0 ? res : - 1 ; } public int [ ] indexesOf ( String [ ] termNumbers , int start , int len ) { int res [ ] = new int [ len ] ; for ( int i = 0 ; i < len ; i ++ ) { res [ i ] = indexOf ( termNumbers [ start + i ] ) ; } return res ; } } 	1
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . util . BitVector ; import org . apache . lucene . store . IndexInput ; class SegmentTermDocs implements TermDocs { protected SegmentReader parent ; protected IndexInput freqStream ; protected int count ; protected int df ; protected BitVector deletedDocs ; int doc = 0 ; int freq ; private int skipInterval ; private int maxSkipLevels ; private DefaultSkipListReader skipListReader ; private long freqBasePointer ; private long proxBasePointer ; private long skipPointer ; private boolean haveSkipped ; protected boolean currentFieldStoresPayloads ; protected SegmentTermDocs ( SegmentReader parent ) { this . parent = parent ; this . freqStream = ( IndexInput ) parent . freqStream . clone ( ) ; this . deletedDocs = parent . deletedDocs ; this . skipInterval = parent . tis . getSkipInterval ( ) ; this . maxSkipLevels = parent . tis . getMaxSkipLevels ( ) ; } public void seek ( Term term ) throws IOException { TermInfo ti = parent . tis . get ( term ) ; seek ( ti , term ) ; } public void seek ( TermEnum termEnum ) throws IOException { TermInfo ti ; Term term ; if ( termEnum instanceof SegmentTermEnum && ( ( SegmentTermEnum ) termEnum ) . fieldInfos == parent . fieldInfos ) { SegmentTermEnum segmentTermEnum = ( ( SegmentTermEnum ) termEnum ) ; term = segmentTermEnum . term ( ) ; ti = segmentTermEnum . termInfo ( ) ; } else { term = termEnum . term ( ) ; ti = parent . tis . get ( term ) ; } seek ( ti , term ) ; } void seek ( TermInfo ti , Term term ) throws IOException { count = 0 ; FieldInfo fi = parent . fieldInfos . fieldInfo ( term . field ) ; currentFieldStoresPayloads = ( fi != null ) ? fi . storePayloads : false ; if ( ti == null ) { df = 0 ; } else { df = ti . docFreq ; doc = 0 ; freqBasePointer = ti . freqPointer ; proxBasePointer = ti . proxPointer ; skipPointer = freqBasePointer + ti . skipOffset ; freqStream . seek ( freqBasePointer ) ; haveSkipped = false ; } } public void close ( ) throws IOException { freqStream . close ( ) ; if ( skipListReader != null ) skipListReader . close ( ) ; } public final int doc ( ) { return doc ; } public final int freq ( ) { return freq ; } protected void skippingDoc ( ) throws IOException { } public boolean next ( ) throws IOException { while ( true ) { if ( count == df ) return false ; int docCode = freqStream . readVInt ( ) ; doc += docCode > > > 1 ; if ( ( docCode & 1 ) != 0 ) freq = 1 ; else freq = freqStream . readVInt ( ) ; count ++ ; if ( deletedDocs == null || ! deletedDocs . get ( doc ) ) break ; skippingDoc ( ) ; } return true ; } public int read ( final int [ ] docs , final int [ ] freqs ) throws IOException { final int length = docs . length ; int i = 0 ; while ( i < length && count < df ) { final int docCode = freqStream . readVInt ( ) ; doc += docCode > > > 1 ; if ( ( docCode & 1 ) != 0 ) freq = 1 ; else freq = freqStream . readVInt ( ) ; count ++ ; if ( deletedDocs == null || ! deletedDocs . get ( doc ) ) { docs [ i ] = doc ; freqs [ i ] = freq ; ++ i ; } } return i ; } protected void skipProx ( long proxPointer , int payloadLength ) throws IOException { } public boolean skipTo ( int target ) throws IOException { if ( df >= skipInterval ) { if ( skipListReader == null ) skipListReader = new DefaultSkipListReader ( ( IndexInput ) freqStream . clone ( ) , maxSkipLevels , skipInterval ) ; if ( ! haveSkipped ) { skipListReader . init ( skipPointer , freqBasePointer , proxBasePointer , df , currentFieldStoresPayloads ) ; haveSkipped = true ; } int newCount = skipListReader . skipTo ( target ) ; if ( newCount > count ) { freqStream . seek ( skipListReader . getFreqPointer ( ) ) ; skipProx ( skipListReader . getProxPointer ( ) , skipListReader . getPayloadLength ( ) ) ; doc = skipListReader . getDoc ( ) ; count = newCount ; } } do { if ( ! next ( ) ) return false ; } while ( target > doc ) ; return true ; } } 	0
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . function . DocValues ; import org . apache . lucene . util . ToStringUtils ; import java . io . IOException ; import java . io . Serializable ; public abstract class ValueSource implements Serializable { public abstract DocValues getValues ( IndexReader reader ) throws IOException ; public abstract String description ( ) ; public String toString ( ) { return description ( ) ; } public abstract boolean equals ( Object o ) ; public abstract int hashCode ( ) ; } 	1
package org . apache . lucene . analysis ; import java . io . Reader ; public final class WhitespaceAnalyzer extends Analyzer { public TokenStream tokenStream ( String fieldName , Reader reader ) { return new WhitespaceTokenizer ( reader ) ; } } 	0
package org . apache . lucene . search ; public class FieldDoc extends ScoreDoc { public Comparable [ ] fields ; public FieldDoc ( int doc , float score ) { super ( doc , score ) ; } public FieldDoc ( int doc , float score , Comparable [ ] fields ) { super ( doc , score ) ; this . fields = fields ; } } 	1
package org . apache . lucene . analysis . standard ; public class TokenMgrError extends Error { static final int LEXICAL_ERROR = 0 ; static final int STATIC_LEXER_ERROR = 1 ; static final int INVALID_LEXICAL_STATE = 2 ; static final int LOOP_DETECTED = 3 ; int errorCode ; protected static final String addEscapes ( String str ) { StringBuffer retval = new StringBuffer ( ) ; char ch ; for ( int i = 0 ; i < str . length ( ) ; i ++ ) { switch ( str . charAt ( i ) ) { case 0 : continue ; case '\b' : retval . append ( "\\b" ) ; continue ; case '\t' : retval . append ( "\\t" ) ; continue ; case '\n' : retval . append ( "\\n" ) ; continue ; case '\f' : retval . append ( "\\f" ) ; continue ; case '\r' : retval . append ( "\\r" ) ; continue ; case '\"' : retval . append ( "\\\"" ) ; continue ; case '\'' : retval . append ( "\\\'" ) ; continue ; case '\\' : retval . append ( "\\\\" ) ; continue ; default : if ( ( ch = str . charAt ( i ) ) < 0x20 || ch > 0x7e ) { String s = "0000" + Integer . toString ( ch , 16 ) ; retval . append ( "\\u" + s . substring ( s . length ( ) - 4 , s . length ( ) ) ) ; } else { retval . append ( ch ) ; } continue ; } } return retval . toString ( ) ; } protected static String LexicalError ( boolean EOFSeen , int lexState , int errorLine , int errorColumn , String errorAfter , char curChar ) { return ( "Lexical error at line " + errorLine + ", column " + errorColumn + ".  Encountered: " + ( EOFSeen ? "<EOF> " : ( "\"" + addEscapes ( String . valueOf ( curChar ) ) + "\"" ) + " (" + ( int ) curChar + "), " ) + "after : \"" + addEscapes ( errorAfter ) + "\"" ) ; } public String getMessage ( ) { return super . getMessage ( ) ; } public TokenMgrError ( ) { } public TokenMgrError ( String message , int reason ) { super ( message ) ; errorCode = reason ; } public TokenMgrError ( boolean EOFSeen , int lexState , int errorLine , int errorColumn , String errorAfter , char curChar , int reason ) { this ( LexicalError ( EOFSeen , lexState , errorLine , errorColumn , errorAfter , curChar ) , reason ) ; } } 	0
package org . apache . lucene . queryParser ; public class TokenMgrError extends Error { static final int LEXICAL_ERROR = 0 ; static final int STATIC_LEXER_ERROR = 1 ; static final int INVALID_LEXICAL_STATE = 2 ; static final int LOOP_DETECTED = 3 ; int errorCode ; protected static final String addEscapes ( String str ) { StringBuffer retval = new StringBuffer ( ) ; char ch ; for ( int i = 0 ; i < str . length ( ) ; i ++ ) { switch ( str . charAt ( i ) ) { case 0 : continue ; case '\b' : retval . append ( "\\b" ) ; continue ; case '\t' : retval . append ( "\\t" ) ; continue ; case '\n' : retval . append ( "\\n" ) ; continue ; case '\f' : retval . append ( "\\f" ) ; continue ; case '\r' : retval . append ( "\\r" ) ; continue ; case '\"' : retval . append ( "\\\"" ) ; continue ; case '\'' : retval . append ( "\\\'" ) ; continue ; case '\\' : retval . append ( "\\\\" ) ; continue ; default : if ( ( ch = str . charAt ( i ) ) < 0x20 || ch > 0x7e ) { String s = "0000" + Integer . toString ( ch , 16 ) ; retval . append ( "\\u" + s . substring ( s . length ( ) - 4 , s . length ( ) ) ) ; } else { retval . append ( ch ) ; } continue ; } } return retval . toString ( ) ; } protected static String LexicalError ( boolean EOFSeen , int lexState , int errorLine , int errorColumn , String errorAfter , char curChar ) { return ( "Lexical error at line " + errorLine + ", column " + errorColumn + ".  Encountered: " + ( EOFSeen ? "<EOF> " : ( "\"" + addEscapes ( String . valueOf ( curChar ) ) + "\"" ) + " (" + ( int ) curChar + "), " ) + "after : \"" + addEscapes ( errorAfter ) + "\"" ) ; } public String getMessage ( ) { return super . getMessage ( ) ; } public TokenMgrError ( ) { } public TokenMgrError ( String message , int reason ) { super ( message ) ; errorCode = reason ; } public TokenMgrError ( boolean EOFSeen , int lexState , int errorLine , int errorColumn , String errorAfter , char curChar , int reason ) { this ( LexicalError ( EOFSeen , lexState , errorLine , errorColumn , errorAfter , curChar ) , reason ) ; } } 	1
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . * ; final class ExactPhraseScorer extends PhraseScorer { ExactPhraseScorer ( Weight weight , TermPositions [ ] tps , int [ ] offsets , Similarity similarity , byte [ ] norms ) { super ( weight , tps , offsets , similarity , norms ) ; } protected final float phraseFreq ( ) throws IOException { pq . clear ( ) ; for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) { pp . firstPosition ( ) ; pq . put ( pp ) ; } pqToList ( ) ; int freq = 0 ; do { while ( first . position < last . position ) { do { if ( ! first . nextPosition ( ) ) return ( float ) freq ; } while ( first . position < last . position ) ; firstToLast ( ) ; } freq ++ ; } while ( last . nextPosition ( ) ) ; return ( float ) freq ; } } 	0
package org . apache . lucene . index ; import java . util . Vector ; import java . util . Iterator ; import java . util . Collection ; import java . io . IOException ; import org . apache . lucene . document . FieldSelector ; import org . apache . lucene . document . FieldSelectorResult ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; final class SegmentMerger { static final byte [ ] NORMS_HEADER = new byte [ ] { 'N' , 'R' , 'M' , - 1 } ; private Directory directory ; private String segment ; private int termIndexInterval = IndexWriter . DEFAULT_TERM_INDEX_INTERVAL ; private Vector readers = new Vector ( ) ; private FieldInfos fieldInfos ; private int mergedDocs ; SegmentMerger ( Directory dir , String name ) { directory = dir ; segment = name ; } SegmentMerger ( IndexWriter writer , String name ) { directory = writer . getDirectory ( ) ; segment = name ; termIndexInterval = writer . getTermIndexInterval ( ) ; } final void add ( IndexReader reader ) { readers . addElement ( reader ) ; } final IndexReader segmentReader ( int i ) { return ( IndexReader ) readers . elementAt ( i ) ; } final int merge ( ) throws CorruptIndexException , IOException { int value ; mergedDocs = mergeFields ( ) ; mergeTerms ( ) ; mergeNorms ( ) ; if ( fieldInfos . hasVectors ( ) ) mergeVectors ( ) ; return mergedDocs ; } final void closeReaders ( ) throws IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader reader = ( IndexReader ) readers . elementAt ( i ) ; reader . close ( ) ; } } final Vector createCompoundFile ( String fileName ) throws IOException { CompoundFileWriter cfsWriter = new CompoundFileWriter ( directory , fileName ) ; Vector files = new Vector ( IndexFileNames . COMPOUND_EXTENSIONS . length + 1 ) ; for ( int i = 0 ; i < IndexFileNames . COMPOUND_EXTENSIONS . length ; i ++ ) { files . add ( segment + "." + IndexFileNames . COMPOUND_EXTENSIONS [ i ] ) ; } for ( int i = 0 ; i < fieldInfos . size ( ) ; i ++ ) { FieldInfo fi = fieldInfos . fieldInfo ( i ) ; if ( fi . isIndexed && ! fi . omitNorms ) { files . add ( segment + "." + IndexFileNames . NORMS_EXTENSION ) ; break ; } } if ( fieldInfos . hasVectors ( ) ) { for ( int i = 0 ; i < IndexFileNames . VECTOR_EXTENSIONS . length ; i ++ ) { files . add ( segment + "." + IndexFileNames . VECTOR_EXTENSIONS [ i ] ) ; } } Iterator it = files . iterator ( ) ; while ( it . hasNext ( ) ) { cfsWriter . addFile ( ( String ) it . next ( ) ) ; } cfsWriter . close ( ) ; return files ; } private void addIndexed ( IndexReader reader , FieldInfos fieldInfos , Collection names , boolean storeTermVectors , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector , boolean storePayloads ) throws IOException { Iterator i = names . iterator ( ) ; while ( i . hasNext ( ) ) { String field = ( String ) i . next ( ) ; fieldInfos . add ( field , true , storeTermVectors , storePositionWithTermVector , storeOffsetWithTermVector , ! reader . hasNorms ( field ) , storePayloads ) ; } } private final int mergeFields ( ) throws CorruptIndexException , IOException { fieldInfos = new FieldInfos ( ) ; int docCount = 0 ; for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader reader = ( IndexReader ) readers . elementAt ( i ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . TERMVECTOR_WITH_POSITION_OFFSET ) , true , true , true , false ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . TERMVECTOR_WITH_POSITION ) , true , true , false , false ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . TERMVECTOR_WITH_OFFSET ) , true , false , true , false ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . TERMVECTOR ) , true , false , false , false ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . STORES_PAYLOADS ) , false , false , false , true ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . INDEXED ) , false , false , false , false ) ; fieldInfos . add ( reader . getFieldNames ( IndexReader . FieldOption . UNINDEXED ) , false ) ; } fieldInfos . write ( directory , segment + ".fnm" ) ; FieldsWriter fieldsWriter = new FieldsWriter ( directory , segment , fieldInfos ) ; FieldSelector fieldSelectorMerge = new FieldSelector ( ) { public FieldSelectorResult accept ( String fieldName ) { return FieldSelectorResult . LOAD_FOR_MERGE ; } } ; try { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader reader = ( IndexReader ) readers . elementAt ( i ) ; int maxDoc = reader . maxDoc ( ) ; for ( int j = 0 ; j < maxDoc ; j ++ ) if ( ! reader . isDeleted ( j ) ) { fieldsWriter . addDocument ( reader . document ( j , fieldSelectorMerge ) ) ; docCount ++ ; } } } finally { fieldsWriter . close ( ) ; } return docCount ; } private final void mergeVectors ( ) throws IOException { TermVectorsWriter termVectorsWriter = new TermVectorsWriter ( directory , segment , fieldInfos ) ; try { for ( int r = 0 ; r < readers . size ( ) ; r ++ ) { IndexReader reader = ( IndexReader ) readers . elementAt ( r ) ; int maxDoc = reader . maxDoc ( ) ; for ( int docNum = 0 ; docNum < maxDoc ; docNum ++ ) { if ( reader . isDeleted ( docNum ) ) continue ; termVectorsWriter . addAllDocVectors ( reader . getTermFreqVectors ( docNum ) ) ; } } } finally { termVectorsWriter . close ( ) ; } } private IndexOutput freqOutput = null ; private IndexOutput proxOutput = null ; private TermInfosWriter termInfosWriter = null ; private int skipInterval ; private int maxSkipLevels ; private SegmentMergeQueue queue = null ; private DefaultSkipListWriter skipListWriter = null ; private final void mergeTerms ( ) throws CorruptIndexException , IOException { try { freqOutput = directory . createOutput ( segment + ".frq" ) ; proxOutput = directory . createOutput ( segment + ".prx" ) ; termInfosWriter = new TermInfosWriter ( directory , segment , fieldInfos , termIndexInterval ) ; skipInterval = termInfosWriter . skipInterval ; maxSkipLevels = termInfosWriter . maxSkipLevels ; skipListWriter = new DefaultSkipListWriter ( skipInterval , maxSkipLevels , mergedDocs , freqOutput , proxOutput ) ; queue = new SegmentMergeQueue ( readers . size ( ) ) ; mergeTermInfos ( ) ; } finally { if ( freqOutput != null ) freqOutput . close ( ) ; if ( proxOutput != null ) proxOutput . close ( ) ; if ( termInfosWriter != null ) termInfosWriter . close ( ) ; if ( queue != null ) queue . close ( ) ; } } private final void mergeTermInfos ( ) throws CorruptIndexException , IOException { int base = 0 ; for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader reader = ( IndexReader ) readers . elementAt ( i ) ; TermEnum termEnum = reader . terms ( ) ; SegmentMergeInfo smi = new SegmentMergeInfo ( base , termEnum , reader ) ; base += reader . numDocs ( ) ; if ( smi . next ( ) ) queue . put ( smi ) ; else smi . close ( ) ; } SegmentMergeInfo [ ] match = new SegmentMergeInfo [ readers . size ( ) ] ; while ( queue . size ( ) > 0 ) { int matchSize = 0 ; match [ matchSize ++ ] = ( SegmentMergeInfo ) queue . pop ( ) ; Term term = match [ 0 ] . term ; SegmentMergeInfo top = ( SegmentMergeInfo ) queue . top ( ) ; while ( top != null && term . compareTo ( top . term ) == 0 ) { match [ matchSize ++ ] = ( SegmentMergeInfo ) queue . pop ( ) ; top = ( SegmentMergeInfo ) queue . top ( ) ; } mergeTermInfo ( match , matchSize ) ; while ( matchSize > 0 ) { SegmentMergeInfo smi = match [ -- matchSize ] ; if ( smi . next ( ) ) queue . put ( smi ) ; else smi . close ( ) ; } } } private final TermInfo termInfo = new TermInfo ( ) ; private final void mergeTermInfo ( SegmentMergeInfo [ ] smis , int n ) throws CorruptIndexException , IOException { long freqPointer = freqOutput . getFilePointer ( ) ; long proxPointer = proxOutput . getFilePointer ( ) ; int df = appendPostings ( smis , n ) ; long skipPointer = skipListWriter . writeSkip ( freqOutput ) ; if ( df > 0 ) { termInfo . set ( df , freqPointer , proxPointer , ( int ) ( skipPointer - freqPointer ) ) ; termInfosWriter . add ( smis [ 0 ] . term , termInfo ) ; } } private byte [ ] payloadBuffer = null ; private final int appendPostings ( SegmentMergeInfo [ ] smis , int n ) throws CorruptIndexException , IOException { int lastDoc = 0 ; int df = 0 ; skipListWriter . resetSkip ( ) ; boolean storePayloads = fieldInfos . fieldInfo ( smis [ 0 ] . term . field ) . storePayloads ; int lastPayloadLength = - 1 ; for ( int i = 0 ; i < n ; i ++ ) { SegmentMergeInfo smi = smis [ i ] ; TermPositions postings = smi . getPositions ( ) ; int base = smi . base ; int [ ] docMap = smi . getDocMap ( ) ; postings . seek ( smi . termEnum ) ; while ( postings . next ( ) ) { int doc = postings . doc ( ) ; if ( docMap != null ) doc = docMap [ doc ] ; doc += base ; if ( doc < 0 || ( df > 0 && doc <= lastDoc ) ) throw new CorruptIndexException ( "docs out of order (" + doc + " <= " + lastDoc + " )" ) ; df ++ ; if ( ( df % skipInterval ) == 0 ) { skipListWriter . setSkipData ( lastDoc , storePayloads , lastPayloadLength ) ; skipListWriter . bufferSkip ( df ) ; } int docCode = ( doc - lastDoc ) << 1 ; lastDoc = doc ; int freq = postings . freq ( ) ; if ( freq == 1 ) { freqOutput . writeVInt ( docCode | 1 ) ; } else { freqOutput . writeVInt ( docCode ) ; freqOutput . writeVInt ( freq ) ; } int lastPosition = 0 ; for ( int j = 0 ; j < freq ; j ++ ) { int position = postings . nextPosition ( ) ; int delta = position - lastPosition ; if ( storePayloads ) { int payloadLength = postings . getPayloadLength ( ) ; if ( payloadLength == lastPayloadLength ) { proxOutput . writeVInt ( delta * 2 ) ; } else { proxOutput . writeVInt ( delta * 2 + 1 ) ; proxOutput . writeVInt ( payloadLength ) ; lastPayloadLength = payloadLength ; } if ( payloadLength > 0 ) { if ( payloadBuffer == null || payloadBuffer . length < payloadLength ) { payloadBuffer = new byte [ payloadLength ] ; } postings . getPayload ( payloadBuffer , 0 ) ; proxOutput . writeBytes ( payloadBuffer , 0 , payloadLength ) ; } } else { proxOutput . writeVInt ( delta ) ; } lastPosition = position ; } } } return df ; } private void mergeNorms ( ) throws IOException { byte [ ] normBuffer = null ; IndexOutput output = null ; try { for ( int i = 0 ; i < fieldInfos . size ( ) ; i ++ ) { FieldInfo fi = fieldInfos . fieldInfo ( i ) ; if ( fi . isIndexed && ! fi . omitNorms ) { if ( output == null ) { output = directory . createOutput ( segment + "." + IndexFileNames . NORMS_EXTENSION ) ; output . writeBytes ( NORMS_HEADER , NORMS_HEADER . length ) ; } for ( int j = 0 ; j < readers . size ( ) ; j ++ ) { IndexReader reader = ( IndexReader ) readers . elementAt ( j ) ; int maxDoc = reader . maxDoc ( ) ; if ( normBuffer == null || normBuffer . length < maxDoc ) { normBuffer = new byte [ maxDoc ] ; } reader . norms ( fi . name , normBuffer , 0 ) ; if ( ! reader . hasDeletions ( ) ) { output . writeBytes ( normBuffer , maxDoc ) ; } else { for ( int k = 0 ; k < maxDoc ; k ++ ) { if ( ! reader . isDeleted ( k ) ) { output . writeByte ( normBuffer [ k ] ) ; } } } } } } } finally { if ( output != null ) { output . close ( ) ; } } } } 	1
package org . apache . lucene . search ; import java . io . IOException ; public class ReqExclScorer extends Scorer { private Scorer reqScorer , exclScorer ; public ReqExclScorer ( Scorer reqScorer , Scorer exclScorer ) { super ( null ) ; this . reqScorer = reqScorer ; this . exclScorer = exclScorer ; } private boolean firstTime = true ; public boolean next ( ) throws IOException { if ( firstTime ) { if ( ! exclScorer . next ( ) ) { exclScorer = null ; } firstTime = false ; } if ( reqScorer == null ) { return false ; } if ( ! reqScorer . next ( ) ) { reqScorer = null ; return false ; } if ( exclScorer == null ) { return true ; } return toNonExcluded ( ) ; } private boolean toNonExcluded ( ) throws IOException { int exclDoc = exclScorer . doc ( ) ; do { int reqDoc = reqScorer . doc ( ) ; if ( reqDoc < exclDoc ) { return true ; } else if ( reqDoc > exclDoc ) { if ( ! exclScorer . skipTo ( reqDoc ) ) { exclScorer = null ; return true ; } exclDoc = exclScorer . doc ( ) ; if ( exclDoc > reqDoc ) { return true ; } } } while ( reqScorer . next ( ) ) ; reqScorer = null ; return false ; } public int doc ( ) { return reqScorer . doc ( ) ; } public float score ( ) throws IOException { return reqScorer . score ( ) ; } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { firstTime = false ; if ( ! exclScorer . skipTo ( target ) ) { exclScorer = null ; } } if ( reqScorer == null ) { return false ; } if ( exclScorer == null ) { return reqScorer . skipTo ( target ) ; } if ( ! reqScorer . skipTo ( target ) ) { reqScorer = null ; return false ; } return toNonExcluded ( ) ; } public Explanation explain ( int doc ) throws IOException { Explanation res = new Explanation ( ) ; if ( exclScorer . skipTo ( doc ) && ( exclScorer . doc ( ) == doc ) ) { res . setDescription ( "excluded" ) ; } else { res . setDescription ( "not excluded" ) ; res . addDetail ( reqScorer . explain ( doc ) ) ; } return res ; } } 	0
package org . apache . lucene . search ; import java . io . IOException ; import java . util . ArrayList ; class DisjunctionMaxScorer extends Scorer { private ArrayList subScorers = new ArrayList ( ) ; private float tieBreakerMultiplier ; private boolean more = false ; private boolean firstTime = true ; public DisjunctionMaxScorer ( float tieBreakerMultiplier , Similarity similarity ) { super ( similarity ) ; this . tieBreakerMultiplier = tieBreakerMultiplier ; } public void add ( Scorer scorer ) throws IOException { if ( scorer . next ( ) ) { subScorers . add ( scorer ) ; more = true ; } } public boolean next ( ) throws IOException { if ( ! more ) return false ; if ( firstTime ) { heapify ( ) ; firstTime = false ; return true ; } int lastdoc = ( ( Scorer ) subScorers . get ( 0 ) ) . doc ( ) ; do { if ( ( ( Scorer ) subScorers . get ( 0 ) ) . next ( ) ) heapAdjust ( 0 ) ; else { heapRemoveRoot ( ) ; if ( subScorers . isEmpty ( ) ) return ( more = false ) ; } } while ( ( ( Scorer ) subScorers . get ( 0 ) ) . doc ( ) == lastdoc ) ; return true ; } public int doc ( ) { return ( ( Scorer ) subScorers . get ( 0 ) ) . doc ( ) ; } public float score ( ) throws IOException { int doc = ( ( Scorer ) subScorers . get ( 0 ) ) . doc ( ) ; float [ ] sum = { ( ( Scorer ) subScorers . get ( 0 ) ) . score ( ) } , max = { sum [ 0 ] } ; int size = subScorers . size ( ) ; scoreAll ( 1 , size , doc , sum , max ) ; scoreAll ( 2 , size , doc , sum , max ) ; return max [ 0 ] + ( sum [ 0 ] - max [ 0 ] ) * tieBreakerMultiplier ; } private void scoreAll ( int root , int size , int doc , float [ ] sum , float [ ] max ) throws IOException { if ( root < size && ( ( Scorer ) subScorers . get ( root ) ) . doc ( ) == doc ) { float sub = ( ( Scorer ) subScorers . get ( root ) ) . score ( ) ; sum [ 0 ] += sub ; max [ 0 ] = Math . max ( max [ 0 ] , sub ) ; scoreAll ( ( root << 1 ) + 1 , size , doc , sum , max ) ; scoreAll ( ( root << 1 ) + 2 , size , doc , sum , max ) ; } } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { if ( ! more ) return false ; heapify ( ) ; firstTime = false ; } while ( subScorers . size ( ) > 0 && ( ( Scorer ) subScorers . get ( 0 ) ) . doc ( ) < target ) { if ( ( ( Scorer ) subScorers . get ( 0 ) ) . skipTo ( target ) ) heapAdjust ( 0 ) ; else heapRemoveRoot ( ) ; } if ( ( subScorers . size ( ) == 0 ) ) return ( more = false ) ; return true ; } public Explanation explain ( int doc ) throws IOException { throw new UnsupportedOperationException ( ) ; } private void heapify ( ) { int size = subScorers . size ( ) ; for ( int i = ( size > > 1 ) - 1 ; i >= 0 ; i -- ) heapAdjust ( i ) ; } private void heapAdjust ( int root ) { Scorer scorer = ( Scorer ) subScorers . get ( root ) ; int doc = scorer . doc ( ) ; int i = root , size = subScorers . size ( ) ; while ( i <= ( size > > 1 ) - 1 ) { int lchild = ( i << 1 ) + 1 ; Scorer lscorer = ( Scorer ) subScorers . get ( lchild ) ; int ldoc = lscorer . doc ( ) ; int rdoc = Integer . MAX_VALUE , rchild = ( i << 1 ) + 2 ; Scorer rscorer = null ; if ( rchild < size ) { rscorer = ( Scorer ) subScorers . get ( rchild ) ; rdoc = rscorer . doc ( ) ; } if ( ldoc < doc ) { if ( rdoc < ldoc ) { subScorers . set ( i , rscorer ) ; subScorers . set ( rchild , scorer ) ; i = rchild ; } else { subScorers . set ( i , lscorer ) ; subScorers . set ( lchild , scorer ) ; i = lchild ; } } else if ( rdoc < doc ) { subScorers . set ( i , rscorer ) ; subScorers . set ( rchild , scorer ) ; i = rchild ; } else return ; } } private void heapRemoveRoot ( ) { int size = subScorers . size ( ) ; if ( size == 1 ) subScorers . remove ( 0 ) ; else { subScorers . set ( 0 , subScorers . get ( size - 1 ) ) ; subScorers . remove ( size - 1 ) ; heapAdjust ( 0 ) ; } } } 	1
package org . apache . lucene . analysis . standard ; public class ParseException extends java . io . IOException { public ParseException ( Token currentTokenVal , int [ ] [ ] expectedTokenSequencesVal , String [ ] tokenImageVal ) { super ( "" ) ; specialConstructor = true ; currentToken = currentTokenVal ; expectedTokenSequences = expectedTokenSequencesVal ; tokenImage = tokenImageVal ; } public ParseException ( ) { super ( ) ; specialConstructor = false ; } public ParseException ( String message ) { super ( message ) ; specialConstructor = false ; } protected boolean specialConstructor ; public Token currentToken ; public int [ ] [ ] expectedTokenSequences ; public String [ ] tokenImage ; public String getMessage ( ) { if ( ! specialConstructor ) { return super . getMessage ( ) ; } String expected = "" ; int maxSize = 0 ; for ( int i = 0 ; i < expectedTokenSequences . length ; i ++ ) { if ( maxSize < expectedTokenSequences [ i ] . length ) { maxSize = expectedTokenSequences [ i ] . length ; } for ( int j = 0 ; j < expectedTokenSequences [ i ] . length ; j ++ ) { expected += tokenImage [ expectedTokenSequences [ i ] [ j ] ] + " " ; } if ( expectedTokenSequences [ i ] [ expectedTokenSequences [ i ] . length - 1 ] != 0 ) { expected += "..." ; } expected += eol + "    " ; } String retval = "Encountered \"" ; Token tok = currentToken . next ; for ( int i = 0 ; i < maxSize ; i ++ ) { if ( i != 0 ) retval += " " ; if ( tok . kind == 0 ) { retval += tokenImage [ 0 ] ; break ; } retval += add_escapes ( tok . image ) ; tok = tok . next ; } retval += "\" at line " + currentToken . next . beginLine + ", column " + currentToken . next . beginColumn + "." + eol ; if ( expectedTokenSequences . length == 1 ) { retval += "Was expecting:" + eol + "    " ; } else { retval += "Was expecting one of:" + eol + "    " ; } retval += expected ; return retval ; } protected String eol = System . getProperty ( "line.separator" , "\n" ) ; protected String add_escapes ( String str ) { StringBuffer retval = new StringBuffer ( ) ; char ch ; for ( int i = 0 ; i < str . length ( ) ; i ++ ) { switch ( str . charAt ( i ) ) { case 0 : continue ; case '\b' : retval . append ( "\\b" ) ; continue ; case '\t' : retval . append ( "\\t" ) ; continue ; case '\n' : retval . append ( "\\n" ) ; continue ; case '\f' : retval . append ( "\\f" ) ; continue ; case '\r' : retval . append ( "\\r" ) ; continue ; case '\"' : retval . append ( "\\\"" ) ; continue ; case '\'' : retval . append ( "\\\'" ) ; continue ; case '\\' : retval . append ( "\\\\" ) ; continue ; default : if ( ( ch = str . charAt ( i ) ) < 0x20 || ch > 0x7e ) { String s = "0000" + Integer . toString ( ch , 16 ) ; retval . append ( "\\u" + s . substring ( s . length ( ) - 4 , s . length ( ) ) ) ; } else { retval . append ( ch ) ; } continue ; } } return retval . toString ( ) ; } } 	0
package org . apache . lucene . search ; public class SimilarityDelegator extends Similarity { private Similarity delegee ; public SimilarityDelegator ( Similarity delegee ) { this . delegee = delegee ; } public float lengthNorm ( String fieldName , int numTerms ) { return delegee . lengthNorm ( fieldName , numTerms ) ; } public float queryNorm ( float sumOfSquaredWeights ) { return delegee . queryNorm ( sumOfSquaredWeights ) ; } public float tf ( float freq ) { return delegee . tf ( freq ) ; } public float sloppyFreq ( int distance ) { return delegee . sloppyFreq ( distance ) ; } public float idf ( int docFreq , int numDocs ) { return delegee . idf ( docFreq , numDocs ) ; } public float coord ( int overlap , int maxOverlap ) { return delegee . coord ( overlap , maxOverlap ) ; } } 	1
package org . apache . lucene . index ; class SegmentTermPositionVector extends SegmentTermVector implements TermPositionVector { protected int [ ] [ ] positions ; protected TermVectorOffsetInfo [ ] [ ] offsets ; public static final int [ ] EMPTY_TERM_POS = new int [ 0 ] ; public SegmentTermPositionVector ( String field , String terms [ ] , int termFreqs [ ] , int [ ] [ ] positions , TermVectorOffsetInfo [ ] [ ] offsets ) { super ( field , terms , termFreqs ) ; this . offsets = offsets ; this . positions = positions ; } public TermVectorOffsetInfo [ ] getOffsets ( int index ) { TermVectorOffsetInfo [ ] result = TermVectorOffsetInfo . EMPTY_OFFSET_INFO ; if ( offsets == null ) return null ; if ( index >= 0 && index < offsets . length ) { result = offsets [ index ] ; } return result ; } public int [ ] getTermPositions ( int index ) { int [ ] result = EMPTY_TERM_POS ; if ( positions == null ) return null ; if ( index >= 0 && index < positions . length ) { result = positions [ index ] ; } return result ; } } 	0
package org . apache . lucene . store ; import java . io . IOException ; import java . util . HashSet ; import java . util . Enumeration ; public class SingleInstanceLockFactory extends LockFactory { private HashSet locks = new HashSet ( ) ; public Lock makeLock ( String lockName ) { return new SingleInstanceLock ( locks , lockName ) ; } public void clearLock ( String lockName ) throws IOException { synchronized ( locks ) { if ( locks . contains ( lockName ) ) { locks . remove ( lockName ) ; } } } } ; class SingleInstanceLock extends Lock { String lockName ; private HashSet locks ; public SingleInstanceLock ( HashSet locks , String lockName ) { this . locks = locks ; this . lockName = lockName ; } public boolean obtain ( ) throws IOException { synchronized ( locks ) { return locks . add ( lockName ) ; } } public void release ( ) { synchronized ( locks ) { locks . remove ( lockName ) ; } } public boolean isLocked ( ) { synchronized ( locks ) { return locks . contains ( lockName ) ; } } public String toString ( ) { return "SingleInstanceLock: " + lockName ; } } 	1
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; final class HitQueue extends PriorityQueue { HitQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object a , Object b ) { ScoreDoc hitA = ( ScoreDoc ) a ; ScoreDoc hitB = ( ScoreDoc ) b ; if ( hitA . score == hitB . score ) return hitA . doc > hitB . doc ; else return hitA . score < hitB . score ; } } 	0
package org . apache . lucene . search ; import java . io . Serializable ; import java . util . Locale ; public class SortField implements Serializable { public static final int SCORE = 0 ; public static final int DOC = 1 ; public static final int AUTO = 2 ; public static final int STRING = 3 ; public static final int INT = 4 ; public static final int FLOAT = 5 ; public static final int CUSTOM = 9 ; public static final SortField FIELD_SCORE = new SortField ( null , SCORE ) ; public static final SortField FIELD_DOC = new SortField ( null , DOC ) ; private String field ; private int type = AUTO ; private Locale locale ; boolean reverse = false ; private SortComparatorSource factory ; public SortField ( String field ) { this . field = field . intern ( ) ; } public SortField ( String field , boolean reverse ) { this . field = field . intern ( ) ; this . reverse = reverse ; } public SortField ( String field , int type ) { this . field = ( field != null ) ? field . intern ( ) : field ; this . type = type ; } public SortField ( String field , int type , boolean reverse ) { this . field = ( field != null ) ? field . intern ( ) : field ; this . type = type ; this . reverse = reverse ; } public SortField ( String field , Locale locale ) { this . field = field . intern ( ) ; this . type = STRING ; this . locale = locale ; } public SortField ( String field , Locale locale , boolean reverse ) { this . field = field . intern ( ) ; this . type = STRING ; this . locale = locale ; this . reverse = reverse ; } public SortField ( String field , SortComparatorSource comparator ) { this . field = ( field != null ) ? field . intern ( ) : field ; this . type = CUSTOM ; this . factory = comparator ; } public SortField ( String field , SortComparatorSource comparator , boolean reverse ) { this . field = ( field != null ) ? field . intern ( ) : field ; this . type = CUSTOM ; this . reverse = reverse ; this . factory = comparator ; } public String getField ( ) { return field ; } public int getType ( ) { return type ; } public Locale getLocale ( ) { return locale ; } public boolean getReverse ( ) { return reverse ; } public SortComparatorSource getFactory ( ) { return factory ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; switch ( type ) { case SCORE : buffer . append ( "<score>" ) ; break ; case DOC : buffer . append ( "<doc>" ) ; break ; case CUSTOM : buffer . append ( "<custom:\"" + field + "\": " + factory + ">" ) ; break ; default : buffer . append ( "\"" + field + "\"" ) ; break ; } if ( locale != null ) buffer . append ( "(" + locale + ")" ) ; if ( reverse ) buffer . append ( '!' ) ; return buffer . toString ( ) ; } } 	1
package org . apache . lucene . search ; import java . io . IOException ; class NonMatchingScorer extends Scorer { public NonMatchingScorer ( ) { super ( null ) ; } public int doc ( ) { throw new UnsupportedOperationException ( ) ; } public boolean next ( ) throws IOException { return false ; } public float score ( ) { throw new UnsupportedOperationException ( ) ; } public boolean skipTo ( int target ) { return false ; } public Explanation explain ( int doc ) { Explanation e = new Explanation ( ) ; e . setDescription ( "No document matches." ) ; return e ; } } 	0
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . IndexWriter ; import org . apache . lucene . index . Term ; import org . apache . lucene . util . SmallFloat ; import java . io . IOException ; import java . io . Serializable ; import java . util . Collection ; import java . util . Iterator ; public abstract class Similarity implements Serializable { private static Similarity defaultImpl = new DefaultSimilarity ( ) ; public static void setDefault ( Similarity similarity ) { Similarity . defaultImpl = similarity ; } public static Similarity getDefault ( ) { return Similarity . defaultImpl ; } private static final float [ ] NORM_TABLE = new float [ 256 ] ; static { for ( int i = 0 ; i < 256 ; i ++ ) NORM_TABLE [ i ] = SmallFloat . byte315ToFloat ( ( byte ) i ) ; } public static float decodeNorm ( byte b ) { return NORM_TABLE [ b & 0xFF ] ; } public static float [ ] getNormDecoder ( ) { return NORM_TABLE ; } public abstract float lengthNorm ( String fieldName , int numTokens ) ; public abstract float queryNorm ( float sumOfSquaredWeights ) ; public static byte encodeNorm ( float f ) { return SmallFloat . floatToByte315 ( f ) ; } public float tf ( int freq ) { return tf ( ( float ) freq ) ; } public abstract float sloppyFreq ( int distance ) ; public abstract float tf ( float freq ) ; public float idf ( Term term , Searcher searcher ) throws IOException { return idf ( searcher . docFreq ( term ) , searcher . maxDoc ( ) ) ; } public float idf ( Collection terms , Searcher searcher ) throws IOException { float idf = 0.0f ; Iterator i = terms . iterator ( ) ; while ( i . hasNext ( ) ) { idf += idf ( ( Term ) i . next ( ) , searcher ) ; } return idf ; } public abstract float idf ( int docFreq , int numDocs ) ; public abstract float coord ( int overlap , int maxOverlap ) ; public float scorePayload ( byte [ ] payload , int offset , int length ) { return 1 ; } } 	1
package org . apache . lucene . index ; import java . io . IOException ; public class StaleReaderException extends IOException { public StaleReaderException ( String message ) { super ( message ) ; } } 	0
package org . apache . lucene . store ; import java . io . IOException ; public abstract class Lock { public static long LOCK_POLL_INTERVAL = 1000 ; public abstract boolean obtain ( ) throws IOException ; protected Throwable failureReason ; public boolean obtain ( long lockWaitTimeout ) throws LockObtainFailedException , IOException { failureReason = null ; boolean locked = obtain ( ) ; int maxSleepCount = ( int ) ( lockWaitTimeout / LOCK_POLL_INTERVAL ) ; int sleepCount = 0 ; while ( ! locked ) { if ( sleepCount ++ == maxSleepCount ) { String reason = "Lock obtain timed out: " + this . toString ( ) ; if ( failureReason != null ) { reason += ": " + failureReason ; } LockObtainFailedException e = new LockObtainFailedException ( reason ) ; if ( failureReason != null ) { e . initCause ( failureReason ) ; } throw e ; } try { Thread . sleep ( LOCK_POLL_INTERVAL ) ; } catch ( InterruptedException e ) { throw new IOException ( e . toString ( ) ) ; } locked = obtain ( ) ; } return locked ; } public abstract void release ( ) ; public abstract boolean isLocked ( ) ; public abstract static class With { private Lock lock ; private long lockWaitTimeout ; public With ( Lock lock , long lockWaitTimeout ) { this . lock = lock ; this . lockWaitTimeout = lockWaitTimeout ; } protected abstract Object doBody ( ) throws IOException ; public Object run ( ) throws LockObtainFailedException , IOException { boolean locked = false ; try { locked = lock . obtain ( lockWaitTimeout ) ; return doBody ( ) ; } finally { if ( locked ) lock . release ( ) ; } } } } 	1
package org . apache . lucene . search . spans ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . search . * ; import java . io . IOException ; import java . util . HashSet ; import java . util . Iterator ; import java . util . Set ; public class SpanWeight implements Weight { protected Similarity similarity ; protected float value ; protected float idf ; protected float queryNorm ; protected float queryWeight ; protected Set terms ; protected SpanQuery query ; public SpanWeight ( SpanQuery query , Searcher searcher ) throws IOException { this . similarity = query . getSimilarity ( searcher ) ; this . query = query ; terms = new HashSet ( ) ; query . extractTerms ( terms ) ; idf = this . query . getSimilarity ( searcher ) . idf ( terms , searcher ) ; } public Query getQuery ( ) { return query ; } public float getValue ( ) { return value ; } public float sumOfSquaredWeights ( ) throws IOException { queryWeight = idf * query . getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float queryNorm ) { this . queryNorm = queryNorm ; queryWeight *= queryNorm ; value = queryWeight * idf ; } public Scorer scorer ( IndexReader reader ) throws IOException { return new SpanScorer ( query . getSpans ( reader ) , this , similarity , reader . norms ( query . getField ( ) ) ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { ComplexExplanation result = new ComplexExplanation ( ) ; result . setDescription ( "weight(" + getQuery ( ) + " in " + doc + "), product of:" ) ; String field = ( ( SpanQuery ) getQuery ( ) ) . getField ( ) ; StringBuffer docFreqs = new StringBuffer ( ) ; Iterator i = terms . iterator ( ) ; while ( i . hasNext ( ) ) { Term term = ( Term ) i . next ( ) ; docFreqs . append ( term . text ( ) ) ; docFreqs . append ( "=" ) ; docFreqs . append ( reader . docFreq ( term ) ) ; if ( i . hasNext ( ) ) { docFreqs . append ( " " ) ; } } Explanation idfExpl = new Explanation ( idf , "idf(" + field + ": " + docFreqs + ")" ) ; Explanation queryExpl = new Explanation ( ) ; queryExpl . setDescription ( "queryWeight(" + getQuery ( ) + "), product of:" ) ; Explanation boostExpl = new Explanation ( getQuery ( ) . getBoost ( ) , "boost" ) ; if ( getQuery ( ) . getBoost ( ) != 1.0f ) queryExpl . addDetail ( boostExpl ) ; queryExpl . addDetail ( idfExpl ) ; Explanation queryNormExpl = new Explanation ( queryNorm , "queryNorm" ) ; queryExpl . addDetail ( queryNormExpl ) ; queryExpl . setValue ( boostExpl . getValue ( ) * idfExpl . getValue ( ) * queryNormExpl . getValue ( ) ) ; result . addDetail ( queryExpl ) ; ComplexExplanation fieldExpl = new ComplexExplanation ( ) ; fieldExpl . setDescription ( "fieldWeight(" + field + ":" + query . toString ( field ) + " in " + doc + "), product of:" ) ; Explanation tfExpl = scorer ( reader ) . explain ( doc ) ; fieldExpl . addDetail ( tfExpl ) ; fieldExpl . addDetail ( idfExpl ) ; Explanation fieldNormExpl = new Explanation ( ) ; byte [ ] fieldNorms = reader . norms ( field ) ; float fieldNorm = fieldNorms != null ? Similarity . decodeNorm ( fieldNorms [ doc ] ) : 0.0f ; fieldNormExpl . setValue ( fieldNorm ) ; fieldNormExpl . setDescription ( "fieldNorm(field=" + field + ", doc=" + doc + ")" ) ; fieldExpl . addDetail ( fieldNormExpl ) ; fieldExpl . setMatch ( Boolean . valueOf ( tfExpl . isMatch ( ) ) ) ; fieldExpl . setValue ( tfExpl . getValue ( ) * idfExpl . getValue ( ) * fieldNormExpl . getValue ( ) ) ; result . addDetail ( fieldExpl ) ; result . setMatch ( fieldExpl . getMatch ( ) ) ; result . setValue ( queryExpl . getValue ( ) * fieldExpl . getValue ( ) ) ; if ( queryExpl . getValue ( ) == 1.0f ) return fieldExpl ; return result ; } } 	0
package org . apache . lucene . search ; public class TopDocs implements java . io . Serializable { public int totalHits ; public ScoreDoc [ ] scoreDocs ; private float maxScore ; public float getMaxScore ( ) { return maxScore ; } public void setMaxScore ( float maxScore ) { this . maxScore = maxScore ; } TopDocs ( int totalHits , ScoreDoc [ ] scoreDocs , float maxScore ) { this . totalHits = totalHits ; this . scoreDocs = scoreDocs ; this . maxScore = maxScore ; } } 	1
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import org . apache . lucene . store . IndexOutput ; class DefaultSkipListWriter extends MultiLevelSkipListWriter { private int [ ] lastSkipDoc ; private int [ ] lastSkipPayloadLength ; private long [ ] lastSkipFreqPointer ; private long [ ] lastSkipProxPointer ; private IndexOutput freqOutput ; private IndexOutput proxOutput ; private int curDoc ; private boolean curStorePayloads ; private int curPayloadLength ; private long curFreqPointer ; private long curProxPointer ; DefaultSkipListWriter ( int skipInterval , int numberOfSkipLevels , int docCount , IndexOutput freqOutput , IndexOutput proxOutput ) { super ( skipInterval , numberOfSkipLevels , docCount ) ; this . freqOutput = freqOutput ; this . proxOutput = proxOutput ; lastSkipDoc = new int [ numberOfSkipLevels ] ; lastSkipPayloadLength = new int [ numberOfSkipLevels ] ; lastSkipFreqPointer = new long [ numberOfSkipLevels ] ; lastSkipProxPointer = new long [ numberOfSkipLevels ] ; } void setSkipData ( int doc , boolean storePayloads , int payloadLength ) { this . curDoc = doc ; this . curStorePayloads = storePayloads ; this . curPayloadLength = payloadLength ; this . curFreqPointer = freqOutput . getFilePointer ( ) ; this . curProxPointer = proxOutput . getFilePointer ( ) ; } protected void resetSkip ( ) { super . resetSkip ( ) ; Arrays . fill ( lastSkipDoc , 0 ) ; Arrays . fill ( lastSkipPayloadLength , - 1 ) ; Arrays . fill ( lastSkipFreqPointer , freqOutput . getFilePointer ( ) ) ; Arrays . fill ( lastSkipProxPointer , proxOutput . getFilePointer ( ) ) ; } protected void writeSkipData ( int level , IndexOutput skipBuffer ) throws IOException { if ( curStorePayloads ) { int delta = curDoc - lastSkipDoc [ level ] ; if ( curPayloadLength == lastSkipPayloadLength [ level ] ) { skipBuffer . writeVInt ( delta * 2 ) ; } else { skipBuffer . writeVInt ( delta * 2 + 1 ) ; skipBuffer . writeVInt ( curPayloadLength ) ; lastSkipPayloadLength [ level ] = curPayloadLength ; } } else { skipBuffer . writeVInt ( curDoc - lastSkipDoc [ level ] ) ; } skipBuffer . writeVInt ( ( int ) ( curFreqPointer - lastSkipFreqPointer [ level ] ) ) ; skipBuffer . writeVInt ( ( int ) ( curProxPointer - lastSkipProxPointer [ level ] ) ) ; lastSkipDoc [ level ] = curDoc ; lastSkipFreqPointer [ level ] = curFreqPointer ; lastSkipProxPointer [ level ] = curProxPointer ; } } 	0
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; public class TopFieldDocCollector extends TopDocCollector { public TopFieldDocCollector ( IndexReader reader , Sort sort , int numHits ) throws IOException { super ( numHits , new FieldSortedHitQueue ( reader , sort . fields , numHits ) ) ; } public void collect ( int doc , float score ) { if ( score > 0.0f ) { totalHits ++ ; hq . insert ( new FieldDoc ( doc , score ) ) ; } } public TopDocs topDocs ( ) { FieldSortedHitQueue fshq = ( FieldSortedHitQueue ) hq ; ScoreDoc [ ] scoreDocs = new ScoreDoc [ fshq . size ( ) ] ; for ( int i = fshq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = fshq . fillFields ( ( FieldDoc ) fshq . pop ( ) ) ; return new TopFieldDocs ( totalHits , scoreDocs , fshq . getFields ( ) , fshq . getMaxScore ( ) ) ; } } 	1
package org . apache . lucene . analysis . standard ; public interface StandardTokenizerConstants { int EOF = 0 ; int ALPHANUM = 1 ; int APOSTROPHE = 2 ; int ACRONYM = 3 ; int COMPANY = 4 ; int EMAIL = 5 ; int HOST = 6 ; int NUM = 7 ; int P = 8 ; int HAS_DIGIT = 9 ; int ALPHA = 10 ; int LETTER = 11 ; int CJ = 12 ; int KOREAN = 13 ; int DIGIT = 14 ; int NOISE = 15 ; int DEFAULT = 0 ; String [ ] tokenImage = { "<EOF>" , "<ALPHANUM>" , "<APOSTROPHE>" , "<ACRONYM>" , "<COMPANY>" , "<EMAIL>" , "<HOST>" , "<NUM>" , "<P>" , "<HAS_DIGIT>" , "<ALPHA>" , "<LETTER>" , "<CJ>" , "<KOREAN>" , "<DIGIT>" , "<NOISE>" , } ; } 	0
package org . apache . lucene . analysis ; import java . io . Reader ; import java . io . IOException ; public abstract class Tokenizer extends TokenStream { protected Reader input ; protected Tokenizer ( ) { } protected Tokenizer ( Reader input ) { this . input = input ; } public void close ( ) throws IOException { input . close ( ) ; } } 	1
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; public abstract class SortComparator implements SortComparatorSource { public ScoreDocComparator newComparator ( final IndexReader reader , final String fieldname ) throws IOException { final String field = fieldname . intern ( ) ; final Comparable [ ] cachedValues = FieldCache . DEFAULT . getCustom ( reader , field , SortComparator . this ) ; return new ScoreDocComparator ( ) { public int compare ( ScoreDoc i , ScoreDoc j ) { return cachedValues [ i . doc ] . compareTo ( cachedValues [ j . doc ] ) ; } public Comparable sortValue ( ScoreDoc i ) { return cachedValues [ i . doc ] ; } public int sortType ( ) { return SortField . CUSTOM ; } } ; } protected abstract Comparable getComparable ( String termtext ) ; } 	0
package org . apache . lucene . search ; import java . util . ArrayList ; public class ComplexExplanation extends Explanation { private Boolean match ; public ComplexExplanation ( ) { super ( ) ; } public ComplexExplanation ( boolean match , float value , String description ) { super ( value , description ) ; this . match = Boolean . valueOf ( match ) ; } public Boolean getMatch ( ) { return match ; } public void setMatch ( Boolean match ) { this . match = match ; } public boolean isMatch ( ) { Boolean m = getMatch ( ) ; return ( null != m ? m . booleanValue ( ) : super . isMatch ( ) ) ; } protected String getSummary ( ) { if ( null == getMatch ( ) ) return super . getSummary ( ) ; return getValue ( ) + " = " + ( isMatch ( ) ? "(MATCH) " : "(NON-MATCH) " ) + getDescription ( ) ; } } 	1
package org . apache . lucene . index ; import java . io . IOException ; public abstract class TermEnum { public abstract boolean next ( ) throws IOException ; public abstract Term term ( ) ; public abstract int docFreq ( ) ; public abstract void close ( ) throws IOException ; public boolean skipTo ( Term target ) throws IOException { do { if ( ! next ( ) ) return false ; } while ( target . compareTo ( term ( ) ) > 0 ) ; return true ; } } 	0
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . document . Document ; import org . apache . lucene . index . CorruptIndexException ; public class Hit implements java . io . Serializable { private Document doc = null ; private boolean resolved = false ; private Hits hits = null ; private int hitNumber ; Hit ( Hits hits , int hitNumber ) { this . hits = hits ; this . hitNumber = hitNumber ; } public Document getDocument ( ) throws CorruptIndexException , IOException { if ( ! resolved ) fetchTheHit ( ) ; return doc ; } public float getScore ( ) throws IOException { return hits . score ( hitNumber ) ; } public int getId ( ) throws IOException { return hits . id ( hitNumber ) ; } private void fetchTheHit ( ) throws CorruptIndexException , IOException { doc = hits . doc ( hitNumber ) ; resolved = true ; } public float getBoost ( ) throws CorruptIndexException , IOException { return getDocument ( ) . getBoost ( ) ; } public String get ( String name ) throws CorruptIndexException , IOException { return getDocument ( ) . get ( name ) ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "Hit<" ) ; buffer . append ( hits . toString ( ) ) ; buffer . append ( " [" ) ; buffer . append ( hitNumber ) ; buffer . append ( "] " ) ; if ( resolved ) { buffer . append ( "resolved" ) ; } else { buffer . append ( "unresolved" ) ; } buffer . append ( ">" ) ; return buffer . toString ( ) ; } } 	1
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . TermDocs ; final class TermScorer extends Scorer { private Weight weight ; private TermDocs termDocs ; private byte [ ] norms ; private float weightValue ; private int doc ; private final int [ ] docs = new int [ 32 ] ; private final int [ ] freqs = new int [ 32 ] ; private int pointer ; private int pointerMax ; private static final int SCORE_CACHE_SIZE = 32 ; private float [ ] scoreCache = new float [ SCORE_CACHE_SIZE ] ; TermScorer ( Weight weight , TermDocs td , Similarity similarity , byte [ ] norms ) { super ( similarity ) ; this . weight = weight ; this . termDocs = td ; this . norms = norms ; this . weightValue = weight . getValue ( ) ; for ( int i = 0 ; i < SCORE_CACHE_SIZE ; i ++ ) scoreCache [ i ] = getSimilarity ( ) . tf ( i ) * weightValue ; } public void score ( HitCollector hc ) throws IOException { next ( ) ; score ( hc , Integer . MAX_VALUE ) ; } protected boolean score ( HitCollector c , int end ) throws IOException { Similarity similarity = getSimilarity ( ) ; float [ ] normDecoder = Similarity . getNormDecoder ( ) ; while ( doc < end ) { int f = freqs [ pointer ] ; float score = f < SCORE_CACHE_SIZE ? scoreCache [ f ] : similarity . tf ( f ) * weightValue ; score *= normDecoder [ norms [ doc ] & 0xFF ] ; c . collect ( doc , score ) ; if ( ++ pointer >= pointerMax ) { pointerMax = termDocs . read ( docs , freqs ) ; if ( pointerMax != 0 ) { pointer = 0 ; } else { termDocs . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } } doc = docs [ pointer ] ; } return true ; } public int doc ( ) { return doc ; } public boolean next ( ) throws IOException { pointer ++ ; if ( pointer >= pointerMax ) { pointerMax = termDocs . read ( docs , freqs ) ; if ( pointerMax != 0 ) { pointer = 0 ; } else { termDocs . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } } doc = docs [ pointer ] ; return true ; } public float score ( ) { int f = freqs [ pointer ] ; float raw = f < SCORE_CACHE_SIZE ? scoreCache [ f ] : getSimilarity ( ) . tf ( f ) * weightValue ; return raw * Similarity . decodeNorm ( norms [ doc ] ) ; } public boolean skipTo ( int target ) throws IOException { for ( pointer ++ ; pointer < pointerMax ; pointer ++ ) { if ( docs [ pointer ] >= target ) { doc = docs [ pointer ] ; return true ; } } boolean result = termDocs . skipTo ( target ) ; if ( result ) { pointerMax = 1 ; pointer = 0 ; docs [ pointer ] = doc = termDocs . doc ( ) ; freqs [ pointer ] = termDocs . freq ( ) ; } else { doc = Integer . MAX_VALUE ; } return result ; } public Explanation explain ( int doc ) throws IOException { TermQuery query = ( TermQuery ) weight . getQuery ( ) ; Explanation tfExplanation = new Explanation ( ) ; int tf = 0 ; while ( pointer < pointerMax ) { if ( docs [ pointer ] == doc ) tf = freqs [ pointer ] ; pointer ++ ; } if ( tf == 0 ) { if ( termDocs . skipTo ( doc ) ) { if ( termDocs . doc ( ) == doc ) { tf = termDocs . freq ( ) ; } } } termDocs . close ( ) ; tfExplanation . setValue ( getSimilarity ( ) . tf ( tf ) ) ; tfExplanation . setDescription ( "tf(termFreq(" + query . getTerm ( ) + ")=" + tf + ")" ) ; return tfExplanation ; } public String toString ( ) { return "scorer(" + weight + ")" ; } } 	0
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . List ; import java . util . ArrayList ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . PriorityQueue ; class NearSpansUnordered implements Spans { private SpanNearQuery query ; private List ordered = new ArrayList ( ) ; private int slop ; private SpansCell first ; private SpansCell last ; private int totalLength ; private CellQueue queue ; private SpansCell max ; private boolean more = true ; private boolean firstTime = true ; private class CellQueue extends PriorityQueue { public CellQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object o1 , Object o2 ) { SpansCell spans1 = ( SpansCell ) o1 ; SpansCell spans2 = ( SpansCell ) o2 ; if ( spans1 . doc ( ) == spans2 . doc ( ) ) { return NearSpansOrdered . docSpansOrdered ( spans1 , spans2 ) ; } else { return spans1 . doc ( ) < spans2 . doc ( ) ; } } } private class SpansCell implements Spans { private Spans spans ; private SpansCell next ; private int length = - 1 ; private int index ; public SpansCell ( Spans spans , int index ) { this . spans = spans ; this . index = index ; } public boolean next ( ) throws IOException { return adjust ( spans . next ( ) ) ; } public boolean skipTo ( int target ) throws IOException { return adjust ( spans . skipTo ( target ) ) ; } private boolean adjust ( boolean condition ) { if ( length != - 1 ) { totalLength -= length ; } if ( condition ) { length = end ( ) - start ( ) ; totalLength += length ; if ( max == null || doc ( ) > max . doc ( ) || ( doc ( ) == max . doc ( ) ) && ( end ( ) > max . end ( ) ) ) { max = this ; } } more = condition ; return condition ; } public int doc ( ) { return spans . doc ( ) ; } public int start ( ) { return spans . start ( ) ; } public int end ( ) { return spans . end ( ) ; } public String toString ( ) { return spans . toString ( ) + "#" + index ; } } public NearSpansUnordered ( SpanNearQuery query , IndexReader reader ) throws IOException { this . query = query ; this . slop = query . getSlop ( ) ; SpanQuery [ ] clauses = query . getClauses ( ) ; queue = new CellQueue ( clauses . length ) ; for ( int i = 0 ; i < clauses . length ; i ++ ) { SpansCell cell = new SpansCell ( clauses [ i ] . getSpans ( reader ) , i ) ; ordered . add ( cell ) ; } } public boolean next ( ) throws IOException { if ( firstTime ) { initList ( true ) ; listToQueue ( ) ; firstTime = false ; } else if ( more ) { if ( min ( ) . next ( ) ) { queue . adjustTop ( ) ; } else { more = false ; } } while ( more ) { boolean queueStale = false ; if ( min ( ) . doc ( ) != max . doc ( ) ) { queueToList ( ) ; queueStale = true ; } while ( more && first . doc ( ) < last . doc ( ) ) { more = first . skipTo ( last . doc ( ) ) ; firstToLast ( ) ; queueStale = true ; } if ( ! more ) return false ; if ( queueStale ) { listToQueue ( ) ; queueStale = false ; } if ( atMatch ( ) ) { return true ; } more = min ( ) . next ( ) ; if ( more ) { queue . adjustTop ( ) ; } } return false ; } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { initList ( false ) ; for ( SpansCell cell = first ; more && cell != null ; cell = cell . next ) { more = cell . skipTo ( target ) ; } if ( more ) { listToQueue ( ) ; } firstTime = false ; } else { while ( more && min ( ) . doc ( ) < target ) { if ( min ( ) . skipTo ( target ) ) { queue . adjustTop ( ) ; } else { more = false ; } } } return more && ( atMatch ( ) || next ( ) ) ; } private SpansCell min ( ) { return ( SpansCell ) queue . top ( ) ; } public int doc ( ) { return min ( ) . doc ( ) ; } public int start ( ) { return min ( ) . start ( ) ; } public int end ( ) { return max . end ( ) ; } public String toString ( ) { return getClass ( ) . getName ( ) + "(" + query . toString ( ) + ")@" + ( firstTime ? "START" : ( more ? ( doc ( ) + ":" + start ( ) + "-" + end ( ) ) : "END" ) ) ; } private void initList ( boolean next ) throws IOException { for ( int i = 0 ; more && i < ordered . size ( ) ; i ++ ) { SpansCell cell = ( SpansCell ) ordered . get ( i ) ; if ( next ) more = cell . next ( ) ; if ( more ) { addToList ( cell ) ; } } } private void addToList ( SpansCell cell ) { if ( last != null ) { last . next = cell ; } else first = cell ; last = cell ; cell . next = null ; } private void firstToLast ( ) { last . next = first ; last = first ; first = first . next ; last . next = null ; } private void queueToList ( ) { last = first = null ; while ( queue . top ( ) != null ) { addToList ( ( SpansCell ) queue . pop ( ) ) ; } } private void listToQueue ( ) { queue . clear ( ) ; for ( SpansCell cell = first ; cell != null ; cell = cell . next ) { queue . put ( cell ) ; } } private boolean atMatch ( ) { return ( min ( ) . doc ( ) == max . doc ( ) ) && ( ( max . end ( ) - min ( ) . start ( ) - totalLength ) <= slop ) ; } } 	1
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermEnum ; public abstract class FilteredTermEnum extends TermEnum { private Term currentTerm = null ; private TermEnum actualEnum = null ; public FilteredTermEnum ( ) { } protected abstract boolean termCompare ( Term term ) ; public abstract float difference ( ) ; protected abstract boolean endEnum ( ) ; protected void setEnum ( TermEnum actualEnum ) throws IOException { this . actualEnum = actualEnum ; Term term = actualEnum . term ( ) ; if ( term != null && termCompare ( term ) ) currentTerm = term ; else next ( ) ; } public int docFreq ( ) { if ( actualEnum == null ) return - 1 ; return actualEnum . docFreq ( ) ; } public boolean next ( ) throws IOException { if ( actualEnum == null ) return false ; currentTerm = null ; while ( currentTerm == null ) { if ( endEnum ( ) ) return false ; if ( actualEnum . next ( ) ) { Term term = actualEnum . term ( ) ; if ( termCompare ( term ) ) { currentTerm = term ; return true ; } } else return false ; } currentTerm = null ; return false ; } public Term term ( ) { return currentTerm ; } public void close ( ) throws IOException { actualEnum . close ( ) ; currentTerm = null ; actualEnum = null ; } } 	0
package org . apache . lucene . document ; import java . io . Reader ; import java . io . Serializable ; import org . apache . lucene . analysis . TokenStream ; public interface Fieldable extends Serializable { void setBoost ( float boost ) ; float getBoost ( ) ; String name ( ) ; public String stringValue ( ) ; public Reader readerValue ( ) ; public byte [ ] binaryValue ( ) ; public TokenStream tokenStreamValue ( ) ; boolean isStored ( ) ; boolean isIndexed ( ) ; boolean isTokenized ( ) ; boolean isCompressed ( ) ; boolean isTermVectorStored ( ) ; boolean isStoreOffsetWithTermVector ( ) ; boolean isStorePositionWithTermVector ( ) ; boolean isBinary ( ) ; boolean getOmitNorms ( ) ; void setOmitNorms ( boolean omitNorms ) ; boolean isLazy ( ) ; } 	1
package org . apache . lucene . index ; import java . io . IOException ; public interface TermDocs { void seek ( Term term ) throws IOException ; void seek ( TermEnum termEnum ) throws IOException ; int doc ( ) ; int freq ( ) ; boolean next ( ) throws IOException ; int read ( int [ ] docs , int [ ] freqs ) throws IOException ; boolean skipTo ( int target ) throws IOException ; void close ( ) throws IOException ; } 	0
package org . apache . lucene . search ; import java . io . IOException ; import java . util . Vector ; import java . util . Iterator ; import org . apache . lucene . document . Document ; import org . apache . lucene . index . CorruptIndexException ; public final class Hits { private Weight weight ; private Searcher searcher ; private Filter filter = null ; private Sort sort = null ; private int length ; private Vector hitDocs = new Vector ( ) ; private HitDoc first ; private HitDoc last ; private int numDocs = 0 ; private int maxDocs = 200 ; Hits ( Searcher s , Query q , Filter f ) throws IOException { weight = q . weight ( s ) ; searcher = s ; filter = f ; getMoreDocs ( 50 ) ; } Hits ( Searcher s , Query q , Filter f , Sort o ) throws IOException { weight = q . weight ( s ) ; searcher = s ; filter = f ; sort = o ; getMoreDocs ( 50 ) ; } private final void getMoreDocs ( int min ) throws IOException { if ( hitDocs . size ( ) > min ) { min = hitDocs . size ( ) ; } int n = min * 2 ; TopDocs topDocs = ( sort == null ) ? searcher . search ( weight , filter , n ) : searcher . search ( weight , filter , n , sort ) ; length = topDocs . totalHits ; ScoreDoc [ ] scoreDocs = topDocs . scoreDocs ; float scoreNorm = 1.0f ; if ( length > 0 && topDocs . getMaxScore ( ) > 1.0f ) { scoreNorm = 1.0f / topDocs . getMaxScore ( ) ; } int end = scoreDocs . length < length ? scoreDocs . length : length ; for ( int i = hitDocs . size ( ) ; i < end ; i ++ ) { hitDocs . addElement ( new HitDoc ( scoreDocs [ i ] . score * scoreNorm , scoreDocs [ i ] . doc ) ) ; } } public final int length ( ) { return length ; } public final Document doc ( int n ) throws CorruptIndexException , IOException { HitDoc hitDoc = hitDoc ( n ) ; remove ( hitDoc ) ; addToFront ( hitDoc ) ; if ( numDocs > maxDocs ) { HitDoc oldLast = last ; remove ( last ) ; oldLast . doc = null ; } if ( hitDoc . doc == null ) { hitDoc . doc = searcher . doc ( hitDoc . id ) ; } return hitDoc . doc ; } public final float score ( int n ) throws IOException { return hitDoc ( n ) . score ; } public final int id ( int n ) throws IOException { return hitDoc ( n ) . id ; } public Iterator iterator ( ) { return new HitIterator ( this ) ; } private final HitDoc hitDoc ( int n ) throws IOException { if ( n >= length ) { throw new IndexOutOfBoundsException ( "Not a valid hit number: " + n ) ; } if ( n >= hitDocs . size ( ) ) { getMoreDocs ( n ) ; } return ( HitDoc ) hitDocs . elementAt ( n ) ; } private final void addToFront ( HitDoc hitDoc ) { if ( first == null ) { last = hitDoc ; } else { first . prev = hitDoc ; } hitDoc . next = first ; first = hitDoc ; hitDoc . prev = null ; numDocs ++ ; } private final void remove ( HitDoc hitDoc ) { if ( hitDoc . doc == null ) { return ; } if ( hitDoc . next == null ) { last = hitDoc . prev ; } else { hitDoc . next . prev = hitDoc . prev ; } if ( hitDoc . prev == null ) { first = hitDoc . next ; } else { hitDoc . prev . next = hitDoc . next ; } numDocs -- ; } } final class HitDoc { float score ; int id ; Document doc = null ; HitDoc next ; HitDoc prev ; HitDoc ( float s , int i ) { score = s ; id = i ; } } 	1
package org . apache . lucene . analysis ; import java . io . Reader ; import java . util . Map ; import java . util . HashMap ; public class PerFieldAnalyzerWrapper extends Analyzer { private Analyzer defaultAnalyzer ; private Map analyzerMap = new HashMap ( ) ; public PerFieldAnalyzerWrapper ( Analyzer defaultAnalyzer ) { this . defaultAnalyzer = defaultAnalyzer ; } public void addAnalyzer ( String fieldName , Analyzer analyzer ) { analyzerMap . put ( fieldName , analyzer ) ; } public TokenStream tokenStream ( String fieldName , Reader reader ) { Analyzer analyzer = ( Analyzer ) analyzerMap . get ( fieldName ) ; if ( analyzer == null ) { analyzer = defaultAnalyzer ; } return analyzer . tokenStream ( fieldName , reader ) ; } public int getPositionIncrementGap ( String fieldName ) { Analyzer analyzer = ( Analyzer ) analyzerMap . get ( fieldName ) ; if ( analyzer == null ) analyzer = defaultAnalyzer ; return analyzer . getPositionIncrementGap ( fieldName ) ; } public String toString ( ) { return "PerFieldAnalyzerWrapper(" + analyzerMap + ", default=" + defaultAnalyzer + ")" ; } } 	0
package org . apache . lucene . queryParser ; public class Token { public int kind ; public int beginLine , beginColumn , endLine , endColumn ; public String image ; public Token next ; public Token specialToken ; public String toString ( ) { return image ; } public static final Token newToken ( int ofKind ) { switch ( ofKind ) { default : return new Token ( ) ; } } } 	1
package org . apache . lucene . search ; public abstract class HitCollector { public abstract void collect ( int doc , float score ) ; } 	0
package org . apache . lucene . store ; import java . io . IOException ; class RAMInputStream extends IndexInput implements Cloneable { static final int BUFFER_SIZE = RAMOutputStream . BUFFER_SIZE ; private RAMFile file ; private long length ; private byte [ ] currentBuffer ; private int currentBufferIndex ; private int bufferPosition ; private long bufferStart ; private int bufferLength ; public RAMInputStream ( RAMFile f ) { file = f ; length = file . length ; currentBufferIndex = - 1 ; currentBuffer = null ; } public void close ( ) { } public long length ( ) { return length ; } public byte readByte ( ) throws IOException { if ( bufferPosition >= bufferLength ) { currentBufferIndex ++ ; switchCurrentBuffer ( ) ; } return currentBuffer [ bufferPosition ++ ] ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { while ( len > 0 ) { if ( bufferPosition >= bufferLength ) { currentBufferIndex ++ ; switchCurrentBuffer ( ) ; } int remainInBuffer = bufferLength - bufferPosition ; int bytesToCopy = len < remainInBuffer ? len : remainInBuffer ; System . arraycopy ( currentBuffer , bufferPosition , b , offset , bytesToCopy ) ; offset += bytesToCopy ; len -= bytesToCopy ; bufferPosition += bytesToCopy ; } } private final void switchCurrentBuffer ( ) throws IOException { if ( currentBufferIndex >= file . buffers . size ( ) ) { throw new IOException ( "Read past EOF" ) ; } else { currentBuffer = ( byte [ ] ) file . buffers . get ( currentBufferIndex ) ; bufferPosition = 0 ; bufferStart = BUFFER_SIZE * currentBufferIndex ; bufferLength = ( int ) ( length - bufferStart ) ; if ( bufferLength > BUFFER_SIZE ) { bufferLength = BUFFER_SIZE ; } } } public long getFilePointer ( ) { return currentBufferIndex < 0 ? 0 : bufferStart + bufferPosition ; } public void seek ( long pos ) throws IOException { long bufferStart = currentBufferIndex * BUFFER_SIZE ; if ( pos < bufferStart || pos >= bufferStart + BUFFER_SIZE ) { currentBufferIndex = ( int ) ( pos / BUFFER_SIZE ) ; switchCurrentBuffer ( ) ; } bufferPosition = ( int ) ( pos % BUFFER_SIZE ) ; } } 	1
package org . apache . lucene . analysis ; import java . io . Reader ; public class WhitespaceTokenizer extends CharTokenizer { public WhitespaceTokenizer ( Reader in ) { super ( in ) ; } protected boolean isTokenChar ( char c ) { return ! Character . isWhitespace ( c ) ; } } 	0
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . util . BitSet ; import java . util . WeakHashMap ; import java . util . Map ; import java . io . IOException ; public class CachingWrapperFilter extends Filter { protected Filter filter ; protected transient Map cache ; public CachingWrapperFilter ( Filter filter ) { this . filter = filter ; } public BitSet bits ( IndexReader reader ) throws IOException { if ( cache == null ) { cache = new WeakHashMap ( ) ; } synchronized ( cache ) { BitSet cached = ( BitSet ) cache . get ( reader ) ; if ( cached != null ) { return cached ; } } final BitSet bits = filter . bits ( reader ) ; synchronized ( cache ) { cache . put ( reader , bits ) ; } return bits ; } public String toString ( ) { return "CachingWrapperFilter(" + filter + ")" ; } public boolean equals ( Object o ) { if ( ! ( o instanceof CachingWrapperFilter ) ) return false ; return this . filter . equals ( ( ( CachingWrapperFilter ) o ) . filter ) ; } public int hashCode ( ) { return filter . hashCode ( ) ^ 0x1117BF25 ; } } 	1
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Query ; import org . apache . lucene . util . ToStringUtils ; public class SpanNotQuery extends SpanQuery { private SpanQuery include ; private SpanQuery exclude ; public SpanNotQuery ( SpanQuery include , SpanQuery exclude ) { this . include = include ; this . exclude = exclude ; if ( ! include . getField ( ) . equals ( exclude . getField ( ) ) ) throw new IllegalArgumentException ( "Clauses must have same field." ) ; } public SpanQuery getInclude ( ) { return include ; } public SpanQuery getExclude ( ) { return exclude ; } public String getField ( ) { return include . getField ( ) ; } public Collection getTerms ( ) { return include . getTerms ( ) ; } public void extractTerms ( Set terms ) { include . extractTerms ( terms ) ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "spanNot(" ) ; buffer . append ( include . toString ( field ) ) ; buffer . append ( ", " ) ; buffer . append ( exclude . toString ( field ) ) ; buffer . append ( ")" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public Spans getSpans ( final IndexReader reader ) throws IOException { return new Spans ( ) { private Spans includeSpans = include . getSpans ( reader ) ; private boolean moreInclude = true ; private Spans excludeSpans = exclude . getSpans ( reader ) ; private boolean moreExclude = excludeSpans . next ( ) ; public boolean next ( ) throws IOException { if ( moreInclude ) moreInclude = includeSpans . next ( ) ; while ( moreInclude && moreExclude ) { if ( includeSpans . doc ( ) > excludeSpans . doc ( ) ) moreExclude = excludeSpans . skipTo ( includeSpans . doc ( ) ) ; while ( moreExclude && includeSpans . doc ( ) == excludeSpans . doc ( ) && excludeSpans . end ( ) <= includeSpans . start ( ) ) { moreExclude = excludeSpans . next ( ) ; } if ( ! moreExclude || includeSpans . doc ( ) != excludeSpans . doc ( ) || includeSpans . end ( ) <= excludeSpans . start ( ) ) break ; moreInclude = includeSpans . next ( ) ; } return moreInclude ; } public boolean skipTo ( int target ) throws IOException { if ( moreInclude ) moreInclude = includeSpans . skipTo ( target ) ; if ( ! moreInclude ) return false ; if ( moreExclude && includeSpans . doc ( ) > excludeSpans . doc ( ) ) moreExclude = excludeSpans . skipTo ( includeSpans . doc ( ) ) ; while ( moreExclude && includeSpans . doc ( ) == excludeSpans . doc ( ) && excludeSpans . end ( ) <= includeSpans . start ( ) ) { moreExclude = excludeSpans . next ( ) ; } if ( ! moreExclude || includeSpans . doc ( ) != excludeSpans . doc ( ) || includeSpans . end ( ) <= excludeSpans . start ( ) ) return true ; return next ( ) ; } public int doc ( ) { return includeSpans . doc ( ) ; } public int start ( ) { return includeSpans . start ( ) ; } public int end ( ) { return includeSpans . end ( ) ; } public String toString ( ) { return "spans(" + SpanNotQuery . this . toString ( ) + ")" ; } } ; } public Query rewrite ( IndexReader reader ) throws IOException { SpanNotQuery clone = null ; SpanQuery rewrittenInclude = ( SpanQuery ) include . rewrite ( reader ) ; if ( rewrittenInclude != include ) { clone = ( SpanNotQuery ) this . clone ( ) ; clone . include = rewrittenInclude ; } SpanQuery rewrittenExclude = ( SpanQuery ) exclude . rewrite ( reader ) ; if ( rewrittenExclude != exclude ) { if ( clone == null ) clone = ( SpanNotQuery ) this . clone ( ) ; clone . exclude = rewrittenExclude ; } if ( clone != null ) { return clone ; } else { return this ; } } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof SpanNotQuery ) ) return false ; SpanNotQuery other = ( SpanNotQuery ) o ; return this . include . equals ( other . include ) && this . exclude . equals ( other . exclude ) && this . getBoost ( ) == other . getBoost ( ) ; } public int hashCode ( ) { int h = include . hashCode ( ) ; h = ( h << 1 ) | ( h > > > 31 ) ; h ^= exclude . hashCode ( ) ; h = ( h << 1 ) | ( h > > > 31 ) ; h ^= Float . floatToRawIntBits ( getBoost ( ) ) ; return h ; } } 	0
package org . apache . lucene . store ; import java . util . ArrayList ; import java . io . Serializable ; class RAMFile implements Serializable { private static final long serialVersionUID = 1l ; ArrayList buffers = new ArrayList ( ) ; long length ; RAMDirectory directory ; long sizeInBytes ; private long lastModified = System . currentTimeMillis ( ) ; RAMFile ( ) { } RAMFile ( RAMDirectory directory ) { this . directory = directory ; } synchronized long getLength ( ) { return length ; } synchronized void setLength ( long length ) { this . length = length ; } synchronized long getLastModified ( ) { return lastModified ; } synchronized void setLastModified ( long lastModified ) { this . lastModified = lastModified ; } final byte [ ] addBuffer ( int size ) { byte [ ] buffer = new byte [ size ] ; if ( directory != null ) synchronized ( directory ) { buffers . add ( buffer ) ; directory . sizeInBytes += size ; sizeInBytes += size ; } else buffers . add ( buffer ) ; return buffer ; } long getSizeInBytes ( ) { synchronized ( directory ) { return sizeInBytes ; } } } 	1
package org . apache . lucene . search ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . FieldSelector ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . CorruptIndexException ; import java . io . IOException ; import java . rmi . Naming ; import java . rmi . RMISecurityManager ; import java . rmi . RemoteException ; import java . rmi . server . UnicastRemoteObject ; public class RemoteSearchable extends UnicastRemoteObject implements Searchable { private Searchable local ; public RemoteSearchable ( Searchable local ) throws RemoteException { super ( ) ; this . local = local ; } public void search ( Weight weight , Filter filter , HitCollector results ) throws IOException { local . search ( weight , filter , results ) ; } public void close ( ) throws IOException { local . close ( ) ; } public int docFreq ( Term term ) throws IOException { return local . docFreq ( term ) ; } public int [ ] docFreqs ( Term [ ] terms ) throws IOException { return local . docFreqs ( terms ) ; } public int maxDoc ( ) throws IOException { return local . maxDoc ( ) ; } public TopDocs search ( Weight weight , Filter filter , int n ) throws IOException { return local . search ( weight , filter , n ) ; } public TopFieldDocs search ( Weight weight , Filter filter , int n , Sort sort ) throws IOException { return local . search ( weight , filter , n , sort ) ; } public Document doc ( int i ) throws CorruptIndexException , IOException { return local . doc ( i ) ; } public Document doc ( int i , FieldSelector fieldSelector ) throws CorruptIndexException , IOException { return local . doc ( i , fieldSelector ) ; } public Query rewrite ( Query original ) throws IOException { return local . rewrite ( original ) ; } public Explanation explain ( Weight weight , int doc ) throws IOException { return local . explain ( weight , doc ) ; } public static void main ( String args [ ] ) throws Exception { String indexName = null ; if ( args != null && args . length == 1 ) indexName = args [ 0 ] ; if ( indexName == null ) { System . out . println ( "Usage: org.apache.lucene.search.RemoteSearchable <index>" ) ; return ; } if ( System . getSecurityManager ( ) == null ) { System . setSecurityManager ( new RMISecurityManager ( ) ) ; } Searchable local = new IndexSearcher ( indexName ) ; RemoteSearchable impl = new RemoteSearchable ( local ) ; Naming . rebind ( "//localhost/Searchable" , impl ) ; } } 	0
package org . apache . lucene . analysis ; import java . io . IOException ; import java . util . HashSet ; import java . util . Set ; public final class StopFilter extends TokenFilter { private final Set stopWords ; private final boolean ignoreCase ; public StopFilter ( TokenStream input , String [ ] stopWords ) { this ( input , stopWords , false ) ; } public StopFilter ( TokenStream in , String [ ] stopWords , boolean ignoreCase ) { super ( in ) ; this . ignoreCase = ignoreCase ; this . stopWords = makeStopSet ( stopWords , ignoreCase ) ; } public StopFilter ( TokenStream input , Set stopWords , boolean ignoreCase ) { super ( input ) ; this . ignoreCase = ignoreCase ; this . stopWords = stopWords ; } public StopFilter ( TokenStream in , Set stopWords ) { this ( in , stopWords , false ) ; } public static final Set makeStopSet ( String [ ] stopWords ) { return makeStopSet ( stopWords , false ) ; } public static final Set makeStopSet ( String [ ] stopWords , boolean ignoreCase ) { HashSet stopTable = new HashSet ( stopWords . length ) ; for ( int i = 0 ; i < stopWords . length ; i ++ ) stopTable . add ( ignoreCase ? stopWords [ i ] . toLowerCase ( ) : stopWords [ i ] ) ; return stopTable ; } public final Token next ( ) throws IOException { for ( Token token = input . next ( ) ; token != null ; token = input . next ( ) ) { String termText = ignoreCase ? token . termText . toLowerCase ( ) : token . termText ; if ( ! stopWords . contains ( termText ) ) return token ; } return null ; } } 	1
package org . apache . lucene . index ; public interface TermFreqVector { public String getField ( ) ; public int size ( ) ; public String [ ] getTerms ( ) ; public int [ ] getTermFrequencies ( ) ; public int indexOf ( String term ) ; public int [ ] indexesOf ( String [ ] terms , int start , int len ) ; } 	0
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; public class ConstantScoreRangeQuery extends Query { private final String fieldName ; private final String lowerVal ; private final String upperVal ; private final boolean includeLower ; private final boolean includeUpper ; public ConstantScoreRangeQuery ( String fieldName , String lowerVal , String upperVal , boolean includeLower , boolean includeUpper ) { if ( lowerVal == null ) { includeLower = true ; } else if ( includeLower && lowerVal . equals ( "" ) ) { lowerVal = null ; } if ( upperVal == null ) { includeUpper = true ; } this . fieldName = fieldName . intern ( ) ; this . lowerVal = lowerVal ; this . upperVal = upperVal ; this . includeLower = includeLower ; this . includeUpper = includeUpper ; } public String getField ( ) { return fieldName ; } public String getLowerVal ( ) { return lowerVal ; } public String getUpperVal ( ) { return upperVal ; } public boolean includesLower ( ) { return includeLower ; } public boolean includesUpper ( ) { return includeUpper ; } public Query rewrite ( IndexReader reader ) throws IOException { RangeFilter rangeFilt = new RangeFilter ( fieldName , lowerVal != null ? lowerVal : "" , upperVal , lowerVal == "" ? false : includeLower , upperVal == null ? false : includeUpper ) ; Query q = new ConstantScoreQuery ( rangeFilt ) ; q . setBoost ( getBoost ( ) ) ; return q ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! getField ( ) . equals ( field ) ) { buffer . append ( getField ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( includeLower ? '[' : '{' ) ; buffer . append ( lowerVal != null ? lowerVal : "*" ) ; buffer . append ( " TO " ) ; buffer . append ( upperVal != null ? upperVal : "*" ) ; buffer . append ( includeUpper ? ']' : '}' ) ; if ( getBoost ( ) != 1.0f ) { buffer . append ( "^" ) ; buffer . append ( Float . toString ( getBoost ( ) ) ) ; } return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof ConstantScoreRangeQuery ) ) return false ; ConstantScoreRangeQuery other = ( ConstantScoreRangeQuery ) o ; if ( this . fieldName != other . fieldName || this . includeLower != other . includeLower || this . includeUpper != other . includeUpper ) { return false ; } if ( this . lowerVal != null ? ! this . lowerVal . equals ( other . lowerVal ) : other . lowerVal != null ) return false ; if ( this . upperVal != null ? ! this . upperVal . equals ( other . upperVal ) : other . upperVal != null ) return false ; return this . getBoost ( ) == other . getBoost ( ) ; } public int hashCode ( ) { int h = Float . floatToIntBits ( getBoost ( ) ) ^ fieldName . hashCode ( ) ; h ^= lowerVal != null ? lowerVal . hashCode ( ) : 0x965a965a ; h ^= ( h << 17 ) | ( h > > > 16 ) ; h ^= ( upperVal != null ? ( upperVal . hashCode ( ) ) : 0x5a695a69 ) ; h ^= ( includeLower ? 0x665599aa : 0 ) ^ ( includeUpper ? 0x99aa5566 : 0 ) ; return h ; } } 	1
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . List ; import java . util . Collection ; import java . util . ArrayList ; import java . util . Iterator ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . PriorityQueue ; import org . apache . lucene . util . ToStringUtils ; import org . apache . lucene . search . Query ; public class SpanOrQuery extends SpanQuery { private List clauses ; private String field ; public SpanOrQuery ( SpanQuery [ ] clauses ) { this . clauses = new ArrayList ( clauses . length ) ; for ( int i = 0 ; i < clauses . length ; i ++ ) { SpanQuery clause = clauses [ i ] ; if ( i == 0 ) { field = clause . getField ( ) ; } else if ( ! clause . getField ( ) . equals ( field ) ) { throw new IllegalArgumentException ( "Clauses must have same field." ) ; } this . clauses . add ( clause ) ; } } public SpanQuery [ ] getClauses ( ) { return ( SpanQuery [ ] ) clauses . toArray ( new SpanQuery [ clauses . size ( ) ] ) ; } public String getField ( ) { return field ; } public Collection getTerms ( ) { Collection terms = new ArrayList ( ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; terms . addAll ( clause . getTerms ( ) ) ; } return terms ; } public void extractTerms ( Set terms ) { Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; clause . extractTerms ( terms ) ; } } public Query rewrite ( IndexReader reader ) throws IOException { SpanOrQuery clone = null ; for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { SpanQuery c = ( SpanQuery ) clauses . get ( i ) ; SpanQuery query = ( SpanQuery ) c . rewrite ( reader ) ; if ( query != c ) { if ( clone == null ) clone = ( SpanOrQuery ) this . clone ( ) ; clone . clauses . set ( i , query ) ; } } if ( clone != null ) { return clone ; } else { return this ; } } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "spanOr([" ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; buffer . append ( clause . toString ( field ) ) ; if ( i . hasNext ( ) ) { buffer . append ( ", " ) ; } } buffer . append ( "])" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( o == null || getClass ( ) != o . getClass ( ) ) return false ; final SpanOrQuery that = ( SpanOrQuery ) o ; if ( ! clauses . equals ( that . clauses ) ) return false ; if ( ! field . equals ( that . field ) ) return false ; return getBoost ( ) == that . getBoost ( ) ; } public int hashCode ( ) { int h = clauses . hashCode ( ) ; h ^= ( h << 10 ) | ( h > > > 23 ) ; h ^= Float . floatToRawIntBits ( getBoost ( ) ) ; return h ; } private class SpanQueue extends PriorityQueue { public SpanQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object o1 , Object o2 ) { Spans spans1 = ( Spans ) o1 ; Spans spans2 = ( Spans ) o2 ; if ( spans1 . doc ( ) == spans2 . doc ( ) ) { if ( spans1 . start ( ) == spans2 . start ( ) ) { return spans1 . end ( ) < spans2 . end ( ) ; } else { return spans1 . start ( ) < spans2 . start ( ) ; } } else { return spans1 . doc ( ) < spans2 . doc ( ) ; } } } public Spans getSpans ( final IndexReader reader ) throws IOException { if ( clauses . size ( ) == 1 ) return ( ( SpanQuery ) clauses . get ( 0 ) ) . getSpans ( reader ) ; return new Spans ( ) { private SpanQueue queue = null ; private boolean initSpanQueue ( int target ) throws IOException { queue = new SpanQueue ( clauses . size ( ) ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { Spans spans = ( ( SpanQuery ) i . next ( ) ) . getSpans ( reader ) ; if ( ( ( target == - 1 ) && spans . next ( ) ) || ( ( target != - 1 ) && spans . skipTo ( target ) ) ) { queue . put ( spans ) ; } } return queue . size ( ) != 0 ; } public boolean next ( ) throws IOException { if ( queue == null ) { return initSpanQueue ( - 1 ) ; } if ( queue . size ( ) == 0 ) { return false ; } if ( top ( ) . next ( ) ) { queue . adjustTop ( ) ; return true ; } queue . pop ( ) ; return queue . size ( ) != 0 ; } private Spans top ( ) { return ( Spans ) queue . top ( ) ; } public boolean skipTo ( int target ) throws IOException { if ( queue == null ) { return initSpanQueue ( target ) ; } while ( queue . size ( ) != 0 && top ( ) . doc ( ) < target ) { if ( top ( ) . skipTo ( target ) ) { queue . adjustTop ( ) ; } else { queue . pop ( ) ; } } return queue . size ( ) != 0 ; } public int doc ( ) { return top ( ) . doc ( ) ; } public int start ( ) { return top ( ) . start ( ) ; } public int end ( ) { return top ( ) . end ( ) ; } public String toString ( ) { return "spans(" + SpanOrQuery . this + ")@" + ( ( queue == null ) ? "START" : ( queue . size ( ) > 0 ? ( doc ( ) + ":" + start ( ) + "-" + end ( ) ) : "END" ) ) ; } } ; } } 	0
package org . apache . lucene . analysis ; import java . io . IOException ; public abstract class TokenFilter extends TokenStream { protected TokenStream input ; protected TokenFilter ( TokenStream input ) { this . input = input ; } public void close ( ) throws IOException { input . close ( ) ; } } 	1
package org . apache . lucene . store ; import java . io . IOException ; public class NoLockFactory extends LockFactory { private static NoLock singletonLock = new NoLock ( ) ; private static NoLockFactory singleton = new NoLockFactory ( ) ; public static NoLockFactory getNoLockFactory ( ) { return singleton ; } public Lock makeLock ( String lockName ) { return singletonLock ; } public void clearLock ( String lockName ) { } ; } ; class NoLock extends Lock { public boolean obtain ( ) throws IOException { return true ; } public void release ( ) { } public boolean isLocked ( ) { return false ; } public String toString ( ) { return "NoLock" ; } } 	0
package org . apache . lucene . index ; import java . util . List ; public final class KeepOnlyLastCommitDeletionPolicy implements IndexDeletionPolicy { public void onInit ( List commits ) { onCommit ( commits ) ; } public void onCommit ( List commits ) { int size = commits . size ( ) ; for ( int i = 0 ; i < size - 1 ; i ++ ) { ( ( IndexCommitPoint ) commits . get ( i ) ) . delete ( ) ; } } } 	1
package org . apache . lucene . search ; public class ScoreDoc implements java . io . Serializable { public float score ; public int doc ; public ScoreDoc ( int doc , float score ) { this . doc = doc ; this . score = score ; } } 	0
package org . apache . lucene . analysis . standard ; import org . apache . lucene . analysis . * ; import java . io . File ; import java . io . IOException ; import java . io . Reader ; import java . util . Set ; public class StandardAnalyzer extends Analyzer { private Set stopSet ; public static final String [ ] STOP_WORDS = StopAnalyzer . ENGLISH_STOP_WORDS ; public StandardAnalyzer ( ) { this ( STOP_WORDS ) ; } public StandardAnalyzer ( Set stopWords ) { stopSet = stopWords ; } public StandardAnalyzer ( String [ ] stopWords ) { stopSet = StopFilter . makeStopSet ( stopWords ) ; } public StandardAnalyzer ( File stopwords ) throws IOException { stopSet = WordlistLoader . getWordSet ( stopwords ) ; } public StandardAnalyzer ( Reader stopwords ) throws IOException { stopSet = WordlistLoader . getWordSet ( stopwords ) ; } public TokenStream tokenStream ( String fieldName , Reader reader ) { TokenStream result = new StandardTokenizer ( reader ) ; result = new StandardFilter ( result ) ; result = new LowerCaseFilter ( result ) ; result = new StopFilter ( result , stopSet ) ; return result ; } } 	1
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Arrays ; import java . util . Comparator ; import org . apache . lucene . index . IndexReader ; class NearSpansOrdered implements Spans { private final int allowedSlop ; private boolean firstTime = true ; private boolean more = false ; private final Spans [ ] subSpans ; private boolean inSameDoc = false ; private int matchDoc = - 1 ; private int matchStart = - 1 ; private int matchEnd = - 1 ; private final Spans [ ] subSpansByDoc ; private final Comparator spanDocComparator = new Comparator ( ) { public int compare ( Object o1 , Object o2 ) { return ( ( Spans ) o1 ) . doc ( ) - ( ( Spans ) o2 ) . doc ( ) ; } } ; private SpanNearQuery query ; public NearSpansOrdered ( SpanNearQuery spanNearQuery , IndexReader reader ) throws IOException { if ( spanNearQuery . getClauses ( ) . length < 2 ) { throw new IllegalArgumentException ( "Less than 2 clauses: " + spanNearQuery ) ; } allowedSlop = spanNearQuery . getSlop ( ) ; SpanQuery [ ] clauses = spanNearQuery . getClauses ( ) ; subSpans = new Spans [ clauses . length ] ; subSpansByDoc = new Spans [ clauses . length ] ; for ( int i = 0 ; i < clauses . length ; i ++ ) { subSpans [ i ] = clauses [ i ] . getSpans ( reader ) ; subSpansByDoc [ i ] = subSpans [ i ] ; } query = spanNearQuery ; } public int doc ( ) { return matchDoc ; } public int start ( ) { return matchStart ; } public int end ( ) { return matchEnd ; } public boolean next ( ) throws IOException { if ( firstTime ) { firstTime = false ; for ( int i = 0 ; i < subSpans . length ; i ++ ) { if ( ! subSpans [ i ] . next ( ) ) { more = false ; return false ; } } more = true ; } return advanceAfterOrdered ( ) ; } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { firstTime = false ; for ( int i = 0 ; i < subSpans . length ; i ++ ) { if ( ! subSpans [ i ] . skipTo ( target ) ) { more = false ; return false ; } } more = true ; } else if ( more && ( subSpans [ 0 ] . doc ( ) < target ) ) { if ( subSpans [ 0 ] . skipTo ( target ) ) { inSameDoc = false ; } else { more = false ; return false ; } } return advanceAfterOrdered ( ) ; } private boolean advanceAfterOrdered ( ) throws IOException { while ( more && ( inSameDoc || toSameDoc ( ) ) ) { if ( stretchToOrder ( ) && shrinkToAfterShortestMatch ( ) ) { return true ; } } return false ; } private boolean toSameDoc ( ) throws IOException { Arrays . sort ( subSpansByDoc , spanDocComparator ) ; int firstIndex = 0 ; int maxDoc = subSpansByDoc [ subSpansByDoc . length - 1 ] . doc ( ) ; while ( subSpansByDoc [ firstIndex ] . doc ( ) != maxDoc ) { if ( ! subSpansByDoc [ firstIndex ] . skipTo ( maxDoc ) ) { more = false ; inSameDoc = false ; return false ; } maxDoc = subSpansByDoc [ firstIndex ] . doc ( ) ; if ( ++ firstIndex == subSpansByDoc . length ) { firstIndex = 0 ; } } for ( int i = 0 ; i < subSpansByDoc . length ; i ++ ) { assert ( subSpansByDoc [ i ] . doc ( ) == maxDoc ) : " NearSpansOrdered.toSameDoc() spans " + subSpansByDoc [ 0 ] + "\n at doc " + subSpansByDoc [ i ] . doc ( ) + ", but should be at " + maxDoc ; } inSameDoc = true ; return true ; } static final boolean docSpansOrdered ( Spans spans1 , Spans spans2 ) { assert spans1 . doc ( ) == spans2 . doc ( ) : "doc1 " + spans1 . doc ( ) + " != doc2 " + spans2 . doc ( ) ; int start1 = spans1 . start ( ) ; int start2 = spans2 . start ( ) ; return ( start1 == start2 ) ? ( spans1 . end ( ) < spans2 . end ( ) ) : ( start1 < start2 ) ; } private static final boolean docSpansOrdered ( int start1 , int end1 , int start2 , int end2 ) { return ( start1 == start2 ) ? ( end1 < end2 ) : ( start1 < start2 ) ; } private boolean stretchToOrder ( ) throws IOException { matchDoc = subSpans [ 0 ] . doc ( ) ; for ( int i = 1 ; inSameDoc && ( i < subSpans . length ) ; i ++ ) { while ( ! docSpansOrdered ( subSpans [ i - 1 ] , subSpans [ i ] ) ) { if ( ! subSpans [ i ] . next ( ) ) { inSameDoc = false ; more = false ; break ; } else if ( matchDoc != subSpans [ i ] . doc ( ) ) { inSameDoc = false ; break ; } } } return inSameDoc ; } private boolean shrinkToAfterShortestMatch ( ) throws IOException { matchStart = subSpans [ subSpans . length - 1 ] . start ( ) ; matchEnd = subSpans [ subSpans . length - 1 ] . end ( ) ; int matchSlop = 0 ; int lastStart = matchStart ; int lastEnd = matchEnd ; for ( int i = subSpans . length - 2 ; i >= 0 ; i -- ) { Spans prevSpans = subSpans [ i ] ; int prevStart = prevSpans . start ( ) ; int prevEnd = prevSpans . end ( ) ; while ( true ) { if ( ! prevSpans . next ( ) ) { inSameDoc = false ; more = false ; break ; } else if ( matchDoc != prevSpans . doc ( ) ) { inSameDoc = false ; break ; } else { int ppStart = prevSpans . start ( ) ; int ppEnd = prevSpans . end ( ) ; if ( ! docSpansOrdered ( ppStart , ppEnd , lastStart , lastEnd ) ) { break ; } else { prevStart = ppStart ; prevEnd = ppEnd ; } } } assert prevStart <= matchStart ; if ( matchStart > prevEnd ) { matchSlop += ( matchStart - prevEnd ) ; } matchStart = prevStart ; lastStart = prevStart ; lastEnd = prevEnd ; } return matchSlop <= allowedSlop ; } public String toString ( ) { return getClass ( ) . getName ( ) + "(" + query . toString ( ) + ")@" + ( firstTime ? "START" : ( more ? ( doc ( ) + ":" + start ( ) + "-" + end ( ) ) : "END" ) ) ; } } 	0
package org . apache . lucene . store ; import java . io . IOException ; public class RAMOutputStream extends IndexOutput { static final int BUFFER_SIZE = 1024 ; private RAMFile file ; private byte [ ] currentBuffer ; private int currentBufferIndex ; private int bufferPosition ; private long bufferStart ; private int bufferLength ; public RAMOutputStream ( ) { this ( new RAMFile ( ) ) ; } RAMOutputStream ( RAMFile f ) { file = f ; currentBufferIndex = - 1 ; currentBuffer = null ; } public void writeTo ( IndexOutput out ) throws IOException { flush ( ) ; final long end = file . length ; long pos = 0 ; int buffer = 0 ; while ( pos < end ) { int length = BUFFER_SIZE ; long nextPos = pos + length ; if ( nextPos > end ) { length = ( int ) ( end - pos ) ; } out . writeBytes ( ( byte [ ] ) file . buffers . get ( buffer ++ ) , length ) ; pos = nextPos ; } } public void reset ( ) { try { seek ( 0 ) ; } catch ( IOException e ) { throw new RuntimeException ( e . toString ( ) ) ; } file . setLength ( 0 ) ; } public void close ( ) throws IOException { flush ( ) ; } public void seek ( long pos ) throws IOException { setFileLength ( ) ; if ( pos < bufferStart || pos >= bufferStart + bufferLength ) { currentBufferIndex = ( int ) ( pos / BUFFER_SIZE ) ; switchCurrentBuffer ( ) ; } bufferPosition = ( int ) ( pos % BUFFER_SIZE ) ; } public long length ( ) { return file . length ; } public void writeByte ( byte b ) throws IOException { if ( bufferPosition == bufferLength ) { currentBufferIndex ++ ; switchCurrentBuffer ( ) ; } currentBuffer [ bufferPosition ++ ] = b ; } public void writeBytes ( byte [ ] b , int offset , int len ) throws IOException { while ( len > 0 ) { if ( bufferPosition == bufferLength ) { currentBufferIndex ++ ; switchCurrentBuffer ( ) ; } int remainInBuffer = currentBuffer . length - bufferPosition ; int bytesToCopy = len < remainInBuffer ? len : remainInBuffer ; System . arraycopy ( b , offset , currentBuffer , bufferPosition , bytesToCopy ) ; offset += bytesToCopy ; len -= bytesToCopy ; bufferPosition += bytesToCopy ; } } private final void switchCurrentBuffer ( ) throws IOException { if ( currentBufferIndex == file . buffers . size ( ) ) { currentBuffer = file . addBuffer ( BUFFER_SIZE ) ; } else { currentBuffer = ( byte [ ] ) file . buffers . get ( currentBufferIndex ) ; } bufferPosition = 0 ; bufferStart = BUFFER_SIZE * currentBufferIndex ; bufferLength = currentBuffer . length ; } private void setFileLength ( ) { long pointer = bufferStart + bufferPosition ; if ( pointer > file . length ) { file . setLength ( pointer ) ; } } public void flush ( ) throws IOException { file . setLastModified ( System . currentTimeMillis ( ) ) ; setFileLength ( ) ; } public long getFilePointer ( ) { return currentBufferIndex < 0 ? 0 : bufferStart + bufferPosition ; } } 	1
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; public interface Weight extends java . io . Serializable { Query getQuery ( ) ; float getValue ( ) ; float sumOfSquaredWeights ( ) throws IOException ; void normalize ( float norm ) ; Scorer scorer ( IndexReader reader ) throws IOException ; Explanation explain ( IndexReader reader , int doc ) throws IOException ; } 	0
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . BufferedIndexInput ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . Lock ; import java . util . HashMap ; import java . io . IOException ; class CompoundFileReader extends Directory { private int readBufferSize ; private static final class FileEntry { long offset ; long length ; } private Directory directory ; private String fileName ; private IndexInput stream ; private HashMap entries = new HashMap ( ) ; public CompoundFileReader ( Directory dir , String name ) throws IOException { this ( dir , name , BufferedIndexInput . BUFFER_SIZE ) ; } public CompoundFileReader ( Directory dir , String name , int readBufferSize ) throws IOException { directory = dir ; fileName = name ; this . readBufferSize = readBufferSize ; boolean success = false ; try { stream = dir . openInput ( name , readBufferSize ) ; int count = stream . readVInt ( ) ; FileEntry entry = null ; for ( int i = 0 ; i < count ; i ++ ) { long offset = stream . readLong ( ) ; String id = stream . readString ( ) ; if ( entry != null ) { entry . length = offset - entry . offset ; } entry = new FileEntry ( ) ; entry . offset = offset ; entries . put ( id , entry ) ; } if ( entry != null ) { entry . length = stream . length ( ) - entry . offset ; } success = true ; } finally { if ( ! success && ( stream != null ) ) { try { stream . close ( ) ; } catch ( IOException e ) { } } } } public Directory getDirectory ( ) { return directory ; } public String getName ( ) { return fileName ; } public synchronized void close ( ) throws IOException { if ( stream == null ) throw new IOException ( "Already closed" ) ; entries . clear ( ) ; stream . close ( ) ; stream = null ; } public synchronized IndexInput openInput ( String id ) throws IOException { return openInput ( id , readBufferSize ) ; } public synchronized IndexInput openInput ( String id , int readBufferSize ) throws IOException { if ( stream == null ) throw new IOException ( "Stream closed" ) ; FileEntry entry = ( FileEntry ) entries . get ( id ) ; if ( entry == null ) throw new IOException ( "No sub-file with id " + id + " found" ) ; return new CSIndexInput ( stream , entry . offset , entry . length , readBufferSize ) ; } public String [ ] list ( ) { String res [ ] = new String [ entries . size ( ) ] ; return ( String [ ] ) entries . keySet ( ) . toArray ( res ) ; } public boolean fileExists ( String name ) { return entries . containsKey ( name ) ; } public long fileModified ( String name ) throws IOException { return directory . fileModified ( fileName ) ; } public void touchFile ( String name ) throws IOException { directory . touchFile ( fileName ) ; } public void deleteFile ( String name ) { throw new UnsupportedOperationException ( ) ; } public void renameFile ( String from , String to ) { throw new UnsupportedOperationException ( ) ; } public long fileLength ( String name ) throws IOException { FileEntry e = ( FileEntry ) entries . get ( name ) ; if ( e == null ) throw new IOException ( "File " + name + " does not exist" ) ; return e . length ; } public IndexOutput createOutput ( String name ) { throw new UnsupportedOperationException ( ) ; } public Lock makeLock ( String name ) { throw new UnsupportedOperationException ( ) ; } static final class CSIndexInput extends BufferedIndexInput { IndexInput base ; long fileOffset ; long length ; CSIndexInput ( final IndexInput base , final long fileOffset , final long length ) { this ( base , fileOffset , length , BufferedIndexInput . BUFFER_SIZE ) ; } CSIndexInput ( final IndexInput base , final long fileOffset , final long length , int readBufferSize ) { super ( readBufferSize ) ; this . base = base ; this . fileOffset = fileOffset ; this . length = length ; } protected void readInternal ( byte [ ] b , int offset , int len ) throws IOException { synchronized ( base ) { long start = getFilePointer ( ) ; if ( start + len > length ) throw new IOException ( "read past EOF" ) ; base . seek ( fileOffset + start ) ; base . readBytes ( b , offset , len ) ; } } protected void seekInternal ( long pos ) { } public void close ( ) { } public long length ( ) { return length ; } } } 	1
package org . apache . lucene . analysis ; import java . io . File ; import java . io . IOException ; import java . io . Reader ; import java . util . Set ; public final class StopAnalyzer extends Analyzer { private Set stopWords ; public static final String [ ] ENGLISH_STOP_WORDS = { "a" , "an" , "and" , "are" , "as" , "at" , "be" , "but" , "by" , "for" , "if" , "in" , "into" , "is" , "it" , "no" , "not" , "of" , "on" , "or" , "such" , "that" , "the" , "their" , "then" , "there" , "these" , "they" , "this" , "to" , "was" , "will" , "with" } ; public StopAnalyzer ( ) { stopWords = StopFilter . makeStopSet ( ENGLISH_STOP_WORDS ) ; } public StopAnalyzer ( Set stopWords ) { this . stopWords = stopWords ; } public StopAnalyzer ( String [ ] stopWords ) { this . stopWords = StopFilter . makeStopSet ( stopWords ) ; } public StopAnalyzer ( File stopwordsFile ) throws IOException { stopWords = WordlistLoader . getWordSet ( stopwordsFile ) ; } public StopAnalyzer ( Reader stopwords ) throws IOException { stopWords = WordlistLoader . getWordSet ( stopwords ) ; } public TokenStream tokenStream ( String fieldName , Reader reader ) { return new StopFilter ( new LowerCaseTokenizer ( reader ) , stopWords ) ; } } 	0
package org . apache . lucene . search ; import java . io . IOException ; import java . util . Set ; import java . util . Vector ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermPositions ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; public class PhraseQuery extends Query { private String field ; private Vector terms = new Vector ( ) ; private Vector positions = new Vector ( ) ; private int slop = 0 ; public PhraseQuery ( ) { } public void setSlop ( int s ) { slop = s ; } public int getSlop ( ) { return slop ; } public void add ( Term term ) { int position = 0 ; if ( positions . size ( ) > 0 ) position = ( ( Integer ) positions . lastElement ( ) ) . intValue ( ) + 1 ; add ( term , position ) ; } public void add ( Term term , int position ) { if ( terms . size ( ) == 0 ) field = term . field ( ) ; else if ( term . field ( ) != field ) throw new IllegalArgumentException ( "All phrase terms must be in the same field: " + term ) ; terms . addElement ( term ) ; positions . addElement ( new Integer ( position ) ) ; } public Term [ ] getTerms ( ) { return ( Term [ ] ) terms . toArray ( new Term [ 0 ] ) ; } public int [ ] getPositions ( ) { int [ ] result = new int [ positions . size ( ) ] ; for ( int i = 0 ; i < positions . size ( ) ; i ++ ) result [ i ] = ( ( Integer ) positions . elementAt ( i ) ) . intValue ( ) ; return result ; } private class PhraseWeight implements Weight { private Similarity similarity ; private float value ; private float idf ; private float queryNorm ; private float queryWeight ; public PhraseWeight ( Searcher searcher ) throws IOException { this . similarity = getSimilarity ( searcher ) ; idf = similarity . idf ( terms , searcher ) ; } public String toString ( ) { return "weight(" + PhraseQuery . this + ")" ; } public Query getQuery ( ) { return PhraseQuery . this ; } public float getValue ( ) { return value ; } public float sumOfSquaredWeights ( ) { queryWeight = idf * getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float queryNorm ) { this . queryNorm = queryNorm ; queryWeight *= queryNorm ; value = queryWeight * idf ; } public Scorer scorer ( IndexReader reader ) throws IOException { if ( terms . size ( ) == 0 ) return null ; TermPositions [ ] tps = new TermPositions [ terms . size ( ) ] ; for ( int i = 0 ; i < terms . size ( ) ; i ++ ) { TermPositions p = reader . termPositions ( ( Term ) terms . elementAt ( i ) ) ; if ( p == null ) return null ; tps [ i ] = p ; } if ( slop == 0 ) return new ExactPhraseScorer ( this , tps , getPositions ( ) , similarity , reader . norms ( field ) ) ; else return new SloppyPhraseScorer ( this , tps , getPositions ( ) , similarity , slop , reader . norms ( field ) ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { Explanation result = new Explanation ( ) ; result . setDescription ( "weight(" + getQuery ( ) + " in " + doc + "), product of:" ) ; StringBuffer docFreqs = new StringBuffer ( ) ; StringBuffer query = new StringBuffer ( ) ; query . append ( '\"' ) ; for ( int i = 0 ; i < terms . size ( ) ; i ++ ) { if ( i != 0 ) { docFreqs . append ( " " ) ; query . append ( " " ) ; } Term term = ( Term ) terms . elementAt ( i ) ; docFreqs . append ( term . text ( ) ) ; docFreqs . append ( "=" ) ; docFreqs . append ( reader . docFreq ( term ) ) ; query . append ( term . text ( ) ) ; } query . append ( '\"' ) ; Explanation idfExpl = new Explanation ( idf , "idf(" + field + ": " + docFreqs + ")" ) ; Explanation queryExpl = new Explanation ( ) ; queryExpl . setDescription ( "queryWeight(" + getQuery ( ) + "), product of:" ) ; Explanation boostExpl = new Explanation ( getBoost ( ) , "boost" ) ; if ( getBoost ( ) != 1.0f ) queryExpl . addDetail ( boostExpl ) ; queryExpl . addDetail ( idfExpl ) ; Explanation queryNormExpl = new Explanation ( queryNorm , "queryNorm" ) ; queryExpl . addDetail ( queryNormExpl ) ; queryExpl . setValue ( boostExpl . getValue ( ) * idfExpl . getValue ( ) * queryNormExpl . getValue ( ) ) ; result . addDetail ( queryExpl ) ; Explanation fieldExpl = new Explanation ( ) ; fieldExpl . setDescription ( "fieldWeight(" + field + ":" + query + " in " + doc + "), product of:" ) ; Explanation tfExpl = scorer ( reader ) . explain ( doc ) ; fieldExpl . addDetail ( tfExpl ) ; fieldExpl . addDetail ( idfExpl ) ; Explanation fieldNormExpl = new Explanation ( ) ; byte [ ] fieldNorms = reader . norms ( field ) ; float fieldNorm = fieldNorms != null ? Similarity . decodeNorm ( fieldNorms [ doc ] ) : 0.0f ; fieldNormExpl . setValue ( fieldNorm ) ; fieldNormExpl . setDescription ( "fieldNorm(field=" + field + ", doc=" + doc + ")" ) ; fieldExpl . addDetail ( fieldNormExpl ) ; fieldExpl . setValue ( tfExpl . getValue ( ) * idfExpl . getValue ( ) * fieldNormExpl . getValue ( ) ) ; result . addDetail ( fieldExpl ) ; result . setValue ( queryExpl . getValue ( ) * fieldExpl . getValue ( ) ) ; if ( queryExpl . getValue ( ) == 1.0f ) return fieldExpl ; return result ; } } protected Weight createWeight ( Searcher searcher ) throws IOException { if ( terms . size ( ) == 1 ) { Term term = ( Term ) terms . elementAt ( 0 ) ; Query termQuery = new TermQuery ( term ) ; termQuery . setBoost ( getBoost ( ) ) ; return termQuery . createWeight ( searcher ) ; } return new PhraseWeight ( searcher ) ; } public void extractTerms ( Set queryTerms ) { queryTerms . addAll ( terms ) ; } public String toString ( String f ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! field . equals ( f ) ) { buffer . append ( field ) ; buffer . append ( ":" ) ; } buffer . append ( "\"" ) ; for ( int i = 0 ; i < terms . size ( ) ; i ++ ) { buffer . append ( ( ( Term ) terms . elementAt ( i ) ) . text ( ) ) ; if ( i != terms . size ( ) - 1 ) buffer . append ( " " ) ; } buffer . append ( "\"" ) ; if ( slop != 0 ) { buffer . append ( "~" ) ; buffer . append ( slop ) ; } buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof PhraseQuery ) ) return false ; PhraseQuery other = ( PhraseQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && ( this . slop == other . slop ) && this . terms . equals ( other . terms ) && this . positions . equals ( other . positions ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ slop ^ terms . hashCode ( ) ^ positions . hashCode ( ) ; } } 	1
package org . apache . lucene . analysis . standard ; import java . io . * ; public class StandardTokenizerTokenManager implements StandardTokenizerConstants { public java . io . PrintStream debugStream = System . out ; public void setDebugStream ( java . io . PrintStream ds ) { debugStream = ds ; } private final int jjMoveStringLiteralDfa0_0 ( ) { return jjMoveNfa_0 ( 0 , 0 ) ; } private final void jjCheckNAdd ( int state ) { if ( jjrounds [ state ] != jjround ) { jjstateSet [ jjnewStateCnt ++ ] = state ; jjrounds [ state ] = jjround ; } } private final void jjAddStates ( int start , int end ) { do { jjstateSet [ jjnewStateCnt ++ ] = jjnextStates [ start ] ; } while ( start ++ != end ) ; } private final void jjCheckNAddTwoStates ( int state1 , int state2 ) { jjCheckNAdd ( state1 ) ; jjCheckNAdd ( state2 ) ; } private final void jjCheckNAddStates ( int start , int end ) { do { jjCheckNAdd ( jjnextStates [ start ] ) ; } while ( start ++ != end ) ; } private final void jjCheckNAddStates ( int start ) { jjCheckNAdd ( jjnextStates [ start ] ) ; jjCheckNAdd ( jjnextStates [ start + 1 ] ) ; } static final long [ ] jjbitVec0 = { 0xfff0000000000000L , 0xffffffffffffdfffL , 0xffffffffL , 0x600000000000000L } ; static final long [ ] jjbitVec2 = { 0x0L , 0xffffffffffffffffL , 0xffffffffffffffffL , 0xffffffffffffffffL } ; static final long [ ] jjbitVec3 = { 0xffffffffffffffffL , 0xffffffffffffffffL , 0xffffL , 0xffff000000000000L } ; static final long [ ] jjbitVec4 = { 0xffffffffffffffffL , 0xffffffffffffffffL , 0x0L , 0x0L } ; static final long [ ] jjbitVec5 = { 0xffffffffffffffffL , 0xffffffffffffffffL , 0xffffffffffffffffL , 0x0L } ; static final long [ ] jjbitVec6 = { 0x0L , 0xffffffe000000000L , 0xffffffffL , 0x0L } ; static final long [ ] jjbitVec7 = { 0x20000L , 0x0L , 0xfffff00000000000L , 0x7fffffL } ; static final long [ ] jjbitVec8 = { 0xffffffffffffffffL , 0xffffffffffffffffL , 0xffffffffffffL , 0x0L } ; static final long [ ] jjbitVec9 = { 0xfffffffeL , 0x0L , 0x0L , 0x0L } ; static final long [ ] jjbitVec10 = { 0x0L , 0x0L , 0x0L , 0xff7fffffff7fffffL } ; static final long [ ] jjbitVec11 = { 0x0L , 0x0L , 0xffffffff00000000L , 0x1fffffffL } ; static final long [ ] jjbitVec12 = { 0x1600L , 0x0L , 0x0L , 0x0L } ; static final long [ ] jjbitVec13 = { 0x0L , 0xffc000000000L , 0x0L , 0xffc000000000L } ; static final long [ ] jjbitVec14 = { 0x0L , 0x3ff00000000L , 0x0L , 0x3ff000000000000L } ; static final long [ ] jjbitVec15 = { 0x0L , 0xffc000000000L , 0x0L , 0xff8000000000L } ; static final long [ ] jjbitVec16 = { 0x0L , 0xffc000000000L , 0x0L , 0x0L } ; static final long [ ] jjbitVec17 = { 0x0L , 0x3ff0000L , 0x0L , 0x3ff0000L } ; static final long [ ] jjbitVec18 = { 0x0L , 0x3ffL , 0x0L , 0x0L } ; static final long [ ] jjbitVec19 = { 0xfffffffeL , 0x0L , 0xfffff00000000000L , 0x7fffffL } ; private final int jjMoveNfa_0 ( int startState , int curPos ) { int [ ] nextStates ; int startsAt = 0 ; jjnewStateCnt = 75 ; int i = 1 ; jjstateSet [ 0 ] = startState ; int j , kind = 0x7fffffff ; for ( ; ; ) { if ( ++ jjround == 0x7fffffff ) ReInitRounds ( ) ; if ( curChar < 64 ) { long l = 1L << curChar ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( ( 0x3ff000000000000L & l ) != 0L ) { if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; } if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 12 , 17 ) ; if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 18 , 23 ) ; break ; case 2 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 18 , 23 ) ; break ; case 3 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 3 , 4 ) ; break ; case 4 : case 5 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 5 , 6 ) ; break ; case 6 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 7 ) ; break ; case 7 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAdd ( 7 ) ; break ; case 8 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 8 , 9 ) ; break ; case 9 : case 10 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 10 , 11 ) ; break ; case 11 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 12 ) ; break ; case 12 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 12 , 13 ) ; break ; case 13 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 14 , 15 ) ; break ; case 14 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 14 , 15 ) ; break ; case 15 : case 16 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 11 , 16 ) ; break ; case 17 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 17 , 18 ) ; break ; case 18 : case 19 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 19 , 20 ) ; break ; case 20 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 21 ) ; break ; case 21 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 21 , 22 ) ; break ; case 22 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 23 , 24 ) ; break ; case 23 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 23 , 24 ) ; break ; case 24 : case 25 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 26 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 27 ) ; break ; case 27 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 22 , 27 ) ; break ; case 28 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 12 , 17 ) ; break ; case 29 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; break ; case 30 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAdd ( 30 ) ; break ; case 31 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 24 , 26 ) ; break ; case 32 : if ( ( 0x600000000000L & l ) != 0L ) jjCheckNAdd ( 33 ) ; break ; case 33 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 27 , 29 ) ; break ; case 35 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 35 , 36 ) ; break ; case 36 : if ( ( 0x600000000000L & l ) != 0L ) jjCheckNAdd ( 37 ) ; break ; case 37 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 5 ) kind = 5 ; jjCheckNAddTwoStates ( 36 , 37 ) ; break ; case 38 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 38 , 39 ) ; break ; case 39 : if ( curChar == 46 ) jjCheckNAdd ( 40 ) ; break ; case 40 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 6 ) kind = 6 ; jjCheckNAddTwoStates ( 39 , 40 ) ; break ; case 41 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 41 , 42 ) ; break ; case 42 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 43 , 44 ) ; break ; case 43 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 43 , 44 ) ; break ; case 44 : case 45 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAdd ( 45 ) ; break ; case 46 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 46 , 47 ) ; break ; case 47 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 48 , 49 ) ; break ; case 48 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 48 , 49 ) ; break ; case 49 : case 50 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 50 , 51 ) ; break ; case 51 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 52 ) ; break ; case 52 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 47 , 52 ) ; break ; case 53 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 53 , 54 ) ; break ; case 54 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 55 , 56 ) ; break ; case 55 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 55 , 56 ) ; break ; case 56 : case 57 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 57 , 58 ) ; break ; case 58 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 59 ) ; break ; case 59 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 59 , 60 ) ; break ; case 60 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 61 , 62 ) ; break ; case 61 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 61 , 62 ) ; break ; case 62 : case 63 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 58 , 63 ) ; break ; case 66 : if ( curChar == 39 ) jjstateSet [ jjnewStateCnt ++ ] = 67 ; break ; case 69 : if ( curChar == 46 ) jjCheckNAdd ( 70 ) ; break ; case 71 : if ( curChar != 46 ) break ; if ( kind > 3 ) kind = 3 ; jjCheckNAdd ( 70 ) ; break ; case 73 : if ( curChar == 38 ) jjstateSet [ jjnewStateCnt ++ ] = 74 ; break ; default : break ; } } while ( i != startsAt ) ; } else if ( curChar < 128 ) { long l = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 30 , 35 ) ; if ( ( 0x7fffffe07fffffeL & l ) != 0L ) { if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; } if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 18 , 23 ) ; break ; case 2 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 18 , 23 ) ; break ; case 3 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 3 , 4 ) ; break ; case 5 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjAddStates ( 36 , 37 ) ; break ; case 6 : if ( curChar == 95 ) jjCheckNAdd ( 7 ) ; break ; case 7 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAdd ( 7 ) ; break ; case 8 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 8 , 9 ) ; break ; case 10 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 10 , 11 ) ; break ; case 11 : if ( curChar == 95 ) jjCheckNAdd ( 12 ) ; break ; case 12 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 12 , 13 ) ; break ; case 13 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 14 , 15 ) ; break ; case 14 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 14 , 15 ) ; break ; case 16 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 11 , 16 ) ; break ; case 17 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 17 , 18 ) ; break ; case 19 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjAddStates ( 38 , 39 ) ; break ; case 20 : if ( curChar == 95 ) jjCheckNAdd ( 21 ) ; break ; case 21 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 21 , 22 ) ; break ; case 22 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 23 , 24 ) ; break ; case 23 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 23 , 24 ) ; break ; case 25 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjAddStates ( 40 , 41 ) ; break ; case 26 : if ( curChar == 95 ) jjCheckNAdd ( 27 ) ; break ; case 27 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 22 , 27 ) ; break ; case 29 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; break ; case 30 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAdd ( 30 ) ; break ; case 31 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 24 , 26 ) ; break ; case 32 : if ( curChar == 95 ) jjCheckNAdd ( 33 ) ; break ; case 33 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 27 , 29 ) ; break ; case 34 : if ( curChar == 64 ) jjCheckNAdd ( 35 ) ; break ; case 35 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 35 , 36 ) ; break ; case 37 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 5 ) kind = 5 ; jjCheckNAddTwoStates ( 36 , 37 ) ; break ; case 38 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 38 , 39 ) ; break ; case 40 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 6 ) kind = 6 ; jjCheckNAddTwoStates ( 39 , 40 ) ; break ; case 41 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 41 , 42 ) ; break ; case 42 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 43 , 44 ) ; break ; case 43 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 43 , 44 ) ; break ; case 45 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjstateSet [ jjnewStateCnt ++ ] = 45 ; break ; case 46 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 46 , 47 ) ; break ; case 47 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 48 , 49 ) ; break ; case 48 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 48 , 49 ) ; break ; case 50 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjAddStates ( 42 , 43 ) ; break ; case 51 : if ( curChar == 95 ) jjCheckNAdd ( 52 ) ; break ; case 52 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 47 , 52 ) ; break ; case 53 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 53 , 54 ) ; break ; case 54 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 55 , 56 ) ; break ; case 55 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 55 , 56 ) ; break ; case 57 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 57 , 58 ) ; break ; case 58 : if ( curChar == 95 ) jjCheckNAdd ( 59 ) ; break ; case 59 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 59 , 60 ) ; break ; case 60 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 61 , 62 ) ; break ; case 61 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 61 , 62 ) ; break ; case 63 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 58 , 63 ) ; break ; case 64 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 30 , 35 ) ; break ; case 65 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 65 , 66 ) ; break ; case 67 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 2 ) kind = 2 ; jjCheckNAddTwoStates ( 66 , 67 ) ; break ; case 68 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 68 , 69 ) ; break ; case 70 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjAddStates ( 44 , 45 ) ; break ; case 72 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 72 , 73 ) ; break ; case 73 : if ( curChar == 64 ) jjCheckNAdd ( 74 ) ; break ; case 74 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 4 ) kind = 4 ; jjCheckNAdd ( 74 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else { int hiByte = ( int ) ( curChar > > 8 ) ; int i1 = hiByte > > 6 ; long l1 = 1L << ( hiByte & 077 ) ; int i2 = ( curChar & 0xff ) > > 6 ; long l2 = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) { if ( kind > 12 ) kind = 12 ; } if ( jjCanMove_1 ( hiByte , i1 , i2 , l1 , l2 ) ) { if ( kind > 13 ) kind = 13 ; } if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 18 , 23 ) ; if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 12 , 17 ) ; if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) { if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; } if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 30 , 35 ) ; break ; case 1 : if ( jjCanMove_1 ( hiByte , i1 , i2 , l1 , l2 ) && kind > 13 ) kind = 13 ; break ; case 2 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 18 , 23 ) ; break ; case 3 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 3 , 4 ) ; break ; case 4 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 5 , 6 ) ; break ; case 5 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 5 , 6 ) ; break ; case 7 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjstateSet [ jjnewStateCnt ++ ] = 7 ; break ; case 8 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 8 , 9 ) ; break ; case 9 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 10 , 11 ) ; break ; case 10 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 10 , 11 ) ; break ; case 12 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 46 , 47 ) ; break ; case 14 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 48 , 49 ) ; break ; case 15 : if ( ! jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 11 , 16 ) ; break ; case 16 : if ( ! jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 11 , 16 ) ; break ; case 17 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 17 , 18 ) ; break ; case 18 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 19 , 20 ) ; break ; case 19 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 19 , 20 ) ; break ; case 21 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 21 , 22 ) ; break ; case 23 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 50 , 51 ) ; break ; case 24 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 25 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 27 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 22 , 27 ) ; break ; case 28 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 12 , 17 ) ; break ; case 29 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; break ; case 30 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAdd ( 30 ) ; break ; case 31 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 24 , 26 ) ; break ; case 33 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 27 , 29 ) ; break ; case 35 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 35 , 36 ) ; break ; case 37 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 5 ) kind = 5 ; jjCheckNAddTwoStates ( 36 , 37 ) ; break ; case 38 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 38 , 39 ) ; break ; case 40 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 6 ) kind = 6 ; jjCheckNAddTwoStates ( 39 , 40 ) ; break ; case 41 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 41 , 42 ) ; break ; case 43 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 52 , 53 ) ; break ; case 44 : if ( ! jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAdd ( 45 ) ; break ; case 45 : if ( ! jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAdd ( 45 ) ; break ; case 46 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 46 , 47 ) ; break ; case 48 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 54 , 55 ) ; break ; case 49 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 50 , 51 ) ; break ; case 50 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 50 , 51 ) ; break ; case 52 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 47 , 52 ) ; break ; case 53 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 53 , 54 ) ; break ; case 55 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 56 , 57 ) ; break ; case 56 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 57 , 58 ) ; break ; case 57 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 57 , 58 ) ; break ; case 59 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 58 , 59 ) ; break ; case 61 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 60 , 61 ) ; break ; case 62 : if ( ! jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 58 , 63 ) ; break ; case 63 : if ( ! jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 58 , 63 ) ; break ; case 64 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 30 , 35 ) ; break ; case 65 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 65 , 66 ) ; break ; case 67 : if ( ! jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 2 ) kind = 2 ; jjCheckNAddTwoStates ( 66 , 67 ) ; break ; case 68 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 68 , 69 ) ; break ; case 70 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 44 , 45 ) ; break ; case 72 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 72 , 73 ) ; break ; case 74 : if ( ! jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 4 ) kind = 4 ; jjstateSet [ jjnewStateCnt ++ ] = 74 ; break ; default : break ; } } while ( i != startsAt ) ; } if ( kind != 0x7fffffff ) { jjmatchedKind = kind ; jjmatchedPos = curPos ; kind = 0x7fffffff ; } ++ curPos ; if ( ( i = jjnewStateCnt ) == ( startsAt = 75 - ( jjnewStateCnt = startsAt ) ) ) return curPos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return curPos ; } } } static final int [ ] jjnextStates = { 30 , 31 , 32 , 34 , 38 , 39 , 41 , 42 , 46 , 47 , 53 , 54 , 5 , 6 , 10 , 11 , 19 , 20 , 3 , 4 , 8 , 9 , 17 , 18 , 31 , 32 , 34 , 32 , 33 , 34 , 65 , 66 , 68 , 69 , 72 , 73 , 5 , 6 , 19 , 20 , 25 , 26 , 50 , 51 , 70 , 71 , 12 , 13 , 14 , 15 , 23 , 24 , 43 , 44 , 48 , 49 , 55 , 56 , 59 , 60 , 61 , 62 , } ; private static final boolean jjCanMove_0 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 48 : return ( ( jjbitVec2 [ i2 ] & l2 ) != 0L ) ; case 49 : return ( ( jjbitVec3 [ i2 ] & l2 ) != 0L ) ; case 51 : return ( ( jjbitVec4 [ i2 ] & l2 ) != 0L ) ; case 77 : return ( ( jjbitVec5 [ i2 ] & l2 ) != 0L ) ; case 255 : return ( ( jjbitVec6 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec0 [ i1 ] & l1 ) != 0L ) return true ; return false ; } } private static final boolean jjCanMove_1 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 215 : return ( ( jjbitVec8 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec7 [ i1 ] & l1 ) != 0L ) return true ; return false ; } } private static final boolean jjCanMove_2 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 0 : return ( ( jjbitVec10 [ i2 ] & l2 ) != 0L ) ; case 255 : return ( ( jjbitVec11 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec9 [ i1 ] & l1 ) != 0L ) return true ; return false ; } } private static final boolean jjCanMove_3 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 6 : return ( ( jjbitVec14 [ i2 ] & l2 ) != 0L ) ; case 11 : return ( ( jjbitVec15 [ i2 ] & l2 ) != 0L ) ; case 13 : return ( ( jjbitVec16 [ i2 ] & l2 ) != 0L ) ; case 14 : return ( ( jjbitVec17 [ i2 ] & l2 ) != 0L ) ; case 16 : return ( ( jjbitVec18 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec12 [ i1 ] & l1 ) != 0L ) if ( ( jjbitVec13 [ i2 ] & l2 ) == 0L ) return false ; else return true ; return false ; } } private static final boolean jjCanMove_4 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 0 : return ( ( jjbitVec10 [ i2 ] & l2 ) != 0L ) ; case 215 : return ( ( jjbitVec8 [ i2 ] & l2 ) != 0L ) ; case 255 : return ( ( jjbitVec11 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec19 [ i1 ] & l1 ) != 0L ) return true ; return false ; } } public static final String [ ] jjstrLiteralImages = { "" , null , null , null , null , null , null , null , null , null , null , null , null , null , null , null , } ; public static final String [ ] lexStateNames = { "DEFAULT" , } ; static final long [ ] jjtoToken = { 0x30ffL , } ; static final long [ ] jjtoSkip = { 0x8000L , } ; protected CharStream input_stream ; private final int [ ] jjrounds = new int [ 75 ] ; private final int [ ] jjstateSet = new int [ 150 ] ; protected char curChar ; public StandardTokenizerTokenManager ( CharStream stream ) { input_stream = stream ; } public StandardTokenizerTokenManager ( CharStream stream , int lexState ) { this ( stream ) ; SwitchTo ( lexState ) ; } public void ReInit ( CharStream stream ) { jjmatchedPos = jjnewStateCnt = 0 ; curLexState = defaultLexState ; input_stream = stream ; ReInitRounds ( ) ; } private final void ReInitRounds ( ) { int i ; jjround = 0x80000001 ; for ( i = 75 ; i -- > 0 ; ) jjrounds [ i ] = 0x80000000 ; } public void ReInit ( CharStream stream , int lexState ) { ReInit ( stream ) ; SwitchTo ( lexState ) ; } public void SwitchTo ( int lexState ) { if ( lexState >= 1 || lexState < 0 ) throw new TokenMgrError ( "Error: Ignoring invalid lexical state : " + lexState + ". State unchanged." , TokenMgrError . INVALID_LEXICAL_STATE ) ; else curLexState = lexState ; } protected Token jjFillToken ( ) { Token t = Token . newToken ( jjmatchedKind ) ; t . kind = jjmatchedKind ; String im = jjstrLiteralImages [ jjmatchedKind ] ; t . image = ( im == null ) ? input_stream . GetImage ( ) : im ; t . beginLine = input_stream . getBeginLine ( ) ; t . beginColumn = input_stream . getBeginColumn ( ) ; t . endLine = input_stream . getEndLine ( ) ; t . endColumn = input_stream . getEndColumn ( ) ; return t ; } int curLexState = 0 ; int defaultLexState = 0 ; int jjnewStateCnt ; int jjround ; int jjmatchedPos ; int jjmatchedKind ; public Token getNextToken ( ) { int kind ; Token specialToken = null ; Token matchedToken ; int curPos = 0 ; EOFLoop : for ( ; ; ) { try { curChar = input_stream . BeginToken ( ) ; } catch ( java . io . IOException e ) { jjmatchedKind = 0 ; matchedToken = jjFillToken ( ) ; return matchedToken ; } jjmatchedKind = 0x7fffffff ; jjmatchedPos = 0 ; curPos = jjMoveStringLiteralDfa0_0 ( ) ; if ( jjmatchedPos == 0 && jjmatchedKind > 15 ) { jjmatchedKind = 15 ; } if ( jjmatchedKind != 0x7fffffff ) { if ( jjmatchedPos + 1 < curPos ) input_stream . backup ( curPos - jjmatchedPos - 1 ) ; if ( ( jjtoToken [ jjmatchedKind > > 6 ] & ( 1L << ( jjmatchedKind & 077 ) ) ) != 0L ) { matchedToken = jjFillToken ( ) ; return matchedToken ; } else { continue EOFLoop ; } } int error_line = input_stream . getEndLine ( ) ; int error_column = input_stream . getEndColumn ( ) ; String error_after = null ; boolean EOFSeen = false ; try { input_stream . readChar ( ) ; input_stream . backup ( 1 ) ; } catch ( java . io . IOException e1 ) { EOFSeen = true ; error_after = curPos <= 1 ? "" : input_stream . GetImage ( ) ; if ( curChar == '\n' || curChar == '\r' ) { error_line ++ ; error_column = 0 ; } else error_column ++ ; } if ( ! EOFSeen ) { input_stream . backup ( 1 ) ; error_after = curPos <= 1 ? "" : input_stream . GetImage ( ) ; } throw new TokenMgrError ( EOFSeen , curLexState , error_line , error_column , error_after , curChar , TokenMgrError . LEXICAL_ERROR ) ; } } } 	0
package org . apache . lucene . search ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . analysis . TokenStream ; import org . apache . lucene . index . TermFreqVector ; import java . io . IOException ; import java . io . StringReader ; import java . util . * ; public class QueryTermVector implements TermFreqVector { private String [ ] terms = new String [ 0 ] ; private int [ ] termFreqs = new int [ 0 ] ; public String getField ( ) { return null ; } public QueryTermVector ( String [ ] queryTerms ) { processTerms ( queryTerms ) ; } public QueryTermVector ( String queryString , Analyzer analyzer ) { if ( analyzer != null ) { TokenStream stream = analyzer . tokenStream ( "" , new StringReader ( queryString ) ) ; if ( stream != null ) { Token next = null ; List terms = new ArrayList ( ) ; try { while ( ( next = stream . next ( ) ) != null ) { terms . add ( next . termText ( ) ) ; } processTerms ( ( String [ ] ) terms . toArray ( new String [ terms . size ( ) ] ) ) ; } catch ( IOException e ) { } } } } private void processTerms ( String [ ] queryTerms ) { if ( queryTerms != null ) { Arrays . sort ( queryTerms ) ; Map tmpSet = new HashMap ( queryTerms . length ) ; List tmpList = new ArrayList ( queryTerms . length ) ; List tmpFreqs = new ArrayList ( queryTerms . length ) ; int j = 0 ; for ( int i = 0 ; i < queryTerms . length ; i ++ ) { String term = queryTerms [ i ] ; Integer position = ( Integer ) tmpSet . get ( term ) ; if ( position == null ) { tmpSet . put ( term , new Integer ( j ++ ) ) ; tmpList . add ( term ) ; tmpFreqs . add ( new Integer ( 1 ) ) ; } else { Integer integer = ( Integer ) tmpFreqs . get ( position . intValue ( ) ) ; tmpFreqs . set ( position . intValue ( ) , new Integer ( integer . intValue ( ) + 1 ) ) ; } } terms = ( String [ ] ) tmpList . toArray ( terms ) ; termFreqs = new int [ tmpFreqs . size ( ) ] ; int i = 0 ; for ( Iterator iter = tmpFreqs . iterator ( ) ; iter . hasNext ( ) ; ) { Integer integer = ( Integer ) iter . next ( ) ; termFreqs [ i ++ ] = integer . intValue ( ) ; } } } public final String toString ( ) { StringBuffer sb = new StringBuffer ( ) ; sb . append ( '{' ) ; for ( int i = 0 ; i < terms . length ; i ++ ) { if ( i > 0 ) sb . append ( ", " ) ; sb . append ( terms [ i ] ) . append ( '/' ) . append ( termFreqs [ i ] ) ; } sb . append ( '}' ) ; return sb . toString ( ) ; } public int size ( ) { return terms . length ; } public String [ ] getTerms ( ) { return terms ; } public int [ ] getTermFrequencies ( ) { return termFreqs ; } public int indexOf ( String term ) { int res = Arrays . binarySearch ( terms , term ) ; return res >= 0 ? res : - 1 ; } public int [ ] indexesOf ( String [ ] terms , int start , int len ) { int res [ ] = new int [ len ] ; for ( int i = 0 ; i < len ; i ++ ) { res [ i ] = indexOf ( terms [ i ] ) ; } return res ; } } 	1
package org . apache . lucene . analysis . standard ; public class Token { public int kind ; public int beginLine , beginColumn , endLine , endColumn ; public String image ; public Token next ; public Token specialToken ; public String toString ( ) { return image ; } public static final Token newToken ( int ofKind ) { switch ( ofKind ) { default : return new Token ( ) ; } } } 	0
package org . apache . lucene . index ; import java . io . ByteArrayOutputStream ; import java . io . IOException ; import java . util . Iterator ; import java . util . zip . Deflater ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; final class FieldsWriter { static final byte FIELD_IS_TOKENIZED = 0x1 ; static final byte FIELD_IS_BINARY = 0x2 ; static final byte FIELD_IS_COMPRESSED = 0x4 ; private FieldInfos fieldInfos ; private IndexOutput fieldsStream ; private IndexOutput indexStream ; FieldsWriter ( Directory d , String segment , FieldInfos fn ) throws IOException { fieldInfos = fn ; fieldsStream = d . createOutput ( segment + ".fdt" ) ; indexStream = d . createOutput ( segment + ".fdx" ) ; } final void close ( ) throws IOException { fieldsStream . close ( ) ; indexStream . close ( ) ; } final void addDocument ( Document doc ) throws IOException { indexStream . writeLong ( fieldsStream . getFilePointer ( ) ) ; int storedCount = 0 ; Iterator fieldIterator = doc . getFields ( ) . iterator ( ) ; while ( fieldIterator . hasNext ( ) ) { Fieldable field = ( Fieldable ) fieldIterator . next ( ) ; if ( field . isStored ( ) ) storedCount ++ ; } fieldsStream . writeVInt ( storedCount ) ; fieldIterator = doc . getFields ( ) . iterator ( ) ; while ( fieldIterator . hasNext ( ) ) { Fieldable field = ( Fieldable ) fieldIterator . next ( ) ; boolean disableCompression = ( field instanceof FieldsReader . FieldForMerge ) ; if ( field . isStored ( ) ) { fieldsStream . writeVInt ( fieldInfos . fieldNumber ( field . name ( ) ) ) ; byte bits = 0 ; if ( field . isTokenized ( ) ) bits |= FieldsWriter . FIELD_IS_TOKENIZED ; if ( field . isBinary ( ) ) bits |= FieldsWriter . FIELD_IS_BINARY ; if ( field . isCompressed ( ) ) bits |= FieldsWriter . FIELD_IS_COMPRESSED ; fieldsStream . writeByte ( bits ) ; if ( field . isCompressed ( ) ) { byte [ ] data = null ; if ( disableCompression ) { data = field . binaryValue ( ) ; } else { if ( field . isBinary ( ) ) { data = compress ( field . binaryValue ( ) ) ; } else { data = compress ( field . stringValue ( ) . getBytes ( "UTF-8" ) ) ; } } final int len = data . length ; fieldsStream . writeVInt ( len ) ; fieldsStream . writeBytes ( data , len ) ; } else { if ( field . isBinary ( ) ) { byte [ ] data = field . binaryValue ( ) ; final int len = data . length ; fieldsStream . writeVInt ( len ) ; fieldsStream . writeBytes ( data , len ) ; } else { fieldsStream . writeString ( field . stringValue ( ) ) ; } } } } } private final byte [ ] compress ( byte [ ] input ) { Deflater compressor = new Deflater ( ) ; compressor . setLevel ( Deflater . BEST_COMPRESSION ) ; compressor . setInput ( input ) ; compressor . finish ( ) ; ByteArrayOutputStream bos = new ByteArrayOutputStream ( input . length ) ; byte [ ] buf = new byte [ 1024 ] ; while ( ! compressor . finished ( ) ) { int count = compressor . deflate ( buf ) ; bos . write ( buf , 0 , count ) ; } compressor . end ( ) ; return bos . toByteArray ( ) ; } } 	1
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermEnum ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; public class PrefixQuery extends Query { private Term prefix ; public PrefixQuery ( Term prefix ) { this . prefix = prefix ; } public Term getPrefix ( ) { return prefix ; } public Query rewrite ( IndexReader reader ) throws IOException { BooleanQuery query = new BooleanQuery ( true ) ; TermEnum enumerator = reader . terms ( prefix ) ; try { String prefixText = prefix . text ( ) ; String prefixField = prefix . field ( ) ; do { Term term = enumerator . term ( ) ; if ( term != null && term . text ( ) . startsWith ( prefixText ) && term . field ( ) == prefixField ) { TermQuery tq = new TermQuery ( term ) ; tq . setBoost ( getBoost ( ) ) ; query . add ( tq , BooleanClause . Occur . SHOULD ) ; } else { break ; } } while ( enumerator . next ( ) ) ; } finally { enumerator . close ( ) ; } return query ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! prefix . field ( ) . equals ( field ) ) { buffer . append ( prefix . field ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( prefix . text ( ) ) ; buffer . append ( '*' ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof PrefixQuery ) ) return false ; PrefixQuery other = ( PrefixQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && this . prefix . equals ( other . prefix ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ prefix . hashCode ( ) ^ 0x6634D93C ; } } 	0
package org . apache . lucene . index ; import org . apache . lucene . store . IndexInput ; import java . io . IOException ; final class SegmentTermPositions extends SegmentTermDocs implements TermPositions { private IndexInput proxStream ; private int proxCount ; private int position ; private int payloadLength ; private boolean needToLoadPayload ; private long lazySkipPointer = 0 ; private int lazySkipProxCount = 0 ; SegmentTermPositions ( SegmentReader p ) { super ( p ) ; this . proxStream = null ; } final void seek ( TermInfo ti , Term term ) throws IOException { super . seek ( ti , term ) ; if ( ti != null ) lazySkipPointer = ti . proxPointer ; lazySkipProxCount = 0 ; proxCount = 0 ; payloadLength = 0 ; needToLoadPayload = false ; } public final void close ( ) throws IOException { super . close ( ) ; if ( proxStream != null ) proxStream . close ( ) ; } public final int nextPosition ( ) throws IOException { lazySkip ( ) ; proxCount -- ; return position += readDeltaPosition ( ) ; } private final int readDeltaPosition ( ) throws IOException { int delta = proxStream . readVInt ( ) ; if ( currentFieldStoresPayloads ) { if ( ( delta & 1 ) != 0 ) { payloadLength = proxStream . readVInt ( ) ; } delta >>>= 1 ; needToLoadPayload = true ; } else { payloadLength = 0 ; needToLoadPayload = false ; } return delta ; } protected final void skippingDoc ( ) throws IOException { lazySkipProxCount += freq ; } public final boolean next ( ) throws IOException { lazySkipProxCount += proxCount ; if ( super . next ( ) ) { proxCount = freq ; position = 0 ; return true ; } return false ; } public final int read ( final int [ ] docs , final int [ ] freqs ) { throw new UnsupportedOperationException ( "TermPositions does not support processing multiple documents in one call. Use TermDocs instead." ) ; } protected void skipProx ( long proxPointer , int payloadLength ) throws IOException { lazySkipPointer = proxPointer ; lazySkipProxCount = 0 ; proxCount = 0 ; this . payloadLength = payloadLength ; needToLoadPayload = false ; } private void skipPositions ( int n ) throws IOException { for ( int f = n ; f > 0 ; f -- ) { readDeltaPosition ( ) ; skipPayload ( ) ; } } private void skipPayload ( ) throws IOException { if ( needToLoadPayload && payloadLength > 0 ) { proxStream . seek ( proxStream . getFilePointer ( ) + payloadLength ) ; } needToLoadPayload = false ; } private void lazySkip ( ) throws IOException { if ( proxStream == null ) { proxStream = ( IndexInput ) parent . proxStream . clone ( ) ; } skipPayload ( ) ; if ( lazySkipPointer != 0 ) { proxStream . seek ( lazySkipPointer ) ; lazySkipPointer = 0 ; } if ( lazySkipProxCount != 0 ) { skipPositions ( lazySkipProxCount ) ; lazySkipProxCount = 0 ; } } public int getPayloadLength ( ) { return payloadLength ; } public byte [ ] getPayload ( byte [ ] data , int offset ) throws IOException { if ( ! needToLoadPayload ) { throw new IOException ( "Payload cannot be loaded more than once for the same term position." ) ; } byte [ ] retArray ; int retOffset ; if ( data == null || data . length - offset < payloadLength ) { retArray = new byte [ payloadLength ] ; retOffset = 0 ; } else { retArray = data ; retOffset = offset ; } proxStream . readBytes ( retArray , retOffset , payloadLength ) ; needToLoadPayload = false ; return retArray ; } public boolean isPayloadAvailable ( ) { return needToLoadPayload && payloadLength > 0 ; } } 	1
package org . apache . lucene . analysis ; import java . io . Reader ; public final class LowerCaseTokenizer extends LetterTokenizer { public LowerCaseTokenizer ( Reader in ) { super ( in ) ; } protected char normalize ( char c ) { return Character . toLowerCase ( c ) ; } } 	0
package org . apache . lucene . store ; import java . io . IOException ; public abstract class IndexInput implements Cloneable { private char [ ] chars ; public abstract byte readByte ( ) throws IOException ; public abstract void readBytes ( byte [ ] b , int offset , int len ) throws IOException ; public int readInt ( ) throws IOException { return ( ( readByte ( ) & 0xFF ) << 24 ) | ( ( readByte ( ) & 0xFF ) << 16 ) | ( ( readByte ( ) & 0xFF ) << 8 ) | ( readByte ( ) & 0xFF ) ; } public int readVInt ( ) throws IOException { byte b = readByte ( ) ; int i = b & 0x7F ; for ( int shift = 7 ; ( b & 0x80 ) != 0 ; shift += 7 ) { b = readByte ( ) ; i |= ( b & 0x7F ) << shift ; } return i ; } public long readLong ( ) throws IOException { return ( ( ( long ) readInt ( ) ) << 32 ) | ( readInt ( ) & 0xFFFFFFFFL ) ; } public long readVLong ( ) throws IOException { byte b = readByte ( ) ; long i = b & 0x7F ; for ( int shift = 7 ; ( b & 0x80 ) != 0 ; shift += 7 ) { b = readByte ( ) ; i |= ( b & 0x7FL ) << shift ; } return i ; } public String readString ( ) throws IOException { int length = readVInt ( ) ; if ( chars == null || length > chars . length ) chars = new char [ length ] ; readChars ( chars , 0 , length ) ; return new String ( chars , 0 , length ) ; } public void readChars ( char [ ] buffer , int start , int length ) throws IOException { final int end = start + length ; for ( int i = start ; i < end ; i ++ ) { byte b = readByte ( ) ; if ( ( b & 0x80 ) == 0 ) buffer [ i ] = ( char ) ( b & 0x7F ) ; else if ( ( b & 0xE0 ) != 0xE0 ) { buffer [ i ] = ( char ) ( ( ( b & 0x1F ) << 6 ) | ( readByte ( ) & 0x3F ) ) ; } else buffer [ i ] = ( char ) ( ( ( b & 0x0F ) << 12 ) | ( ( readByte ( ) & 0x3F ) << 6 ) | ( readByte ( ) & 0x3F ) ) ; } } public void skipChars ( int length ) throws IOException { for ( int i = 0 ; i < length ; i ++ ) { byte b = readByte ( ) ; if ( ( b & 0x80 ) == 0 ) { } else if ( ( b & 0xE0 ) != 0xE0 ) { readByte ( ) ; } else { readByte ( ) ; readByte ( ) ; } } } public abstract void close ( ) throws IOException ; public abstract long getFilePointer ( ) ; public abstract void seek ( long pos ) throws IOException ; public abstract long length ( ) ; public Object clone ( ) { IndexInput clone = null ; try { clone = ( IndexInput ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . chars = null ; return clone ; } } 	1
package org . apache . lucene . util ; import java . io . IOException ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . IndexOutput ; public final class BitVector { private byte [ ] bits ; private int size ; private int count = - 1 ; public BitVector ( int n ) { size = n ; bits = new byte [ ( size > > 3 ) + 1 ] ; } public final void set ( int bit ) { if ( bit >= size ) { throw new ArrayIndexOutOfBoundsException ( bit ) ; } bits [ bit > > 3 ] |= 1 << ( bit & 7 ) ; count = - 1 ; } public final void clear ( int bit ) { if ( bit >= size ) { throw new ArrayIndexOutOfBoundsException ( bit ) ; } bits [ bit > > 3 ] &= ~ ( 1 << ( bit & 7 ) ) ; count = - 1 ; } public final boolean get ( int bit ) { if ( bit >= size ) { throw new ArrayIndexOutOfBoundsException ( bit ) ; } return ( bits [ bit > > 3 ] & ( 1 << ( bit & 7 ) ) ) != 0 ; } public final int size ( ) { return size ; } public final int count ( ) { if ( count == - 1 ) { int c = 0 ; int end = bits . length ; for ( int i = 0 ; i < end ; i ++ ) c += BYTE_COUNTS [ bits [ i ] & 0xFF ] ; count = c ; } return count ; } private static final byte [ ] BYTE_COUNTS = { 0 , 1 , 1 , 2 , 1 , 2 , 2 , 3 , 1 , 2 , 2 , 3 , 2 , 3 , 3 , 4 , 1 , 2 , 2 , 3 , 2 , 3 , 3 , 4 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 1 , 2 , 2 , 3 , 2 , 3 , 3 , 4 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 1 , 2 , 2 , 3 , 2 , 3 , 3 , 4 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 4 , 5 , 5 , 6 , 5 , 6 , 6 , 7 , 1 , 2 , 2 , 3 , 2 , 3 , 3 , 4 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 4 , 5 , 5 , 6 , 5 , 6 , 6 , 7 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 4 , 5 , 5 , 6 , 5 , 6 , 6 , 7 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 4 , 5 , 5 , 6 , 5 , 6 , 6 , 7 , 4 , 5 , 5 , 6 , 5 , 6 , 6 , 7 , 5 , 6 , 6 , 7 , 6 , 7 , 7 , 8 } ; public final void write ( Directory d , String name ) throws IOException { IndexOutput output = d . createOutput ( name ) ; try { if ( isSparse ( ) ) { writeDgaps ( output ) ; } else { writeBits ( output ) ; } } finally { output . close ( ) ; } } private void writeBits ( IndexOutput output ) throws IOException { output . writeInt ( size ( ) ) ; output . writeInt ( count ( ) ) ; output . writeBytes ( bits , bits . length ) ; } private void writeDgaps ( IndexOutput output ) throws IOException { output . writeInt ( - 1 ) ; output . writeInt ( size ( ) ) ; output . writeInt ( count ( ) ) ; int last = 0 ; int n = count ( ) ; int m = bits . length ; for ( int i = 0 ; i < m && n > 0 ; i ++ ) { if ( bits [ i ] != 0 ) { output . writeVInt ( i - last ) ; output . writeByte ( bits [ i ] ) ; last = i ; n -= BYTE_COUNTS [ bits [ i ] & 0xFF ] ; } } } private boolean isSparse ( ) { int factor = 10 ; if ( bits . length < ( 1 << 7 ) ) return factor * ( 4 + ( 8 + 8 ) * count ( ) ) < size ( ) ; if ( bits . length < ( 1 << 14 ) ) return factor * ( 4 + ( 8 + 16 ) * count ( ) ) < size ( ) ; if ( bits . length < ( 1 << 21 ) ) return factor * ( 4 + ( 8 + 24 ) * count ( ) ) < size ( ) ; if ( bits . length < ( 1 << 28 ) ) return factor * ( 4 + ( 8 + 32 ) * count ( ) ) < size ( ) ; return factor * ( 4 + ( 8 + 40 ) * count ( ) ) < size ( ) ; } public BitVector ( Directory d , String name ) throws IOException { IndexInput input = d . openInput ( name ) ; try { size = input . readInt ( ) ; if ( size == - 1 ) { readDgaps ( input ) ; } else { readBits ( input ) ; } } finally { input . close ( ) ; } } private void readBits ( IndexInput input ) throws IOException { count = input . readInt ( ) ; bits = new byte [ ( size > > 3 ) + 1 ] ; input . readBytes ( bits , 0 , bits . length ) ; } private void readDgaps ( IndexInput input ) throws IOException { size = input . readInt ( ) ; count = input . readInt ( ) ; bits = new byte [ ( size > > 3 ) + 1 ] ; int last = 0 ; int n = count ( ) ; while ( n > 0 ) { last += input . readVInt ( ) ; bits [ last ] = input . readByte ( ) ; n -= BYTE_COUNTS [ bits [ last ] & 0xFF ] ; } } } 	0
package org . apache . lucene . analysis ; import java . io . IOException ; public abstract class TokenStream { public abstract Token next ( ) throws IOException ; public void reset ( ) throws IOException { } public void close ( ) throws IOException { } } 	1
package org . apache . lucene . index ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . analysis . TokenStream ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . search . Similarity ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; import java . io . IOException ; import java . io . PrintStream ; import java . io . Reader ; import java . io . StringReader ; import java . util . Arrays ; import java . util . BitSet ; import java . util . Enumeration ; import java . util . Hashtable ; import java . util . Iterator ; import java . util . LinkedList ; import java . util . List ; final class DocumentWriter { private Analyzer analyzer ; private Directory directory ; private Similarity similarity ; private FieldInfos fieldInfos ; private int maxFieldLength ; private int termIndexInterval = IndexWriter . DEFAULT_TERM_INDEX_INTERVAL ; private PrintStream infoStream ; DocumentWriter ( Directory directory , Analyzer analyzer , Similarity similarity , int maxFieldLength ) { this . directory = directory ; this . analyzer = analyzer ; this . similarity = similarity ; this . maxFieldLength = maxFieldLength ; } DocumentWriter ( Directory directory , Analyzer analyzer , IndexWriter writer ) { this . directory = directory ; this . analyzer = analyzer ; this . similarity = writer . getSimilarity ( ) ; this . maxFieldLength = writer . getMaxFieldLength ( ) ; this . termIndexInterval = writer . getTermIndexInterval ( ) ; } final void addDocument ( String segment , Document doc ) throws CorruptIndexException , IOException { fieldInfos = new FieldInfos ( ) ; fieldInfos . add ( doc ) ; postingTable . clear ( ) ; fieldLengths = new int [ fieldInfos . size ( ) ] ; fieldPositions = new int [ fieldInfos . size ( ) ] ; fieldOffsets = new int [ fieldInfos . size ( ) ] ; fieldStoresPayloads = new BitSet ( fieldInfos . size ( ) ) ; fieldBoosts = new float [ fieldInfos . size ( ) ] ; Arrays . fill ( fieldBoosts , doc . getBoost ( ) ) ; try { invertDocument ( doc ) ; Posting [ ] postings = sortPostingTable ( ) ; fieldInfos . write ( directory , segment + ".fnm" ) ; FieldsWriter fieldsWriter = new FieldsWriter ( directory , segment , fieldInfos ) ; try { fieldsWriter . addDocument ( doc ) ; } finally { fieldsWriter . close ( ) ; } writePostings ( postings , segment ) ; writeNorms ( segment ) ; } finally { IOException ex = null ; Iterator it = openTokenStreams . iterator ( ) ; while ( it . hasNext ( ) ) { try { ( ( TokenStream ) it . next ( ) ) . close ( ) ; } catch ( IOException e ) { if ( ex != null ) { ex = e ; } } } openTokenStreams . clear ( ) ; if ( ex != null ) { throw ex ; } } } private final Hashtable postingTable = new Hashtable ( ) ; private int [ ] fieldLengths ; private int [ ] fieldPositions ; private int [ ] fieldOffsets ; private float [ ] fieldBoosts ; private BitSet fieldStoresPayloads ; private List openTokenStreams = new LinkedList ( ) ; private final void invertDocument ( Document doc ) throws IOException { Iterator fieldIterator = doc . getFields ( ) . iterator ( ) ; while ( fieldIterator . hasNext ( ) ) { Fieldable field = ( Fieldable ) fieldIterator . next ( ) ; String fieldName = field . name ( ) ; int fieldNumber = fieldInfos . fieldNumber ( fieldName ) ; int length = fieldLengths [ fieldNumber ] ; int position = fieldPositions [ fieldNumber ] ; if ( length > 0 ) position += analyzer . getPositionIncrementGap ( fieldName ) ; int offset = fieldOffsets [ fieldNumber ] ; if ( field . isIndexed ( ) ) { if ( ! field . isTokenized ( ) ) { String stringValue = field . stringValue ( ) ; if ( field . isStoreOffsetWithTermVector ( ) ) addPosition ( fieldName , stringValue , position ++ , null , new TermVectorOffsetInfo ( offset , offset + stringValue . length ( ) ) ) ; else addPosition ( fieldName , stringValue , position ++ , null , null ) ; offset += stringValue . length ( ) ; length ++ ; } else { TokenStream stream = field . tokenStreamValue ( ) ; if ( stream == null ) { Reader reader ; if ( field . readerValue ( ) != null ) reader = field . readerValue ( ) ; else if ( field . stringValue ( ) != null ) reader = new StringReader ( field . stringValue ( ) ) ; else throw new IllegalArgumentException ( "field must have either String or Reader value" ) ; stream = analyzer . tokenStream ( fieldName , reader ) ; } openTokenStreams . add ( stream ) ; stream . reset ( ) ; Token lastToken = null ; for ( Token t = stream . next ( ) ; t != null ; t = stream . next ( ) ) { position += ( t . getPositionIncrement ( ) - 1 ) ; Payload payload = t . getPayload ( ) ; if ( payload != null ) { fieldStoresPayloads . set ( fieldNumber ) ; } TermVectorOffsetInfo termVectorOffsetInfo ; if ( field . isStoreOffsetWithTermVector ( ) ) { termVectorOffsetInfo = new TermVectorOffsetInfo ( offset + t . startOffset ( ) , offset + t . endOffset ( ) ) ; } else { termVectorOffsetInfo = null ; } addPosition ( fieldName , t . termText ( ) , position ++ , payload , termVectorOffsetInfo ) ; lastToken = t ; if ( ++ length >= maxFieldLength ) { if ( infoStream != null ) infoStream . println ( "maxFieldLength " + maxFieldLength + " reached, ignoring following tokens" ) ; break ; } } if ( lastToken != null ) offset += lastToken . endOffset ( ) + 1 ; } fieldLengths [ fieldNumber ] = length ; fieldPositions [ fieldNumber ] = position ; fieldBoosts [ fieldNumber ] *= field . getBoost ( ) ; fieldOffsets [ fieldNumber ] = offset ; } } for ( int i = fieldStoresPayloads . nextSetBit ( 0 ) ; i >= 0 ; i = fieldStoresPayloads . nextSetBit ( i + 1 ) ) { fieldInfos . fieldInfo ( i ) . storePayloads = true ; } } private final Term termBuffer = new Term ( "" , "" ) ; private final void addPosition ( String field , String text , int position , Payload payload , TermVectorOffsetInfo offset ) { termBuffer . set ( field , text ) ; Posting ti = ( Posting ) postingTable . get ( termBuffer ) ; if ( ti != null ) { int freq = ti . freq ; if ( ti . positions . length == freq ) { int [ ] newPositions = new int [ freq * 2 ] ; int [ ] positions = ti . positions ; System . arraycopy ( positions , 0 , newPositions , 0 , freq ) ; ti . positions = newPositions ; if ( ti . payloads != null ) { Payload [ ] newPayloads = new Payload [ freq * 2 ] ; Payload [ ] payloads = ti . payloads ; System . arraycopy ( payloads , 0 , newPayloads , 0 , payloads . length ) ; ti . payloads = newPayloads ; } } ti . positions [ freq ] = position ; if ( payload != null ) { if ( ti . payloads == null ) { ti . payloads = new Payload [ ti . positions . length ] ; } ti . payloads [ freq ] = payload ; } if ( offset != null ) { if ( ti . offsets . length == freq ) { TermVectorOffsetInfo [ ] newOffsets = new TermVectorOffsetInfo [ freq * 2 ] ; TermVectorOffsetInfo [ ] offsets = ti . offsets ; System . arraycopy ( offsets , 0 , newOffsets , 0 , freq ) ; ti . offsets = newOffsets ; } ti . offsets [ freq ] = offset ; } ti . freq = freq + 1 ; } else { Term term = new Term ( field , text , false ) ; postingTable . put ( term , new Posting ( term , position , payload , offset ) ) ; } } private final Posting [ ] sortPostingTable ( ) { Posting [ ] array = new Posting [ postingTable . size ( ) ] ; Enumeration postings = postingTable . elements ( ) ; for ( int i = 0 ; postings . hasMoreElements ( ) ; i ++ ) array [ i ] = ( Posting ) postings . nextElement ( ) ; quickSort ( array , 0 , array . length - 1 ) ; return array ; } private static final void quickSort ( Posting [ ] postings , int lo , int hi ) { if ( lo >= hi ) return ; int mid = ( lo + hi ) / 2 ; if ( postings [ lo ] . term . compareTo ( postings [ mid ] . term ) > 0 ) { Posting tmp = postings [ lo ] ; postings [ lo ] = postings [ mid ] ; postings [ mid ] = tmp ; } if ( postings [ mid ] . term . compareTo ( postings [ hi ] . term ) > 0 ) { Posting tmp = postings [ mid ] ; postings [ mid ] = postings [ hi ] ; postings [ hi ] = tmp ; if ( postings [ lo ] . term . compareTo ( postings [ mid ] . term ) > 0 ) { Posting tmp2 = postings [ lo ] ; postings [ lo ] = postings [ mid ] ; postings [ mid ] = tmp2 ; } } int left = lo + 1 ; int right = hi - 1 ; if ( left >= right ) return ; Term partition = postings [ mid ] . term ; for ( ; ; ) { while ( postings [ right ] . term . compareTo ( partition ) > 0 ) -- right ; while ( left < right && postings [ left ] . term . compareTo ( partition ) <= 0 ) ++ left ; if ( left < right ) { Posting tmp = postings [ left ] ; postings [ left ] = postings [ right ] ; postings [ right ] = tmp ; -- right ; } else { break ; } } quickSort ( postings , lo , left ) ; quickSort ( postings , left + 1 , hi ) ; } private final void writePostings ( Posting [ ] postings , String segment ) throws CorruptIndexException , IOException { IndexOutput freq = null , prox = null ; TermInfosWriter tis = null ; TermVectorsWriter termVectorWriter = null ; try { freq = directory . createOutput ( segment + ".frq" ) ; prox = directory . createOutput ( segment + ".prx" ) ; tis = new TermInfosWriter ( directory , segment , fieldInfos , termIndexInterval ) ; TermInfo ti = new TermInfo ( ) ; String currentField = null ; boolean currentFieldHasPayloads = false ; for ( int i = 0 ; i < postings . length ; i ++ ) { Posting posting = postings [ i ] ; String termField = posting . term . field ( ) ; if ( currentField != termField ) { currentField = termField ; FieldInfo fi = fieldInfos . fieldInfo ( currentField ) ; currentFieldHasPayloads = fi . storePayloads ; if ( fi . storeTermVector ) { if ( termVectorWriter == null ) { termVectorWriter = new TermVectorsWriter ( directory , segment , fieldInfos ) ; termVectorWriter . openDocument ( ) ; } termVectorWriter . openField ( currentField ) ; } else if ( termVectorWriter != null ) { termVectorWriter . closeField ( ) ; } } ti . set ( 1 , freq . getFilePointer ( ) , prox . getFilePointer ( ) , - 1 ) ; tis . add ( posting . term , ti ) ; int postingFreq = posting . freq ; if ( postingFreq == 1 ) freq . writeVInt ( 1 ) ; else { freq . writeVInt ( 0 ) ; freq . writeVInt ( postingFreq ) ; } int lastPosition = 0 ; int [ ] positions = posting . positions ; Payload [ ] payloads = posting . payloads ; int lastPayloadLength = - 1 ; for ( int j = 0 ; j < postingFreq ; j ++ ) { int position = positions [ j ] ; int delta = position - lastPosition ; if ( currentFieldHasPayloads ) { int payloadLength = 0 ; Payload payload = null ; if ( payloads != null ) { payload = payloads [ j ] ; if ( payload != null ) { payloadLength = payload . length ; } } if ( payloadLength == lastPayloadLength ) { prox . writeVInt ( delta * 2 ) ; } else { prox . writeVInt ( delta * 2 + 1 ) ; prox . writeVInt ( payloadLength ) ; lastPayloadLength = payloadLength ; } if ( payloadLength > 0 ) { prox . writeBytes ( payload . data , payload . offset , payload . length ) ; } } else { prox . writeVInt ( delta ) ; } lastPosition = position ; } if ( termVectorWriter != null && termVectorWriter . isFieldOpen ( ) ) { termVectorWriter . addTerm ( posting . term . text ( ) , postingFreq , posting . positions , posting . offsets ) ; } } if ( termVectorWriter != null ) termVectorWriter . closeDocument ( ) ; } finally { IOException keep = null ; if ( freq != null ) try { freq . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( prox != null ) try { prox . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tis != null ) try { tis . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( termVectorWriter != null ) try { termVectorWriter . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( keep != null ) throw ( IOException ) keep . fillInStackTrace ( ) ; } } private final void writeNorms ( String segment ) throws IOException { for ( int n = 0 ; n < fieldInfos . size ( ) ; n ++ ) { FieldInfo fi = fieldInfos . fieldInfo ( n ) ; if ( fi . isIndexed && ! fi . omitNorms ) { float norm = fieldBoosts [ n ] * similarity . lengthNorm ( fi . name , fieldLengths [ n ] ) ; IndexOutput norms = directory . createOutput ( segment + ".f" + n ) ; try { norms . writeByte ( Similarity . encodeNorm ( norm ) ) ; } finally { norms . close ( ) ; } } } } void setInfoStream ( PrintStream infoStream ) { this . infoStream = infoStream ; } int getNumFields ( ) { return fieldInfos . size ( ) ; } } final class Posting { Term term ; int freq ; int [ ] positions ; Payload [ ] payloads ; TermVectorOffsetInfo [ ] offsets ; Posting ( Term t , int position , Payload payload , TermVectorOffsetInfo offset ) { term = t ; freq = 1 ; positions = new int [ 1 ] ; positions [ 0 ] = position ; if ( payload != null ) { payloads = new Payload [ 1 ] ; payloads [ 0 ] = payload ; } else payloads = null ; if ( offset != null ) { offsets = new TermVectorOffsetInfo [ 1 ] ; offsets [ 0 ] = offset ; } else offsets = null ; } } 	0
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermDocs ; import org . apache . lucene . index . TermEnum ; import java . io . IOException ; import java . util . BitSet ; public class RangeFilter extends Filter { private String fieldName ; private String lowerTerm ; private String upperTerm ; private boolean includeLower ; private boolean includeUpper ; public RangeFilter ( String fieldName , String lowerTerm , String upperTerm , boolean includeLower , boolean includeUpper ) { this . fieldName = fieldName ; this . lowerTerm = lowerTerm ; this . upperTerm = upperTerm ; this . includeLower = includeLower ; this . includeUpper = includeUpper ; if ( null == lowerTerm && null == upperTerm ) { throw new IllegalArgumentException ( "At least one value must be non-null" ) ; } if ( includeLower && null == lowerTerm ) { throw new IllegalArgumentException ( "The lower bound must be non-null to be inclusive" ) ; } if ( includeUpper && null == upperTerm ) { throw new IllegalArgumentException ( "The upper bound must be non-null to be inclusive" ) ; } } public static RangeFilter Less ( String fieldName , String upperTerm ) { return new RangeFilter ( fieldName , null , upperTerm , false , true ) ; } public static RangeFilter More ( String fieldName , String lowerTerm ) { return new RangeFilter ( fieldName , lowerTerm , null , true , false ) ; } public BitSet bits ( IndexReader reader ) throws IOException { BitSet bits = new BitSet ( reader . maxDoc ( ) ) ; TermEnum enumerator = ( null != lowerTerm ? reader . terms ( new Term ( fieldName , lowerTerm ) ) : reader . terms ( new Term ( fieldName , "" ) ) ) ; try { if ( enumerator . term ( ) == null ) { return bits ; } boolean checkLower = false ; if ( ! includeLower ) checkLower = true ; TermDocs termDocs = reader . termDocs ( ) ; try { do { Term term = enumerator . term ( ) ; if ( term != null && term . field ( ) . equals ( fieldName ) ) { if ( ! checkLower || null == lowerTerm || term . text ( ) . compareTo ( lowerTerm ) > 0 ) { checkLower = false ; if ( upperTerm != null ) { int compare = upperTerm . compareTo ( term . text ( ) ) ; if ( ( compare < 0 ) || ( ! includeUpper && compare == 0 ) ) { break ; } } termDocs . seek ( enumerator . term ( ) ) ; while ( termDocs . next ( ) ) { bits . set ( termDocs . doc ( ) ) ; } } } else { break ; } } while ( enumerator . next ( ) ) ; } finally { termDocs . close ( ) ; } } finally { enumerator . close ( ) ; } return bits ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( fieldName ) ; buffer . append ( ":" ) ; buffer . append ( includeLower ? "[" : "{" ) ; if ( null != lowerTerm ) { buffer . append ( lowerTerm ) ; } buffer . append ( "-" ) ; if ( null != upperTerm ) { buffer . append ( upperTerm ) ; } buffer . append ( includeUpper ? "]" : "}" ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof RangeFilter ) ) return false ; RangeFilter other = ( RangeFilter ) o ; if ( ! this . fieldName . equals ( other . fieldName ) || this . includeLower != other . includeLower || this . includeUpper != other . includeUpper ) { return false ; } if ( this . lowerTerm != null ? ! this . lowerTerm . equals ( other . lowerTerm ) : other . lowerTerm != null ) return false ; if ( this . upperTerm != null ? ! this . upperTerm . equals ( other . upperTerm ) : other . upperTerm != null ) return false ; return true ; } public int hashCode ( ) { int h = fieldName . hashCode ( ) ; h ^= lowerTerm != null ? lowerTerm . hashCode ( ) : 0xB6ECE882 ; h = ( h << 1 ) | ( h > > > 31 ) ; h ^= ( upperTerm != null ? ( upperTerm . hashCode ( ) ) : 0x91BEC2C2 ) ; h ^= ( includeLower ? 0xD484B933 : 0 ) ^ ( includeUpper ? 0x6AE423AC : 0 ) ; return h ; } } 	1
package org . apache . lucene . util ; public final class Constants { private Constants ( ) { } public static final String JAVA_VERSION = System . getProperty ( "java.version" ) ; public static final boolean JAVA_1_1 = JAVA_VERSION . startsWith ( "1.1." ) ; public static final boolean JAVA_1_2 = JAVA_VERSION . startsWith ( "1.2." ) ; public static final boolean JAVA_1_3 = JAVA_VERSION . startsWith ( "1.3." ) ; public static final String OS_NAME = System . getProperty ( "os.name" ) ; public static final boolean LINUX = OS_NAME . startsWith ( "Linux" ) ; public static final boolean WINDOWS = OS_NAME . startsWith ( "Windows" ) ; public static final boolean SUN_OS = OS_NAME . startsWith ( "SunOS" ) ; } 	0
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . FieldCache ; import java . io . IOException ; public class ReverseOrdFieldSource extends ValueSource { public String field ; public ReverseOrdFieldSource ( String field ) { this . field = field ; } public String description ( ) { return "rord(" + field + ')' ; } public DocValues getValues ( IndexReader reader ) throws IOException { final FieldCache . StringIndex sindex = FieldCache . DEFAULT . getStringIndex ( reader , field ) ; final int arr [ ] = sindex . order ; final int end = sindex . lookup . length ; return new DocValues ( arr . length ) { public float floatVal ( int doc ) { return ( float ) ( end - arr [ doc ] ) ; } public int intVal ( int doc ) { return end - arr [ doc ] ; } public String strVal ( int doc ) { return Integer . toString ( intVal ( doc ) ) ; } public String toString ( int doc ) { return description ( ) + '=' + strVal ( doc ) ; } Object getInnerArray ( ) { return arr ; } } ; } public boolean equals ( Object o ) { if ( o . getClass ( ) != ReverseOrdFieldSource . class ) return false ; ReverseOrdFieldSource other = ( ReverseOrdFieldSource ) o ; return this . field . equals ( other . field ) ; } private static final int hcode = ReverseOrdFieldSource . class . hashCode ( ) ; public int hashCode ( ) { return hcode + field . hashCode ( ) ; } } 	1
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . util . ToStringUtils ; public abstract class MultiTermQuery extends Query { private Term term ; public MultiTermQuery ( Term term ) { this . term = term ; } public Term getTerm ( ) { return term ; } protected abstract FilteredTermEnum getEnum ( IndexReader reader ) throws IOException ; public Query rewrite ( IndexReader reader ) throws IOException { FilteredTermEnum enumerator = getEnum ( reader ) ; BooleanQuery query = new BooleanQuery ( true ) ; try { do { Term t = enumerator . term ( ) ; if ( t != null ) { TermQuery tq = new TermQuery ( t ) ; tq . setBoost ( getBoost ( ) * enumerator . difference ( ) ) ; query . add ( tq , BooleanClause . Occur . SHOULD ) ; } } while ( enumerator . next ( ) ) ; } finally { enumerator . close ( ) ; } return query ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! term . field ( ) . equals ( field ) ) { buffer . append ( term . field ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( term . text ( ) ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof MultiTermQuery ) ) return false ; final MultiTermQuery multiTermQuery = ( MultiTermQuery ) o ; if ( ! term . equals ( multiTermQuery . term ) ) return false ; return getBoost ( ) == multiTermQuery . getBoost ( ) ; } public int hashCode ( ) { return term . hashCode ( ) + Float . floatToRawIntBits ( getBoost ( ) ) ; } } 	0
package org . apache . lucene . analysis ; import java . io . IOException ; public final class PorterStemFilter extends TokenFilter { private PorterStemmer stemmer ; public PorterStemFilter ( TokenStream in ) { super ( in ) ; stemmer = new PorterStemmer ( ) ; } public final Token next ( ) throws IOException { Token token = input . next ( ) ; if ( token == null ) return null ; else { String s = stemmer . stem ( token . termText ) ; if ( s != token . termText ) token . termText = s ; return token ; } } } 	1
package org . apache . lucene . search ; import java . io . IOException ; final class BooleanScorer extends Scorer { private SubScorer scorers = null ; private BucketTable bucketTable = new BucketTable ( ) ; private int maxCoord = 1 ; private float [ ] coordFactors = null ; private int requiredMask = 0 ; private int prohibitedMask = 0 ; private int nextMask = 1 ; private final int minNrShouldMatch ; BooleanScorer ( Similarity similarity ) { this ( similarity , 1 ) ; } BooleanScorer ( Similarity similarity , int minNrShouldMatch ) { super ( similarity ) ; this . minNrShouldMatch = minNrShouldMatch ; } static final class SubScorer { public Scorer scorer ; public boolean done ; public boolean required = false ; public boolean prohibited = false ; public HitCollector collector ; public SubScorer next ; public SubScorer ( Scorer scorer , boolean required , boolean prohibited , HitCollector collector , SubScorer next ) throws IOException { this . scorer = scorer ; this . done = ! scorer . next ( ) ; this . required = required ; this . prohibited = prohibited ; this . collector = collector ; this . next = next ; } } final void add ( Scorer scorer , boolean required , boolean prohibited ) throws IOException { int mask = 0 ; if ( required || prohibited ) { if ( nextMask == 0 ) throw new IndexOutOfBoundsException ( "More than 32 required/prohibited clauses in query." ) ; mask = nextMask ; nextMask = nextMask << 1 ; } else mask = 0 ; if ( ! prohibited ) maxCoord ++ ; if ( prohibited ) prohibitedMask |= mask ; else if ( required ) requiredMask |= mask ; scorers = new SubScorer ( scorer , required , prohibited , bucketTable . newCollector ( mask ) , scorers ) ; } private final void computeCoordFactors ( ) { coordFactors = new float [ maxCoord ] ; for ( int i = 0 ; i < maxCoord ; i ++ ) coordFactors [ i ] = getSimilarity ( ) . coord ( i , maxCoord - 1 ) ; } private int end ; private Bucket current ; public void score ( HitCollector hc ) throws IOException { next ( ) ; score ( hc , Integer . MAX_VALUE ) ; } protected boolean score ( HitCollector hc , int max ) throws IOException { if ( coordFactors == null ) computeCoordFactors ( ) ; boolean more ; Bucket tmp ; do { bucketTable . first = null ; while ( current != null ) { if ( ( current . bits & prohibitedMask ) == 0 && ( current . bits & requiredMask ) == requiredMask ) { if ( current . doc >= max ) { tmp = current ; current = current . next ; tmp . next = bucketTable . first ; bucketTable . first = tmp ; continue ; } if ( current . coord >= minNrShouldMatch ) { hc . collect ( current . doc , current . score * coordFactors [ current . coord ] ) ; } } current = current . next ; } if ( bucketTable . first != null ) { current = bucketTable . first ; bucketTable . first = current . next ; return true ; } more = false ; end += BucketTable . SIZE ; for ( SubScorer sub = scorers ; sub != null ; sub = sub . next ) { if ( ! sub . done ) { sub . done = ! sub . scorer . score ( sub . collector , end ) ; if ( ! sub . done ) more = true ; } } current = bucketTable . first ; } while ( current != null || more ) ; return false ; } public int doc ( ) { return current . doc ; } public boolean next ( ) throws IOException { boolean more ; do { while ( bucketTable . first != null ) { current = bucketTable . first ; bucketTable . first = current . next ; if ( ( current . bits & prohibitedMask ) == 0 && ( current . bits & requiredMask ) == requiredMask && current . coord >= minNrShouldMatch ) { return true ; } } more = false ; end += BucketTable . SIZE ; for ( SubScorer sub = scorers ; sub != null ; sub = sub . next ) { Scorer scorer = sub . scorer ; while ( ! sub . done && scorer . doc ( ) < end ) { sub . collector . collect ( scorer . doc ( ) , scorer . score ( ) ) ; sub . done = ! scorer . next ( ) ; } if ( ! sub . done ) { more = true ; } } } while ( bucketTable . first != null || more ) ; return false ; } public float score ( ) { if ( coordFactors == null ) computeCoordFactors ( ) ; return current . score * coordFactors [ current . coord ] ; } static final class Bucket { int doc = - 1 ; float score ; int bits ; int coord ; Bucket next ; } static final class BucketTable { public static final int SIZE = 1 << 11 ; public static final int MASK = SIZE - 1 ; final Bucket [ ] buckets = new Bucket [ SIZE ] ; Bucket first = null ; public BucketTable ( ) { } public final int size ( ) { return SIZE ; } public HitCollector newCollector ( int mask ) { return new Collector ( mask , this ) ; } } static final class Collector extends HitCollector { private BucketTable bucketTable ; private int mask ; public Collector ( int mask , BucketTable bucketTable ) { this . mask = mask ; this . bucketTable = bucketTable ; } public final void collect ( final int doc , final float score ) { final BucketTable table = bucketTable ; final int i = doc & BucketTable . MASK ; Bucket bucket = table . buckets [ i ] ; if ( bucket == null ) table . buckets [ i ] = bucket = new Bucket ( ) ; if ( bucket . doc != doc ) { bucket . doc = doc ; bucket . score = score ; bucket . bits = mask ; bucket . coord = 1 ; bucket . next = table . first ; table . first = bucket ; } else { bucket . score += score ; bucket . bits |= mask ; bucket . coord ++ ; } } } public boolean skipTo ( int target ) { throw new UnsupportedOperationException ( ) ; } public Explanation explain ( int doc ) { throw new UnsupportedOperationException ( ) ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "boolean(" ) ; for ( SubScorer sub = scorers ; sub != null ; sub = sub . next ) { buffer . append ( sub . scorer . toString ( ) ) ; buffer . append ( " " ) ; } buffer . append ( ")" ) ; return buffer . toString ( ) ; } } 	0
package org . apache . lucene . index ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . document . Document ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . FSDirectory ; import org . apache . lucene . store . LockObtainFailedException ; import java . io . File ; import java . io . IOException ; import java . io . PrintStream ; public class IndexModifier { protected IndexWriter indexWriter = null ; protected IndexReader indexReader = null ; protected Directory directory = null ; protected Analyzer analyzer = null ; protected boolean open = false ; protected PrintStream infoStream = null ; protected boolean useCompoundFile = true ; protected int maxBufferedDocs = IndexWriter . DEFAULT_MAX_BUFFERED_DOCS ; protected int maxFieldLength = IndexWriter . DEFAULT_MAX_FIELD_LENGTH ; protected int mergeFactor = IndexWriter . DEFAULT_MERGE_FACTOR ; public IndexModifier ( Directory directory , Analyzer analyzer , boolean create ) throws CorruptIndexException , LockObtainFailedException , IOException { init ( directory , analyzer , create ) ; } public IndexModifier ( String dirName , Analyzer analyzer , boolean create ) throws CorruptIndexException , LockObtainFailedException , IOException { Directory dir = FSDirectory . getDirectory ( dirName ) ; init ( dir , analyzer , create ) ; } public IndexModifier ( File file , Analyzer analyzer , boolean create ) throws CorruptIndexException , LockObtainFailedException , IOException { Directory dir = FSDirectory . getDirectory ( file ) ; init ( dir , analyzer , create ) ; } protected void init ( Directory directory , Analyzer analyzer , boolean create ) throws CorruptIndexException , LockObtainFailedException , IOException { this . directory = directory ; synchronized ( this . directory ) { this . analyzer = analyzer ; indexWriter = new IndexWriter ( directory , analyzer , create ) ; open = true ; } } protected void assureOpen ( ) { if ( ! open ) { throw new IllegalStateException ( "Index is closed" ) ; } } protected void createIndexWriter ( ) throws CorruptIndexException , LockObtainFailedException , IOException { if ( indexWriter == null ) { if ( indexReader != null ) { indexReader . close ( ) ; indexReader = null ; } indexWriter = new IndexWriter ( directory , analyzer , false ) ; indexWriter . setInfoStream ( infoStream ) ; indexWriter . setUseCompoundFile ( useCompoundFile ) ; indexWriter . setMaxBufferedDocs ( maxBufferedDocs ) ; indexWriter . setMaxFieldLength ( maxFieldLength ) ; indexWriter . setMergeFactor ( mergeFactor ) ; } } protected void createIndexReader ( ) throws CorruptIndexException , IOException { if ( indexReader == null ) { if ( indexWriter != null ) { indexWriter . close ( ) ; indexWriter = null ; } indexReader = IndexReader . open ( directory ) ; } } public void flush ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . close ( ) ; indexWriter = null ; createIndexWriter ( ) ; } else { indexReader . close ( ) ; indexReader = null ; createIndexReader ( ) ; } } } public void addDocument ( Document doc , Analyzer docAnalyzer ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; if ( docAnalyzer != null ) indexWriter . addDocument ( doc , docAnalyzer ) ; else indexWriter . addDocument ( doc ) ; } } public void addDocument ( Document doc ) throws CorruptIndexException , LockObtainFailedException , IOException { addDocument ( doc , null ) ; } public int deleteDocuments ( Term term ) throws StaleReaderException , CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexReader ( ) ; return indexReader . deleteDocuments ( term ) ; } } public void deleteDocument ( int docNum ) throws StaleReaderException , CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexReader ( ) ; indexReader . deleteDocument ( docNum ) ; } } public int docCount ( ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { return indexWriter . docCount ( ) ; } else { return indexReader . numDocs ( ) ; } } } public void optimize ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; indexWriter . optimize ( ) ; } } public void setInfoStream ( PrintStream infoStream ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . setInfoStream ( infoStream ) ; } this . infoStream = infoStream ; } } public PrintStream getInfoStream ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; return indexWriter . getInfoStream ( ) ; } } public void setUseCompoundFile ( boolean useCompoundFile ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . setUseCompoundFile ( useCompoundFile ) ; } this . useCompoundFile = useCompoundFile ; } } public boolean getUseCompoundFile ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; return indexWriter . getUseCompoundFile ( ) ; } } public void setMaxFieldLength ( int maxFieldLength ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . setMaxFieldLength ( maxFieldLength ) ; } this . maxFieldLength = maxFieldLength ; } } public int getMaxFieldLength ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; return indexWriter . getMaxFieldLength ( ) ; } } public void setMaxBufferedDocs ( int maxBufferedDocs ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . setMaxBufferedDocs ( maxBufferedDocs ) ; } this . maxBufferedDocs = maxBufferedDocs ; } } public int getMaxBufferedDocs ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; return indexWriter . getMaxBufferedDocs ( ) ; } } public void setMergeFactor ( int mergeFactor ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . setMergeFactor ( mergeFactor ) ; } this . mergeFactor = mergeFactor ; } } public int getMergeFactor ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; return indexWriter . getMergeFactor ( ) ; } } public void close ( ) throws CorruptIndexException , IOException { synchronized ( directory ) { if ( ! open ) throw new IllegalStateException ( "Index is closed already" ) ; if ( indexWriter != null ) { indexWriter . close ( ) ; indexWriter = null ; } else { indexReader . close ( ) ; indexReader = null ; } open = false ; } } public String toString ( ) { return "Index@" + directory ; } } 	1
package org . apache . lucene . search ; public class QueryFilter extends CachingWrapperFilter { public QueryFilter ( Query query ) { super ( new QueryWrapperFilter ( query ) ) ; } public boolean equals ( Object o ) { return super . equals ( ( QueryFilter ) o ) ; } public int hashCode ( ) { return super . hashCode ( ) ^ 0x923F64B9 ; } } 	0
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexInput ; final class SegmentTermEnum extends TermEnum implements Cloneable { private IndexInput input ; FieldInfos fieldInfos ; long size ; long position = - 1 ; private TermBuffer termBuffer = new TermBuffer ( ) ; private TermBuffer prevBuffer = new TermBuffer ( ) ; private TermBuffer scratch ; private TermInfo termInfo = new TermInfo ( ) ; private int format ; private boolean isIndex = false ; long indexPointer = 0 ; int indexInterval ; int skipInterval ; int maxSkipLevels ; private int formatM1SkipInterval ; SegmentTermEnum ( IndexInput i , FieldInfos fis , boolean isi ) throws CorruptIndexException , IOException { input = i ; fieldInfos = fis ; isIndex = isi ; maxSkipLevels = 1 ; int firstInt = input . readInt ( ) ; if ( firstInt >= 0 ) { format = 0 ; size = firstInt ; indexInterval = 128 ; skipInterval = Integer . MAX_VALUE ; } else { format = firstInt ; if ( format < TermInfosWriter . FORMAT ) throw new CorruptIndexException ( "Unknown format version:" + format ) ; size = input . readLong ( ) ; if ( format == - 1 ) { if ( ! isIndex ) { indexInterval = input . readInt ( ) ; formatM1SkipInterval = input . readInt ( ) ; } skipInterval = Integer . MAX_VALUE ; } else { indexInterval = input . readInt ( ) ; skipInterval = input . readInt ( ) ; if ( format == - 3 ) { maxSkipLevels = input . readInt ( ) ; } } } } protected Object clone ( ) { SegmentTermEnum clone = null ; try { clone = ( SegmentTermEnum ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . input = ( IndexInput ) input . clone ( ) ; clone . termInfo = new TermInfo ( termInfo ) ; clone . termBuffer = ( TermBuffer ) termBuffer . clone ( ) ; clone . prevBuffer = ( TermBuffer ) prevBuffer . clone ( ) ; clone . scratch = null ; return clone ; } final void seek ( long pointer , int p , Term t , TermInfo ti ) throws IOException { input . seek ( pointer ) ; position = p ; termBuffer . set ( t ) ; prevBuffer . reset ( ) ; termInfo . set ( ti ) ; } public final boolean next ( ) throws IOException { if ( position ++ >= size - 1 ) { termBuffer . reset ( ) ; return false ; } prevBuffer . set ( termBuffer ) ; termBuffer . read ( input , fieldInfos ) ; termInfo . docFreq = input . readVInt ( ) ; termInfo . freqPointer += input . readVLong ( ) ; termInfo . proxPointer += input . readVLong ( ) ; if ( format == - 1 ) { if ( ! isIndex ) { if ( termInfo . docFreq > formatM1SkipInterval ) { termInfo . skipOffset = input . readVInt ( ) ; } } } else { if ( termInfo . docFreq >= skipInterval ) termInfo . skipOffset = input . readVInt ( ) ; } if ( isIndex ) indexPointer += input . readVLong ( ) ; return true ; } final void scanTo ( Term term ) throws IOException { if ( scratch == null ) scratch = new TermBuffer ( ) ; scratch . set ( term ) ; while ( scratch . compareTo ( termBuffer ) > 0 && next ( ) ) { } } public final Term term ( ) { return termBuffer . toTerm ( ) ; } final Term prev ( ) { return prevBuffer . toTerm ( ) ; } final TermInfo termInfo ( ) { return new TermInfo ( termInfo ) ; } final void termInfo ( TermInfo ti ) { ti . set ( termInfo ) ; } public final int docFreq ( ) { return termInfo . docFreq ; } final long freqPointer ( ) { return termInfo . freqPointer ; } final long proxPointer ( ) { return termInfo . proxPointer ; } public final void close ( ) throws IOException { input . close ( ) ; } } 	1
package org . apache . lucene . queryParser ; import java . io . * ; public final class FastCharStream implements CharStream { char [ ] buffer = null ; int bufferLength = 0 ; int bufferPosition = 0 ; int tokenStart = 0 ; int bufferStart = 0 ; Reader input ; public FastCharStream ( Reader r ) { input = r ; } public final char readChar ( ) throws IOException { if ( bufferPosition >= bufferLength ) refill ( ) ; return buffer [ bufferPosition ++ ] ; } private final void refill ( ) throws IOException { int newPosition = bufferLength - tokenStart ; if ( tokenStart == 0 ) { if ( buffer == null ) { buffer = new char [ 2048 ] ; } else if ( bufferLength == buffer . length ) { char [ ] newBuffer = new char [ buffer . length * 2 ] ; System . arraycopy ( buffer , 0 , newBuffer , 0 , bufferLength ) ; buffer = newBuffer ; } } else { System . arraycopy ( buffer , tokenStart , buffer , 0 , newPosition ) ; } bufferLength = newPosition ; bufferPosition = newPosition ; bufferStart += tokenStart ; tokenStart = 0 ; int charsRead = input . read ( buffer , newPosition , buffer . length - newPosition ) ; if ( charsRead == - 1 ) throw new IOException ( "read past eof" ) ; else bufferLength += charsRead ; } public final char BeginToken ( ) throws IOException { tokenStart = bufferPosition ; return readChar ( ) ; } public final void backup ( int amount ) { bufferPosition -= amount ; } public final String GetImage ( ) { return new String ( buffer , tokenStart , bufferPosition - tokenStart ) ; } public final char [ ] GetSuffix ( int len ) { char [ ] value = new char [ len ] ; System . arraycopy ( buffer , bufferPosition - len , value , 0 , len ) ; return value ; } public final void Done ( ) { try { input . close ( ) ; } catch ( IOException e ) { System . err . println ( "Caught: " + e + "; ignoring." ) ; } } public final int getColumn ( ) { return bufferStart + bufferPosition ; } public final int getLine ( ) { return 1 ; } public final int getEndColumn ( ) { return bufferStart + bufferPosition ; } public final int getEndLine ( ) { return 1 ; } public final int getBeginColumn ( ) { return bufferStart + tokenStart ; } public final int getBeginLine ( ) { return 1 ; } } 	0
package org . apache . lucene . search . function ; import java . io . IOException ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . ComplexExplanation ; import org . apache . lucene . search . Explanation ; import org . apache . lucene . search . Query ; import org . apache . lucene . search . Scorer ; import org . apache . lucene . search . Searcher ; import org . apache . lucene . search . Similarity ; import org . apache . lucene . search . Weight ; import org . apache . lucene . util . ToStringUtils ; public class CustomScoreQuery extends Query { private Query subQuery ; private ValueSourceQuery valSrcQuery ; private boolean strict = false ; public CustomScoreQuery ( Query subQuery ) { this ( subQuery , null ) ; } public CustomScoreQuery ( Query subQuery , ValueSourceQuery valSrcQuery ) { super ( ) ; this . subQuery = subQuery ; this . valSrcQuery = valSrcQuery ; if ( subQuery == null ) throw new IllegalArgumentException ( "<subqyery> must not be null!" ) ; } public Query rewrite ( IndexReader reader ) throws IOException { subQuery = subQuery . rewrite ( reader ) ; if ( valSrcQuery != null ) { valSrcQuery = ( ValueSourceQuery ) valSrcQuery . rewrite ( reader ) ; } return this ; } public void extractTerms ( Set terms ) { subQuery . extractTerms ( terms ) ; if ( valSrcQuery != null ) { valSrcQuery . extractTerms ( terms ) ; } } public Object clone ( ) { CustomScoreQuery clone = ( CustomScoreQuery ) super . clone ( ) ; clone . subQuery = ( Query ) subQuery . clone ( ) ; if ( valSrcQuery != null ) { clone . valSrcQuery = ( ValueSourceQuery ) valSrcQuery . clone ( ) ; } return clone ; } public String toString ( String field ) { StringBuffer sb = new StringBuffer ( name ( ) ) . append ( "(" ) ; sb . append ( subQuery . toString ( field ) ) ; if ( valSrcQuery != null ) { sb . append ( ", " ) . append ( valSrcQuery . toString ( field ) ) ; } sb . append ( ")" ) ; sb . append ( strict ? " STRICT" : "" ) ; return sb . toString ( ) + ToStringUtils . boost ( getBoost ( ) ) ; } public boolean equals ( Object o ) { if ( getClass ( ) != o . getClass ( ) ) { return false ; } CustomScoreQuery other = ( CustomScoreQuery ) o ; return this . getBoost ( ) == other . getBoost ( ) && this . subQuery . equals ( other . subQuery ) && ( this . valSrcQuery == null ? other . valSrcQuery == null : this . valSrcQuery . equals ( other . valSrcQuery ) ) ; } public int hashCode ( ) { int valSrcHash = valSrcQuery == null ? 0 : valSrcQuery . hashCode ( ) ; return ( getClass ( ) . hashCode ( ) + subQuery . hashCode ( ) + valSrcHash ) ^ Float . floatToIntBits ( getBoost ( ) ) ; } public float customScore ( int doc , float subQueryScore , float valSrcScore ) { return valSrcScore * subQueryScore ; } public Explanation customExplain ( int doc , Explanation subQueryExpl , Explanation valSrcExpl ) { float valSrcScore = valSrcExpl == null ? 1 : valSrcExpl . getValue ( ) ; Explanation exp = new Explanation ( valSrcScore * subQueryExpl . getValue ( ) , "custom score: product of:" ) ; exp . addDetail ( subQueryExpl ) ; if ( valSrcExpl != null ) { exp . addDetail ( valSrcExpl ) ; } return exp ; } private class CustomWeight implements Weight { Searcher searcher ; Weight subQueryWeight ; Weight valSrcWeight ; boolean qStrict ; public CustomWeight ( Searcher searcher ) throws IOException { this . searcher = searcher ; this . subQueryWeight = subQuery . weight ( searcher ) ; if ( valSrcQuery != null ) { this . valSrcWeight = valSrcQuery . createWeight ( searcher ) ; } this . qStrict = strict ; } public Query getQuery ( ) { return CustomScoreQuery . this ; } public float getValue ( ) { return getBoost ( ) ; } public float sumOfSquaredWeights ( ) throws IOException { float sum = subQueryWeight . sumOfSquaredWeights ( ) ; if ( valSrcWeight != null ) { if ( qStrict ) { valSrcWeight . sumOfSquaredWeights ( ) ; } else { sum += valSrcWeight . sumOfSquaredWeights ( ) ; } } sum *= getBoost ( ) * getBoost ( ) ; return sum ; } public void normalize ( float norm ) { norm *= getBoost ( ) ; subQueryWeight . normalize ( norm ) ; if ( valSrcWeight != null ) { if ( qStrict ) { valSrcWeight . normalize ( 1 ) ; } else { valSrcWeight . normalize ( norm ) ; } } } public Scorer scorer ( IndexReader reader ) throws IOException { Scorer subQueryScorer = subQueryWeight . scorer ( reader ) ; Scorer valSrcScorer = ( valSrcWeight == null ? null : valSrcWeight . scorer ( reader ) ) ; return new CustomScorer ( getSimilarity ( searcher ) , reader , this , subQueryScorer , valSrcScorer ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { return scorer ( reader ) . explain ( doc ) ; } } private class CustomScorer extends Scorer { private final CustomWeight weight ; private final float qWeight ; private Scorer subQueryScorer ; private Scorer valSrcScorer ; private IndexReader reader ; private CustomScorer ( Similarity similarity , IndexReader reader , CustomWeight w , Scorer subQueryScorer , Scorer valSrcScorer ) throws IOException { super ( similarity ) ; this . weight = w ; this . qWeight = w . getValue ( ) ; this . subQueryScorer = subQueryScorer ; this . valSrcScorer = valSrcScorer ; this . reader = reader ; } public boolean next ( ) throws IOException { boolean hasNext = subQueryScorer . next ( ) ; if ( valSrcScorer != null && hasNext ) { valSrcScorer . skipTo ( subQueryScorer . doc ( ) ) ; } return hasNext ; } public int doc ( ) { return subQueryScorer . doc ( ) ; } public float score ( ) throws IOException { float valSrcScore = ( valSrcScorer == null ? 1 : valSrcScorer . score ( ) ) ; return qWeight * customScore ( subQueryScorer . doc ( ) , subQueryScorer . score ( ) , valSrcScore ) ; } public boolean skipTo ( int target ) throws IOException { boolean hasNext = subQueryScorer . skipTo ( target ) ; if ( valSrcScorer != null && hasNext ) { valSrcScorer . skipTo ( subQueryScorer . doc ( ) ) ; } return hasNext ; } public Explanation explain ( int doc ) throws IOException { Explanation subQueryExpl = weight . subQueryWeight . explain ( reader , doc ) ; if ( ! subQueryExpl . isMatch ( ) ) { return subQueryExpl ; } Explanation valSrcExpl = valSrcScorer == null ? null : valSrcScorer . explain ( doc ) ; Explanation customExp = customExplain ( doc , subQueryExpl , valSrcExpl ) ; float sc = qWeight * customExp . getValue ( ) ; Explanation res = new ComplexExplanation ( true , sc , CustomScoreQuery . this . toString ( ) + ", product of:" ) ; res . addDetail ( customExp ) ; res . addDetail ( new Explanation ( qWeight , "queryBoost" ) ) ; return res ; } } protected Weight createWeight ( Searcher searcher ) throws IOException { return new CustomWeight ( searcher ) ; } public boolean isStrict ( ) { return strict ; } public void setStrict ( boolean strict ) { this . strict = strict ; } public String name ( ) { return "custom" ; } } 	1
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; public class WildcardTermEnum extends FilteredTermEnum { Term searchTerm ; String field = "" ; String text = "" ; String pre = "" ; int preLen = 0 ; boolean endEnum = false ; public WildcardTermEnum ( IndexReader reader , Term term ) throws IOException { super ( ) ; searchTerm = term ; field = searchTerm . field ( ) ; text = searchTerm . text ( ) ; int sidx = text . indexOf ( WILDCARD_STRING ) ; int cidx = text . indexOf ( WILDCARD_CHAR ) ; int idx = sidx ; if ( idx == - 1 ) { idx = cidx ; } else if ( cidx >= 0 ) { idx = Math . min ( idx , cidx ) ; } pre = searchTerm . text ( ) . substring ( 0 , idx ) ; preLen = pre . length ( ) ; text = text . substring ( preLen ) ; setEnum ( reader . terms ( new Term ( searchTerm . field ( ) , pre ) ) ) ; } protected final boolean termCompare ( Term term ) { if ( field == term . field ( ) ) { String searchText = term . text ( ) ; if ( searchText . startsWith ( pre ) ) { return wildcardEquals ( text , 0 , searchText , preLen ) ; } } endEnum = true ; return false ; } public final float difference ( ) { return 1.0f ; } public final boolean endEnum ( ) { return endEnum ; } public static final char WILDCARD_STRING = '*' ; public static final char WILDCARD_CHAR = '?' ; public static final boolean wildcardEquals ( String pattern , int patternIdx , String string , int stringIdx ) { int p = patternIdx ; for ( int s = stringIdx ; ; ++ p , ++ s ) { boolean sEnd = ( s >= string . length ( ) ) ; boolean pEnd = ( p >= pattern . length ( ) ) ; if ( sEnd ) { boolean justWildcardsLeft = true ; int wildcardSearchPos = p ; while ( wildcardSearchPos < pattern . length ( ) && justWildcardsLeft ) { char wildchar = pattern . charAt ( wildcardSearchPos ) ; if ( wildchar != WILDCARD_CHAR && wildchar != WILDCARD_STRING ) { justWildcardsLeft = false ; } else { if ( wildchar == WILDCARD_CHAR ) { return false ; } wildcardSearchPos ++ ; } } if ( justWildcardsLeft ) { return true ; } } if ( sEnd || pEnd ) { break ; } if ( pattern . charAt ( p ) == WILDCARD_CHAR ) { continue ; } if ( pattern . charAt ( p ) == WILDCARD_STRING ) { ++ p ; for ( int i = string . length ( ) ; i >= s ; -- i ) { if ( wildcardEquals ( pattern , p , string , i ) ) { return true ; } } break ; } if ( pattern . charAt ( p ) != string . charAt ( s ) ) { break ; } } return false ; } public void close ( ) throws IOException { super . close ( ) ; searchTerm = null ; field = null ; text = null ; } } 	0
package org . apache . lucene . store ; import java . io . IOException ; public abstract class LockFactory { protected String lockPrefix = "" ; public void setLockPrefix ( String lockPrefix ) { this . lockPrefix = lockPrefix ; } public String getLockPrefix ( ) { return this . lockPrefix ; } public abstract Lock makeLock ( String lockName ) ; abstract public void clearLock ( String lockName ) throws IOException ; } 	1
package org . apache . lucene . store ; import java . io . IOException ; public class LockObtainFailedException extends IOException { public LockObtainFailedException ( String message ) { super ( message ) ; } } 	0
package org . apache . lucene . search ; public class TopFieldDocs extends TopDocs { public SortField [ ] fields ; TopFieldDocs ( int totalHits , ScoreDoc [ ] scoreDocs , SortField [ ] fields , float maxScore ) { super ( totalHits , scoreDocs , maxScore ) ; this . fields = fields ; } } 	1
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . * ; abstract class PhraseScorer extends Scorer { private Weight weight ; protected byte [ ] norms ; protected float value ; private boolean firstTime = true ; private boolean more = true ; protected PhraseQueue pq ; protected PhrasePositions first , last ; private float freq ; PhraseScorer ( Weight weight , TermPositions [ ] tps , int [ ] offsets , Similarity similarity , byte [ ] norms ) { super ( similarity ) ; this . norms = norms ; this . weight = weight ; this . value = weight . getValue ( ) ; for ( int i = 0 ; i < tps . length ; i ++ ) { PhrasePositions pp = new PhrasePositions ( tps [ i ] , offsets [ i ] ) ; if ( last != null ) { last . next = pp ; } else first = pp ; last = pp ; } pq = new PhraseQueue ( tps . length ) ; } public int doc ( ) { return first . doc ; } public boolean next ( ) throws IOException { if ( firstTime ) { init ( ) ; firstTime = false ; } else if ( more ) { more = last . next ( ) ; } return doNext ( ) ; } private boolean doNext ( ) throws IOException { while ( more ) { while ( more && first . doc < last . doc ) { more = first . skipTo ( last . doc ) ; firstToLast ( ) ; } if ( more ) { freq = phraseFreq ( ) ; if ( freq == 0.0f ) more = last . next ( ) ; else return true ; } } return false ; } public float score ( ) throws IOException { float raw = getSimilarity ( ) . tf ( freq ) * value ; return raw * Similarity . decodeNorm ( norms [ first . doc ] ) ; } public boolean skipTo ( int target ) throws IOException { firstTime = false ; for ( PhrasePositions pp = first ; more && pp != null ; pp = pp . next ) { more = pp . skipTo ( target ) ; } if ( more ) sort ( ) ; return doNext ( ) ; } protected abstract float phraseFreq ( ) throws IOException ; private void init ( ) throws IOException { for ( PhrasePositions pp = first ; more && pp != null ; pp = pp . next ) more = pp . next ( ) ; if ( more ) sort ( ) ; } private void sort ( ) { pq . clear ( ) ; for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) pq . put ( pp ) ; pqToList ( ) ; } protected final void pqToList ( ) { last = first = null ; while ( pq . top ( ) != null ) { PhrasePositions pp = ( PhrasePositions ) pq . pop ( ) ; if ( last != null ) { last . next = pp ; } else first = pp ; last = pp ; pp . next = null ; } } protected final void firstToLast ( ) { last . next = first ; last = first ; first = first . next ; last . next = null ; } public Explanation explain ( final int doc ) throws IOException { Explanation tfExplanation = new Explanation ( ) ; while ( next ( ) && doc ( ) < doc ) { } float phraseFreq = ( doc ( ) == doc ) ? freq : 0.0f ; tfExplanation . setValue ( getSimilarity ( ) . tf ( phraseFreq ) ) ; tfExplanation . setDescription ( "tf(phraseFreq=" + phraseFreq + ")" ) ; return tfExplanation ; } public String toString ( ) { return "scorer(" + weight + ")" ; } } 	0
package org . apache . lucene . queryParser ; import java . util . Vector ; import java . io . * ; import java . text . * ; import java . util . * ; import org . apache . lucene . index . Term ; import org . apache . lucene . analysis . * ; import org . apache . lucene . document . * ; import org . apache . lucene . search . * ; import org . apache . lucene . util . Parameter ; public class QueryParserTokenManager implements QueryParserConstants { public java . io . PrintStream debugStream = System . out ; public void setDebugStream ( java . io . PrintStream ds ) { debugStream = ds ; } private final int jjStopStringLiteralDfa_3 ( int pos , long active0 ) { switch ( pos ) { default : return - 1 ; } } private final int jjStartNfa_3 ( int pos , long active0 ) { return jjMoveNfa_3 ( jjStopStringLiteralDfa_3 ( pos , active0 ) , pos + 1 ) ; } private final int jjStopAtPos ( int pos , int kind ) { jjmatchedKind = kind ; jjmatchedPos = pos ; return pos + 1 ; } private final int jjStartNfaWithStates_3 ( int pos , int kind , int state ) { jjmatchedKind = kind ; jjmatchedPos = pos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return pos + 1 ; } return jjMoveNfa_3 ( state , pos + 1 ) ; } private final int jjMoveStringLiteralDfa0_3 ( ) { switch ( curChar ) { case 40 : return jjStopAtPos ( 0 , 12 ) ; case 41 : return jjStopAtPos ( 0 , 13 ) ; case 42 : return jjStartNfaWithStates_3 ( 0 , 15 , 36 ) ; case 43 : return jjStopAtPos ( 0 , 10 ) ; case 45 : return jjStopAtPos ( 0 , 11 ) ; case 58 : return jjStopAtPos ( 0 , 14 ) ; case 91 : return jjStopAtPos ( 0 , 22 ) ; case 94 : return jjStopAtPos ( 0 , 16 ) ; case 123 : return jjStopAtPos ( 0 , 23 ) ; default : return jjMoveNfa_3 ( 0 , 0 ) ; } } private final void jjCheckNAdd ( int state ) { if ( jjrounds [ state ] != jjround ) { jjstateSet [ jjnewStateCnt ++ ] = state ; jjrounds [ state ] = jjround ; } } private final void jjAddStates ( int start , int end ) { do { jjstateSet [ jjnewStateCnt ++ ] = jjnextStates [ start ] ; } while ( start ++ != end ) ; } private final void jjCheckNAddTwoStates ( int state1 , int state2 ) { jjCheckNAdd ( state1 ) ; jjCheckNAdd ( state2 ) ; } private final void jjCheckNAddStates ( int start , int end ) { do { jjCheckNAdd ( jjnextStates [ start ] ) ; } while ( start ++ != end ) ; } private final void jjCheckNAddStates ( int start ) { jjCheckNAdd ( jjnextStates [ start ] ) ; jjCheckNAdd ( jjnextStates [ start + 1 ] ) ; } static final long [ ] jjbitVec0 = { 0xfffffffffffffffeL , 0xffffffffffffffffL , 0xffffffffffffffffL , 0xffffffffffffffffL } ; static final long [ ] jjbitVec2 = { 0x0L , 0x0L , 0xffffffffffffffffL , 0xffffffffffffffffL } ; private final int jjMoveNfa_3 ( int startState , int curPos ) { int [ ] nextStates ; int startsAt = 0 ; jjnewStateCnt = 36 ; int i = 1 ; jjstateSet [ 0 ] = startState ; int j , kind = 0x7fffffff ; for ( ; ; ) { if ( ++ jjround == 0x7fffffff ) ReInitRounds ( ) ; if ( curChar < 64 ) { long l = 1L << curChar ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 36 : case 25 : if ( ( 0xfbfffcf8ffffd9ffL & l ) == 0L ) break ; if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 0 : if ( ( 0xfbffd4f8ffffd9ffL & l ) != 0L ) { if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; } else if ( ( 0x100002600L & l ) != 0L ) { if ( kind > 6 ) kind = 6 ; } else if ( curChar == 34 ) jjCheckNAddTwoStates ( 15 , 17 ) ; else if ( curChar == 33 ) { if ( kind > 9 ) kind = 9 ; } if ( ( 0x7bffd0f8ffffd9ffL & l ) != 0L ) { if ( kind > 18 ) kind = 18 ; jjCheckNAddStates ( 0 , 4 ) ; } else if ( curChar == 42 ) { if ( kind > 20 ) kind = 20 ; } if ( curChar == 38 ) jjstateSet [ jjnewStateCnt ++ ] = 4 ; break ; case 4 : if ( curChar == 38 && kind > 7 ) kind = 7 ; break ; case 5 : if ( curChar == 38 ) jjstateSet [ jjnewStateCnt ++ ] = 4 ; break ; case 13 : if ( curChar == 33 && kind > 9 ) kind = 9 ; break ; case 14 : if ( curChar == 34 ) jjCheckNAddTwoStates ( 15 , 17 ) ; break ; case 15 : if ( ( 0xfffffffbffffffffL & l ) != 0L ) jjCheckNAddStates ( 5 , 7 ) ; break ; case 16 : if ( curChar == 34 ) jjCheckNAddStates ( 5 , 7 ) ; break ; case 18 : if ( curChar == 34 && kind > 17 ) kind = 17 ; break ; case 20 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 19 ) kind = 19 ; jjAddStates ( 8 , 9 ) ; break ; case 21 : if ( curChar == 46 ) jjCheckNAdd ( 22 ) ; break ; case 22 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 19 ) kind = 19 ; jjCheckNAdd ( 22 ) ; break ; case 23 : if ( curChar == 42 && kind > 20 ) kind = 20 ; break ; case 24 : if ( ( 0xfbffd4f8ffffd9ffL & l ) == 0L ) break ; if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 27 : if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 28 : if ( ( 0x7bffd0f8ffffd9ffL & l ) == 0L ) break ; if ( kind > 18 ) kind = 18 ; jjCheckNAddStates ( 0 , 4 ) ; break ; case 29 : if ( ( 0x7bfff8f8ffffd9ffL & l ) == 0L ) break ; if ( kind > 18 ) kind = 18 ; jjCheckNAddTwoStates ( 29 , 30 ) ; break ; case 31 : if ( kind > 18 ) kind = 18 ; jjCheckNAddTwoStates ( 29 , 30 ) ; break ; case 32 : if ( ( 0x7bfff8f8ffffd9ffL & l ) != 0L ) jjCheckNAddStates ( 10 , 12 ) ; break ; case 34 : jjCheckNAddStates ( 10 , 12 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else if ( curChar < 128 ) { long l = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 36 : if ( ( 0x97ffffff87ffffffL & l ) != 0L ) { if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; } else if ( curChar == 92 ) jjCheckNAddTwoStates ( 27 , 27 ) ; break ; case 0 : if ( ( 0x97ffffff87ffffffL & l ) != 0L ) { if ( kind > 18 ) kind = 18 ; jjCheckNAddStates ( 0 , 4 ) ; } else if ( curChar == 92 ) jjCheckNAddStates ( 13 , 15 ) ; else if ( curChar == 126 ) { if ( kind > 19 ) kind = 19 ; jjstateSet [ jjnewStateCnt ++ ] = 20 ; } if ( ( 0x97ffffff87ffffffL & l ) != 0L ) { if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; } if ( curChar == 78 ) jjstateSet [ jjnewStateCnt ++ ] = 11 ; else if ( curChar == 124 ) jjstateSet [ jjnewStateCnt ++ ] = 8 ; else if ( curChar == 79 ) jjstateSet [ jjnewStateCnt ++ ] = 6 ; else if ( curChar == 65 ) jjstateSet [ jjnewStateCnt ++ ] = 2 ; break ; case 1 : if ( curChar == 68 && kind > 7 ) kind = 7 ; break ; case 2 : if ( curChar == 78 ) jjstateSet [ jjnewStateCnt ++ ] = 1 ; break ; case 3 : if ( curChar == 65 ) jjstateSet [ jjnewStateCnt ++ ] = 2 ; break ; case 6 : if ( curChar == 82 && kind > 8 ) kind = 8 ; break ; case 7 : if ( curChar == 79 ) jjstateSet [ jjnewStateCnt ++ ] = 6 ; break ; case 8 : if ( curChar == 124 && kind > 8 ) kind = 8 ; break ; case 9 : if ( curChar == 124 ) jjstateSet [ jjnewStateCnt ++ ] = 8 ; break ; case 10 : if ( curChar == 84 && kind > 9 ) kind = 9 ; break ; case 11 : if ( curChar == 79 ) jjstateSet [ jjnewStateCnt ++ ] = 10 ; break ; case 12 : if ( curChar == 78 ) jjstateSet [ jjnewStateCnt ++ ] = 11 ; break ; case 15 : jjAddStates ( 5 , 7 ) ; break ; case 17 : if ( curChar == 92 ) jjstateSet [ jjnewStateCnt ++ ] = 16 ; break ; case 19 : if ( curChar != 126 ) break ; if ( kind > 19 ) kind = 19 ; jjstateSet [ jjnewStateCnt ++ ] = 20 ; break ; case 24 : if ( ( 0x97ffffff87ffffffL & l ) == 0L ) break ; if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 25 : if ( ( 0x97ffffff87ffffffL & l ) == 0L ) break ; if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 26 : if ( curChar == 92 ) jjCheckNAddTwoStates ( 27 , 27 ) ; break ; case 27 : if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 28 : if ( ( 0x97ffffff87ffffffL & l ) == 0L ) break ; if ( kind > 18 ) kind = 18 ; jjCheckNAddStates ( 0 , 4 ) ; break ; case 29 : if ( ( 0x97ffffff87ffffffL & l ) == 0L ) break ; if ( kind > 18 ) kind = 18 ; jjCheckNAddTwoStates ( 29 , 30 ) ; break ; case 30 : if ( curChar == 92 ) jjCheckNAddTwoStates ( 31 , 31 ) ; break ; case 31 : if ( kind > 18 ) kind = 18 ; jjCheckNAddTwoStates ( 29 , 30 ) ; break ; case 32 : if ( ( 0x97ffffff87ffffffL & l ) != 0L ) jjCheckNAddStates ( 10 , 12 ) ; break ; case 33 : if ( curChar == 92 ) jjCheckNAddTwoStates ( 34 , 34 ) ; break ; case 34 : jjCheckNAddStates ( 10 , 12 ) ; break ; case 35 : if ( curChar == 92 ) jjCheckNAddStates ( 13 , 15 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else { int hiByte = ( int ) ( curChar > > 8 ) ; int i1 = hiByte > > 6 ; long l1 = 1L << ( hiByte & 077 ) ; int i2 = ( curChar & 0xff ) > > 6 ; long l2 = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 36 : case 25 : case 27 : if ( ! jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 0 : if ( jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) { if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; } if ( jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) { if ( kind > 18 ) kind = 18 ; jjCheckNAddStates ( 0 , 4 ) ; } break ; case 15 : if ( jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 5 , 7 ) ; break ; case 24 : if ( ! jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 28 : if ( ! jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 18 ) kind = 18 ; jjCheckNAddStates ( 0 , 4 ) ; break ; case 29 : case 31 : if ( ! jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 18 ) kind = 18 ; jjCheckNAddTwoStates ( 29 , 30 ) ; break ; case 32 : case 34 : if ( jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 10 , 12 ) ; break ; default : break ; } } while ( i != startsAt ) ; } if ( kind != 0x7fffffff ) { jjmatchedKind = kind ; jjmatchedPos = curPos ; kind = 0x7fffffff ; } ++ curPos ; if ( ( i = jjnewStateCnt ) == ( startsAt = 36 - ( jjnewStateCnt = startsAt ) ) ) return curPos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return curPos ; } } } private final int jjStopStringLiteralDfa_1 ( int pos , long active0 ) { switch ( pos ) { case 0 : if ( ( active0 & 0x20000000L ) != 0L ) { jjmatchedKind = 32 ; return 6 ; } return - 1 ; default : return - 1 ; } } private final int jjStartNfa_1 ( int pos , long active0 ) { return jjMoveNfa_1 ( jjStopStringLiteralDfa_1 ( pos , active0 ) , pos + 1 ) ; } private final int jjStartNfaWithStates_1 ( int pos , int kind , int state ) { jjmatchedKind = kind ; jjmatchedPos = pos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return pos + 1 ; } return jjMoveNfa_1 ( state , pos + 1 ) ; } private final int jjMoveStringLiteralDfa0_1 ( ) { switch ( curChar ) { case 84 : return jjMoveStringLiteralDfa1_1 ( 0x20000000L ) ; case 125 : return jjStopAtPos ( 0 , 30 ) ; default : return jjMoveNfa_1 ( 0 , 0 ) ; } } private final int jjMoveStringLiteralDfa1_1 ( long active0 ) { try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { jjStopStringLiteralDfa_1 ( 0 , active0 ) ; return 1 ; } switch ( curChar ) { case 79 : if ( ( active0 & 0x20000000L ) != 0L ) return jjStartNfaWithStates_1 ( 1 , 29 , 6 ) ; break ; default : break ; } return jjStartNfa_1 ( 0 , active0 ) ; } private final int jjMoveNfa_1 ( int startState , int curPos ) { int [ ] nextStates ; int startsAt = 0 ; jjnewStateCnt = 7 ; int i = 1 ; jjstateSet [ 0 ] = startState ; int j , kind = 0x7fffffff ; for ( ; ; ) { if ( ++ jjround == 0x7fffffff ) ReInitRounds ( ) ; if ( curChar < 64 ) { long l = 1L << curChar ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( ( 0xfffffffeffffffffL & l ) != 0L ) { if ( kind > 32 ) kind = 32 ; jjCheckNAdd ( 6 ) ; } if ( ( 0x100002600L & l ) != 0L ) { if ( kind > 6 ) kind = 6 ; } else if ( curChar == 34 ) jjCheckNAddTwoStates ( 2 , 4 ) ; break ; case 1 : if ( curChar == 34 ) jjCheckNAddTwoStates ( 2 , 4 ) ; break ; case 2 : if ( ( 0xfffffffbffffffffL & l ) != 0L ) jjCheckNAddStates ( 16 , 18 ) ; break ; case 3 : if ( curChar == 34 ) jjCheckNAddStates ( 16 , 18 ) ; break ; case 5 : if ( curChar == 34 && kind > 31 ) kind = 31 ; break ; case 6 : if ( ( 0xfffffffeffffffffL & l ) == 0L ) break ; if ( kind > 32 ) kind = 32 ; jjCheckNAdd ( 6 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else if ( curChar < 128 ) { long l = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : case 6 : if ( ( 0xdfffffffffffffffL & l ) == 0L ) break ; if ( kind > 32 ) kind = 32 ; jjCheckNAdd ( 6 ) ; break ; case 2 : jjAddStates ( 16 , 18 ) ; break ; case 4 : if ( curChar == 92 ) jjstateSet [ jjnewStateCnt ++ ] = 3 ; break ; default : break ; } } while ( i != startsAt ) ; } else { int hiByte = ( int ) ( curChar > > 8 ) ; int i1 = hiByte > > 6 ; long l1 = 1L << ( hiByte & 077 ) ; int i2 = ( curChar & 0xff ) > > 6 ; long l2 = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : case 6 : if ( ! jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 32 ) kind = 32 ; jjCheckNAdd ( 6 ) ; break ; case 2 : if ( jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 16 , 18 ) ; break ; default : break ; } } while ( i != startsAt ) ; } if ( kind != 0x7fffffff ) { jjmatchedKind = kind ; jjmatchedPos = curPos ; kind = 0x7fffffff ; } ++ curPos ; if ( ( i = jjnewStateCnt ) == ( startsAt = 7 - ( jjnewStateCnt = startsAt ) ) ) return curPos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return curPos ; } } } private final int jjMoveStringLiteralDfa0_0 ( ) { return jjMoveNfa_0 ( 0 , 0 ) ; } private final int jjMoveNfa_0 ( int startState , int curPos ) { int [ ] nextStates ; int startsAt = 0 ; jjnewStateCnt = 3 ; int i = 1 ; jjstateSet [ 0 ] = startState ; int j , kind = 0x7fffffff ; for ( ; ; ) { if ( ++ jjround == 0x7fffffff ) ReInitRounds ( ) ; if ( curChar < 64 ) { long l = 1L << curChar ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 24 ) kind = 24 ; jjAddStates ( 19 , 20 ) ; break ; case 1 : if ( curChar == 46 ) jjCheckNAdd ( 2 ) ; break ; case 2 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 24 ) kind = 24 ; jjCheckNAdd ( 2 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else if ( curChar < 128 ) { long l = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { default : break ; } } while ( i != startsAt ) ; } else { int hiByte = ( int ) ( curChar > > 8 ) ; int i1 = hiByte > > 6 ; long l1 = 1L << ( hiByte & 077 ) ; int i2 = ( curChar & 0xff ) > > 6 ; long l2 = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { default : break ; } } while ( i != startsAt ) ; } if ( kind != 0x7fffffff ) { jjmatchedKind = kind ; jjmatchedPos = curPos ; kind = 0x7fffffff ; } ++ curPos ; if ( ( i = jjnewStateCnt ) == ( startsAt = 3 - ( jjnewStateCnt = startsAt ) ) ) return curPos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return curPos ; } } } private final int jjStopStringLiteralDfa_2 ( int pos , long active0 ) { switch ( pos ) { case 0 : if ( ( active0 & 0x2000000L ) != 0L ) { jjmatchedKind = 28 ; return 6 ; } return - 1 ; default : return - 1 ; } } private final int jjStartNfa_2 ( int pos , long active0 ) { return jjMoveNfa_2 ( jjStopStringLiteralDfa_2 ( pos , active0 ) , pos + 1 ) ; } private final int jjStartNfaWithStates_2 ( int pos , int kind , int state ) { jjmatchedKind = kind ; jjmatchedPos = pos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return pos + 1 ; } return jjMoveNfa_2 ( state , pos + 1 ) ; } private final int jjMoveStringLiteralDfa0_2 ( ) { switch ( curChar ) { case 84 : return jjMoveStringLiteralDfa1_2 ( 0x2000000L ) ; case 93 : return jjStopAtPos ( 0 , 26 ) ; default : return jjMoveNfa_2 ( 0 , 0 ) ; } } private final int jjMoveStringLiteralDfa1_2 ( long active0 ) { try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { jjStopStringLiteralDfa_2 ( 0 , active0 ) ; return 1 ; } switch ( curChar ) { case 79 : if ( ( active0 & 0x2000000L ) != 0L ) return jjStartNfaWithStates_2 ( 1 , 25 , 6 ) ; break ; default : break ; } return jjStartNfa_2 ( 0 , active0 ) ; } private final int jjMoveNfa_2 ( int startState , int curPos ) { int [ ] nextStates ; int startsAt = 0 ; jjnewStateCnt = 7 ; int i = 1 ; jjstateSet [ 0 ] = startState ; int j , kind = 0x7fffffff ; for ( ; ; ) { if ( ++ jjround == 0x7fffffff ) ReInitRounds ( ) ; if ( curChar < 64 ) { long l = 1L << curChar ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( ( 0xfffffffeffffffffL & l ) != 0L ) { if ( kind > 28 ) kind = 28 ; jjCheckNAdd ( 6 ) ; } if ( ( 0x100002600L & l ) != 0L ) { if ( kind > 6 ) kind = 6 ; } else if ( curChar == 34 ) jjCheckNAddTwoStates ( 2 , 4 ) ; break ; case 1 : if ( curChar == 34 ) jjCheckNAddTwoStates ( 2 , 4 ) ; break ; case 2 : if ( ( 0xfffffffbffffffffL & l ) != 0L ) jjCheckNAddStates ( 16 , 18 ) ; break ; case 3 : if ( curChar == 34 ) jjCheckNAddStates ( 16 , 18 ) ; break ; case 5 : if ( curChar == 34 && kind > 27 ) kind = 27 ; break ; case 6 : if ( ( 0xfffffffeffffffffL & l ) == 0L ) break ; if ( kind > 28 ) kind = 28 ; jjCheckNAdd ( 6 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else if ( curChar < 128 ) { long l = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : case 6 : if ( ( 0xffffffffdfffffffL & l ) == 0L ) break ; if ( kind > 28 ) kind = 28 ; jjCheckNAdd ( 6 ) ; break ; case 2 : jjAddStates ( 16 , 18 ) ; break ; case 4 : if ( curChar == 92 ) jjstateSet [ jjnewStateCnt ++ ] = 3 ; break ; default : break ; } } while ( i != startsAt ) ; } else { int hiByte = ( int ) ( curChar > > 8 ) ; int i1 = hiByte > > 6 ; long l1 = 1L << ( hiByte & 077 ) ; int i2 = ( curChar & 0xff ) > > 6 ; long l2 = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : case 6 : if ( ! jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 28 ) kind = 28 ; jjCheckNAdd ( 6 ) ; break ; case 2 : if ( jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 16 , 18 ) ; break ; default : break ; } } while ( i != startsAt ) ; } if ( kind != 0x7fffffff ) { jjmatchedKind = kind ; jjmatchedPos = curPos ; kind = 0x7fffffff ; } ++ curPos ; if ( ( i = jjnewStateCnt ) == ( startsAt = 7 - ( jjnewStateCnt = startsAt ) ) ) return curPos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return curPos ; } } } static final int [ ] jjnextStates = { 29 , 32 , 23 , 33 , 30 , 15 , 17 , 18 , 20 , 21 , 32 , 23 , 33 , 31 , 34 , 27 , 2 , 4 , 5 , 0 , 1 , } ; private static final boolean jjCanMove_0 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 0 : return ( ( jjbitVec2 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec0 [ i1 ] & l1 ) != 0L ) return true ; return false ; } } public static final String [ ] jjstrLiteralImages = { "" , null , null , null , null , null , null , null , null , null , "\53" , "\55" , "\50" , "\51" , "\72" , "\52" , "\136" , null , null , null , null , null , "\133" , "\173" , null , "\124\117" , "\135" , null , null , "\124\117" , "\175" , null , null , } ; public static final String [ ] lexStateNames = { "Boost" , "RangeEx" , "RangeIn" , "DEFAULT" , } ; public static final int [ ] jjnewLexState = { - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , 0 , - 1 , - 1 , - 1 , - 1 , - 1 , 2 , 1 , 3 , - 1 , 3 , - 1 , - 1 , - 1 , 3 , - 1 , - 1 , } ; static final long [ ] jjtoToken = { 0x1ffffff81L , } ; static final long [ ] jjtoSkip = { 0x40L , } ; protected CharStream input_stream ; private final int [ ] jjrounds = new int [ 36 ] ; private final int [ ] jjstateSet = new int [ 72 ] ; protected char curChar ; public QueryParserTokenManager ( CharStream stream ) { input_stream = stream ; } public QueryParserTokenManager ( CharStream stream , int lexState ) { this ( stream ) ; SwitchTo ( lexState ) ; } public void ReInit ( CharStream stream ) { jjmatchedPos = jjnewStateCnt = 0 ; curLexState = defaultLexState ; input_stream = stream ; ReInitRounds ( ) ; } private final void ReInitRounds ( ) { int i ; jjround = 0x80000001 ; for ( i = 36 ; i -- > 0 ; ) jjrounds [ i ] = 0x80000000 ; } public void ReInit ( CharStream stream , int lexState ) { ReInit ( stream ) ; SwitchTo ( lexState ) ; } public void SwitchTo ( int lexState ) { if ( lexState >= 4 || lexState < 0 ) throw new TokenMgrError ( "Error: Ignoring invalid lexical state : " + lexState + ". State unchanged." , TokenMgrError . INVALID_LEXICAL_STATE ) ; else curLexState = lexState ; } protected Token jjFillToken ( ) { Token t = Token . newToken ( jjmatchedKind ) ; t . kind = jjmatchedKind ; String im = jjstrLiteralImages [ jjmatchedKind ] ; t . image = ( im == null ) ? input_stream . GetImage ( ) : im ; t . beginLine = input_stream . getBeginLine ( ) ; t . beginColumn = input_stream . getBeginColumn ( ) ; t . endLine = input_stream . getEndLine ( ) ; t . endColumn = input_stream . getEndColumn ( ) ; return t ; } int curLexState = 3 ; int defaultLexState = 3 ; int jjnewStateCnt ; int jjround ; int jjmatchedPos ; int jjmatchedKind ; public Token getNextToken ( ) { int kind ; Token specialToken = null ; Token matchedToken ; int curPos = 0 ; EOFLoop : for ( ; ; ) { try { curChar = input_stream . BeginToken ( ) ; } catch ( java . io . IOException e ) { jjmatchedKind = 0 ; matchedToken = jjFillToken ( ) ; return matchedToken ; } switch ( curLexState ) { case 0 : jjmatchedKind = 0x7fffffff ; jjmatchedPos = 0 ; curPos = jjMoveStringLiteralDfa0_0 ( ) ; break ; case 1 : jjmatchedKind = 0x7fffffff ; jjmatchedPos = 0 ; curPos = jjMoveStringLiteralDfa0_1 ( ) ; break ; case 2 : jjmatchedKind = 0x7fffffff ; jjmatchedPos = 0 ; curPos = jjMoveStringLiteralDfa0_2 ( ) ; break ; case 3 : jjmatchedKind = 0x7fffffff ; jjmatchedPos = 0 ; curPos = jjMoveStringLiteralDfa0_3 ( ) ; break ; } if ( jjmatchedKind != 0x7fffffff ) { if ( jjmatchedPos + 1 < curPos ) input_stream . backup ( curPos - jjmatchedPos - 1 ) ; if ( ( jjtoToken [ jjmatchedKind > > 6 ] & ( 1L << ( jjmatchedKind & 077 ) ) ) != 0L ) { matchedToken = jjFillToken ( ) ; if ( jjnewLexState [ jjmatchedKind ] != - 1 ) curLexState = jjnewLexState [ jjmatchedKind ] ; return matchedToken ; } else { if ( jjnewLexState [ jjmatchedKind ] != - 1 ) curLexState = jjnewLexState [ jjmatchedKind ] ; continue EOFLoop ; } } int error_line = input_stream . getEndLine ( ) ; int error_column = input_stream . getEndColumn ( ) ; String error_after = null ; boolean EOFSeen = false ; try { input_stream . readChar ( ) ; input_stream . backup ( 1 ) ; } catch ( java . io . IOException e1 ) { EOFSeen = true ; error_after = curPos <= 1 ? "" : input_stream . GetImage ( ) ; if ( curChar == '\n' || curChar == '\r' ) { error_line ++ ; error_column = 0 ; } else error_column ++ ; } if ( ! EOFSeen ) { input_stream . backup ( 1 ) ; error_after = curPos <= 1 ? "" : input_stream . GetImage ( ) ; } throw new TokenMgrError ( EOFSeen , curLexState , error_line , error_column , error_after , curChar , TokenMgrError . LEXICAL_ERROR ) ; } } } 	1
package org . apache . lucene . document ; import java . util . Set ; public class SetBasedFieldSelector implements FieldSelector { private Set fieldsToLoad ; private Set lazyFieldsToLoad ; public SetBasedFieldSelector ( Set fieldsToLoad , Set lazyFieldsToLoad ) { this . fieldsToLoad = fieldsToLoad ; this . lazyFieldsToLoad = lazyFieldsToLoad ; } public FieldSelectorResult accept ( String fieldName ) { FieldSelectorResult result = FieldSelectorResult . NO_LOAD ; if ( fieldsToLoad . contains ( fieldName ) == true ) { result = FieldSelectorResult . LOAD ; } if ( lazyFieldsToLoad . contains ( fieldName ) == true ) { result = FieldSelectorResult . LAZY_LOAD ; } return result ; } } 	0
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . * ; import org . apache . lucene . util . ToStringUtils ; import java . io . IOException ; import java . util . Set ; public class ValueSourceQuery extends Query { ValueSource valSrc ; public ValueSourceQuery ( ValueSource valSrc ) { this . valSrc = valSrc ; } public Query rewrite ( IndexReader reader ) throws IOException { return this ; } public void extractTerms ( Set terms ) { } private class ValueSourceWeight implements Weight { Searcher searcher ; float queryNorm ; float queryWeight ; public ValueSourceWeight ( Searcher searcher ) { this . searcher = searcher ; } public Query getQuery ( ) { return ValueSourceQuery . this ; } public float getValue ( ) { return queryWeight ; } public float sumOfSquaredWeights ( ) throws IOException { queryWeight = getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float norm ) { this . queryNorm = norm ; queryWeight *= this . queryNorm ; } public Scorer scorer ( IndexReader reader ) throws IOException { return new ValueSourceScorer ( getSimilarity ( searcher ) , reader , this ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { return scorer ( reader ) . explain ( doc ) ; } } private class ValueSourceScorer extends Scorer { private final IndexReader reader ; private final ValueSourceWeight weight ; private final int maxDoc ; private final float qWeight ; private int doc = - 1 ; private final DocValues vals ; private ValueSourceScorer ( Similarity similarity , IndexReader reader , ValueSourceWeight w ) throws IOException { super ( similarity ) ; this . weight = w ; this . qWeight = w . getValue ( ) ; this . reader = reader ; this . maxDoc = reader . maxDoc ( ) ; vals = valSrc . getValues ( reader ) ; } public boolean next ( ) throws IOException { for ( ; ; ) { ++ doc ; if ( doc >= maxDoc ) { return false ; } if ( reader . isDeleted ( doc ) ) { continue ; } return true ; } } public int doc ( ) { return doc ; } public float score ( ) throws IOException { return qWeight * vals . floatVal ( doc ) ; } public boolean skipTo ( int target ) throws IOException { doc = target - 1 ; return next ( ) ; } public Explanation explain ( int doc ) throws IOException { float sc = qWeight * vals . floatVal ( doc ) ; Explanation result = new ComplexExplanation ( true , sc , ValueSourceQuery . this . toString ( ) + ", product of:" ) ; result . addDetail ( vals . explain ( doc ) ) ; result . addDetail ( new Explanation ( getBoost ( ) , "boost" ) ) ; result . addDetail ( new Explanation ( weight . queryNorm , "queryNorm" ) ) ; return result ; } } protected Weight createWeight ( Searcher searcher ) { return new ValueSourceQuery . ValueSourceWeight ( searcher ) ; } public String toString ( String field ) { return valSrc . toString ( ) + ToStringUtils . boost ( getBoost ( ) ) ; } public boolean equals ( Object o ) { if ( getClass ( ) != o . getClass ( ) ) { return false ; } ValueSourceQuery other = ( ValueSourceQuery ) o ; return this . getBoost ( ) == other . getBoost ( ) && this . valSrc . equals ( other . valSrc ) ; } public int hashCode ( ) { return ( getClass ( ) . hashCode ( ) + valSrc . hashCode ( ) ) ^ Float . floatToIntBits ( getBoost ( ) ) ; } } 	1
package org . apache . lucene . search ; import java . util . ArrayList ; public class Explanation implements java . io . Serializable { private float value ; private String description ; private ArrayList details ; public Explanation ( ) { } public Explanation ( float value , String description ) { this . value = value ; this . description = description ; } public boolean isMatch ( ) { return ( 0.0f < getValue ( ) ) ; } public float getValue ( ) { return value ; } public void setValue ( float value ) { this . value = value ; } public String getDescription ( ) { return description ; } public void setDescription ( String description ) { this . description = description ; } protected String getSummary ( ) { return getValue ( ) + " = " + getDescription ( ) ; } public Explanation [ ] getDetails ( ) { if ( details == null ) return null ; return ( Explanation [ ] ) details . toArray ( new Explanation [ 0 ] ) ; } public void addDetail ( Explanation detail ) { if ( details == null ) details = new ArrayList ( ) ; details . add ( detail ) ; } public String toString ( ) { return toString ( 0 ) ; } protected String toString ( int depth ) { StringBuffer buffer = new StringBuffer ( ) ; for ( int i = 0 ; i < depth ; i ++ ) { buffer . append ( "  " ) ; } buffer . append ( getSummary ( ) ) ; buffer . append ( "\n" ) ; Explanation [ ] details = getDetails ( ) ; if ( details != null ) { for ( int i = 0 ; i < details . length ; i ++ ) { buffer . append ( details [ i ] . toString ( depth + 1 ) ) ; } } return buffer . toString ( ) ; } public String toHtml ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "<ul>\n" ) ; buffer . append ( "<li>" ) ; buffer . append ( getSummary ( ) ) ; buffer . append ( "<br />\n" ) ; Explanation [ ] details = getDetails ( ) ; if ( details != null ) { for ( int i = 0 ; i < details . length ; i ++ ) { buffer . append ( details [ i ] . toHtml ( ) ) ; } } buffer . append ( "</li>\n" ) ; buffer . append ( "</ul>\n" ) ; return buffer . toString ( ) ; } } 	0
package org . apache . lucene . index ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . FieldSelector ; import org . apache . lucene . document . FieldSelectorResult ; import org . apache . lucene . document . Fieldable ; import java . io . IOException ; import java . util . * ; public class ParallelReader extends IndexReader { private List readers = new ArrayList ( ) ; private SortedMap fieldToReader = new TreeMap ( ) ; private Map readerToFields = new HashMap ( ) ; private List storedFieldReaders = new ArrayList ( ) ; private int maxDoc ; private int numDocs ; private boolean hasDeletions ; public ParallelReader ( ) throws IOException { super ( null ) ; } public void add ( IndexReader reader ) throws IOException { ensureOpen ( ) ; add ( reader , false ) ; } public void add ( IndexReader reader , boolean ignoreStoredFields ) throws IOException { ensureOpen ( ) ; if ( readers . size ( ) == 0 ) { this . maxDoc = reader . maxDoc ( ) ; this . numDocs = reader . numDocs ( ) ; this . hasDeletions = reader . hasDeletions ( ) ; } if ( reader . maxDoc ( ) != maxDoc ) throw new IllegalArgumentException ( "All readers must have same maxDoc: " + maxDoc + "!=" + reader . maxDoc ( ) ) ; if ( reader . numDocs ( ) != numDocs ) throw new IllegalArgumentException ( "All readers must have same numDocs: " + numDocs + "!=" + reader . numDocs ( ) ) ; Collection fields = reader . getFieldNames ( IndexReader . FieldOption . ALL ) ; readerToFields . put ( reader , fields ) ; Iterator i = fields . iterator ( ) ; while ( i . hasNext ( ) ) { String field = ( String ) i . next ( ) ; if ( fieldToReader . get ( field ) == null ) fieldToReader . put ( field , reader ) ; } if ( ! ignoreStoredFields ) storedFieldReaders . add ( reader ) ; readers . add ( reader ) ; } public int numDocs ( ) { return numDocs ; } public int maxDoc ( ) { return maxDoc ; } public boolean hasDeletions ( ) { return hasDeletions ; } public boolean isDeleted ( int n ) { if ( readers . size ( ) > 0 ) return ( ( IndexReader ) readers . get ( 0 ) ) . isDeleted ( n ) ; return false ; } protected void doDelete ( int n ) throws CorruptIndexException , IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { ( ( IndexReader ) readers . get ( i ) ) . deleteDocument ( n ) ; } hasDeletions = true ; } protected void doUndeleteAll ( ) throws CorruptIndexException , IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { ( ( IndexReader ) readers . get ( i ) ) . undeleteAll ( ) ; } hasDeletions = false ; } public Document document ( int n , FieldSelector fieldSelector ) throws CorruptIndexException , IOException { ensureOpen ( ) ; Document result = new Document ( ) ; for ( int i = 0 ; i < storedFieldReaders . size ( ) ; i ++ ) { IndexReader reader = ( IndexReader ) storedFieldReaders . get ( i ) ; boolean include = ( fieldSelector == null ) ; if ( ! include ) { Iterator it = ( ( Collection ) readerToFields . get ( reader ) ) . iterator ( ) ; while ( it . hasNext ( ) ) if ( fieldSelector . accept ( ( String ) it . next ( ) ) != FieldSelectorResult . NO_LOAD ) { include = true ; break ; } } if ( include ) { Iterator fieldIterator = reader . document ( n , fieldSelector ) . getFields ( ) . iterator ( ) ; while ( fieldIterator . hasNext ( ) ) { result . add ( ( Fieldable ) fieldIterator . next ( ) ) ; } } } return result ; } public TermFreqVector [ ] getTermFreqVectors ( int n ) throws IOException { ensureOpen ( ) ; ArrayList results = new ArrayList ( ) ; Iterator i = fieldToReader . entrySet ( ) . iterator ( ) ; while ( i . hasNext ( ) ) { Map . Entry e = ( Map . Entry ) i . next ( ) ; String field = ( String ) e . getKey ( ) ; IndexReader reader = ( IndexReader ) e . getValue ( ) ; TermFreqVector vector = reader . getTermFreqVector ( n , field ) ; if ( vector != null ) results . add ( vector ) ; } return ( TermFreqVector [ ] ) results . toArray ( new TermFreqVector [ results . size ( ) ] ) ; } public TermFreqVector getTermFreqVector ( int n , String field ) throws IOException { ensureOpen ( ) ; IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; return reader == null ? null : reader . getTermFreqVector ( n , field ) ; } public boolean hasNorms ( String field ) throws IOException { ensureOpen ( ) ; IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; return reader == null ? false : reader . hasNorms ( field ) ; } public byte [ ] norms ( String field ) throws IOException { ensureOpen ( ) ; IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; return reader == null ? null : reader . norms ( field ) ; } public void norms ( String field , byte [ ] result , int offset ) throws IOException { ensureOpen ( ) ; IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; if ( reader != null ) reader . norms ( field , result , offset ) ; } protected void doSetNorm ( int n , String field , byte value ) throws CorruptIndexException , IOException { IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; if ( reader != null ) reader . doSetNorm ( n , field , value ) ; } public TermEnum terms ( ) throws IOException { ensureOpen ( ) ; return new ParallelTermEnum ( ) ; } public TermEnum terms ( Term term ) throws IOException { ensureOpen ( ) ; return new ParallelTermEnum ( term ) ; } public int docFreq ( Term term ) throws IOException { ensureOpen ( ) ; IndexReader reader = ( ( IndexReader ) fieldToReader . get ( term . field ( ) ) ) ; return reader == null ? 0 : reader . docFreq ( term ) ; } public TermDocs termDocs ( Term term ) throws IOException { ensureOpen ( ) ; return new ParallelTermDocs ( term ) ; } public TermDocs termDocs ( ) throws IOException { ensureOpen ( ) ; return new ParallelTermDocs ( ) ; } public TermPositions termPositions ( Term term ) throws IOException { ensureOpen ( ) ; return new ParallelTermPositions ( term ) ; } public TermPositions termPositions ( ) throws IOException { ensureOpen ( ) ; return new ParallelTermPositions ( ) ; } protected void doCommit ( ) throws IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) ( ( IndexReader ) readers . get ( i ) ) . commit ( ) ; } protected synchronized void doClose ( ) throws IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) ( ( IndexReader ) readers . get ( i ) ) . close ( ) ; } public Collection getFieldNames ( IndexReader . FieldOption fieldNames ) { ensureOpen ( ) ; Set fieldSet = new HashSet ( ) ; for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader reader = ( ( IndexReader ) readers . get ( i ) ) ; Collection names = reader . getFieldNames ( fieldNames ) ; fieldSet . addAll ( names ) ; } return fieldSet ; } private class ParallelTermEnum extends TermEnum { private String field ; private Iterator fieldIterator ; private TermEnum termEnum ; public ParallelTermEnum ( ) throws IOException { field = ( String ) fieldToReader . firstKey ( ) ; if ( field != null ) termEnum = ( ( IndexReader ) fieldToReader . get ( field ) ) . terms ( ) ; } public ParallelTermEnum ( Term term ) throws IOException { field = term . field ( ) ; IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; if ( reader != null ) termEnum = reader . terms ( term ) ; } public boolean next ( ) throws IOException { if ( termEnum == null ) return false ; if ( termEnum . next ( ) && termEnum . term ( ) . field ( ) == field ) return true ; termEnum . close ( ) ; if ( fieldIterator == null ) { fieldIterator = fieldToReader . tailMap ( field ) . keySet ( ) . iterator ( ) ; fieldIterator . next ( ) ; } while ( fieldIterator . hasNext ( ) ) { field = ( String ) fieldIterator . next ( ) ; termEnum = ( ( IndexReader ) fieldToReader . get ( field ) ) . terms ( new Term ( field , "" ) ) ; Term term = termEnum . term ( ) ; if ( term != null && term . field ( ) == field ) return true ; else termEnum . close ( ) ; } return false ; } public Term term ( ) { if ( termEnum == null ) return null ; return termEnum . term ( ) ; } public int docFreq ( ) { if ( termEnum == null ) return 0 ; return termEnum . docFreq ( ) ; } public void close ( ) throws IOException { if ( termEnum != null ) termEnum . close ( ) ; } } private class ParallelTermDocs implements TermDocs { protected TermDocs termDocs ; public ParallelTermDocs ( ) { } public ParallelTermDocs ( Term term ) throws IOException { seek ( term ) ; } public int doc ( ) { return termDocs . doc ( ) ; } public int freq ( ) { return termDocs . freq ( ) ; } public void seek ( Term term ) throws IOException { IndexReader reader = ( ( IndexReader ) fieldToReader . get ( term . field ( ) ) ) ; termDocs = reader != null ? reader . termDocs ( term ) : null ; } public void seek ( TermEnum termEnum ) throws IOException { seek ( termEnum . term ( ) ) ; } public boolean next ( ) throws IOException { if ( termDocs == null ) return false ; return termDocs . next ( ) ; } public int read ( final int [ ] docs , final int [ ] freqs ) throws IOException { if ( termDocs == null ) return 0 ; return termDocs . read ( docs , freqs ) ; } public boolean skipTo ( int target ) throws IOException { if ( termDocs == null ) return false ; return termDocs . skipTo ( target ) ; } public void close ( ) throws IOException { if ( termDocs != null ) termDocs . close ( ) ; } } private class ParallelTermPositions extends ParallelTermDocs implements TermPositions { public ParallelTermPositions ( ) { } public ParallelTermPositions ( Term term ) throws IOException { seek ( term ) ; } public void seek ( Term term ) throws IOException { IndexReader reader = ( ( IndexReader ) fieldToReader . get ( term . field ( ) ) ) ; termDocs = reader != null ? reader . termPositions ( term ) : null ; } public int nextPosition ( ) throws IOException { return ( ( TermPositions ) termDocs ) . nextPosition ( ) ; } public int getPayloadLength ( ) { return ( ( TermPositions ) termDocs ) . getPayloadLength ( ) ; } public byte [ ] getPayload ( byte [ ] data , int offset ) throws IOException { return ( ( TermPositions ) termDocs ) . getPayload ( data , offset ) ; } public boolean isPayloadAvailable ( ) { return ( ( TermPositions ) termDocs ) . isPayloadAvailable ( ) ; } } } 	1
package org . apache . lucene . index ; public class FieldReaderException extends RuntimeException { public FieldReaderException ( ) { } public FieldReaderException ( Throwable cause ) { super ( cause ) ; } public FieldReaderException ( String message ) { super ( message ) ; } public FieldReaderException ( String message , Throwable cause ) { super ( message , cause ) ; } } 	0
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . FieldCache ; import org . apache . lucene . search . function . DocValues ; import java . io . IOException ; public class ShortFieldSource extends FieldCacheSource { private FieldCache . ShortParser parser ; public ShortFieldSource ( String field ) { this ( field , null ) ; } public ShortFieldSource ( String field , FieldCache . ShortParser parser ) { super ( field ) ; this . parser = parser ; } public String description ( ) { return "short(" + super . description ( ) + ')' ; } public DocValues getCachedFieldValues ( FieldCache cache , String field , IndexReader reader ) throws IOException { final short [ ] arr = ( parser == null ) ? cache . getShorts ( reader , field ) : cache . getShorts ( reader , field , parser ) ; return new DocValues ( reader . maxDoc ( ) ) { public float floatVal ( int doc ) { return ( float ) arr [ doc ] ; } public int intVal ( int doc ) { return arr [ doc ] ; } public String toString ( int doc ) { return description ( ) + '=' + intVal ( doc ) ; } Object getInnerArray ( ) { return arr ; } } ; } public boolean cachedFieldSourceEquals ( FieldCacheSource o ) { if ( o . getClass ( ) != ShortFieldSource . class ) { return false ; } ShortFieldSource other = ( ShortFieldSource ) o ; return this . parser == null ? other . parser == null : this . parser . getClass ( ) == other . parser . getClass ( ) ; } public int cachedFieldSourceHashCode ( ) { return parser == null ? Short . class . hashCode ( ) : parser . getClass ( ) . hashCode ( ) ; } } 	1
