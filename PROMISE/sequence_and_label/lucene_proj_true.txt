package org . apache . lucene . analysis ; import java . io . Reader ; public final class WhitespaceAnalyzer extends Analyzer { public TokenStream tokenStream ( String fieldName , Reader reader ) { return new WhitespaceTokenizer ( reader ) ; } } 	0	['2', '2', '0', '3', '4', '1', '0', '3', '2', '2', '10', '0', '0', '0.666666667', '0.666666667', '0', '0', '4', '1', '0.5', '0']
package org . apache . lucene . search ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . analysis . TokenStream ; import org . apache . lucene . index . TermFreqVector ; import java . io . IOException ; import java . io . StringReader ; import java . util . * ; public class QueryTermVector implements TermFreqVector { private String [ ] terms = new String [ 0 ] ; private int [ ] termFreqs = new int [ 0 ] ; public String getField ( ) { return null ; } public QueryTermVector ( String [ ] queryTerms ) { processTerms ( queryTerms ) ; } public QueryTermVector ( String queryString , Analyzer analyzer ) { if ( analyzer != null ) { TokenStream stream = analyzer . tokenStream ( "" , new StringReader ( queryString ) ) ; if ( stream != null ) { Token next = null ; List terms = new ArrayList ( ) ; try { while ( ( next = stream . next ( ) ) != null ) { terms . add ( next . termText ( ) ) ; } processTerms ( ( String [ ] ) terms . toArray ( new String [ terms . size ( ) ] ) ) ; } catch ( IOException e ) { } } } } private void processTerms ( String [ ] queryTerms ) { if ( queryTerms != null ) { Arrays . sort ( queryTerms ) ; Map tmpSet = new HashMap ( queryTerms . length ) ; List tmpList = new ArrayList ( queryTerms . length ) ; List tmpFreqs = new ArrayList ( queryTerms . length ) ; int j = 0 ; for ( int i = 0 ; i < queryTerms . length ; i ++ ) { String term = queryTerms [ i ] ; Integer position = ( Integer ) tmpSet . get ( term ) ; if ( position == null ) { tmpSet . put ( term , new Integer ( j ++ ) ) ; tmpList . add ( term ) ; tmpFreqs . add ( new Integer ( 1 ) ) ; } else { Integer integer = ( Integer ) tmpFreqs . get ( position . intValue ( ) ) ; tmpFreqs . set ( position . intValue ( ) , new Integer ( integer . intValue ( ) + 1 ) ) ; } } terms = ( String [ ] ) tmpList . toArray ( terms ) ; termFreqs = new int [ tmpFreqs . size ( ) ] ; int i = 0 ; for ( Iterator iter = tmpFreqs . iterator ( ) ; iter . hasNext ( ) ; ) { Integer integer = ( Integer ) iter . next ( ) ; termFreqs [ i ++ ] = integer . intValue ( ) ; } } } public final String toString ( ) { StringBuffer sb = new StringBuffer ( ) ; sb . append ( '{' ) ; for ( int i = 0 ; i < terms . length ; i ++ ) { if ( i > 0 ) sb . append ( ", " ) ; sb . append ( terms [ i ] ) . append ( '/' ) . append ( termFreqs [ i ] ) ; } sb . append ( '}' ) ; return sb . toString ( ) ; } public int size ( ) { return terms . length ; } public String [ ] getTerms ( ) { return terms ; } public int [ ] getTermFrequencies ( ) { return termFreqs ; } public int indexOf ( String term ) { int res = Arrays . binarySearch ( terms , term ) ; return res >= 0 ? res : - 1 ; } public int [ ] indexesOf ( String [ ] terms , int start , int len ) { int res [ ] = new int [ len ] ; for ( int i = 0 ; i < len ; i ++ ) { res [ i ] = indexOf ( terms [ i ] ) ; } return res ; } } 	0	['10', '1', '0', '4', '37', '0', '0', '4', '9', '0.388888889', '278', '1', '0', '0', '0.34', '0', '0', '26.6', '5', '1.6', '0']
package org . apache . lucene . analysis ; import java . io . * ; class PorterStemmer { private char [ ] b ; private int i , j , k , k0 ; private boolean dirty = false ; private static final int INC = 50 ; private static final int EXTRA = 1 ; public PorterStemmer ( ) { b = new char [ INC ] ; i = 0 ; } public void reset ( ) { i = 0 ; dirty = false ; } public void add ( char ch ) { if ( b . length <= i + EXTRA ) { char [ ] new_b = new char [ b . length + INC ] ; for ( int c = 0 ; c < b . length ; c ++ ) new_b [ c ] = b [ c ] ; b = new_b ; } b [ i ++ ] = ch ; } public String toString ( ) { return new String ( b , 0 , i ) ; } public int getResultLength ( ) { return i ; } public char [ ] getResultBuffer ( ) { return b ; } private final boolean cons ( int i ) { switch ( b [ i ] ) { case 'a' : case 'e' : case 'i' : case 'o' : case 'u' : return false ; case 'y' : return ( i == k0 ) ? true : ! cons ( i - 1 ) ; default : return true ; } } private final int m ( ) { int n = 0 ; int i = k0 ; while ( true ) { if ( i > j ) return n ; if ( ! cons ( i ) ) break ; i ++ ; } i ++ ; while ( true ) { while ( true ) { if ( i > j ) return n ; if ( cons ( i ) ) break ; i ++ ; } i ++ ; n ++ ; while ( true ) { if ( i > j ) return n ; if ( ! cons ( i ) ) break ; i ++ ; } i ++ ; } } private final boolean vowelinstem ( ) { int i ; for ( i = k0 ; i <= j ; i ++ ) if ( ! cons ( i ) ) return true ; return false ; } private final boolean doublec ( int j ) { if ( j < k0 + 1 ) return false ; if ( b [ j ] != b [ j - 1 ] ) return false ; return cons ( j ) ; } private final boolean cvc ( int i ) { if ( i < k0 + 2 || ! cons ( i ) || cons ( i - 1 ) || ! cons ( i - 2 ) ) return false ; else { int ch = b [ i ] ; if ( ch == 'w' || ch == 'x' || ch == 'y' ) return false ; } return true ; } private final boolean ends ( String s ) { int l = s . length ( ) ; int o = k - l + 1 ; if ( o < k0 ) return false ; for ( int i = 0 ; i < l ; i ++ ) if ( b [ o + i ] != s . charAt ( i ) ) return false ; j = k - l ; return true ; } void setto ( String s ) { int l = s . length ( ) ; int o = j + 1 ; for ( int i = 0 ; i < l ; i ++ ) b [ o + i ] = s . charAt ( i ) ; k = j + l ; dirty = true ; } void r ( String s ) { if ( m ( ) > 0 ) setto ( s ) ; } private final void step1 ( ) { if ( b [ k ] == 's' ) { if ( ends ( "sses" ) ) k -= 2 ; else if ( ends ( "ies" ) ) setto ( "i" ) ; else if ( b [ k - 1 ] != 's' ) k -- ; } if ( ends ( "eed" ) ) { if ( m ( ) > 0 ) k -- ; } else if ( ( ends ( "ed" ) || ends ( "ing" ) ) && vowelinstem ( ) ) { k = j ; if ( ends ( "at" ) ) setto ( "ate" ) ; else if ( ends ( "bl" ) ) setto ( "ble" ) ; else if ( ends ( "iz" ) ) setto ( "ize" ) ; else if ( doublec ( k ) ) { int ch = b [ k -- ] ; if ( ch == 'l' || ch == 's' || ch == 'z' ) k ++ ; } else if ( m ( ) == 1 && cvc ( k ) ) setto ( "e" ) ; } } private final void step2 ( ) { if ( ends ( "y" ) && vowelinstem ( ) ) { b [ k ] = 'i' ; dirty = true ; } } private final void step3 ( ) { if ( k == k0 ) return ; switch ( b [ k - 1 ] ) { case 'a' : if ( ends ( "ational" ) ) { r ( "ate" ) ; break ; } if ( ends ( "tional" ) ) { r ( "tion" ) ; break ; } break ; case 'c' : if ( ends ( "enci" ) ) { r ( "ence" ) ; break ; } if ( ends ( "anci" ) ) { r ( "ance" ) ; break ; } break ; case 'e' : if ( ends ( "izer" ) ) { r ( "ize" ) ; break ; } break ; case 'l' : if ( ends ( "bli" ) ) { r ( "ble" ) ; break ; } if ( ends ( "alli" ) ) { r ( "al" ) ; break ; } if ( ends ( "entli" ) ) { r ( "ent" ) ; break ; } if ( ends ( "eli" ) ) { r ( "e" ) ; break ; } if ( ends ( "ousli" ) ) { r ( "ous" ) ; break ; } break ; case 'o' : if ( ends ( "ization" ) ) { r ( "ize" ) ; break ; } if ( ends ( "ation" ) ) { r ( "ate" ) ; break ; } if ( ends ( "ator" ) ) { r ( "ate" ) ; break ; } break ; case 's' : if ( ends ( "alism" ) ) { r ( "al" ) ; break ; } if ( ends ( "iveness" ) ) { r ( "ive" ) ; break ; } if ( ends ( "fulness" ) ) { r ( "ful" ) ; break ; } if ( ends ( "ousness" ) ) { r ( "ous" ) ; break ; } break ; case 't' : if ( ends ( "aliti" ) ) { r ( "al" ) ; break ; } if ( ends ( "iviti" ) ) { r ( "ive" ) ; break ; } if ( ends ( "biliti" ) ) { r ( "ble" ) ; break ; } break ; case 'g' : if ( ends ( "logi" ) ) { r ( "log" ) ; break ; } } } private final void step4 ( ) { switch ( b [ k ] ) { case 'e' : if ( ends ( "icate" ) ) { r ( "ic" ) ; break ; } if ( ends ( "ative" ) ) { r ( "" ) ; break ; } if ( ends ( "alize" ) ) { r ( "al" ) ; break ; } break ; case 'i' : if ( ends ( "iciti" ) ) { r ( "ic" ) ; break ; } break ; case 'l' : if ( ends ( "ical" ) ) { r ( "ic" ) ; break ; } if ( ends ( "ful" ) ) { r ( "" ) ; break ; } break ; case 's' : if ( ends ( "ness" ) ) { r ( "" ) ; break ; } break ; } } private final void step5 ( ) { if ( k == k0 ) return ; switch ( b [ k - 1 ] ) { case 'a' : if ( ends ( "al" ) ) break ; return ; case 'c' : if ( ends ( "ance" ) ) break ; if ( ends ( "ence" ) ) break ; return ; case 'e' : if ( ends ( "er" ) ) break ; return ; case 'i' : if ( ends ( "ic" ) ) break ; return ; case 'l' : if ( ends ( "able" ) ) break ; if ( ends ( "ible" ) ) break ; return ; case 'n' : if ( ends ( "ant" ) ) break ; if ( ends ( "ement" ) ) break ; if ( ends ( "ment" ) ) break ; if ( ends ( "ent" ) ) break ; return ; case 'o' : if ( ends ( "ion" ) && j >= 0 && ( b [ j ] == 's' || b [ j ] == 't' ) ) break ; if ( ends ( "ou" ) ) break ; return ; case 's' : if ( ends ( "ism" ) ) break ; return ; case 't' : if ( ends ( "ate" ) ) break ; if ( ends ( "iti" ) ) break ; return ; case 'u' : if ( ends ( "ous" ) ) break ; return ; case 'v' : if ( ends ( "ive" ) ) break ; return ; case 'z' : if ( ends ( "ize" ) ) break ; return ; default : return ; } if ( m ( ) > 1 ) k = j ; } private final void step6 ( ) { j = k ; if ( b [ k ] == 'e' ) { int a = m ( ) ; if ( a > 1 || a == 1 && ! cvc ( k - 1 ) ) k -- ; } if ( b [ k ] == 'l' && doublec ( k ) && m ( ) > 1 ) k -- ; } public String stem ( String s ) { if ( stem ( s . toCharArray ( ) , s . length ( ) ) ) return toString ( ) ; else return s ; } public boolean stem ( char [ ] word ) { return stem ( word , word . length ) ; } public boolean stem ( char [ ] wordBuffer , int offset , int wordLen ) { reset ( ) ; if ( b . length < wordLen ) { char [ ] new_b = new char [ wordLen + EXTRA ] ; b = new_b ; } for ( int j = 0 ; j < wordLen ; j ++ ) b [ j ] = wordBuffer [ offset + j ] ; i = wordLen ; return stem ( 0 ) ; } public boolean stem ( char [ ] word , int wordLen ) { return stem ( word , 0 , wordLen ) ; } public boolean stem ( ) { return stem ( 0 ) ; } public boolean stem ( int i0 ) { k = i - 1 ; k0 = i0 ; if ( k > k0 + 1 ) { step1 ( ) ; step2 ( ) ; step3 ( ) ; step4 ( ) ; step5 ( ) ; step6 ( ) ; } if ( i != k + 1 ) dirty = true ; i = k + 1 ; return dirty ; } public static void main ( String [ ] args ) { PorterStemmer s = new PorterStemmer ( ) ; for ( int i = 0 ; i < args . length ; i ++ ) { try { InputStream in = new FileInputStream ( args [ i ] ) ; byte [ ] buffer = new byte [ 1024 ] ; int bufferLen , offset , ch ; bufferLen = in . read ( buffer ) ; offset = 0 ; s . reset ( ) ; while ( true ) { if ( offset < bufferLen ) ch = buffer [ offset ++ ] ; else { bufferLen = in . read ( buffer ) ; offset = 0 ; if ( bufferLen < 0 ) ch = - 1 ; else ch = buffer [ offset ++ ] ; } if ( Character . isLetter ( ( char ) ch ) ) { s . add ( Character . toLowerCase ( ( char ) ch ) ) ; } else { s . stem ( ) ; System . out . print ( s . toString ( ) ) ; s . reset ( ) ; if ( ch < 0 ) break ; else { System . out . print ( ( char ) ch ) ; } } } in . close ( ) ; } catch ( IOException e ) { System . out . println ( "error reading " + args [ i ] ) ; } } } } 	0	['27', '1', '0', '1', '43', '13', '1', '0', '13', '0.600961538', '1174', '1', '0', '0', '0.25308642', '0', '0', '42.18518519', '26', '5.7407', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermDocs ; import org . apache . lucene . index . TermEnum ; import java . io . IOException ; import java . util . BitSet ; public class RangeFilter extends Filter { private String fieldName ; private String lowerTerm ; private String upperTerm ; private boolean includeLower ; private boolean includeUpper ; public RangeFilter ( String fieldName , String lowerTerm , String upperTerm , boolean includeLower , boolean includeUpper ) { this . fieldName = fieldName ; this . lowerTerm = lowerTerm ; this . upperTerm = upperTerm ; this . includeLower = includeLower ; this . includeUpper = includeUpper ; if ( null == lowerTerm && null == upperTerm ) { throw new IllegalArgumentException ( "At least one value must be non-null" ) ; } if ( includeLower && null == lowerTerm ) { throw new IllegalArgumentException ( "The lower bound must be non-null to be inclusive" ) ; } if ( includeUpper && null == upperTerm ) { throw new IllegalArgumentException ( "The upper bound must be non-null to be inclusive" ) ; } } public static RangeFilter Less ( String fieldName , String upperTerm ) { return new RangeFilter ( fieldName , null , upperTerm , false , true ) ; } public static RangeFilter More ( String fieldName , String lowerTerm ) { return new RangeFilter ( fieldName , lowerTerm , null , true , false ) ; } public BitSet bits ( IndexReader reader ) throws IOException { BitSet bits = new BitSet ( reader . maxDoc ( ) ) ; TermEnum enumerator = ( null != lowerTerm ? reader . terms ( new Term ( fieldName , lowerTerm ) ) : reader . terms ( new Term ( fieldName , "" ) ) ) ; try { if ( enumerator . term ( ) == null ) { return bits ; } boolean checkLower = false ; if ( ! includeLower ) checkLower = true ; TermDocs termDocs = reader . termDocs ( ) ; try { do { Term term = enumerator . term ( ) ; if ( term != null && term . field ( ) . equals ( fieldName ) ) { if ( ! checkLower || null == lowerTerm || term . text ( ) . compareTo ( lowerTerm ) > 0 ) { checkLower = false ; if ( upperTerm != null ) { int compare = upperTerm . compareTo ( term . text ( ) ) ; if ( ( compare < 0 ) || ( ! includeUpper && compare == 0 ) ) { break ; } } termDocs . seek ( enumerator . term ( ) ) ; while ( termDocs . next ( ) ) { bits . set ( termDocs . doc ( ) ) ; } } } else { break ; } } while ( enumerator . next ( ) ) ; } finally { termDocs . close ( ) ; } } finally { enumerator . close ( ) ; } return bits ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( fieldName ) ; buffer . append ( ":" ) ; buffer . append ( includeLower ? "[" : "{" ) ; if ( null != lowerTerm ) { buffer . append ( lowerTerm ) ; } buffer . append ( "-" ) ; if ( null != upperTerm ) { buffer . append ( upperTerm ) ; } buffer . append ( includeUpper ? "]" : "}" ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof RangeFilter ) ) return false ; RangeFilter other = ( RangeFilter ) o ; if ( ! this . fieldName . equals ( other . fieldName ) || this . includeLower != other . includeLower || this . includeUpper != other . includeUpper ) { return false ; } if ( this . lowerTerm != null ? ! this . lowerTerm . equals ( other . lowerTerm ) : other . lowerTerm != null ) return false ; if ( this . upperTerm != null ? ! this . upperTerm . equals ( other . upperTerm ) : other . upperTerm != null ) return false ; return true ; } public int hashCode ( ) { int h = fieldName . hashCode ( ) ; h ^= lowerTerm != null ? lowerTerm . hashCode ( ) : 0xB6ECE882 ; h = ( h << 1 ) | ( h > > > 31 ) ; h ^= ( upperTerm != null ? ( upperTerm . hashCode ( ) ) : 0x91BEC2C2 ) ; h ^= ( includeLower ? 0xD484B933 : 0 ) ^ ( includeUpper ? 0x6AE423AC : 0 ) ; return h ; } } 	0	['7', '2', '0', '6', '30', '1', '1', '5', '7', '0', '373', '1', '0', '0.142857143', '0.314285714', '1', '1', '51.57142857', '12', '3.5714', '0']
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; final class HitQueue extends PriorityQueue { HitQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object a , Object b ) { ScoreDoc hitA = ( ScoreDoc ) a ; ScoreDoc hitB = ( ScoreDoc ) b ; if ( hitA . score == hitB . score ) return hitA . doc > hitB . doc ; else return hitA . score < hitB . score ; } } 	0	['2', '2', '0', '6', '4', '1', '4', '2', '0', '2', '39', '0', '0', '0.916666667', '0.666666667', '1', '3', '18.5', '4', '2', '0']
package org . apache . lucene . search ; import java . io . IOException ; class NonMatchingScorer extends Scorer { public NonMatchingScorer ( ) { super ( null ) ; } public int doc ( ) { throw new UnsupportedOperationException ( ) ; } public boolean next ( ) throws IOException { return false ; } public float score ( ) { throw new UnsupportedOperationException ( ) ; } public boolean skipTo ( int target ) { return false ; } public Explanation explain ( int doc ) { Explanation e = new Explanation ( ) ; e . setDescription ( "No document matches." ) ; return e ; } } 	0	['6', '2', '0', '4', '10', '15', '1', '3', '6', '2', '31', '0', '0', '0.615384615', '0.666666667', '1', '3', '4.166666667', '1', '0.8333', '0']
package org . apache . lucene . search ; public class TopFieldDocs extends TopDocs { public SortField [ ] fields ; TopFieldDocs ( int totalHits , ScoreDoc [ ] scoreDocs , SortField [ ] fields , float maxScore ) { super ( totalHits , scoreDocs , maxScore ) ; this . fields = fields ; } } 	0	['1', '2', '0', '14', '2', '0', '11', '3', '0', '2', '11', '0', '1', '1', '1', '0', '0', '9', '0', '0', '0']
package org . apache . lucene . analysis . standard ; public interface StandardTokenizerConstants { int EOF = 0 ; int ALPHANUM = 1 ; int APOSTROPHE = 2 ; int ACRONYM = 3 ; int COMPANY = 4 ; int EMAIL = 5 ; int HOST = 6 ; int NUM = 7 ; int P = 8 ; int HAS_DIGIT = 9 ; int ALPHA = 10 ; int LETTER = 11 ; int CJ = 12 ; int KOREAN = 13 ; int DIGIT = 14 ; int NOISE = 15 ; int DEFAULT = 0 ; String [ ] tokenImage = { "<EOF>" , "<ALPHANUM>" , "<APOSTROPHE>" , "<ACRONYM>" , "<COMPANY>" , "<EMAIL>" , "<HOST>" , "<NUM>" , "<P>" , "<HAS_DIGIT>" , "<ALPHA>" , "<LETTER>" , "<CJ>" , "<KOREAN>" , "<DIGIT>" , "<NOISE>" , } ; } 	0	['1', '1', '0', '3', '1', '0', '3', '0', '0', '2', '87', '0', '0', '0', '0', '0', '0', '68', '0', '0', '0']
package org . apache . lucene . index ; import java . io . IOException ; public abstract class TermEnum { public abstract boolean next ( ) throws IOException ; public abstract Term term ( ) ; public abstract int docFreq ( ) ; public abstract void close ( ) throws IOException ; public boolean skipTo ( Term target ) throws IOException { do { if ( ! next ( ) ) return false ; } while ( target . compareTo ( term ( ) ) > 0 ) ; return true ; } } 	0	['6', '1', '5', '25', '8', '15', '24', '1', '6', '2', '21', '0', '0', '0', '0.583333333', '0', '0', '2.5', '1', '0.8333', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . util . StringHelper ; import java . io . IOException ; import java . util . Vector ; final class TermVectorsWriter { static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x1 ; static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x2 ; static final int FORMAT_VERSION = 2 ; static final int FORMAT_SIZE = 4 ; static final String TVX_EXTENSION = ".tvx" ; static final String TVD_EXTENSION = ".tvd" ; static final String TVF_EXTENSION = ".tvf" ; private IndexOutput tvx = null , tvd = null , tvf = null ; private Vector fields = null ; private Vector terms = null ; private FieldInfos fieldInfos ; private TVField currentField = null ; private long currentDocPointer = - 1 ; public TermVectorsWriter ( Directory directory , String segment , FieldInfos fieldInfos ) throws IOException { tvx = directory . createOutput ( segment + TVX_EXTENSION ) ; tvx . writeInt ( FORMAT_VERSION ) ; tvd = directory . createOutput ( segment + TVD_EXTENSION ) ; tvd . writeInt ( FORMAT_VERSION ) ; tvf = directory . createOutput ( segment + TVF_EXTENSION ) ; tvf . writeInt ( FORMAT_VERSION ) ; this . fieldInfos = fieldInfos ; fields = new Vector ( fieldInfos . size ( ) ) ; terms = new Vector ( ) ; } public final void openDocument ( ) throws IOException { closeDocument ( ) ; currentDocPointer = tvd . getFilePointer ( ) ; } public final void closeDocument ( ) throws IOException { if ( isDocumentOpen ( ) ) { closeField ( ) ; writeDoc ( ) ; fields . clear ( ) ; currentDocPointer = - 1 ; } } public final boolean isDocumentOpen ( ) { return currentDocPointer != - 1 ; } public final void openField ( String field ) throws IOException { FieldInfo fieldInfo = fieldInfos . fieldInfo ( field ) ; openField ( fieldInfo . number , fieldInfo . storePositionWithTermVector , fieldInfo . storeOffsetWithTermVector ) ; } private void openField ( int fieldNumber , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector ) throws IOException { if ( ! isDocumentOpen ( ) ) throw new IllegalStateException ( "Cannot open field when no document is open." ) ; closeField ( ) ; currentField = new TVField ( fieldNumber , storePositionWithTermVector , storeOffsetWithTermVector ) ; } public final void closeField ( ) throws IOException { if ( isFieldOpen ( ) ) { writeField ( ) ; fields . add ( currentField ) ; terms . clear ( ) ; currentField = null ; } } public final boolean isFieldOpen ( ) { return currentField != null ; } public final void addTerm ( String termText , int freq ) { addTerm ( termText , freq , null , null ) ; } public final void addTerm ( String termText , int freq , int [ ] positions , TermVectorOffsetInfo [ ] offsets ) { if ( ! isDocumentOpen ( ) ) throw new IllegalStateException ( "Cannot add terms when document is not open" ) ; if ( ! isFieldOpen ( ) ) throw new IllegalStateException ( "Cannot add terms when field is not open" ) ; addTermInternal ( termText , freq , positions , offsets ) ; } private final void addTermInternal ( String termText , int freq , int [ ] positions , TermVectorOffsetInfo [ ] offsets ) { TVTerm term = new TVTerm ( ) ; term . termText = termText ; term . freq = freq ; term . positions = positions ; term . offsets = offsets ; terms . add ( term ) ; } public final void addAllDocVectors ( TermFreqVector [ ] vectors ) throws IOException { openDocument ( ) ; if ( vectors != null ) { for ( int i = 0 ; i < vectors . length ; i ++ ) { boolean storePositionWithTermVector = false ; boolean storeOffsetWithTermVector = false ; try { TermPositionVector tpVector = ( TermPositionVector ) vectors [ i ] ; if ( tpVector . size ( ) > 0 && tpVector . getTermPositions ( 0 ) != null ) storePositionWithTermVector = true ; if ( tpVector . size ( ) > 0 && tpVector . getOffsets ( 0 ) != null ) storeOffsetWithTermVector = true ; FieldInfo fieldInfo = fieldInfos . fieldInfo ( tpVector . getField ( ) ) ; openField ( fieldInfo . number , storePositionWithTermVector , storeOffsetWithTermVector ) ; for ( int j = 0 ; j < tpVector . size ( ) ; j ++ ) addTermInternal ( tpVector . getTerms ( ) [ j ] , tpVector . getTermFrequencies ( ) [ j ] , tpVector . getTermPositions ( j ) , tpVector . getOffsets ( j ) ) ; closeField ( ) ; } catch ( ClassCastException ignore ) { TermFreqVector tfVector = vectors [ i ] ; FieldInfo fieldInfo = fieldInfos . fieldInfo ( tfVector . getField ( ) ) ; openField ( fieldInfo . number , storePositionWithTermVector , storeOffsetWithTermVector ) ; for ( int j = 0 ; j < tfVector . size ( ) ; j ++ ) addTermInternal ( tfVector . getTerms ( ) [ j ] , tfVector . getTermFrequencies ( ) [ j ] , null , null ) ; closeField ( ) ; } } } closeDocument ( ) ; } final void close ( ) throws IOException { try { closeDocument ( ) ; } finally { IOException keep = null ; if ( tvx != null ) try { tvx . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tvd != null ) try { tvd . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tvf != null ) try { tvf . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( keep != null ) throw ( IOException ) keep . fillInStackTrace ( ) ; } } private void writeField ( ) throws IOException { currentField . tvfPointer = tvf . getFilePointer ( ) ; final int size = terms . size ( ) ; tvf . writeVInt ( size ) ; boolean storePositions = currentField . storePositions ; boolean storeOffsets = currentField . storeOffsets ; byte bits = 0x0 ; if ( storePositions ) bits |= STORE_POSITIONS_WITH_TERMVECTOR ; if ( storeOffsets ) bits |= STORE_OFFSET_WITH_TERMVECTOR ; tvf . writeByte ( bits ) ; String lastTermText = "" ; for ( int i = 0 ; i < size ; i ++ ) { TVTerm term = ( TVTerm ) terms . elementAt ( i ) ; int start = StringHelper . stringDifference ( lastTermText , term . termText ) ; int length = term . termText . length ( ) - start ; tvf . writeVInt ( start ) ; tvf . writeVInt ( length ) ; tvf . writeChars ( term . termText , start , length ) ; tvf . writeVInt ( term . freq ) ; lastTermText = term . termText ; if ( storePositions ) { if ( term . positions == null ) throw new IllegalStateException ( "Trying to write positions that are null!" ) ; int position = 0 ; for ( int j = 0 ; j < term . freq ; j ++ ) { tvf . writeVInt ( term . positions [ j ] - position ) ; position = term . positions [ j ] ; } } if ( storeOffsets ) { if ( term . offsets == null ) throw new IllegalStateException ( "Trying to write offsets that are null!" ) ; int position = 0 ; for ( int j = 0 ; j < term . freq ; j ++ ) { tvf . writeVInt ( term . offsets [ j ] . getStartOffset ( ) - position ) ; tvf . writeVInt ( term . offsets [ j ] . getEndOffset ( ) - term . offsets [ j ] . getStartOffset ( ) ) ; position = term . offsets [ j ] . getEndOffset ( ) ; } } } } private void writeDoc ( ) throws IOException { if ( isFieldOpen ( ) ) throw new IllegalStateException ( "Field is still open while writing document" ) ; tvx . writeLong ( currentDocPointer ) ; final int size = fields . size ( ) ; tvd . writeVInt ( size ) ; for ( int i = 0 ; i < size ; i ++ ) { TVField field = ( TVField ) fields . elementAt ( i ) ; tvd . writeVInt ( field . number ) ; } long lastFieldPointer = 0 ; for ( int i = 0 ; i < size ; i ++ ) { TVField field = ( TVField ) fields . elementAt ( i ) ; tvd . writeVLong ( field . tvfPointer - lastFieldPointer ) ; lastFieldPointer = field . tvfPointer ; } } private static class TVField { int number ; long tvfPointer = 0 ; boolean storePositions = false ; boolean storeOffsets = false ; TVField ( int number , boolean storePos , boolean storeOff ) { this . number = number ; storePositions = storePos ; storeOffsets = storeOff ; } } private static class TVTerm { String termText ; int freq = 0 ; int positions [ ] = null ; TermVectorOffsetInfo [ ] offsets = null ; } } 	0	['15', '1', '0', '13', '54', '41', '2', '11', '10', '0.804761905', '675', '0.533333333', '5', '0', '0.237037037', '0', '0', '43', '3', '1.2', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . HashSet ; import java . util . Iterator ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; public abstract class Query implements java . io . Serializable , Cloneable { private float boost = 1.0f ; public void setBoost ( float b ) { boost = b ; } public float getBoost ( ) { return boost ; } public abstract String toString ( String field ) ; public String toString ( ) { return toString ( "" ) ; } protected Weight createWeight ( Searcher searcher ) throws IOException { throw new UnsupportedOperationException ( ) ; } public Weight weight ( Searcher searcher ) throws IOException { Query query = searcher . rewrite ( this ) ; Weight weight = query . createWeight ( searcher ) ; float sum = weight . sumOfSquaredWeights ( ) ; float norm = getSimilarity ( searcher ) . queryNorm ( sum ) ; weight . normalize ( norm ) ; return weight ; } public Query rewrite ( IndexReader reader ) throws IOException { return this ; } public Query combine ( Query [ ] queries ) { HashSet uniques = new HashSet ( ) ; for ( int i = 0 ; i < queries . length ; i ++ ) { Query query = queries [ i ] ; BooleanClause [ ] clauses = null ; boolean splittable = ( query instanceof BooleanQuery ) ; if ( splittable ) { BooleanQuery bq = ( BooleanQuery ) query ; splittable = bq . isCoordDisabled ( ) ; clauses = bq . getClauses ( ) ; for ( int j = 0 ; splittable && j < clauses . length ; j ++ ) { splittable = ( clauses [ j ] . getOccur ( ) == BooleanClause . Occur . SHOULD ) ; } } if ( splittable ) { for ( int j = 0 ; j < clauses . length ; j ++ ) { uniques . add ( clauses [ j ] . getQuery ( ) ) ; } } else { uniques . add ( query ) ; } } if ( uniques . size ( ) == 1 ) { return ( Query ) uniques . iterator ( ) . next ( ) ; } Iterator it = uniques . iterator ( ) ; BooleanQuery result = new BooleanQuery ( true ) ; while ( it . hasNext ( ) ) result . add ( ( Query ) it . next ( ) , BooleanClause . Occur . SHOULD ) ; return result ; } public void extractTerms ( Set terms ) { throw new UnsupportedOperationException ( ) ; } public static Query mergeBooleanQueries ( Query [ ] queries ) { HashSet allClauses = new HashSet ( ) ; for ( int i = 0 ; i < queries . length ; i ++ ) { BooleanClause [ ] clauses = ( ( BooleanQuery ) queries [ i ] ) . getClauses ( ) ; for ( int j = 0 ; j < clauses . length ; j ++ ) { allClauses . add ( clauses [ j ] ) ; } } boolean coordDisabled = queries . length == 0 ? false : ( ( BooleanQuery ) queries [ 0 ] ) . isCoordDisabled ( ) ; BooleanQuery result = new BooleanQuery ( coordDisabled ) ; Iterator i = allClauses . iterator ( ) ; while ( i . hasNext ( ) ) { result . add ( ( BooleanClause ) i . next ( ) ) ; } return result ; } public Similarity getSimilarity ( Searcher searcher ) { return searcher . getSimilarity ( ) ; } public Object clone ( ) { try { return ( Query ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { throw new RuntimeException ( "Clone not supported: " + e . getMessage ( ) ) ; } } } 	0	['13', '1', '13', '45', '39', '72', '42', '7', '12', '0.833333333', '249', '1', '0', '0', '0.230769231', '0', '0', '18.07692308', '9', '1.8462', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermEnum ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; public class RangeQuery extends Query { private Term lowerTerm ; private Term upperTerm ; private boolean inclusive ; public RangeQuery ( Term lowerTerm , Term upperTerm , boolean inclusive ) { if ( lowerTerm == null && upperTerm == null ) { throw new IllegalArgumentException ( "At least one term must be non-null" ) ; } if ( lowerTerm != null && upperTerm != null && lowerTerm . field ( ) != upperTerm . field ( ) ) { throw new IllegalArgumentException ( "Both terms must be for the same field" ) ; } if ( lowerTerm != null ) { this . lowerTerm = lowerTerm ; } else { this . lowerTerm = new Term ( upperTerm . field ( ) , "" ) ; } this . upperTerm = upperTerm ; this . inclusive = inclusive ; } public Query rewrite ( IndexReader reader ) throws IOException { BooleanQuery query = new BooleanQuery ( true ) ; TermEnum enumerator = reader . terms ( lowerTerm ) ; try { boolean checkLower = false ; if ( ! inclusive ) checkLower = true ; String testField = getField ( ) ; do { Term term = enumerator . term ( ) ; if ( term != null && term . field ( ) == testField ) { if ( ! checkLower || term . text ( ) . compareTo ( lowerTerm . text ( ) ) > 0 ) { checkLower = false ; if ( upperTerm != null ) { int compare = upperTerm . text ( ) . compareTo ( term . text ( ) ) ; if ( ( compare < 0 ) || ( ! inclusive && compare == 0 ) ) break ; } TermQuery tq = new TermQuery ( term ) ; tq . setBoost ( getBoost ( ) ) ; query . add ( tq , BooleanClause . Occur . SHOULD ) ; } } else { break ; } } while ( enumerator . next ( ) ) ; } finally { enumerator . close ( ) ; } return query ; } public String getField ( ) { return ( lowerTerm != null ? lowerTerm . field ( ) : upperTerm . field ( ) ) ; } public Term getLowerTerm ( ) { return lowerTerm ; } public Term getUpperTerm ( ) { return upperTerm ; } public boolean isInclusive ( ) { return inclusive ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! getField ( ) . equals ( field ) ) { buffer . append ( getField ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( inclusive ? "[" : "{" ) ; buffer . append ( lowerTerm != null ? lowerTerm . text ( ) : "null" ) ; buffer . append ( " TO " ) ; buffer . append ( upperTerm != null ? upperTerm . text ( ) : "null" ) ; buffer . append ( inclusive ? "]" : "}" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof RangeQuery ) ) return false ; final RangeQuery other = ( RangeQuery ) o ; if ( this . getBoost ( ) != other . getBoost ( ) ) return false ; if ( this . inclusive != other . inclusive ) return false ; if ( this . lowerTerm != null ? ! this . lowerTerm . equals ( other . lowerTerm ) : other . lowerTerm != null ) return false ; if ( this . upperTerm != null ? ! this . upperTerm . equals ( other . upperTerm ) : other . upperTerm != null ) return false ; return true ; } public int hashCode ( ) { int h = Float . floatToIntBits ( getBoost ( ) ) ; h ^= lowerTerm != null ? lowerTerm . hashCode ( ) : 0 ; h ^= ( h << 25 ) | ( h > > > 8 ) ; h ^= upperTerm != null ? upperTerm . hashCode ( ) : 0 ; h ^= this . inclusive ? 0x2742E74A : 0 ; return h ; } } 	0	['9', '2', '0', '9', '32', '0', '1', '8', '9', '0.291666667', '340', '1', '2', '0.6', '0.259259259', '2', '3', '36.44444444', '11', '3', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermEnum ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; public class PrefixQuery extends Query { private Term prefix ; public PrefixQuery ( Term prefix ) { this . prefix = prefix ; } public Term getPrefix ( ) { return prefix ; } public Query rewrite ( IndexReader reader ) throws IOException { BooleanQuery query = new BooleanQuery ( true ) ; TermEnum enumerator = reader . terms ( prefix ) ; try { String prefixText = prefix . text ( ) ; String prefixField = prefix . field ( ) ; do { Term term = enumerator . term ( ) ; if ( term != null && term . text ( ) . startsWith ( prefixText ) && term . field ( ) == prefixField ) { TermQuery tq = new TermQuery ( term ) ; tq . setBoost ( getBoost ( ) ) ; query . add ( tq , BooleanClause . Occur . SHOULD ) ; } else { break ; } } while ( enumerator . next ( ) ) ; } finally { enumerator . close ( ) ; } return query ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! prefix . field ( ) . equals ( field ) ) { buffer . append ( prefix . field ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( prefix . text ( ) ) ; buffer . append ( '*' ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof PrefixQuery ) ) return false ; PrefixQuery other = ( PrefixQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && this . prefix . equals ( other . prefix ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ prefix . hashCode ( ) ^ 0x6634D93C ; } } 	0	['6', '2', '0', '9', '28', '0', '1', '8', '6', '0', '147', '1', '1', '0.705882353', '0.333333333', '2', '3', '23.33333333', '4', '1.5', '0']
package org . apache . lucene . analysis . standard ; import java . io . * ; public class StandardTokenizer extends org . apache . lucene . analysis . Tokenizer implements StandardTokenizerConstants { public StandardTokenizer ( Reader reader ) { this ( new FastCharStream ( reader ) ) ; this . input = reader ; } final public org . apache . lucene . analysis . Token next ( ) throws ParseException , IOException { Token token = null ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case ALPHANUM : token = jj_consume_token ( ALPHANUM ) ; break ; case APOSTROPHE : token = jj_consume_token ( APOSTROPHE ) ; break ; case ACRONYM : token = jj_consume_token ( ACRONYM ) ; break ; case COMPANY : token = jj_consume_token ( COMPANY ) ; break ; case EMAIL : token = jj_consume_token ( EMAIL ) ; break ; case HOST : token = jj_consume_token ( HOST ) ; break ; case NUM : token = jj_consume_token ( NUM ) ; break ; case CJ : token = jj_consume_token ( CJ ) ; break ; case 0 : token = jj_consume_token ( 0 ) ; break ; default : jj_la1 [ 0 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } if ( token . kind == EOF ) { { if ( true ) return null ; } } else { { if ( true ) return new org . apache . lucene . analysis . Token ( token . image , token . beginColumn , token . endColumn , tokenImage [ token . kind ] ) ; } } throw new Error ( "Missing return statement in function" ) ; } public StandardTokenizerTokenManager token_source ; public Token token , jj_nt ; private int jj_ntk ; private int jj_gen ; final private int [ ] jj_la1 = new int [ 1 ] ; static private int [ ] jj_la1_0 ; static { jj_la1_0 ( ) ; } private static void jj_la1_0 ( ) { jj_la1_0 = new int [ ] { 0x10ff , } ; } public StandardTokenizer ( CharStream stream ) { token_source = new StandardTokenizerTokenManager ( stream ) ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 1 ; i ++ ) jj_la1 [ i ] = - 1 ; } public void ReInit ( CharStream stream ) { token_source . ReInit ( stream ) ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 1 ; i ++ ) jj_la1 [ i ] = - 1 ; } public StandardTokenizer ( StandardTokenizerTokenManager tm ) { token_source = tm ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 1 ; i ++ ) jj_la1 [ i ] = - 1 ; } public void ReInit ( StandardTokenizerTokenManager tm ) { token_source = tm ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 1 ; i ++ ) jj_la1 [ i ] = - 1 ; } final private Token jj_consume_token ( int kind ) throws ParseException { Token oldToken ; if ( ( oldToken = token ) . next != null ) token = token . next ; else token = token . next = token_source . getNextToken ( ) ; jj_ntk = - 1 ; if ( token . kind == kind ) { jj_gen ++ ; return token ; } token = oldToken ; jj_kind = kind ; throw generateParseException ( ) ; } final public Token getNextToken ( ) { if ( token . next != null ) token = token . next ; else token = token . next = token_source . getNextToken ( ) ; jj_ntk = - 1 ; jj_gen ++ ; return token ; } final public Token getToken ( int index ) { Token t = token ; for ( int i = 0 ; i < index ; i ++ ) { if ( t . next != null ) t = t . next ; else t = t . next = token_source . getNextToken ( ) ; } return t ; } final private int jj_ntk ( ) { if ( ( jj_nt = token . next ) == null ) return ( jj_ntk = ( token . next = token_source . getNextToken ( ) ) . kind ) ; else return ( jj_ntk = jj_nt . kind ) ; } private java . util . Vector jj_expentries = new java . util . Vector ( ) ; private int [ ] jj_expentry ; private int jj_kind = - 1 ; public ParseException generateParseException ( ) { jj_expentries . removeAllElements ( ) ; boolean [ ] la1tokens = new boolean [ 16 ] ; for ( int i = 0 ; i < 16 ; i ++ ) { la1tokens [ i ] = false ; } if ( jj_kind >= 0 ) { la1tokens [ jj_kind ] = true ; jj_kind = - 1 ; } for ( int i = 0 ; i < 1 ; i ++ ) { if ( jj_la1 [ i ] == jj_gen ) { for ( int j = 0 ; j < 32 ; j ++ ) { if ( ( jj_la1_0 [ i ] & ( 1 << j ) ) != 0 ) { la1tokens [ j ] = true ; } } } } for ( int i = 0 ; i < 16 ; i ++ ) { if ( la1tokens [ i ] ) { jj_expentry = new int [ 1 ] ; jj_expentry [ 0 ] = i ; jj_expentries . addElement ( jj_expentry ) ; } } int [ ] [ ] exptokseq = new int [ jj_expentries . size ( ) ] [ ] ; for ( int i = 0 ; i < jj_expentries . size ( ) ; i ++ ) { exptokseq [ i ] = ( int [ ] ) jj_expentries . elementAt ( i ) ; } return new ParseException ( token , exptokseq , tokenImage ) ; } final public void enable_tracing ( ) { } final public void disable_tracing ( ) { } } 	0	['15', '3', '0', '9', '29', '15', '1', '8', '11', '0.6', '523', '0.7', '3', '0.214285714', '0.285714286', '1', '1', '33.2', '10', '1.7333', '0']
package org . apache . lucene . search ; public class TopDocs implements java . io . Serializable { public int totalHits ; public ScoreDoc [ ] scoreDocs ; private float maxScore ; public float getMaxScore ( ) { return maxScore ; } public void setMaxScore ( float maxScore ) { this . maxScore = maxScore ; } TopDocs ( int totalHits , ScoreDoc [ ] scoreDocs , float maxScore ) { this . totalHits = totalHits ; this . scoreDocs = scoreDocs ; this . maxScore = maxScore ; } } 	0	['3', '1', '1', '14', '4', '0', '13', '1', '2', '0.666666667', '25', '0.333333333', '1', '0', '0.583333333', '0', '0', '6.333333333', '1', '0.6667', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; import java . io . IOException ; public abstract class Tokenizer extends TokenStream { protected Reader input ; protected Tokenizer ( ) { } protected Tokenizer ( Reader input ) { this . input = input ; } public void close ( ) throws IOException { input . close ( ) ; } } 	0	['3', '2', '3', '4', '5', '1', '3', '1', '1', '0.5', '17', '1', '0', '0.666666667', '0.666666667', '0', '0', '4.333333333', '1', '0.3333', '0']
package org . apache . lucene ; public final class LucenePackage { private LucenePackage ( ) { } public static Package get ( ) { return LucenePackage . class . getPackage ( ) ; } } 	0	['3', '1', '0', '0', '8', '3', '0', '0', '1', '1', '27', '0', '0', '0', '0.333333333', '0', '0', '7.666666667', '2', '1', '0']
package org . apache . lucene . util ; import java . io . ObjectStreamException ; import java . io . Serializable ; import java . io . StreamCorruptedException ; import java . util . HashMap ; import java . util . Map ; public abstract class Parameter implements Serializable { static Map allParameters = new HashMap ( ) ; private String name ; private Parameter ( ) { } protected Parameter ( String name ) { this . name = name ; String key = makeKey ( name ) ; if ( allParameters . containsKey ( key ) ) throw new IllegalArgumentException ( "Parameter name " + key + " already used!" ) ; allParameters . put ( key , this ) ; } private String makeKey ( String name ) { return getClass ( ) + " " + name ; } public String toString ( ) { return name ; } protected Object readResolve ( ) throws ObjectStreamException { Object par = allParameters . get ( makeKey ( name ) ) ; if ( par == null ) throw new StreamCorruptedException ( "Unknown parameter value: " + name ) ; return par ; } } 	0	['6', '1', '5', '5', '18', '5', '5', '0', '1', '0.6', '88', '0.5', '0', '0', '0.7', '0', '0', '13.33333333', '1', '0.5', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Query ; import org . apache . lucene . search . Weight ; import org . apache . lucene . search . Searcher ; public abstract class SpanQuery extends Query { public abstract Spans getSpans ( IndexReader reader ) throws IOException ; public abstract String getField ( ) ; public abstract Collection getTerms ( ) ; protected Weight createWeight ( Searcher searcher ) throws IOException { return new SpanWeight ( this , searcher ) ; } } 	0	['5', '2', '5', '15', '7', '10', '10', '6', '4', '2', '14', '0', '0', '0.75', '0.466666667', '1', '1', '1.8', '1', '0.8', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; public final class SimpleAnalyzer extends Analyzer { public TokenStream tokenStream ( String fieldName , Reader reader ) { return new LowerCaseTokenizer ( reader ) ; } } 	0	['2', '2', '0', '4', '4', '1', '1', '3', '2', '2', '10', '0', '0', '0.666666667', '0.666666667', '0', '0', '4', '1', '0.5', '0']
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; import java . text . Collator ; import java . util . Locale ; class FieldDocSortedHitQueue extends PriorityQueue { volatile SortField [ ] fields ; volatile Collator [ ] collators ; FieldDocSortedHitQueue ( SortField [ ] fields , int size ) { this . fields = fields ; this . collators = hasCollators ( fields ) ; initialize ( size ) ; } synchronized void setFields ( SortField [ ] fields ) { if ( this . fields == null ) { this . fields = fields ; this . collators = hasCollators ( fields ) ; } } SortField [ ] getFields ( ) { return fields ; } private Collator [ ] hasCollators ( final SortField [ ] fields ) { if ( fields == null ) return null ; Collator [ ] ret = new Collator [ fields . length ] ; for ( int i = 0 ; i < fields . length ; ++ i ) { Locale locale = fields [ i ] . getLocale ( ) ; if ( locale != null ) ret [ i ] = Collator . getInstance ( locale ) ; } return ret ; } protected final boolean lessThan ( final Object a , final Object b ) { final FieldDoc docA = ( FieldDoc ) a ; final FieldDoc docB = ( FieldDoc ) b ; final int n = fields . length ; int c = 0 ; for ( int i = 0 ; i < n && c == 0 ; ++ i ) { final int type = fields [ i ] . getType ( ) ; switch ( type ) { case SortField . SCORE : float r1 = ( ( Float ) docA . fields [ i ] ) . floatValue ( ) ; float r2 = ( ( Float ) docB . fields [ i ] ) . floatValue ( ) ; if ( r1 > r2 ) c = - 1 ; if ( r1 < r2 ) c = 1 ; break ; case SortField . DOC : case SortField . INT : int i1 = ( ( Integer ) docA . fields [ i ] ) . intValue ( ) ; int i2 = ( ( Integer ) docB . fields [ i ] ) . intValue ( ) ; if ( i1 < i2 ) c = - 1 ; if ( i1 > i2 ) c = 1 ; break ; case SortField . STRING : String s1 = ( String ) docA . fields [ i ] ; String s2 = ( String ) docB . fields [ i ] ; if ( s1 == null ) c = ( s2 == null ) ? 0 : - 1 ; else if ( s2 == null ) c = 1 ; else if ( fields [ i ] . getLocale ( ) == null ) { c = s1 . compareTo ( s2 ) ; } else { c = collators [ i ] . compare ( s1 , s2 ) ; } break ; case SortField . FLOAT : float f1 = ( ( Float ) docA . fields [ i ] ) . floatValue ( ) ; float f2 = ( ( Float ) docB . fields [ i ] ) . floatValue ( ) ; if ( f1 < f2 ) c = - 1 ; if ( f1 > f2 ) c = 1 ; break ; case SortField . CUSTOM : c = docA . fields [ i ] . compareTo ( docB . fields [ i ] ) ; break ; case SortField . AUTO : throw new RuntimeException ( "FieldDocSortedHitQueue cannot use an AUTO SortField" ) ; default : throw new RuntimeException ( "invalid SortField type: " + type ) ; } if ( fields [ i ] . getReverse ( ) ) { c = - c ; } } if ( c == 0 ) return docA . doc > docB . doc ; return c > 0 ; } } 	0	['5', '2', '0', '6', '21', '0', '3', '3', '0', '0.375', '274', '0', '1', '0.733333333', '0.5', '1', '3', '53.4', '18', '5', '0']
package org . apache . lucene . search ; public abstract class HitCollector { public abstract void collect ( int doc , float score ) ; } 	0	['2', '1', '6', '21', '3', '1', '21', '0', '2', '2', '5', '0', '0', '0', '0.666666667', '0', '0', '1.5', '1', '0.5', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; public class WhitespaceTokenizer extends CharTokenizer { public WhitespaceTokenizer ( Reader in ) { super ( in ) ; } protected boolean isTokenChar ( char c ) { return ! Character . isWhitespace ( c ) ; } } 	0	['2', '4', '0', '2', '4', '1', '1', '1', '1', '2', '13', '0', '0', '0.857142857', '0.666666667', '1', '1', '5.5', '2', '1', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Query ; import org . apache . lucene . util . ToStringUtils ; public class SpanNotQuery extends SpanQuery { private SpanQuery include ; private SpanQuery exclude ; public SpanNotQuery ( SpanQuery include , SpanQuery exclude ) { this . include = include ; this . exclude = exclude ; if ( ! include . getField ( ) . equals ( exclude . getField ( ) ) ) throw new IllegalArgumentException ( "Clauses must have same field." ) ; } public SpanQuery getInclude ( ) { return include ; } public SpanQuery getExclude ( ) { return exclude ; } public String getField ( ) { return include . getField ( ) ; } public Collection getTerms ( ) { return include . getTerms ( ) ; } public void extractTerms ( Set terms ) { include . extractTerms ( terms ) ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "spanNot(" ) ; buffer . append ( include . toString ( field ) ) ; buffer . append ( ", " ) ; buffer . append ( exclude . toString ( field ) ) ; buffer . append ( ")" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public Spans getSpans ( final IndexReader reader ) throws IOException { return new Spans ( ) { private Spans includeSpans = include . getSpans ( reader ) ; private boolean moreInclude = true ; private Spans excludeSpans = exclude . getSpans ( reader ) ; private boolean moreExclude = excludeSpans . next ( ) ; public boolean next ( ) throws IOException { if ( moreInclude ) moreInclude = includeSpans . next ( ) ; while ( moreInclude && moreExclude ) { if ( includeSpans . doc ( ) > excludeSpans . doc ( ) ) moreExclude = excludeSpans . skipTo ( includeSpans . doc ( ) ) ; while ( moreExclude && includeSpans . doc ( ) == excludeSpans . doc ( ) && excludeSpans . end ( ) <= includeSpans . start ( ) ) { moreExclude = excludeSpans . next ( ) ; } if ( ! moreExclude || includeSpans . doc ( ) != excludeSpans . doc ( ) || includeSpans . end ( ) <= excludeSpans . start ( ) ) break ; moreInclude = includeSpans . next ( ) ; } return moreInclude ; } public boolean skipTo ( int target ) throws IOException { if ( moreInclude ) moreInclude = includeSpans . skipTo ( target ) ; if ( ! moreInclude ) return false ; if ( moreExclude && includeSpans . doc ( ) > excludeSpans . doc ( ) ) moreExclude = excludeSpans . skipTo ( includeSpans . doc ( ) ) ; while ( moreExclude && includeSpans . doc ( ) == excludeSpans . doc ( ) && excludeSpans . end ( ) <= includeSpans . start ( ) ) { moreExclude = excludeSpans . next ( ) ; } if ( ! moreExclude || includeSpans . doc ( ) != excludeSpans . doc ( ) || includeSpans . end ( ) <= excludeSpans . start ( ) ) return true ; return next ( ) ; } public int doc ( ) { return includeSpans . doc ( ) ; } public int start ( ) { return includeSpans . start ( ) ; } public int end ( ) { return includeSpans . end ( ) ; } public String toString ( ) { return "spans(" + SpanNotQuery . this . toString ( ) + ")" ; } } ; } public Query rewrite ( IndexReader reader ) throws IOException { SpanNotQuery clone = null ; SpanQuery rewrittenInclude = ( SpanQuery ) include . rewrite ( reader ) ; if ( rewrittenInclude != include ) { clone = ( SpanNotQuery ) this . clone ( ) ; clone . include = rewrittenInclude ; } SpanQuery rewrittenExclude = ( SpanQuery ) exclude . rewrite ( reader ) ; if ( rewrittenExclude != exclude ) { if ( clone == null ) clone = ( SpanNotQuery ) this . clone ( ) ; clone . exclude = rewrittenExclude ; } if ( clone != null ) { return clone ; } else { return this ; } } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof SpanNotQuery ) ) return false ; SpanNotQuery other = ( SpanNotQuery ) o ; return this . include . equals ( other . include ) && this . exclude . equals ( other . exclude ) && this . getBoost ( ) == other . getBoost ( ) ; } public int hashCode ( ) { int h = include . hashCode ( ) ; h = ( h << 1 ) | ( h > > > 31 ) ; h ^= exclude . hashCode ( ) ; h = ( h << 1 ) | ( h > > > 31 ) ; h ^= Float . floatToRawIntBits ( getBoost ( ) ) ; return h ; } } 	0	['13', '3', '0', '6', '31', '0', '1', '6', '11', '0.375', '218', '1', '2', '0.571428571', '0.208791209', '2', '2', '15.61538462', '6', '1.3077', '0']
package org . apache . lucene . analysis . standard ; import org . apache . lucene . analysis . * ; public final class StandardFilter extends TokenFilter implements StandardTokenizerConstants { public StandardFilter ( TokenStream in ) { super ( in ) ; } private static final String APOSTROPHE_TYPE = tokenImage [ APOSTROPHE ] ; private static final String ACRONYM_TYPE = tokenImage [ ACRONYM ] ; public final org . apache . lucene . analysis . Token next ( ) throws java . io . IOException { org . apache . lucene . analysis . Token t = input . next ( ) ; if ( t == null ) return null ; String text = t . termText ( ) ; String type = t . type ( ) ; if ( type == APOSTROPHE_TYPE && ( text . endsWith ( "'s" ) || text . endsWith ( "'S" ) ) ) { return new org . apache . lucene . analysis . Token ( text . substring ( 0 , text . length ( ) - 2 ) , t . startOffset ( ) , t . endOffset ( ) , type ) ; } else if ( type == ACRONYM_TYPE ) { StringBuffer trimmed = new StringBuffer ( ) ; for ( int i = 0 ; i < text . length ( ) ; i ++ ) { char c = text . charAt ( i ) ; if ( c != '.' ) trimmed . append ( c ) ; } return new org . apache . lucene . analysis . Token ( trimmed . toString ( ) , t . startOffset ( ) , t . endOffset ( ) , type ) ; } else { return t ; } } } 	0	['3', '3', '0', '5', '17', '1', '1', '4', '2', '0.5', '98', '1', '0', '0.75', '0.75', '0', '0', '31', '1', '0.3333', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; import java . io . Reader ; public abstract class CharTokenizer extends Tokenizer { public CharTokenizer ( Reader input ) { super ( input ) ; } private int offset = 0 , bufferIndex = 0 , dataLen = 0 ; private static final int MAX_WORD_LEN = 255 ; private static final int IO_BUFFER_SIZE = 1024 ; private final char [ ] buffer = new char [ MAX_WORD_LEN ] ; private final char [ ] ioBuffer = new char [ IO_BUFFER_SIZE ] ; protected abstract boolean isTokenChar ( char c ) ; protected char normalize ( char c ) { return c ; } public final Token next ( ) throws IOException { int length = 0 ; int start = offset ; while ( true ) { final char c ; offset ++ ; if ( bufferIndex >= dataLen ) { dataLen = input . read ( ioBuffer ) ; bufferIndex = 0 ; } ; if ( dataLen == - 1 ) { if ( length > 0 ) break ; else return null ; } else c = ioBuffer [ bufferIndex ++ ] ; if ( isTokenChar ( c ) ) { if ( length == 0 ) start = offset - 1 ; buffer [ length ++ ] = normalize ( c ) ; if ( length == MAX_WORD_LEN ) break ; } else if ( length > 0 ) break ; } return new Token ( new String ( buffer , 0 , length ) , start , start + length ) ; } } 	0	['4', '3', '2', '4', '8', '4', '2', '2', '2', '0.857142857', '122', '1', '0', '0.5', '0.583333333', '0', '0', '27.75', '1', '0.75', '0']
package org . apache . lucene . document ; import java . text . ParseException ; import java . text . SimpleDateFormat ; import java . util . Calendar ; import java . util . Date ; import java . util . TimeZone ; public class DateTools { private final static TimeZone GMT = TimeZone . getTimeZone ( "GMT" ) ; private DateTools ( ) { } public static String dateToString ( Date date , Resolution resolution ) { return timeToString ( date . getTime ( ) , resolution ) ; } public static String timeToString ( long time , Resolution resolution ) { Calendar cal = Calendar . getInstance ( GMT ) ; cal . setTime ( new Date ( round ( time , resolution ) ) ) ; SimpleDateFormat sdf = new SimpleDateFormat ( ) ; sdf . setTimeZone ( GMT ) ; String pattern = null ; if ( resolution == Resolution . YEAR ) { pattern = "yyyy" ; } else if ( resolution == Resolution . MONTH ) { pattern = "yyyyMM" ; } else if ( resolution == Resolution . DAY ) { pattern = "yyyyMMdd" ; } else if ( resolution == Resolution . HOUR ) { pattern = "yyyyMMddHH" ; } else if ( resolution == Resolution . MINUTE ) { pattern = "yyyyMMddHHmm" ; } else if ( resolution == Resolution . SECOND ) { pattern = "yyyyMMddHHmmss" ; } else if ( resolution == Resolution . MILLISECOND ) { pattern = "yyyyMMddHHmmssSSS" ; } else { throw new IllegalArgumentException ( "unknown resolution " + resolution ) ; } sdf . applyPattern ( pattern ) ; return sdf . format ( cal . getTime ( ) ) ; } public static long stringToTime ( String dateString ) throws ParseException { return stringToDate ( dateString ) . getTime ( ) ; } public static Date stringToDate ( String dateString ) throws ParseException { String pattern = null ; if ( dateString . length ( ) == 4 ) pattern = "yyyy" ; else if ( dateString . length ( ) == 6 ) pattern = "yyyyMM" ; else if ( dateString . length ( ) == 8 ) pattern = "yyyyMMdd" ; else if ( dateString . length ( ) == 10 ) pattern = "yyyyMMddHH" ; else if ( dateString . length ( ) == 12 ) pattern = "yyyyMMddHHmm" ; else if ( dateString . length ( ) == 14 ) pattern = "yyyyMMddHHmmss" ; else if ( dateString . length ( ) == 17 ) pattern = "yyyyMMddHHmmssSSS" ; else throw new ParseException ( "Input is not valid date string: " + dateString , 0 ) ; SimpleDateFormat sdf = new SimpleDateFormat ( pattern ) ; sdf . setTimeZone ( GMT ) ; Date date = sdf . parse ( dateString ) ; return date ; } public static Date round ( Date date , Resolution resolution ) { return new Date ( round ( date . getTime ( ) , resolution ) ) ; } public static long round ( long time , Resolution resolution ) { Calendar cal = Calendar . getInstance ( GMT ) ; cal . setTime ( new Date ( time ) ) ; if ( resolution == Resolution . YEAR ) { cal . set ( Calendar . MONTH , 0 ) ; cal . set ( Calendar . DAY_OF_MONTH , 1 ) ; cal . set ( Calendar . HOUR_OF_DAY , 0 ) ; cal . set ( Calendar . MINUTE , 0 ) ; cal . set ( Calendar . SECOND , 0 ) ; cal . set ( Calendar . MILLISECOND , 0 ) ; } else if ( resolution == Resolution . MONTH ) { cal . set ( Calendar . DAY_OF_MONTH , 1 ) ; cal . set ( Calendar . HOUR_OF_DAY , 0 ) ; cal . set ( Calendar . MINUTE , 0 ) ; cal . set ( Calendar . SECOND , 0 ) ; cal . set ( Calendar . MILLISECOND , 0 ) ; } else if ( resolution == Resolution . DAY ) { cal . set ( Calendar . HOUR_OF_DAY , 0 ) ; cal . set ( Calendar . MINUTE , 0 ) ; cal . set ( Calendar . SECOND , 0 ) ; cal . set ( Calendar . MILLISECOND , 0 ) ; } else if ( resolution == Resolution . HOUR ) { cal . set ( Calendar . MINUTE , 0 ) ; cal . set ( Calendar . SECOND , 0 ) ; cal . set ( Calendar . MILLISECOND , 0 ) ; } else if ( resolution == Resolution . MINUTE ) { cal . set ( Calendar . SECOND , 0 ) ; cal . set ( Calendar . MILLISECOND , 0 ) ; } else if ( resolution == Resolution . SECOND ) { cal . set ( Calendar . MILLISECOND , 0 ) ; } else if ( resolution == Resolution . MILLISECOND ) { } else { throw new IllegalArgumentException ( "unknown resolution " + resolution ) ; } return cal . getTime ( ) . getTime ( ) ; } public static class Resolution { public static final Resolution YEAR = new Resolution ( "year" ) ; public static final Resolution MONTH = new Resolution ( "month" ) ; public static final Resolution DAY = new Resolution ( "day" ) ; public static final Resolution HOUR = new Resolution ( "hour" ) ; public static final Resolution MINUTE = new Resolution ( "minute" ) ; public static final Resolution SECOND = new Resolution ( "second" ) ; public static final Resolution MILLISECOND = new Resolution ( "millisecond" ) ; private String resolution ; private Resolution ( ) { } private Resolution ( String resolution ) { this . resolution = resolution ; } public String toString ( ) { return resolution ; } } } 	0	['8', '1', '0', '1', '29', '16', '0', '1', '6', '0.142857143', '330', '1', '0', '0', '0.314285714', '0', '0', '40.125', '8', '2.5', '0']
package org . apache . lucene . search ; public interface ScoreDocComparator { static final ScoreDocComparator RELEVANCE = new ScoreDocComparator ( ) { public int compare ( ScoreDoc i , ScoreDoc j ) { if ( i . score > j . score ) return - 1 ; if ( i . score < j . score ) return 1 ; return 0 ; } public Comparable sortValue ( ScoreDoc i ) { return new Float ( i . score ) ; } public int sortType ( ) { return SortField . SCORE ; } } ; static final ScoreDocComparator INDEXORDER = new ScoreDocComparator ( ) { public int compare ( ScoreDoc i , ScoreDoc j ) { if ( i . doc < j . doc ) return - 1 ; if ( i . doc > j . doc ) return 1 ; return 0 ; } public Comparable sortValue ( ScoreDoc i ) { return new Integer ( i . doc ) ; } public int sortType ( ) { return SortField . DOC ; } } ; int compare ( ScoreDoc i , ScoreDoc j ) ; Comparable sortValue ( ScoreDoc i ) ; int sortType ( ) ; } 	0	['4', '1', '0', '11', '6', '6', '10', '3', '3', '1', '15', '0', '2', '0', '0.833333333', '0', '0', '2.25', '1', '0.75', '0']
package org . apache . lucene . index ; import java . util . * ; class SegmentTermVector implements TermFreqVector { private String field ; private String terms [ ] ; private int termFreqs [ ] ; SegmentTermVector ( String field , String terms [ ] , int termFreqs [ ] ) { this . field = field ; this . terms = terms ; this . termFreqs = termFreqs ; } public String getField ( ) { return field ; } public String toString ( ) { StringBuffer sb = new StringBuffer ( ) ; sb . append ( '{' ) ; sb . append ( field ) . append ( ": " ) ; if ( terms != null ) { for ( int i = 0 ; i < terms . length ; i ++ ) { if ( i > 0 ) sb . append ( ", " ) ; sb . append ( terms [ i ] ) . append ( '/' ) . append ( termFreqs [ i ] ) ; } } sb . append ( '}' ) ; return sb . toString ( ) ; } public int size ( ) { return terms == null ? 0 : terms . length ; } public String [ ] getTerms ( ) { return terms ; } public int [ ] getTermFrequencies ( ) { return termFreqs ; } public int indexOf ( String termText ) { if ( terms == null ) return - 1 ; int res = Arrays . binarySearch ( terms , termText ) ; return res >= 0 ? res : - 1 ; } public int [ ] indexesOf ( String [ ] termNumbers , int start , int len ) { int res [ ] = new int [ len ] ; for ( int i = 0 ; i < len ; i ++ ) { res [ i ] = indexOf ( termNumbers [ start + i ] ) ; } return res ; } } 	0	['8', '1', '1', '3', '15', '0', '2', '1', '7', '0.571428571', '133', '1', '0', '0', '0.35', '0', '0', '15.25', '4', '1.75', '0']
package org . apache . lucene . document ; import org . apache . lucene . search . PrefixQuery ; import org . apache . lucene . search . RangeQuery ; import java . util . Date ; public class DateField { private DateField ( ) { } private static int DATE_LEN = Long . toString ( 1000L * 365 * 24 * 60 * 60 * 1000 , Character . MAX_RADIX ) . length ( ) ; public static String MIN_DATE_STRING ( ) { return timeToString ( 0 ) ; } public static String MAX_DATE_STRING ( ) { char [ ] buffer = new char [ DATE_LEN ] ; char c = Character . forDigit ( Character . MAX_RADIX - 1 , Character . MAX_RADIX ) ; for ( int i = 0 ; i < DATE_LEN ; i ++ ) buffer [ i ] = c ; return new String ( buffer ) ; } public static String dateToString ( Date date ) { return timeToString ( date . getTime ( ) ) ; } public static String timeToString ( long time ) { if ( time < 0 ) throw new RuntimeException ( "time '" + time + "' is too early, must be >= 0" ) ; String s = Long . toString ( time , Character . MAX_RADIX ) ; if ( s . length ( ) > DATE_LEN ) throw new RuntimeException ( "time '" + time + "' is too late, length of string " + "representation must be <= " + DATE_LEN ) ; if ( s . length ( ) < DATE_LEN ) { StringBuffer sb = new StringBuffer ( s ) ; while ( sb . length ( ) < DATE_LEN ) sb . insert ( 0 , 0 ) ; s = sb . toString ( ) ; } return s ; } public static long stringToTime ( String s ) { return Long . parseLong ( s , Character . MAX_RADIX ) ; } public static Date stringToDate ( String s ) { return new Date ( stringToTime ( s ) ) ; } } 	0	['8', '1', '0', '1', '25', '22', '1', '0', '6', '0.428571429', '126', '1', '0', '0', '0.178571429', '0', '0', '14.625', '5', '1.375', '0']
package org . apache . lucene . search ; public class FieldDoc extends ScoreDoc { public Comparable [ ] fields ; public FieldDoc ( int doc , float score ) { super ( doc , score ) ; } public FieldDoc ( int doc , float score , Comparable [ ] fields ) { super ( doc , score ) ; this . fields = fields ; } } 	0	['2', '2', '0', '4', '3', '1', '3', '1', '2', '1', '16', '0', '0', '0', '0.875', '0', '0', '6.5', '0', '0', '0']
package org . apache . lucene . queryParser ; public class TokenMgrError extends Error { static final int LEXICAL_ERROR = 0 ; static final int STATIC_LEXER_ERROR = 1 ; static final int INVALID_LEXICAL_STATE = 2 ; static final int LOOP_DETECTED = 3 ; int errorCode ; protected static final String addEscapes ( String str ) { StringBuffer retval = new StringBuffer ( ) ; char ch ; for ( int i = 0 ; i < str . length ( ) ; i ++ ) { switch ( str . charAt ( i ) ) { case 0 : continue ; case '\b' : retval . append ( "\\b" ) ; continue ; case '\t' : retval . append ( "\\t" ) ; continue ; case '\n' : retval . append ( "\\n" ) ; continue ; case '\f' : retval . append ( "\\f" ) ; continue ; case '\r' : retval . append ( "\\r" ) ; continue ; case '\"' : retval . append ( "\\\"" ) ; continue ; case '\'' : retval . append ( "\\\'" ) ; continue ; case '\\' : retval . append ( "\\\\" ) ; continue ; default : if ( ( ch = str . charAt ( i ) ) < 0x20 || ch > 0x7e ) { String s = "0000" + Integer . toString ( ch , 16 ) ; retval . append ( "\\u" + s . substring ( s . length ( ) - 4 , s . length ( ) ) ) ; } else { retval . append ( ch ) ; } continue ; } } return retval . toString ( ) ; } protected static String LexicalError ( boolean EOFSeen , int lexState , int errorLine , int errorColumn , String errorAfter , char curChar ) { return ( "Lexical error at line " + errorLine + ", column " + errorColumn + ".  Encountered: " + ( EOFSeen ? "<EOF> " : ( "\"" + addEscapes ( String . valueOf ( curChar ) ) + "\"" ) + " (" + ( int ) curChar + "), " ) + "after : \"" + addEscapes ( errorAfter ) + "\"" ) ; } public String getMessage ( ) { return super . getMessage ( ) ; } public TokenMgrError ( ) { } public TokenMgrError ( String message , int reason ) { super ( message ) ; errorCode = reason ; } public TokenMgrError ( boolean EOFSeen , int lexState , int errorLine , int errorColumn , String errorAfter , char curChar , int reason ) { this ( LexicalError ( EOFSeen , lexState , errorLine , errorColumn , errorAfter , curChar ) , reason ) ; } } 	0	['6', '3', '0', '2', '19', '15', '2', '0', '4', '1.12', '184', '0', '0', '0.8125', '0.5', '1', '1', '28.83333333', '14', '2.8333', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; import java . io . Serializable ; public interface SortComparatorSource extends Serializable { ScoreDocComparator newComparator ( IndexReader reader , String fieldname ) throws IOException ; } 	0	['1', '1', '0', '5', '1', '0', '3', '2', '1', '2', '1', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search ; import java . io . IOException ; public class ReqOptSumScorer extends Scorer { private Scorer reqScorer ; private Scorer optScorer ; public ReqOptSumScorer ( Scorer reqScorer , Scorer optScorer ) { super ( null ) ; this . reqScorer = reqScorer ; this . optScorer = optScorer ; } private boolean firstTimeOptScorer = true ; public boolean next ( ) throws IOException { return reqScorer . next ( ) ; } public boolean skipTo ( int target ) throws IOException { return reqScorer . skipTo ( target ) ; } public int doc ( ) { return reqScorer . doc ( ) ; } public float score ( ) throws IOException { int curDoc = reqScorer . doc ( ) ; float reqScore = reqScorer . score ( ) ; if ( firstTimeOptScorer ) { firstTimeOptScorer = false ; if ( ! optScorer . skipTo ( curDoc ) ) { optScorer = null ; return reqScore ; } } else if ( optScorer == null ) { return reqScore ; } else if ( ( optScorer . doc ( ) < curDoc ) && ( ! optScorer . skipTo ( curDoc ) ) ) { optScorer = null ; return reqScore ; } return ( optScorer . doc ( ) == curDoc ) ? reqScore + optScorer . score ( ) : reqScore ; } public Explanation explain ( int doc ) throws IOException { Explanation res = new Explanation ( ) ; res . setDescription ( "required, optional" ) ; res . addDetail ( reqScorer . explain ( doc ) ) ; res . addDetail ( optScorer . explain ( doc ) ) ; return res ; } } 	0	['6', '2', '0', '4', '15', '0', '1', '3', '6', '0.466666667', '113', '1', '2', '0.615384615', '0.5', '1', '3', '17.33333333', '1', '0.8333', '0']
package org . apache . lucene . document ; public class NumberTools { private static final int RADIX = 36 ; private static final char NEGATIVE_PREFIX = '-' ; private static final char POSITIVE_PREFIX = '0' ; public static final String MIN_STRING_VALUE = NEGATIVE_PREFIX + "0000000000000" ; public static final String MAX_STRING_VALUE = POSITIVE_PREFIX + "1y2p0ij32e8e7" ; public static final int STR_SIZE = MIN_STRING_VALUE . length ( ) ; public static String longToString ( long l ) { if ( l == Long . MIN_VALUE ) { return MIN_STRING_VALUE ; } StringBuffer buf = new StringBuffer ( STR_SIZE ) ; if ( l < 0 ) { buf . append ( NEGATIVE_PREFIX ) ; l = Long . MAX_VALUE + l + 1 ; } else { buf . append ( POSITIVE_PREFIX ) ; } String num = Long . toString ( l , RADIX ) ; int padLen = STR_SIZE - num . length ( ) - buf . length ( ) ; while ( padLen -- > 0 ) { buf . append ( '0' ) ; } buf . append ( num ) ; return buf . toString ( ) ; } public static long stringToLong ( String str ) { if ( str == null ) { throw new NullPointerException ( "string cannot be null" ) ; } if ( str . length ( ) != STR_SIZE ) { throw new NumberFormatException ( "string is the wrong size" ) ; } if ( str . equals ( MIN_STRING_VALUE ) ) { return Long . MIN_VALUE ; } char prefix = str . charAt ( 0 ) ; long l = Long . parseLong ( str . substring ( 1 ) , RADIX ) ; if ( prefix == POSITIVE_PREFIX ) { } else if ( prefix == NEGATIVE_PREFIX ) { l = l - Long . MAX_VALUE - 1 ; } else { throw new NumberFormatException ( "string does not begin with the correct prefix" ) ; } return l ; } } 	0	['4', '1', '0', '0', '18', '0', '0', '0', '3', '1.166666667', '130', '0.5', '0', '0', '0.333333333', '0', '0', '30', '6', '2.5', '0']
package org . apache . lucene . search ; public class SimilarityDelegator extends Similarity { private Similarity delegee ; public SimilarityDelegator ( Similarity delegee ) { this . delegee = delegee ; } public float lengthNorm ( String fieldName , int numTerms ) { return delegee . lengthNorm ( fieldName , numTerms ) ; } public float queryNorm ( float sumOfSquaredWeights ) { return delegee . queryNorm ( sumOfSquaredWeights ) ; } public float tf ( float freq ) { return delegee . tf ( freq ) ; } public float sloppyFreq ( int distance ) { return delegee . sloppyFreq ( distance ) ; } public float idf ( int docFreq , int numDocs ) { return delegee . idf ( docFreq , numDocs ) ; } public float coord ( int overlap , int maxOverlap ) { return delegee . coord ( overlap , maxOverlap ) ; } } 	0	['7', '2', '1', '2', '14', '0', '1', '1', '7', '0', '47', '1', '1', '0.7', '0.428571429', '1', '2', '5.571428571', '1', '0.8571', '0']
package org . apache . lucene . analysis . standard ; public class Token { public int kind ; public int beginLine , beginColumn , endLine , endColumn ; public String image ; public Token next ; public Token specialToken ; public String toString ( ) { return image ; } public static final Token newToken ( int ofKind ) { switch ( ofKind ) { default : return new Token ( ) ; } } } 	0	['3', '1', '0', '3', '4', '3', '3', '0', '3', '1.4375', '23', '0', '2', '0', '0.5', '0', '0', '4', '2', '1', '0']
package org . apache . lucene . index ; public interface TermPositionVector extends TermFreqVector { public int [ ] getTermPositions ( int index ) ; public TermVectorOffsetInfo [ ] getOffsets ( int index ) ; } 	0	['2', '1', '0', '4', '2', '1', '2', '2', '2', '2', '2', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; import java . util . HashSet ; import java . util . Set ; public final class StopFilter extends TokenFilter { private final Set stopWords ; private final boolean ignoreCase ; public StopFilter ( TokenStream input , String [ ] stopWords ) { this ( input , stopWords , false ) ; } public StopFilter ( TokenStream in , String [ ] stopWords , boolean ignoreCase ) { super ( in ) ; this . ignoreCase = ignoreCase ; this . stopWords = makeStopSet ( stopWords , ignoreCase ) ; } public StopFilter ( TokenStream input , Set stopWords , boolean ignoreCase ) { super ( input ) ; this . ignoreCase = ignoreCase ; this . stopWords = stopWords ; } public StopFilter ( TokenStream in , Set stopWords ) { this ( in , stopWords , false ) ; } public static final Set makeStopSet ( String [ ] stopWords ) { return makeStopSet ( stopWords , false ) ; } public static final Set makeStopSet ( String [ ] stopWords , boolean ignoreCase ) { HashSet stopTable = new HashSet ( stopWords . length ) ; for ( int i = 0 ; i < stopWords . length ; i ++ ) stopTable . add ( ignoreCase ? stopWords [ i ] . toLowerCase ( ) : stopWords [ i ] ) ; return stopTable ; } public final Token next ( ) throws IOException { for ( Token token = input . next ( ) ; token != null ; token = input . next ( ) ) { String termText = ignoreCase ? token . termText . toLowerCase ( ) : token . termText ; if ( ! stopWords . contains ( termText ) ) return token ; } return null ; } } 	0	['7', '3', '0', '5', '13', '15', '2', '3', '7', '0.333333333', '106', '1', '0', '0.5', '0.514285714', '0', '0', '13.85714286', '3', '0.7143', '0']
package org . apache . lucene . analysis . standard ; import org . apache . lucene . analysis . * ; import java . io . File ; import java . io . IOException ; import java . io . Reader ; import java . util . Set ; public class StandardAnalyzer extends Analyzer { private Set stopSet ; public static final String [ ] STOP_WORDS = StopAnalyzer . ENGLISH_STOP_WORDS ; public StandardAnalyzer ( ) { this ( STOP_WORDS ) ; } public StandardAnalyzer ( Set stopWords ) { stopSet = stopWords ; } public StandardAnalyzer ( String [ ] stopWords ) { stopSet = StopFilter . makeStopSet ( stopWords ) ; } public StandardAnalyzer ( File stopwords ) throws IOException { stopSet = WordlistLoader . getWordSet ( stopwords ) ; } public StandardAnalyzer ( Reader stopwords ) throws IOException { stopSet = WordlistLoader . getWordSet ( stopwords ) ; } public TokenStream tokenStream ( String fieldName , Reader reader ) { TokenStream result = new StandardTokenizer ( reader ) ; result = new StandardFilter ( result ) ; result = new LowerCaseFilter ( result ) ; result = new StopFilter ( result , stopSet ) ; return result ; } } 	0	['7', '2', '0', '8', '15', '0', '0', '8', '6', '0.5', '67', '0.5', '0', '0.666666667', '0.333333333', '0', '0', '8.285714286', '1', '0.1429', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; public interface Spans { boolean next ( ) throws IOException ; boolean skipTo ( int target ) throws IOException ; int doc ( ) ; int start ( ) ; int end ( ) ; } 	0	['5', '1', '0', '15', '5', '10', '15', '0', '5', '2', '5', '0', '0', '0', '0.6', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search ; import java . util . Iterator ; import java . util . NoSuchElementException ; public class HitIterator implements Iterator { private Hits hits ; private int hitNumber = 0 ; HitIterator ( Hits hits ) { this . hits = hits ; } public boolean hasNext ( ) { return hitNumber < hits . length ( ) ; } public Object next ( ) { if ( hitNumber == hits . length ( ) ) throw new NoSuchElementException ( ) ; Object next = new Hit ( hits , hitNumber ) ; hitNumber ++ ; return next ; } public void remove ( ) { throw new UnsupportedOperationException ( ) ; } public int length ( ) { return hits . length ( ) ; } } 	0	['5', '1', '0', '2', '10', '0', '1', '2', '4', '0.375', '60', '1', '1', '0', '0.6', '0', '0', '10.6', '2', '1.2', '0']
package org . apache . lucene . util ; public final class Constants { private Constants ( ) { } public static final String JAVA_VERSION = System . getProperty ( "java.version" ) ; public static final boolean JAVA_1_1 = JAVA_VERSION . startsWith ( "1.1." ) ; public static final boolean JAVA_1_2 = JAVA_VERSION . startsWith ( "1.2." ) ; public static final boolean JAVA_1_3 = JAVA_VERSION . startsWith ( "1.3." ) ; public static final String OS_NAME = System . getProperty ( "os.name" ) ; public static final boolean LINUX = OS_NAME . startsWith ( "Linux" ) ; public static final boolean WINDOWS = OS_NAME . startsWith ( "Windows" ) ; public static final boolean SUN_OS = OS_NAME . startsWith ( "SunOS" ) ; } 	0	['2', '1', '0', '0', '5', '1', '0', '0', '0', '1', '44', '0', '0', '0', '1', '0', '0', '17', '0', '0', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . util . ToStringUtils ; public abstract class MultiTermQuery extends Query { private Term term ; public MultiTermQuery ( Term term ) { this . term = term ; } public Term getTerm ( ) { return term ; } protected abstract FilteredTermEnum getEnum ( IndexReader reader ) throws IOException ; public Query rewrite ( IndexReader reader ) throws IOException { FilteredTermEnum enumerator = getEnum ( reader ) ; BooleanQuery query = new BooleanQuery ( true ) ; try { do { Term t = enumerator . term ( ) ; if ( t != null ) { TermQuery tq = new TermQuery ( t ) ; tq . setBoost ( getBoost ( ) * enumerator . difference ( ) ) ; query . add ( tq , BooleanClause . Occur . SHOULD ) ; } } while ( enumerator . next ( ) ) ; } finally { enumerator . close ( ) ; } return query ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! term . field ( ) . equals ( field ) ) { buffer . append ( term . field ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( term . text ( ) ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof MultiTermQuery ) ) return false ; final MultiTermQuery multiTermQuery = ( MultiTermQuery ) o ; if ( ! term . equals ( multiTermQuery . term ) ) return false ; return getBoost ( ) == multiTermQuery . getBoost ( ) ; } public int hashCode ( ) { return term . hashCode ( ) + Float . floatToRawIntBits ( getBoost ( ) ) ; } } 	0	['7', '2', '2', '10', '27', '1', '2', '8', '6', '0.333333333', '134', '1', '1', '0.666666667', '0.342857143', '2', '3', '18', '5', '1.5714', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . util . PriorityQueue ; public class ParallelMultiSearcher extends MultiSearcher { private Searchable [ ] searchables ; private int [ ] starts ; public ParallelMultiSearcher ( Searchable [ ] searchables ) throws IOException { super ( searchables ) ; this . searchables = searchables ; this . starts = getStarts ( ) ; } public int docFreq ( Term term ) throws IOException { return super . docFreq ( term ) ; } public TopDocs search ( Weight weight , Filter filter , int nDocs ) throws IOException { HitQueue hq = new HitQueue ( nDocs ) ; int totalHits = 0 ; MultiSearcherThread [ ] msta = new MultiSearcherThread [ searchables . length ] ; for ( int i = 0 ; i < searchables . length ; i ++ ) { msta [ i ] = new MultiSearcherThread ( searchables [ i ] , weight , filter , nDocs , hq , i , starts , "MultiSearcher thread #" + ( i + 1 ) ) ; msta [ i ] . start ( ) ; } for ( int i = 0 ; i < searchables . length ; i ++ ) { try { msta [ i ] . join ( ) ; } catch ( InterruptedException ie ) { ; } IOException ioe = msta [ i ] . getIOException ( ) ; if ( ioe == null ) { totalHits += msta [ i ] . hits ( ) ; } else { throw ioe ; } } ScoreDoc [ ] scoreDocs = new ScoreDoc [ hq . size ( ) ] ; for ( int i = hq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = ( ScoreDoc ) hq . pop ( ) ; float maxScore = ( totalHits == 0 ) ? Float . NEGATIVE_INFINITY : scoreDocs [ 0 ] . score ; return new TopDocs ( totalHits , scoreDocs , maxScore ) ; } public TopFieldDocs search ( Weight weight , Filter filter , int nDocs , Sort sort ) throws IOException { FieldDocSortedHitQueue hq = new FieldDocSortedHitQueue ( null , nDocs ) ; int totalHits = 0 ; MultiSearcherThread [ ] msta = new MultiSearcherThread [ searchables . length ] ; for ( int i = 0 ; i < searchables . length ; i ++ ) { msta [ i ] = new MultiSearcherThread ( searchables [ i ] , weight , filter , nDocs , hq , sort , i , starts , "MultiSearcher thread #" + ( i + 1 ) ) ; msta [ i ] . start ( ) ; } float maxScore = Float . NEGATIVE_INFINITY ; for ( int i = 0 ; i < searchables . length ; i ++ ) { try { msta [ i ] . join ( ) ; } catch ( InterruptedException ie ) { ; } IOException ioe = msta [ i ] . getIOException ( ) ; if ( ioe == null ) { totalHits += msta [ i ] . hits ( ) ; maxScore = Math . max ( maxScore , msta [ i ] . getMaxScore ( ) ) ; } else { throw ioe ; } } ScoreDoc [ ] scoreDocs = new ScoreDoc [ hq . size ( ) ] ; for ( int i = hq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = ( ScoreDoc ) hq . pop ( ) ; return new TopFieldDocs ( totalHits , scoreDocs , hq . getFields ( ) , maxScore ) ; } public void search ( Weight weight , Filter filter , final HitCollector results ) throws IOException { for ( int i = 0 ; i < searchables . length ; i ++ ) { final int start = starts [ i ] ; searchables [ i ] . search ( weight , filter , new HitCollector ( ) { public void collect ( int doc , float score ) { results . collect ( doc + start , score ) ; } } ) ; } } public Query rewrite ( Query original ) throws IOException { return super . rewrite ( original ) ; } } class MultiSearcherThread extends Thread { private Searchable searchable ; private Weight weight ; private Filter filter ; private int nDocs ; private TopDocs docs ; private int i ; private PriorityQueue hq ; private int [ ] starts ; private IOException ioe ; private Sort sort ; public MultiSearcherThread ( Searchable searchable , Weight weight , Filter filter , int nDocs , HitQueue hq , int i , int [ ] starts , String name ) { super ( name ) ; this . searchable = searchable ; this . weight = weight ; this . filter = filter ; this . nDocs = nDocs ; this . hq = hq ; this . i = i ; this . starts = starts ; } public MultiSearcherThread ( Searchable searchable , Weight weight , Filter filter , int nDocs , FieldDocSortedHitQueue hq , Sort sort , int i , int [ ] starts , String name ) { super ( name ) ; this . searchable = searchable ; this . weight = weight ; this . filter = filter ; this . nDocs = nDocs ; this . hq = hq ; this . i = i ; this . starts = starts ; this . sort = sort ; } public void run ( ) { try { docs = ( sort == null ) ? searchable . search ( weight , filter , nDocs ) : searchable . search ( weight , filter , nDocs , sort ) ; } catch ( IOException ioe ) { this . ioe = ioe ; } if ( ioe == null ) { if ( sort != null ) { ( ( FieldDocSortedHitQueue ) hq ) . setFields ( ( ( TopFieldDocs ) docs ) . fields ) ; } ScoreDoc [ ] scoreDocs = docs . scoreDocs ; for ( int j = 0 ; j < scoreDocs . length ; j ++ ) { ScoreDoc scoreDoc = scoreDocs [ j ] ; scoreDoc . doc += starts [ i ] ; synchronized ( hq ) { if ( ! hq . insert ( scoreDoc ) ) break ; } } } } public int hits ( ) { return docs . totalHits ; } public float getMaxScore ( ) { return docs . getMaxScore ( ) ; } public IOException getIOException ( ) { return ioe ; } } 	0	['6', '3', '0', '16', '33', '3', '1', '16', '6', '0.4', '297', '1', '1', '0.87804878', '0.351851852', '2', '3', '48.16666667', '1', '0.8333', '0']
package org . apache . lucene . util ; public class SmallFloat { public static byte floatToByte ( float f , int numMantissaBits , int zeroExp ) { int fzero = ( 63 - zeroExp ) << numMantissaBits ; int bits = Float . floatToRawIntBits ( f ) ; int smallfloat = bits > > ( 24 - numMantissaBits ) ; if ( smallfloat < fzero ) { return ( bits <= 0 ) ? ( byte ) 0 : ( byte ) 1 ; } else if ( smallfloat >= fzero + 0x100 ) { return - 1 ; } else { return ( byte ) ( smallfloat - fzero ) ; } } public static float byteToFloat ( byte b , int numMantissaBits , int zeroExp ) { if ( b == 0 ) return 0.0f ; int bits = ( b & 0xff ) << ( 24 - numMantissaBits ) ; bits += ( 63 - zeroExp ) << 24 ; return Float . intBitsToFloat ( bits ) ; } public static byte floatToByte315 ( float f ) { int bits = Float . floatToRawIntBits ( f ) ; int smallfloat = bits > > ( 24 - 3 ) ; if ( smallfloat < ( 63 - 15 ) << 3 ) { return ( bits <= 0 ) ? ( byte ) 0 : ( byte ) 1 ; } if ( smallfloat >= ( ( 63 - 15 ) << 3 ) + 0x100 ) { return - 1 ; } return ( byte ) ( smallfloat - ( ( 63 - 15 ) << 3 ) ) ; } public static float byte315ToFloat ( byte b ) { if ( b == 0 ) return 0.0f ; int bits = ( b & 0xff ) << ( 24 - 3 ) ; bits += ( 63 - 15 ) << 24 ; return Float . intBitsToFloat ( bits ) ; } public static byte floatToByte52 ( float f ) { int bits = Float . floatToRawIntBits ( f ) ; int smallfloat = bits > > ( 24 - 5 ) ; if ( smallfloat < ( 63 - 2 ) << 5 ) { return ( bits <= 0 ) ? ( byte ) 0 : ( byte ) 1 ; } if ( smallfloat >= ( ( 63 - 2 ) << 5 ) + 0x100 ) { return - 1 ; } return ( byte ) ( smallfloat - ( ( 63 - 2 ) << 5 ) ) ; } public static float byte52ToFloat ( byte b ) { if ( b == 0 ) return 0.0f ; int bits = ( b & 0xff ) << ( 24 - 5 ) ; bits += ( 63 - 2 ) << 24 ; return Float . intBitsToFloat ( bits ) ; } } 	0	['7', '1', '0', '1', '10', '21', '1', '0', '7', '2', '155', '0', '0', '0', '0.321428571', '0', '0', '21.14285714', '4', '2.5714', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; import java . io . Reader ; public class KeywordTokenizer extends Tokenizer { private static final int DEFAULT_BUFFER_SIZE = 256 ; private boolean done ; private final char [ ] buffer ; public KeywordTokenizer ( Reader input ) { this ( input , DEFAULT_BUFFER_SIZE ) ; } public KeywordTokenizer ( Reader input , int bufferSize ) { super ( input ) ; this . buffer = new char [ bufferSize ] ; this . done = false ; } public Token next ( ) throws IOException { if ( ! done ) { done = true ; StringBuffer buffer = new StringBuffer ( ) ; int length ; while ( true ) { length = input . read ( this . buffer ) ; if ( length == - 1 ) break ; buffer . append ( this . buffer , 0 , length ) ; } String text = buffer . toString ( ) ; return new Token ( text , 0 , text . length ( ) ) ; } return null ; } } 	0	['3', '3', '0', '3', '10', '1', '1', '2', '3', '0.5', '63', '1', '0', '0.75', '0.666666667', '0', '0', '19', '1', '0.3333', '0']
package org . apache . lucene . index ; public interface TermFreqVector { public String getField ( ) ; public int size ( ) ; public String [ ] getTerms ( ) ; public int [ ] getTermFrequencies ( ) ; public int indexOf ( String term ) ; public int [ ] indexesOf ( String [ ] terms , int start , int len ) ; } 	0	['6', '1', '0', '11', '6', '15', '11', '0', '6', '2', '6', '0', '0', '0', '0.375', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; public class TopDocCollector extends HitCollector { private int numHits ; private float minScore = 0.0f ; int totalHits ; PriorityQueue hq ; public TopDocCollector ( int numHits ) { this ( numHits , new HitQueue ( numHits ) ) ; } TopDocCollector ( int numHits , PriorityQueue hq ) { this . numHits = numHits ; this . hq = hq ; } public void collect ( int doc , float score ) { if ( score > 0.0f ) { totalHits ++ ; if ( hq . size ( ) < numHits || score >= minScore ) { hq . insert ( new ScoreDoc ( doc , score ) ) ; minScore = ( ( ScoreDoc ) hq . top ( ) ) . score ; } } } public int getTotalHits ( ) { return totalHits ; } public TopDocs topDocs ( ) { ScoreDoc [ ] scoreDocs = new ScoreDoc [ hq . size ( ) ] ; for ( int i = hq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = ( ScoreDoc ) hq . pop ( ) ; float maxScore = ( totalHits == 0 ) ? Float . NEGATIVE_INFINITY : scoreDocs [ 0 ] . score ; return new TopDocs ( totalHits , scoreDocs , maxScore ) ; } } 	0	['5', '2', '1', '7', '13', '0', '2', '5', '4', '0.4375', '110', '0.5', '1', '0.25', '0.5', '0', '0', '20.2', '4', '1.6', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; public class LetterTokenizer extends CharTokenizer { public LetterTokenizer ( Reader in ) { super ( in ) ; } protected boolean isTokenChar ( char c ) { return Character . isLetter ( c ) ; } } 	0	['2', '4', '1', '2', '4', '1', '1', '1', '1', '2', '9', '0', '0', '0.857142857', '0.666666667', '1', '1', '3.5', '1', '0.5', '0']
package org . apache . lucene . analysis . standard ; public class TokenMgrError extends Error { static final int LEXICAL_ERROR = 0 ; static final int STATIC_LEXER_ERROR = 1 ; static final int INVALID_LEXICAL_STATE = 2 ; static final int LOOP_DETECTED = 3 ; int errorCode ; protected static final String addEscapes ( String str ) { StringBuffer retval = new StringBuffer ( ) ; char ch ; for ( int i = 0 ; i < str . length ( ) ; i ++ ) { switch ( str . charAt ( i ) ) { case 0 : continue ; case '\b' : retval . append ( "\\b" ) ; continue ; case '\t' : retval . append ( "\\t" ) ; continue ; case '\n' : retval . append ( "\\n" ) ; continue ; case '\f' : retval . append ( "\\f" ) ; continue ; case '\r' : retval . append ( "\\r" ) ; continue ; case '\"' : retval . append ( "\\\"" ) ; continue ; case '\'' : retval . append ( "\\\'" ) ; continue ; case '\\' : retval . append ( "\\\\" ) ; continue ; default : if ( ( ch = str . charAt ( i ) ) < 0x20 || ch > 0x7e ) { String s = "0000" + Integer . toString ( ch , 16 ) ; retval . append ( "\\u" + s . substring ( s . length ( ) - 4 , s . length ( ) ) ) ; } else { retval . append ( ch ) ; } continue ; } } return retval . toString ( ) ; } protected static String LexicalError ( boolean EOFSeen , int lexState , int errorLine , int errorColumn , String errorAfter , char curChar ) { return ( "Lexical error at line " + errorLine + ", column " + errorColumn + ".  Encountered: " + ( EOFSeen ? "<EOF> " : ( "\"" + addEscapes ( String . valueOf ( curChar ) ) + "\"" ) + " (" + ( int ) curChar + "), " ) + "after : \"" + addEscapes ( errorAfter ) + "\"" ) ; } public String getMessage ( ) { return super . getMessage ( ) ; } public TokenMgrError ( ) { } public TokenMgrError ( String message , int reason ) { super ( message ) ; errorCode = reason ; } public TokenMgrError ( boolean EOFSeen , int lexState , int errorLine , int errorColumn , String errorAfter , char curChar , int reason ) { this ( LexicalError ( EOFSeen , lexState , errorLine , errorColumn , errorAfter , curChar ) , reason ) ; } } 	0	['6', '3', '0', '1', '19', '15', '1', '0', '4', '1.12', '184', '0', '0', '0.8125', '0.5', '1', '1', '28.83333333', '14', '2.8333', '0']
package org . apache . lucene . search ; import java . util . BitSet ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; public abstract class Filter implements java . io . Serializable { public abstract BitSet bits ( IndexReader reader ) throws IOException ; } 	0	['2', '1', '3', '20', '3', '1', '19', '1', '2', '2', '5', '0', '0', '0', '0.75', '0', '0', '1.5', '1', '0.5', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; public interface FieldCache { public static final int STRING_INDEX = - 1 ; public static class StringIndex { public final String [ ] lookup ; public final int [ ] order ; public StringIndex ( int [ ] values , String [ ] lookup ) { this . order = values ; this . lookup = lookup ; } } public interface IntParser { public int parseInt ( String string ) ; } public interface FloatParser { public float parseFloat ( String string ) ; } public static FieldCache DEFAULT = new FieldCacheImpl ( ) ; public int [ ] getInts ( IndexReader reader , String field ) throws IOException ; public int [ ] getInts ( IndexReader reader , String field , IntParser parser ) throws IOException ; public float [ ] getFloats ( IndexReader reader , String field ) throws IOException ; public float [ ] getFloats ( IndexReader reader , String field , FloatParser parser ) throws IOException ; public String [ ] getStrings ( IndexReader reader , String field ) throws IOException ; public StringIndex getStringIndex ( IndexReader reader , String field ) throws IOException ; public Object getAuto ( IndexReader reader , String field ) throws IOException ; public Comparable [ ] getCustom ( IndexReader reader , String field , SortComparator comparator ) throws IOException ; } 	0	['9', '1', '0', '7', '10', '36', '3', '6', '8', '1.0625', '16', '0', '1', '0', '0.5625', '0', '0', '0.555555556', '1', '0.8889', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class IndexInput implements Cloneable { private char [ ] chars ; public abstract byte readByte ( ) throws IOException ; public abstract void readBytes ( byte [ ] b , int offset , int len ) throws IOException ; public int readInt ( ) throws IOException { return ( ( readByte ( ) & 0xFF ) << 24 ) | ( ( readByte ( ) & 0xFF ) << 16 ) | ( ( readByte ( ) & 0xFF ) << 8 ) | ( readByte ( ) & 0xFF ) ; } public int readVInt ( ) throws IOException { byte b = readByte ( ) ; int i = b & 0x7F ; for ( int shift = 7 ; ( b & 0x80 ) != 0 ; shift += 7 ) { b = readByte ( ) ; i |= ( b & 0x7F ) << shift ; } return i ; } public long readLong ( ) throws IOException { return ( ( ( long ) readInt ( ) ) << 32 ) | ( readInt ( ) & 0xFFFFFFFFL ) ; } public long readVLong ( ) throws IOException { byte b = readByte ( ) ; long i = b & 0x7F ; for ( int shift = 7 ; ( b & 0x80 ) != 0 ; shift += 7 ) { b = readByte ( ) ; i |= ( b & 0x7FL ) << shift ; } return i ; } public String readString ( ) throws IOException { int length = readVInt ( ) ; if ( chars == null || length > chars . length ) chars = new char [ length ] ; readChars ( chars , 0 , length ) ; return new String ( chars , 0 , length ) ; } public void readChars ( char [ ] buffer , int start , int length ) throws IOException { final int end = start + length ; for ( int i = start ; i < end ; i ++ ) { byte b = readByte ( ) ; if ( ( b & 0x80 ) == 0 ) buffer [ i ] = ( char ) ( b & 0x7F ) ; else if ( ( b & 0xE0 ) != 0xE0 ) { buffer [ i ] = ( char ) ( ( ( b & 0x1F ) << 6 ) | ( readByte ( ) & 0x3F ) ) ; } else buffer [ i ] = ( char ) ( ( ( b & 0x0F ) << 12 ) | ( ( readByte ( ) & 0x3F ) << 6 ) | ( readByte ( ) & 0x3F ) ) ; } } public abstract void close ( ) throws IOException ; public abstract long getFilePointer ( ) ; public abstract void seek ( long pos ) throws IOException ; public abstract long length ( ) ; public Object clone ( ) { IndexInput clone = null ; try { clone = ( IndexInput ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . chars = null ; return clone ; } } 	0	['14', '1', '3', '24', '17', '89', '24', '0', '14', '0.923076923', '224', '1', '0', '0', '0.271428571', '0', '0', '14.92857143', '1', '0.9286', '0']
package org . apache . lucene . analysis . standard ; import java . io . * ; public final class FastCharStream implements CharStream { char [ ] buffer = null ; int bufferLength = 0 ; int bufferPosition = 0 ; int tokenStart = 0 ; int bufferStart = 0 ; Reader input ; public FastCharStream ( Reader r ) { input = r ; } public final char readChar ( ) throws IOException { if ( bufferPosition >= bufferLength ) refill ( ) ; return buffer [ bufferPosition ++ ] ; } private final void refill ( ) throws IOException { int newPosition = bufferLength - tokenStart ; if ( tokenStart == 0 ) { if ( buffer == null ) { buffer = new char [ 2048 ] ; } else if ( bufferLength == buffer . length ) { char [ ] newBuffer = new char [ buffer . length * 2 ] ; System . arraycopy ( buffer , 0 , newBuffer , 0 , bufferLength ) ; buffer = newBuffer ; } } else { System . arraycopy ( buffer , tokenStart , buffer , 0 , newPosition ) ; } bufferLength = newPosition ; bufferPosition = newPosition ; bufferStart += tokenStart ; tokenStart = 0 ; int charsRead = input . read ( buffer , newPosition , buffer . length - newPosition ) ; if ( charsRead == - 1 ) throw new IOException ( "read past eof" ) ; else bufferLength += charsRead ; } public final char BeginToken ( ) throws IOException { tokenStart = bufferPosition ; return readChar ( ) ; } public final void backup ( int amount ) { bufferPosition -= amount ; } public final String GetImage ( ) { return new String ( buffer , tokenStart , bufferPosition - tokenStart ) ; } public final char [ ] GetSuffix ( int len ) { char [ ] value = new char [ len ] ; System . arraycopy ( buffer , bufferPosition - len , value , 0 , len ) ; return value ; } public final void Done ( ) { try { input . close ( ) ; } catch ( IOException e ) { System . err . println ( "Caught: " + e + "; ignoring." ) ; } } public final int getColumn ( ) { return bufferStart + bufferPosition ; } public final int getLine ( ) { return 1 ; } public final int getEndColumn ( ) { return bufferStart + bufferPosition ; } public final int getEndLine ( ) { return 1 ; } public final int getBeginColumn ( ) { return bufferStart + tokenStart ; } public final int getBeginLine ( ) { return 1 ; } } 	0	['14', '1', '0', '2', '25', '3', '1', '1', '13', '0.602564103', '237', '0', '0', '0', '0.404761905', '0', '0', '15.5', '1', '0.9286', '0']
package org . apache . lucene . search ; import java . io . IOException ; public class ReqExclScorer extends Scorer { private Scorer reqScorer , exclScorer ; public ReqExclScorer ( Scorer reqScorer , Scorer exclScorer ) { super ( null ) ; this . reqScorer = reqScorer ; this . exclScorer = exclScorer ; } private boolean firstTime = true ; public boolean next ( ) throws IOException { if ( firstTime ) { if ( ! exclScorer . next ( ) ) { exclScorer = null ; } firstTime = false ; } if ( reqScorer == null ) { return false ; } if ( ! reqScorer . next ( ) ) { reqScorer = null ; return false ; } if ( exclScorer == null ) { return true ; } return toNonExcluded ( ) ; } private boolean toNonExcluded ( ) throws IOException { int exclDoc = exclScorer . doc ( ) ; do { int reqDoc = reqScorer . doc ( ) ; if ( reqDoc < exclDoc ) { return true ; } else if ( reqDoc > exclDoc ) { if ( ! exclScorer . skipTo ( reqDoc ) ) { exclScorer = null ; return true ; } exclDoc = exclScorer . doc ( ) ; if ( exclDoc > reqDoc ) { return true ; } } } while ( reqScorer . next ( ) ) ; reqScorer = null ; return false ; } public int doc ( ) { return reqScorer . doc ( ) ; } public float score ( ) throws IOException { return reqScorer . score ( ) ; } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { firstTime = false ; if ( ! exclScorer . skipTo ( target ) ) { exclScorer = null ; } } if ( reqScorer == null ) { return false ; } if ( exclScorer == null ) { return reqScorer . skipTo ( target ) ; } if ( ! reqScorer . skipTo ( target ) ) { reqScorer = null ; return false ; } return toNonExcluded ( ) ; } public Explanation explain ( int doc ) throws IOException { Explanation res = new Explanation ( ) ; if ( exclScorer . skipTo ( doc ) && ( exclScorer . doc ( ) == doc ) ) { res . setDescription ( "excluded" ) ; } else { res . setDescription ( "not excluded" ) ; res . addDetail ( reqScorer . explain ( doc ) ) ; } return res ; } } 	0	['7', '2', '0', '4', '16', '0', '1', '3', '6', '0.333333333', '179', '1', '2', '0.571428571', '0.476190476', '1', '3', '24.14285714', '1', '0.8571', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; public final class PorterStemFilter extends TokenFilter { private PorterStemmer stemmer ; public PorterStemFilter ( TokenStream in ) { super ( in ) ; stemmer = new PorterStemmer ( ) ; } public final Token next ( ) throws IOException { Token token = input . next ( ) ; if ( token == null ) return null ; else { String s = stemmer . stem ( token . termText ) ; if ( s != token . termText ) token . termText = s ; return token ; } } } 	0	['2', '3', '0', '4', '6', '0', '0', '4', '2', '0', '35', '1', '1', '0.75', '0.75', '0', '0', '16', '1', '0.5', '0']
package org . apache . lucene . analysis ; import java . io . File ; import java . io . FileReader ; import java . io . IOException ; import java . io . Reader ; import java . io . BufferedReader ; import java . util . HashSet ; import java . util . Hashtable ; import java . util . Iterator ; public class WordlistLoader { public static HashSet getWordSet ( File wordfile ) throws IOException { HashSet result = new HashSet ( ) ; FileReader reader = null ; try { reader = new FileReader ( wordfile ) ; result = getWordSet ( reader ) ; } finally { if ( reader != null ) reader . close ( ) ; } return result ; } public static HashSet getWordSet ( Reader reader ) throws IOException { HashSet result = new HashSet ( ) ; BufferedReader br = null ; try { if ( reader instanceof BufferedReader ) { br = ( BufferedReader ) reader ; } else { br = new BufferedReader ( reader ) ; } String word = null ; while ( ( word = br . readLine ( ) ) != null ) { result . add ( word . trim ( ) ) ; } } finally { if ( br != null ) br . close ( ) ; } return result ; } private static Hashtable makeWordTable ( HashSet wordSet ) { Hashtable table = new Hashtable ( ) ; for ( Iterator iter = wordSet . iterator ( ) ; iter . hasNext ( ) ; ) { String word = ( String ) iter . next ( ) ; table . put ( word , word ) ; } return table ; } } 	0	['4', '1', '0', '2', '18', '6', '2', '0', '3', '2', '102', '0', '0', '0', '0.25', '0', '0', '24.5', '2', '1', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; public final class LowerCaseFilter extends TokenFilter { public LowerCaseFilter ( TokenStream in ) { super ( in ) ; } public final Token next ( ) throws IOException { Token t = input . next ( ) ; if ( t == null ) return null ; t . termText = t . termText . toLowerCase ( ) ; return t ; } } 	0	['2', '3', '0', '4', '5', '1', '1', '3', '2', '2', '21', '0', '0', '0.75', '0.75', '0', '0', '9.5', '1', '0.5', '0']
package org . apache . lucene . queryParser ; public interface CharStream { char readChar ( ) throws java . io . IOException ; int getEndColumn ( ) ; int getEndLine ( ) ; int getBeginColumn ( ) ; int getBeginLine ( ) ; void backup ( int amount ) ; char BeginToken ( ) throws java . io . IOException ; String GetImage ( ) ; char [ ] GetSuffix ( int len ) ; void Done ( ) ; } 	0	['10', '1', '0', '3', '10', '45', '3', '0', '10', '2', '10', '0', '0', '0', '0.6', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; public abstract class SortComparator implements SortComparatorSource { public ScoreDocComparator newComparator ( final IndexReader reader , final String fieldname ) throws IOException { final String field = fieldname . intern ( ) ; final Comparable [ ] cachedValues = FieldCache . DEFAULT . getCustom ( reader , field , SortComparator . this ) ; return new ScoreDocComparator ( ) { public int compare ( ScoreDoc i , ScoreDoc j ) { return cachedValues [ i . doc ] . compareTo ( cachedValues [ j . doc ] ) ; } public Comparable sortValue ( ScoreDoc i ) { return cachedValues [ i . doc ] ; } public int sortType ( ) { return SortField . CUSTOM ; } } ; } protected abstract Comparable getComparable ( String termtext ) ; } 	0	['3', '1', '0', '6', '7', '3', '3', '5', '2', '2', '21', '0', '0', '0', '0.666666667', '0', '0', '6', '1', '0.6667', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . TermDocs ; final class TermScorer extends Scorer { private Weight weight ; private TermDocs termDocs ; private byte [ ] norms ; private float weightValue ; private int doc ; private final int [ ] docs = new int [ 32 ] ; private final int [ ] freqs = new int [ 32 ] ; private int pointer ; private int pointerMax ; private static final int SCORE_CACHE_SIZE = 32 ; private float [ ] scoreCache = new float [ SCORE_CACHE_SIZE ] ; TermScorer ( Weight weight , TermDocs td , Similarity similarity , byte [ ] norms ) { super ( similarity ) ; this . weight = weight ; this . termDocs = td ; this . norms = norms ; this . weightValue = weight . getValue ( ) ; for ( int i = 0 ; i < SCORE_CACHE_SIZE ; i ++ ) scoreCache [ i ] = getSimilarity ( ) . tf ( i ) * weightValue ; } public void score ( HitCollector hc ) throws IOException { next ( ) ; score ( hc , Integer . MAX_VALUE ) ; } protected boolean score ( HitCollector c , int end ) throws IOException { Similarity similarity = getSimilarity ( ) ; float [ ] normDecoder = Similarity . getNormDecoder ( ) ; while ( doc < end ) { int f = freqs [ pointer ] ; float score = f < SCORE_CACHE_SIZE ? scoreCache [ f ] : similarity . tf ( f ) * weightValue ; score *= normDecoder [ norms [ doc ] & 0xFF ] ; c . collect ( doc , score ) ; if ( ++ pointer >= pointerMax ) { pointerMax = termDocs . read ( docs , freqs ) ; if ( pointerMax != 0 ) { pointer = 0 ; } else { termDocs . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } } doc = docs [ pointer ] ; } return true ; } public int doc ( ) { return doc ; } public boolean next ( ) throws IOException { pointer ++ ; if ( pointer >= pointerMax ) { pointerMax = termDocs . read ( docs , freqs ) ; if ( pointerMax != 0 ) { pointer = 0 ; } else { termDocs . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } } doc = docs [ pointer ] ; return true ; } public float score ( ) { int f = freqs [ pointer ] ; float raw = f < SCORE_CACHE_SIZE ? scoreCache [ f ] : getSimilarity ( ) . tf ( f ) * weightValue ; return raw * Similarity . decodeNorm ( norms [ doc ] ) ; } public boolean skipTo ( int target ) throws IOException { for ( pointer ++ ; pointer < pointerMax ; pointer ++ ) { if ( docs [ pointer ] >= target ) { doc = docs [ pointer ] ; return true ; } } boolean result = termDocs . skipTo ( target ) ; if ( result ) { pointerMax = 1 ; pointer = 0 ; docs [ pointer ] = doc = termDocs . doc ( ) ; freqs [ pointer ] = termDocs . freq ( ) ; } else { doc = Integer . MAX_VALUE ; } return result ; } public Explanation explain ( int doc ) throws IOException { TermQuery query = ( TermQuery ) weight . getQuery ( ) ; Explanation tfExplanation = new Explanation ( ) ; int tf = 0 ; while ( pointer < pointerMax ) { if ( docs [ pointer ] == doc ) tf = freqs [ pointer ] ; pointer ++ ; } if ( tf == 0 ) { while ( termDocs . next ( ) ) { if ( termDocs . doc ( ) == doc ) { tf = termDocs . freq ( ) ; } } } termDocs . close ( ) ; tfExplanation . setValue ( getSimilarity ( ) . tf ( tf ) ) ; tfExplanation . setDescription ( "tf(termFreq(" + query . getTerm ( ) + ")=" + tf + ")" ) ; return tfExplanation ; } public String toString ( ) { return "scorer(" + weight + ")" ; } } 	0	['9', '2', '0', '10', '32', '0', '1', '9', '7', '0.545454545', '409', '1', '2', '0.5', '0.285714286', '1', '3', '43.22222222', '2', '1', '0']
package org . apache . lucene . util ; public abstract class PriorityQueue { private Object [ ] heap ; private int size ; private int maxSize ; protected abstract boolean lessThan ( Object a , Object b ) ; protected final void initialize ( int maxSize ) { size = 0 ; int heapSize = maxSize + 1 ; heap = new Object [ heapSize ] ; this . maxSize = maxSize ; } public final void put ( Object element ) { size ++ ; heap [ size ] = element ; upHeap ( ) ; } public boolean insert ( Object element ) { if ( size < maxSize ) { put ( element ) ; return true ; } else if ( size > 0 && ! lessThan ( element , top ( ) ) ) { heap [ 1 ] = element ; adjustTop ( ) ; return true ; } else return false ; } public final Object top ( ) { if ( size > 0 ) return heap [ 1 ] ; else return null ; } public final Object pop ( ) { if ( size > 0 ) { Object result = heap [ 1 ] ; heap [ 1 ] = heap [ size ] ; heap [ size ] = null ; size -- ; downHeap ( ) ; return result ; } else return null ; } public final void adjustTop ( ) { downHeap ( ) ; } public final int size ( ) { return size ; } public final void clear ( ) { for ( int i = 0 ; i <= size ; i ++ ) heap [ i ] = null ; size = 0 ; } private final void upHeap ( ) { int i = size ; Object node = heap [ i ] ; int j = i > > > 1 ; while ( j > 0 && lessThan ( node , heap [ j ] ) ) { heap [ i ] = heap [ j ] ; i = j ; j = j > > > 1 ; } heap [ i ] = node ; } private final void downHeap ( ) { int i = 1 ; Object node = heap [ i ] ; int j = i << 1 ; int k = j + 1 ; if ( k <= size && lessThan ( heap [ k ] , heap [ j ] ) ) { j = k ; } while ( j <= size && lessThan ( heap [ j ] , node ) ) { heap [ i ] = heap [ j ] ; i = j ; j = i << 1 ; k = j + 1 ; if ( k <= size && lessThan ( heap [ k ] , heap [ j ] ) ) { j = k ; } } heap [ i ] = node ; } } 	0	['12', '1', '10', '13', '13', '0', '13', '0', '8', '0.454545455', '275', '1', '0', '0', '0.444444444', '0', '0', '21.66666667', '7', '2.0833', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import java . io . IOException ; public final class FuzzyTermEnum extends FilteredTermEnum { private static final int TYPICAL_LONGEST_WORD_IN_INDEX = 19 ; private int [ ] [ ] d ; private float similarity ; private boolean endEnum = false ; private Term searchTerm = null ; private final String field ; private final String text ; private final String prefix ; private final float minimumSimilarity ; private final float scale_factor ; private final int [ ] maxDistances = new int [ TYPICAL_LONGEST_WORD_IN_INDEX ] ; public FuzzyTermEnum ( IndexReader reader , Term term ) throws IOException { this ( reader , term , FuzzyQuery . defaultMinSimilarity , FuzzyQuery . defaultPrefixLength ) ; } public FuzzyTermEnum ( IndexReader reader , Term term , float minSimilarity ) throws IOException { this ( reader , term , minSimilarity , FuzzyQuery . defaultPrefixLength ) ; } public FuzzyTermEnum ( IndexReader reader , Term term , final float minSimilarity , final int prefixLength ) throws IOException { super ( ) ; if ( minSimilarity >= 1.0f ) throw new IllegalArgumentException ( "minimumSimilarity cannot be greater than or equal to 1" ) ; else if ( minSimilarity < 0.0f ) throw new IllegalArgumentException ( "minimumSimilarity cannot be less than 0" ) ; if ( prefixLength < 0 ) throw new IllegalArgumentException ( "prefixLength cannot be less than 0" ) ; this . minimumSimilarity = minSimilarity ; this . scale_factor = 1.0f / ( 1.0f - minimumSimilarity ) ; this . searchTerm = term ; this . field = searchTerm . field ( ) ; final int fullSearchTermLength = searchTerm . text ( ) . length ( ) ; final int realPrefixLength = prefixLength > fullSearchTermLength ? fullSearchTermLength : prefixLength ; this . text = searchTerm . text ( ) . substring ( realPrefixLength ) ; this . prefix = searchTerm . text ( ) . substring ( 0 , realPrefixLength ) ; initializeMaxDistances ( ) ; this . d = initDistanceArray ( ) ; setEnum ( reader . terms ( new Term ( searchTerm . field ( ) , prefix ) ) ) ; } protected final boolean termCompare ( Term term ) { if ( field == term . field ( ) && term . text ( ) . startsWith ( prefix ) ) { final String target = term . text ( ) . substring ( prefix . length ( ) ) ; this . similarity = similarity ( target ) ; return ( similarity > minimumSimilarity ) ; } endEnum = true ; return false ; } public final float difference ( ) { return ( float ) ( ( similarity - minimumSimilarity ) * scale_factor ) ; } public final boolean endEnum ( ) { return endEnum ; } private static final int min ( int a , int b , int c ) { final int t = ( a < b ) ? a : b ; return ( t < c ) ? t : c ; } private final int [ ] [ ] initDistanceArray ( ) { return new int [ this . text . length ( ) + 1 ] [ TYPICAL_LONGEST_WORD_IN_INDEX ] ; } private synchronized final float similarity ( final String target ) { final int m = target . length ( ) ; final int n = text . length ( ) ; if ( n == 0 ) { return prefix . length ( ) == 0 ? 0.0f : 1.0f - ( ( float ) m / prefix . length ( ) ) ; } if ( m == 0 ) { return prefix . length ( ) == 0 ? 0.0f : 1.0f - ( ( float ) n / prefix . length ( ) ) ; } final int maxDistance = getMaxDistance ( m ) ; if ( maxDistance < Math . abs ( m - n ) ) { return 0.0f ; } if ( d [ 0 ] . length <= m ) { growDistanceArray ( m ) ; } for ( int i = 0 ; i <= n ; i ++ ) d [ i ] [ 0 ] = i ; for ( int j = 0 ; j <= m ; j ++ ) d [ 0 ] [ j ] = j ; for ( int i = 1 ; i <= n ; i ++ ) { int bestPossibleEditDistance = m ; final char s_i = text . charAt ( i - 1 ) ; for ( int j = 1 ; j <= m ; j ++ ) { if ( s_i != target . charAt ( j - 1 ) ) { d [ i ] [ j ] = min ( d [ i - 1 ] [ j ] , d [ i ] [ j - 1 ] , d [ i - 1 ] [ j - 1 ] ) + 1 ; } else { d [ i ] [ j ] = min ( d [ i - 1 ] [ j ] + 1 , d [ i ] [ j - 1 ] + 1 , d [ i - 1 ] [ j - 1 ] ) ; } bestPossibleEditDistance = Math . min ( bestPossibleEditDistance , d [ i ] [ j ] ) ; } if ( i > maxDistance && bestPossibleEditDistance > maxDistance ) { return 0.0f ; } } return 1.0f - ( ( float ) d [ n ] [ m ] / ( float ) ( prefix . length ( ) + Math . min ( n , m ) ) ) ; } private void growDistanceArray ( int m ) { for ( int i = 0 ; i < d . length ; i ++ ) { d [ i ] = new int [ m + 1 ] ; } } private final int getMaxDistance ( int m ) { return ( m < maxDistances . length ) ? maxDistances [ m ] : calculateMaxDistance ( m ) ; } private void initializeMaxDistances ( ) { for ( int i = 0 ; i < maxDistances . length ; i ++ ) { maxDistances [ i ] = calculateMaxDistance ( i ) ; } } private int calculateMaxDistance ( int m ) { return ( int ) ( ( 1 - minimumSimilarity ) * ( Math . min ( text . length ( ) , m ) + prefix . length ( ) ) ) ; } public void close ( ) throws IOException { super . close ( ) ; } } 	0	['14', '3', '0', '5', '29', '53', '1', '4', '6', '0.692307692', '514', '1', '1', '0.541666667', '0.333333333', '1', '4', '34.92857143', '14', '2.2857', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Query ; import org . apache . lucene . util . ToStringUtils ; public class SpanFirstQuery extends SpanQuery { private SpanQuery match ; private int end ; public SpanFirstQuery ( SpanQuery match , int end ) { this . match = match ; this . end = end ; } public SpanQuery getMatch ( ) { return match ; } public int getEnd ( ) { return end ; } public String getField ( ) { return match . getField ( ) ; } public Collection getTerms ( ) { return match . getTerms ( ) ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "spanFirst(" ) ; buffer . append ( match . toString ( field ) ) ; buffer . append ( ", " ) ; buffer . append ( end ) ; buffer . append ( ")" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public void extractTerms ( Set terms ) { match . extractTerms ( terms ) ; } public Spans getSpans ( final IndexReader reader ) throws IOException { return new Spans ( ) { private Spans spans = match . getSpans ( reader ) ; public boolean next ( ) throws IOException { while ( spans . next ( ) ) { if ( end ( ) <= end ) return true ; } return false ; } public boolean skipTo ( int target ) throws IOException { if ( ! spans . skipTo ( target ) ) return false ; if ( spans . end ( ) <= end ) return true ; return next ( ) ; } public int doc ( ) { return spans . doc ( ) ; } public int start ( ) { return spans . start ( ) ; } public int end ( ) { return spans . end ( ) ; } public String toString ( ) { return "spans(" + SpanFirstQuery . this . toString ( ) + ")" ; } } ; } public Query rewrite ( IndexReader reader ) throws IOException { SpanFirstQuery clone = null ; SpanQuery rewritten = ( SpanQuery ) match . rewrite ( reader ) ; if ( rewritten != match ) { clone = ( SpanFirstQuery ) this . clone ( ) ; clone . match = rewritten ; } if ( clone != null ) { return clone ; } else { return this ; } } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof SpanFirstQuery ) ) return false ; SpanFirstQuery other = ( SpanFirstQuery ) o ; return this . end == other . end && this . match . equals ( other . match ) && this . getBoost ( ) == other . getBoost ( ) ; } public int hashCode ( ) { int h = match . hashCode ( ) ; h ^= ( h << 8 ) | ( h > > > 25 ) ; h ^= Float . floatToRawIntBits ( getBoost ( ) ) ^ end ; return h ; } } 	0	['13', '3', '0', '6', '30', '0', '1', '6', '11', '0.416666667', '176', '1', '1', '0.571428571', '0.192307692', '2', '2', '12.38461538', '6', '1.3077', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; public class TopFieldDocCollector extends TopDocCollector { public TopFieldDocCollector ( IndexReader reader , Sort sort , int numHits ) throws IOException { super ( numHits , new FieldSortedHitQueue ( reader , sort . fields , numHits ) ) ; } public void collect ( int doc , float score ) { if ( score > 0.0f ) { totalHits ++ ; hq . insert ( new FieldDoc ( doc , score ) ) ; } } public TopDocs topDocs ( ) { FieldSortedHitQueue fshq = ( FieldSortedHitQueue ) hq ; ScoreDoc [ ] scoreDocs = new ScoreDoc [ fshq . size ( ) ] ; for ( int i = fshq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = fshq . fillFields ( ( FieldDoc ) fshq . pop ( ) ) ; return new TopFieldDocs ( totalHits , scoreDocs , fshq . getFields ( ) , fshq . getMaxScore ( ) ) ; } } 	0	['3', '3', '0', '11', '13', '1', '1', '10', '3', '2', '70', '0', '0', '0.666666667', '0.533333333', '1', '3', '22.33333333', '2', '1.3333', '0']
package org . apache . lucene . index ; import java . io . File ; import java . io . FilenameFilter ; public class IndexFileNameFilter implements FilenameFilter { public boolean accept ( File dir , String name ) { for ( int i = 0 ; i < IndexFileNames . INDEX_EXTENSIONS . length ; i ++ ) { if ( name . endsWith ( "." + IndexFileNames . INDEX_EXTENSIONS [ i ] ) ) return true ; } if ( name . equals ( IndexFileNames . DELETABLE ) ) return true ; else if ( name . equals ( IndexFileNames . SEGMENTS ) ) return true ; else if ( name . matches ( ".+\\.f\\d+" ) ) return true ; return false ; } } 	0	['2', '1', '0', '2', '9', '1', '1', '1', '2', '2', '48', '0', '0', '0', '0.666666667', '0', '0', '23', '6', '3', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; public final class LowerCaseTokenizer extends LetterTokenizer { public LowerCaseTokenizer ( Reader in ) { super ( in ) ; } protected char normalize ( char c ) { return Character . toLowerCase ( c ) ; } } 	0	['2', '5', '0', '3', '4', '1', '2', '1', '1', '2', '9', '0', '0', '0.875', '0.666666667', '1', '1', '3.5', '1', '0.5', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermEnum ; public abstract class FilteredTermEnum extends TermEnum { private Term currentTerm = null ; private TermEnum actualEnum = null ; public FilteredTermEnum ( ) { } protected abstract boolean termCompare ( Term term ) ; public abstract float difference ( ) ; protected abstract boolean endEnum ( ) ; protected void setEnum ( TermEnum actualEnum ) throws IOException { this . actualEnum = actualEnum ; Term term = actualEnum . term ( ) ; if ( term != null && termCompare ( term ) ) currentTerm = term ; else next ( ) ; } public int docFreq ( ) { if ( actualEnum == null ) return - 1 ; return actualEnum . docFreq ( ) ; } public boolean next ( ) throws IOException { if ( actualEnum == null ) return false ; currentTerm = null ; while ( currentTerm == null ) { if ( endEnum ( ) ) return false ; if ( actualEnum . next ( ) ) { Term term = actualEnum . term ( ) ; if ( termCompare ( term ) ) { currentTerm = term ; return true ; } } else return false ; } currentTerm = null ; return false ; } public Term term ( ) { return currentTerm ; } public void close ( ) throws IOException { actualEnum . close ( ) ; currentTerm = null ; actualEnum = null ; } } 	0	['9', '2', '2', '7', '14', '8', '5', '2', '6', '0.5', '103', '1', '2', '0.384615385', '0.407407407', '1', '2', '10.22222222', '2', '1', '0']
package org . apache . lucene . index ; import java . io . IOException ; public interface TermDocs { void seek ( Term term ) throws IOException ; void seek ( TermEnum termEnum ) throws IOException ; int doc ( ) ; int freq ( ) ; boolean next ( ) throws IOException ; int read ( int [ ] docs , int [ ] freqs ) throws IOException ; boolean skipTo ( int target ) throws IOException ; void close ( ) throws IOException ; } 	0	['8', '1', '0', '19', '8', '28', '17', '2', '8', '2', '8', '0', '0', '0', '0.3', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . index ; final class TermInfo { int docFreq = 0 ; long freqPointer = 0 ; long proxPointer = 0 ; int skipOffset ; TermInfo ( ) { } TermInfo ( int df , long fp , long pp ) { docFreq = df ; freqPointer = fp ; proxPointer = pp ; } TermInfo ( TermInfo ti ) { docFreq = ti . docFreq ; freqPointer = ti . freqPointer ; proxPointer = ti . proxPointer ; skipOffset = ti . skipOffset ; } final void set ( int docFreq , long freqPointer , long proxPointer , int skipOffset ) { this . docFreq = docFreq ; this . freqPointer = freqPointer ; this . proxPointer = proxPointer ; this . skipOffset = skipOffset ; } final void set ( TermInfo ti ) { docFreq = ti . docFreq ; freqPointer = ti . freqPointer ; proxPointer = ti . proxPointer ; skipOffset = ti . skipOffset ; } } 	0	['5', '1', '0', '8', '6', '0', '8', '0', '0', '0.125', '100', '0', '0', '0', '0.55', '0', '0', '18.2', '1', '0.4', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . util . PriorityQueue ; final class SegmentMergeQueue extends PriorityQueue { SegmentMergeQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object a , Object b ) { SegmentMergeInfo stiA = ( SegmentMergeInfo ) a ; SegmentMergeInfo stiB = ( SegmentMergeInfo ) b ; int comparison = stiA . term . compareTo ( stiB . term ) ; if ( comparison == 0 ) return stiA . base < stiB . base ; else return comparison < 0 ; } final void close ( ) throws IOException { while ( top ( ) != null ) ( ( SegmentMergeInfo ) pop ( ) ) . close ( ) ; } } 	0	['3', '2', '0', '5', '9', '3', '2', '3', '0', '2', '47', '0', '0', '0.846153846', '0.555555556', '1', '3', '14.66666667', '4', '1.6667', '0']
package org . apache . lucene . util ; public class ToStringUtils { public static String boost ( float boost ) { if ( boost != 1.0f ) { return "^" + Float . toString ( boost ) ; } else return "" ; } } 	0	['2', '1', '0', '15', '7', '1', '15', '0', '2', '2', '21', '0', '0', '0', '0.5', '0', '0', '9.5', '2', '1', '0']
package org . apache . lucene . search ; public class ScoreDoc implements java . io . Serializable { public float score ; public int doc ; public ScoreDoc ( int doc , float score ) { this . doc = doc ; this . score = score ; } } 	0	['1', '1', '1', '19', '2', '0', '19', '0', '1', '2', '12', '0', '0', '0', '1', '0', '0', '9', '0', '0', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; public final class LengthFilter extends TokenFilter { final int min ; final int max ; public LengthFilter ( TokenStream in , int min , int max ) { super ( in ) ; this . min = min ; this . max = max ; } public final Token next ( ) throws IOException { for ( Token token = input . next ( ) ; token != null ; token = input . next ( ) ) { int len = token . termText ( ) . length ( ) ; if ( len >= min && len <= max ) { return token ; } } return null ; } } 	0	['2', '3', '0', '3', '6', '0', '0', '3', '2', '0', '41', '0', '0', '0.75', '0.666666667', '0', '0', '18.5', '1', '0.5', '0']
package org . apache . lucene . queryParser ; public class ParseException extends Exception { public ParseException ( Token currentTokenVal , int [ ] [ ] expectedTokenSequencesVal , String [ ] tokenImageVal ) { super ( "" ) ; specialConstructor = true ; currentToken = currentTokenVal ; expectedTokenSequences = expectedTokenSequencesVal ; tokenImage = tokenImageVal ; } public ParseException ( ) { super ( ) ; specialConstructor = false ; } public ParseException ( String message ) { super ( message ) ; specialConstructor = false ; } protected boolean specialConstructor ; public Token currentToken ; public int [ ] [ ] expectedTokenSequences ; public String [ ] tokenImage ; public String getMessage ( ) { if ( ! specialConstructor ) { return super . getMessage ( ) ; } String expected = "" ; int maxSize = 0 ; for ( int i = 0 ; i < expectedTokenSequences . length ; i ++ ) { if ( maxSize < expectedTokenSequences [ i ] . length ) { maxSize = expectedTokenSequences [ i ] . length ; } for ( int j = 0 ; j < expectedTokenSequences [ i ] . length ; j ++ ) { expected += tokenImage [ expectedTokenSequences [ i ] [ j ] ] + " " ; } if ( expectedTokenSequences [ i ] [ expectedTokenSequences [ i ] . length - 1 ] != 0 ) { expected += "..." ; } expected += eol + "    " ; } String retval = "Encountered \"" ; Token tok = currentToken . next ; for ( int i = 0 ; i < maxSize ; i ++ ) { if ( i != 0 ) retval += " " ; if ( tok . kind == 0 ) { retval += tokenImage [ 0 ] ; break ; } retval += add_escapes ( tok . image ) ; tok = tok . next ; } retval += "\" at line " + currentToken . next . beginLine + ", column " + currentToken . next . beginColumn ; retval += "." + eol ; if ( expectedTokenSequences . length == 1 ) { retval += "Was expecting:" + eol + "    " ; } else { retval += "Was expecting one of:" + eol + "    " ; } retval += expected ; return retval ; } protected String eol = System . getProperty ( "line.separator" , "\n" ) ; protected String add_escapes ( String str ) { StringBuffer retval = new StringBuffer ( ) ; char ch ; for ( int i = 0 ; i < str . length ( ) ; i ++ ) { switch ( str . charAt ( i ) ) { case 0 : continue ; case '\b' : retval . append ( "\\b" ) ; continue ; case '\t' : retval . append ( "\\t" ) ; continue ; case '\n' : retval . append ( "\\n" ) ; continue ; case '\f' : retval . append ( "\\f" ) ; continue ; case '\r' : retval . append ( "\\r" ) ; continue ; case '\"' : retval . append ( "\\\"" ) ; continue ; case '\'' : retval . append ( "\\\'" ) ; continue ; case '\\' : retval . append ( "\\\\" ) ; continue ; default : if ( ( ch = str . charAt ( i ) ) < 0x20 || ch > 0x7e ) { String s = "0000" + Integer . toString ( ch , 16 ) ; retval . append ( "\\u" + s . substring ( s . length ( ) - 4 , s . length ( ) ) ) ; } else { retval . append ( ch ) ; } continue ; } } return retval . toString ( ) ; } } 	0	['5', '3', '0', '3', '18', '0', '2', '1', '4', '0.55', '387', '0.4', '1', '0.866666667', '0.4', '1', '1', '75.4', '14', '4.8', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; public interface Weight extends java . io . Serializable { Query getQuery ( ) ; float getValue ( ) ; float sumOfSquaredWeights ( ) throws IOException ; void normalize ( float norm ) ; Scorer scorer ( IndexReader reader ) throws IOException ; Explanation explain ( IndexReader reader , int doc ) throws IOException ; } 	0	['6', '1', '0', '40', '6', '15', '37', '4', '6', '2', '6', '0', '0', '0', '0.416666667', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search ; import java . io . IOException ; public abstract class Scorer { private Similarity similarity ; protected Scorer ( Similarity similarity ) { this . similarity = similarity ; } public Similarity getSimilarity ( ) { return this . similarity ; } public void score ( HitCollector hc ) throws IOException { while ( next ( ) ) { hc . collect ( doc ( ) , score ( ) ) ; } } protected boolean score ( HitCollector hc , int max ) throws IOException { while ( doc ( ) < max ) { hc . collect ( doc ( ) , score ( ) ) ; if ( ! next ( ) ) return false ; } return true ; } public abstract boolean next ( ) throws IOException ; public abstract int doc ( ) ; public abstract float score ( ) throws IOException ; public abstract boolean skipTo ( int target ) throws IOException ; public abstract Explanation explain ( int doc ) throws IOException ; } 	0	['9', '1', '15', '33', '11', '34', '30', '3', '7', '0.875', '47', '1', '1', '0', '0.416666667', '0', '0', '4.111111111', '1', '0.8889', '0']
package org . apache . lucene . analysis ; import java . io . File ; import java . io . IOException ; import java . io . Reader ; import java . util . Set ; public final class StopAnalyzer extends Analyzer { private Set stopWords ; public static final String [ ] ENGLISH_STOP_WORDS = { "a" , "an" , "and" , "are" , "as" , "at" , "be" , "but" , "by" , "for" , "if" , "in" , "into" , "is" , "it" , "no" , "not" , "of" , "on" , "or" , "s" , "such" , "t" , "that" , "the" , "their" , "then" , "there" , "these" , "they" , "this" , "to" , "was" , "will" , "with" } ; public StopAnalyzer ( ) { stopWords = StopFilter . makeStopSet ( ENGLISH_STOP_WORDS ) ; } public StopAnalyzer ( Set stopWords ) { this . stopWords = stopWords ; } public StopAnalyzer ( String [ ] stopWords ) { this . stopWords = StopFilter . makeStopSet ( stopWords ) ; } public StopAnalyzer ( File stopwordsFile ) throws IOException { stopWords = WordlistLoader . getWordSet ( stopwordsFile ) ; } public StopAnalyzer ( Reader stopwords ) throws IOException { stopWords = WordlistLoader . getWordSet ( stopwords ) ; } public TokenStream tokenStream ( String fieldName , Reader reader ) { return new StopFilter ( new LowerCaseTokenizer ( reader ) , stopWords ) ; } } 	0	['7', '2', '0', '6', '13', '0', '1', '5', '6', '0.5', '197', '0.5', '0', '0.666666667', '0.333333333', '0', '0', '26.85714286', '1', '0.1429', '0']
package org . apache . lucene . search ; import java . io . Serializable ; import java . util . Locale ; public class SortField implements Serializable { public static final int SCORE = 0 ; public static final int DOC = 1 ; public static final int AUTO = 2 ; public static final int STRING = 3 ; public static final int INT = 4 ; public static final int FLOAT = 5 ; public static final int CUSTOM = 9 ; public static final SortField FIELD_SCORE = new SortField ( null , SCORE ) ; public static final SortField FIELD_DOC = new SortField ( null , DOC ) ; private String field ; private int type = AUTO ; private Locale locale ; boolean reverse = false ; private SortComparatorSource factory ; public SortField ( String field ) { this . field = field . intern ( ) ; } public SortField ( String field , boolean reverse ) { this . field = field . intern ( ) ; this . reverse = reverse ; } public SortField ( String field , int type ) { this . field = ( field != null ) ? field . intern ( ) : field ; this . type = type ; } public SortField ( String field , int type , boolean reverse ) { this . field = ( field != null ) ? field . intern ( ) : field ; this . type = type ; this . reverse = reverse ; } public SortField ( String field , Locale locale ) { this . field = field . intern ( ) ; this . type = STRING ; this . locale = locale ; } public SortField ( String field , Locale locale , boolean reverse ) { this . field = field . intern ( ) ; this . type = STRING ; this . locale = locale ; this . reverse = reverse ; } public SortField ( String field , SortComparatorSource comparator ) { this . field = ( field != null ) ? field . intern ( ) : field ; this . type = CUSTOM ; this . factory = comparator ; } public SortField ( String field , SortComparatorSource comparator , boolean reverse ) { this . field = ( field != null ) ? field . intern ( ) : field ; this . type = CUSTOM ; this . reverse = reverse ; this . factory = comparator ; } public String getField ( ) { return field ; } public int getType ( ) { return type ; } public Locale getLocale ( ) { return locale ; } public boolean getReverse ( ) { return reverse ; } public SortComparatorSource getFactory ( ) { return factory ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; switch ( type ) { case SCORE : buffer . append ( "<score>" ) ; break ; case DOC : buffer . append ( "<doc>" ) ; break ; case CUSTOM : buffer . append ( "<custom:\"" + field + "\": " + factory + ">" ) ; break ; default : buffer . append ( "\"" + field + "\"" ) ; break ; } if ( locale != null ) buffer . append ( "(" + locale + ")" ) ; if ( reverse ) buffer . append ( '!' ) ; return buffer . toString ( ) ; } } 	0	['15', '1', '0', '9', '22', '0', '8', '1', '14', '0.852040816', '297', '0.285714286', '3', '0', '0.380952381', '0', '0', '17.86666667', '7', '0.8', '0']
package org . apache . lucene . queryParser ; import java . io . * ; public final class FastCharStream implements CharStream { char [ ] buffer = null ; int bufferLength = 0 ; int bufferPosition = 0 ; int tokenStart = 0 ; int bufferStart = 0 ; Reader input ; public FastCharStream ( Reader r ) { input = r ; } public final char readChar ( ) throws IOException { if ( bufferPosition >= bufferLength ) refill ( ) ; return buffer [ bufferPosition ++ ] ; } private final void refill ( ) throws IOException { int newPosition = bufferLength - tokenStart ; if ( tokenStart == 0 ) { if ( buffer == null ) { buffer = new char [ 2048 ] ; } else if ( bufferLength == buffer . length ) { char [ ] newBuffer = new char [ buffer . length * 2 ] ; System . arraycopy ( buffer , 0 , newBuffer , 0 , bufferLength ) ; buffer = newBuffer ; } } else { System . arraycopy ( buffer , tokenStart , buffer , 0 , newPosition ) ; } bufferLength = newPosition ; bufferPosition = newPosition ; bufferStart += tokenStart ; tokenStart = 0 ; int charsRead = input . read ( buffer , newPosition , buffer . length - newPosition ) ; if ( charsRead == - 1 ) throw new IOException ( "read past eof" ) ; else bufferLength += charsRead ; } public final char BeginToken ( ) throws IOException { tokenStart = bufferPosition ; return readChar ( ) ; } public final void backup ( int amount ) { bufferPosition -= amount ; } public final String GetImage ( ) { return new String ( buffer , tokenStart , bufferPosition - tokenStart ) ; } public final char [ ] GetSuffix ( int len ) { char [ ] value = new char [ len ] ; System . arraycopy ( buffer , bufferPosition - len , value , 0 , len ) ; return value ; } public final void Done ( ) { try { input . close ( ) ; } catch ( IOException e ) { System . err . println ( "Caught: " + e + "; ignoring." ) ; } } public final int getColumn ( ) { return bufferStart + bufferPosition ; } public final int getLine ( ) { return 1 ; } public final int getEndColumn ( ) { return bufferStart + bufferPosition ; } public final int getEndLine ( ) { return 1 ; } public final int getBeginColumn ( ) { return bufferStart + tokenStart ; } public final int getBeginLine ( ) { return 1 ; } } 	0	['14', '1', '0', '2', '25', '3', '1', '1', '13', '0.602564103', '237', '0', '0', '0', '0.404761905', '0', '0', '15.5', '1', '0.9286', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; public class WildcardTermEnum extends FilteredTermEnum { Term searchTerm ; String field = "" ; String text = "" ; String pre = "" ; int preLen = 0 ; boolean endEnum = false ; public WildcardTermEnum ( IndexReader reader , Term term ) throws IOException { super ( ) ; searchTerm = term ; field = searchTerm . field ( ) ; text = searchTerm . text ( ) ; int sidx = text . indexOf ( WILDCARD_STRING ) ; int cidx = text . indexOf ( WILDCARD_CHAR ) ; int idx = sidx ; if ( idx == - 1 ) { idx = cidx ; } else if ( cidx >= 0 ) { idx = Math . min ( idx , cidx ) ; } pre = searchTerm . text ( ) . substring ( 0 , idx ) ; preLen = pre . length ( ) ; text = text . substring ( preLen ) ; setEnum ( reader . terms ( new Term ( searchTerm . field ( ) , pre ) ) ) ; } protected final boolean termCompare ( Term term ) { if ( field == term . field ( ) ) { String searchText = term . text ( ) ; if ( searchText . startsWith ( pre ) ) { return wildcardEquals ( text , 0 , searchText , preLen ) ; } } endEnum = true ; return false ; } public final float difference ( ) { return 1.0f ; } public final boolean endEnum ( ) { return endEnum ; } public static final char WILDCARD_STRING = '*' ; public static final char WILDCARD_CHAR = '?' ; public static final boolean wildcardEquals ( String pattern , int patternIdx , String string , int stringIdx ) { int p = patternIdx ; for ( int s = stringIdx ; ; ++ p , ++ s ) { boolean sEnd = ( s >= string . length ( ) ) ; boolean pEnd = ( p >= pattern . length ( ) ) ; if ( sEnd ) { boolean justWildcardsLeft = true ; int wildcardSearchPos = p ; while ( wildcardSearchPos < pattern . length ( ) && justWildcardsLeft ) { char wildchar = pattern . charAt ( wildcardSearchPos ) ; if ( wildchar != WILDCARD_CHAR && wildchar != WILDCARD_STRING ) { justWildcardsLeft = false ; } else { if ( wildchar == WILDCARD_CHAR ) { return false ; } wildcardSearchPos ++ ; } } if ( justWildcardsLeft ) { return true ; } } if ( sEnd || pEnd ) { break ; } if ( pattern . charAt ( p ) == WILDCARD_CHAR ) { continue ; } if ( pattern . charAt ( p ) == WILDCARD_STRING ) { ++ p ; for ( int i = string . length ( ) ; i >= s ; -- i ) { if ( wildcardEquals ( pattern , p , string , i ) ) { return true ; } } break ; } if ( pattern . charAt ( p ) != string . charAt ( s ) ) { break ; } } return false ; } public void close ( ) throws IOException { super . close ( ) ; searchTerm = null ; field = null ; text = null ; } } 	0	['6', '3', '0', '5', '20', '5', '1', '4', '5', '0.825', '247', '0', '1', '0.722222222', '0.333333333', '1', '4', '38.83333333', '16', '3.6667', '0']
package org . apache . lucene . search ; import java . io . Serializable ; public class Sort implements Serializable { public static final Sort RELEVANCE = new Sort ( ) ; public static final Sort INDEXORDER = new Sort ( SortField . FIELD_DOC ) ; SortField [ ] fields ; public Sort ( ) { this ( new SortField [ ] { SortField . FIELD_SCORE , SortField . FIELD_DOC } ) ; } public Sort ( String field ) { setSort ( field , false ) ; } public Sort ( String field , boolean reverse ) { setSort ( field , reverse ) ; } public Sort ( String [ ] fields ) { setSort ( fields ) ; } public Sort ( SortField field ) { setSort ( field ) ; } public Sort ( SortField [ ] fields ) { setSort ( fields ) ; } public final void setSort ( String field ) { setSort ( field , false ) ; } public void setSort ( String field , boolean reverse ) { SortField [ ] nfields = new SortField [ ] { new SortField ( field , SortField . AUTO , reverse ) , SortField . FIELD_DOC } ; fields = nfields ; } public void setSort ( String [ ] fieldnames ) { final int n = fieldnames . length ; SortField [ ] nfields = new SortField [ n ] ; for ( int i = 0 ; i < n ; ++ i ) { nfields [ i ] = new SortField ( fieldnames [ i ] , SortField . AUTO ) ; } fields = nfields ; } public void setSort ( SortField field ) { this . fields = new SortField [ ] { field } ; } public void setSort ( SortField [ ] fields ) { this . fields = fields ; } public SortField [ ] getSort ( ) { return fields ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { buffer . append ( fields [ i ] . toString ( ) ) ; if ( ( i + 1 ) < fields . length ) buffer . append ( ',' ) ; } return buffer . toString ( ) ; } } 	0	['14', '1', '0', '12', '22', '61', '11', '1', '13', '0.692307692', '175', '0', '3', '0', '0.320512821', '0', '0', '11.28571429', '3', '0.7143', '0']
package org . apache . lucene . search ; public class DefaultSimilarity extends Similarity { public float lengthNorm ( String fieldName , int numTerms ) { return ( float ) ( 1.0 / Math . sqrt ( numTerms ) ) ; } public float queryNorm ( float sumOfSquaredWeights ) { return ( float ) ( 1.0 / Math . sqrt ( sumOfSquaredWeights ) ) ; } public float tf ( float freq ) { return ( float ) Math . sqrt ( freq ) ; } public float sloppyFreq ( int distance ) { return 1.0f / ( distance + 1 ) ; } public float idf ( int docFreq , int numDocs ) { return ( float ) ( Math . log ( numDocs / ( double ) ( docFreq + 1 ) ) + 1.0 ) ; } public float coord ( int overlap , int maxOverlap ) { return overlap / ( float ) maxOverlap ; } } 	0	['7', '2', '0', '3', '10', '21', '3', '1', '7', '2', '54', '0', '0', '0.7', '0.5', '1', '2', '6.714285714', '1', '0.8571', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; public class ConstantScoreRangeQuery extends Query { private final String fieldName ; private final String lowerVal ; private final String upperVal ; private final boolean includeLower ; private final boolean includeUpper ; public ConstantScoreRangeQuery ( String fieldName , String lowerVal , String upperVal , boolean includeLower , boolean includeUpper ) { if ( lowerVal == null ) { includeLower = true ; } else if ( includeLower && lowerVal . equals ( "" ) ) { lowerVal = null ; } if ( upperVal == null ) { includeUpper = true ; } this . fieldName = fieldName . intern ( ) ; this . lowerVal = lowerVal ; this . upperVal = upperVal ; this . includeLower = includeLower ; this . includeUpper = includeUpper ; } public String getField ( ) { return fieldName ; } public String getLowerVal ( ) { return lowerVal ; } public String getUpperVal ( ) { return upperVal ; } public boolean includesLower ( ) { return includeLower ; } public boolean includesUpper ( ) { return includeUpper ; } public Query rewrite ( IndexReader reader ) throws IOException { RangeFilter rangeFilt = new RangeFilter ( fieldName , lowerVal != null ? lowerVal : "" , upperVal , lowerVal == "" ? false : includeLower , upperVal == null ? false : includeUpper ) ; Query q = new ConstantScoreQuery ( rangeFilt ) ; q . setBoost ( getBoost ( ) ) ; return q ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! getField ( ) . equals ( field ) ) { buffer . append ( getField ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( includeLower ? '[' : '{' ) ; buffer . append ( lowerVal != null ? lowerVal : "*" ) ; buffer . append ( " TO " ) ; buffer . append ( upperVal != null ? upperVal : "*" ) ; buffer . append ( includeUpper ? ']' : '}' ) ; if ( getBoost ( ) != 1.0f ) { buffer . append ( "^" ) ; buffer . append ( Float . toString ( getBoost ( ) ) ) ; } return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof ConstantScoreRangeQuery ) ) return false ; ConstantScoreRangeQuery other = ( ConstantScoreRangeQuery ) o ; if ( this . fieldName != other . fieldName || this . includeLower != other . includeLower || this . includeUpper != other . includeUpper ) { return false ; } if ( this . lowerVal != null ? ! this . lowerVal . equals ( other . lowerVal ) : other . lowerVal != null ) return false ; if ( this . upperVal != null ? ! this . upperVal . equals ( other . upperVal ) : other . upperVal != null ) return false ; return this . getBoost ( ) == other . getBoost ( ) ; } public int hashCode ( ) { int h = Float . floatToIntBits ( getBoost ( ) ) ^ fieldName . hashCode ( ) ; h ^= lowerVal != null ? lowerVal . hashCode ( ) : 0x965a965a ; h ^= ( h << 17 ) | ( h > > > 16 ) ; h ^= ( upperVal != null ? ( upperVal . hashCode ( ) ) : 0x5a695a69 ) ; h ^= ( includeLower ? 0x665599aa : 0 ) ^ ( includeUpper ? 0x99aa5566 : 0 ) ; return h ; } } 	0	['10', '2', '0', '5', '24', '0', '0', '5', '10', '0.444444444', '313', '1', '0', '0.571428571', '0.3', '2', '3', '29.8', '13', '3.1', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; public abstract class TokenFilter extends TokenStream { protected TokenStream input ; protected TokenFilter ( TokenStream input ) { this . input = input ; } public void close ( ) throws IOException { input . close ( ) ; } } 	0	['2', '2', '6', '7', '4', '0', '6', '1', '1', '0', '13', '1', '1', '0.666666667', '0.75', '0', '0', '5', '1', '0.5', '0']
package org . apache . lucene . store ; import java . io . IOException ; import java . io . File ; import java . io . RandomAccessFile ; import java . nio . ByteBuffer ; import java . nio . channels . FileChannel ; import java . nio . channels . FileChannel . MapMode ; public class MMapDirectory extends FSDirectory { private static class MMapIndexInput extends IndexInput { private ByteBuffer buffer ; private final long length ; private MMapIndexInput ( RandomAccessFile raf ) throws IOException { this . length = raf . length ( ) ; this . buffer = raf . getChannel ( ) . map ( MapMode . READ_ONLY , 0 , length ) ; } public byte readByte ( ) throws IOException { return buffer . get ( ) ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { buffer . get ( b , offset , len ) ; } public long getFilePointer ( ) { return buffer . position ( ) ; } public void seek ( long pos ) throws IOException { buffer . position ( ( int ) pos ) ; } public long length ( ) { return length ; } public Object clone ( ) { MMapIndexInput clone = ( MMapIndexInput ) super . clone ( ) ; clone . buffer = buffer . duplicate ( ) ; return clone ; } public void close ( ) throws IOException { } } private static class MultiMMapIndexInput extends IndexInput { private ByteBuffer [ ] buffers ; private int [ ] bufSizes ; private final long length ; private int curBufIndex ; private final int maxBufSize ; private ByteBuffer curBuf ; private int curAvail ; public MultiMMapIndexInput ( RandomAccessFile raf , int maxBufSize ) throws IOException { this . length = raf . length ( ) ; this . maxBufSize = maxBufSize ; if ( maxBufSize <= 0 ) throw new IllegalArgumentException ( "Non positive maxBufSize: " + maxBufSize ) ; if ( ( length / maxBufSize ) > Integer . MAX_VALUE ) throw new IllegalArgumentException ( "RandomAccessFile too big for maximum buffer size: " + raf . toString ( ) ) ; int nrBuffers = ( int ) ( length / maxBufSize ) ; if ( ( nrBuffers * maxBufSize ) < length ) nrBuffers ++ ; this . buffers = new ByteBuffer [ nrBuffers ] ; this . bufSizes = new int [ nrBuffers ] ; long bufferStart = 0 ; FileChannel rafc = raf . getChannel ( ) ; for ( int bufNr = 0 ; bufNr < nrBuffers ; bufNr ++ ) { int bufSize = ( length > ( bufferStart + maxBufSize ) ) ? maxBufSize : ( int ) ( length - bufferStart ) ; this . buffers [ bufNr ] = rafc . map ( MapMode . READ_ONLY , bufferStart , bufSize ) ; this . bufSizes [ bufNr ] = bufSize ; bufferStart += bufSize ; } seek ( 0L ) ; } public byte readByte ( ) throws IOException { if ( curAvail == 0 ) { curBufIndex ++ ; curBuf = buffers [ curBufIndex ] ; curBuf . position ( 0 ) ; curAvail = bufSizes [ curBufIndex ] ; } curAvail -- ; return curBuf . get ( ) ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { while ( len > curAvail ) { curBuf . get ( b , offset , curAvail ) ; len -= curAvail ; offset += curAvail ; curBufIndex ++ ; curBuf = buffers [ curBufIndex ] ; curBuf . position ( 0 ) ; curAvail = bufSizes [ curBufIndex ] ; } curBuf . get ( b , offset , len ) ; curAvail -= len ; } public long getFilePointer ( ) { return ( curBufIndex * ( long ) maxBufSize ) + curBuf . position ( ) ; } public void seek ( long pos ) throws IOException { curBufIndex = ( int ) ( pos / maxBufSize ) ; curBuf = buffers [ curBufIndex ] ; int bufOffset = ( int ) ( pos - ( curBufIndex * maxBufSize ) ) ; curBuf . position ( bufOffset ) ; curAvail = bufSizes [ curBufIndex ] - bufOffset ; } public long length ( ) { return length ; } public Object clone ( ) { MultiMMapIndexInput clone = ( MultiMMapIndexInput ) super . clone ( ) ; clone . buffers = new ByteBuffer [ buffers . length ] ; for ( int bufNr = 0 ; bufNr < buffers . length ; bufNr ++ ) { clone . buffers [ bufNr ] = buffers [ bufNr ] . duplicate ( ) ; } try { clone . seek ( getFilePointer ( ) ) ; } catch ( IOException ioe ) { RuntimeException newException = new RuntimeException ( ioe ) ; newException . initCause ( ioe ) ; throw newException ; } ; return clone ; } public void close ( ) throws IOException { } } private final int MAX_BBUF = Integer . MAX_VALUE ; public IndexInput openInput ( String name ) throws IOException { File f = new File ( getFile ( ) , name ) ; RandomAccessFile raf = new RandomAccessFile ( f , "r" ) ; try { return ( raf . length ( ) <= MAX_BBUF ) ? ( IndexInput ) new MMapIndexInput ( raf ) : ( IndexInput ) new MultiMMapIndexInput ( raf , MAX_BBUF ) ; } finally { raf . close ( ) ; } } } 	0	['2', '3', '0', '5', '10', '1', '0', '5', '2', '1', '48', '1', '0', '0.972222222', '0.75', '0', '0', '22.5', '1', '0.5', '0']
package org . apache . lucene . index ; import java . io . IOException ; final class SegmentMergeInfo { Term term ; int base ; TermEnum termEnum ; IndexReader reader ; private TermPositions postings ; private int [ ] docMap ; SegmentMergeInfo ( int b , TermEnum te , IndexReader r ) throws IOException { base = b ; reader = r ; termEnum = te ; term = te . term ( ) ; } int [ ] getDocMap ( ) { if ( docMap == null ) { if ( reader . hasDeletions ( ) ) { int maxDoc = reader . maxDoc ( ) ; docMap = new int [ maxDoc ] ; int j = 0 ; for ( int i = 0 ; i < maxDoc ; i ++ ) { if ( reader . isDeleted ( i ) ) docMap [ i ] = - 1 ; else docMap [ i ] = j ++ ; } } } return docMap ; } TermPositions getPositions ( ) throws IOException { if ( postings == null ) { postings = reader . termPositions ( ) ; } return postings ; } final boolean next ( ) throws IOException { if ( termEnum . next ( ) ) { term = termEnum . term ( ) ; return true ; } else { term = null ; return false ; } } final void close ( ) throws IOException { termEnum . close ( ) ; if ( postings != null ) { postings . close ( ) ; } } } 	0	['5', '1', '0', '7', '14', '0', '3', '4', '0', '0.75', '108', '0.333333333', '4', '0', '0.4', '0', '0', '19.4', '5', '1.6', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . Set ; import java . util . Vector ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermPositions ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; public class PhraseQuery extends Query { private String field ; private Vector terms = new Vector ( ) ; private Vector positions = new Vector ( ) ; private int slop = 0 ; public PhraseQuery ( ) { } public void setSlop ( int s ) { slop = s ; } public int getSlop ( ) { return slop ; } public void add ( Term term ) { int position = 0 ; if ( positions . size ( ) > 0 ) position = ( ( Integer ) positions . lastElement ( ) ) . intValue ( ) + 1 ; add ( term , position ) ; } public void add ( Term term , int position ) { if ( terms . size ( ) == 0 ) field = term . field ( ) ; else if ( term . field ( ) != field ) throw new IllegalArgumentException ( "All phrase terms must be in the same field: " + term ) ; terms . addElement ( term ) ; positions . addElement ( new Integer ( position ) ) ; } public Term [ ] getTerms ( ) { return ( Term [ ] ) terms . toArray ( new Term [ 0 ] ) ; } public int [ ] getPositions ( ) { int [ ] result = new int [ positions . size ( ) ] ; for ( int i = 0 ; i < positions . size ( ) ; i ++ ) result [ i ] = ( ( Integer ) positions . elementAt ( i ) ) . intValue ( ) ; return result ; } private class PhraseWeight implements Weight { private Similarity similarity ; private float value ; private float idf ; private float queryNorm ; private float queryWeight ; public PhraseWeight ( Searcher searcher ) throws IOException { this . similarity = getSimilarity ( searcher ) ; idf = similarity . idf ( terms , searcher ) ; } public String toString ( ) { return "weight(" + PhraseQuery . this + ")" ; } public Query getQuery ( ) { return PhraseQuery . this ; } public float getValue ( ) { return value ; } public float sumOfSquaredWeights ( ) { queryWeight = idf * getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float queryNorm ) { this . queryNorm = queryNorm ; queryWeight *= queryNorm ; value = queryWeight * idf ; } public Scorer scorer ( IndexReader reader ) throws IOException { if ( terms . size ( ) == 0 ) return null ; TermPositions [ ] tps = new TermPositions [ terms . size ( ) ] ; for ( int i = 0 ; i < terms . size ( ) ; i ++ ) { TermPositions p = reader . termPositions ( ( Term ) terms . elementAt ( i ) ) ; if ( p == null ) return null ; tps [ i ] = p ; } if ( slop == 0 ) return new ExactPhraseScorer ( this , tps , getPositions ( ) , similarity , reader . norms ( field ) ) ; else return new SloppyPhraseScorer ( this , tps , getPositions ( ) , similarity , slop , reader . norms ( field ) ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { Explanation result = new Explanation ( ) ; result . setDescription ( "weight(" + getQuery ( ) + " in " + doc + "), product of:" ) ; StringBuffer docFreqs = new StringBuffer ( ) ; StringBuffer query = new StringBuffer ( ) ; query . append ( '\"' ) ; for ( int i = 0 ; i < terms . size ( ) ; i ++ ) { if ( i != 0 ) { docFreqs . append ( " " ) ; query . append ( " " ) ; } Term term = ( Term ) terms . elementAt ( i ) ; docFreqs . append ( term . text ( ) ) ; docFreqs . append ( "=" ) ; docFreqs . append ( reader . docFreq ( term ) ) ; query . append ( term . text ( ) ) ; } query . append ( '\"' ) ; Explanation idfExpl = new Explanation ( idf , "idf(" + field + ": " + docFreqs + ")" ) ; Explanation queryExpl = new Explanation ( ) ; queryExpl . setDescription ( "queryWeight(" + getQuery ( ) + "), product of:" ) ; Explanation boostExpl = new Explanation ( getBoost ( ) , "boost" ) ; if ( getBoost ( ) != 1.0f ) queryExpl . addDetail ( boostExpl ) ; queryExpl . addDetail ( idfExpl ) ; Explanation queryNormExpl = new Explanation ( queryNorm , "queryNorm" ) ; queryExpl . addDetail ( queryNormExpl ) ; queryExpl . setValue ( boostExpl . getValue ( ) * idfExpl . getValue ( ) * queryNormExpl . getValue ( ) ) ; result . addDetail ( queryExpl ) ; Explanation fieldExpl = new Explanation ( ) ; fieldExpl . setDescription ( "fieldWeight(" + field + ":" + query + " in " + doc + "), product of:" ) ; Explanation tfExpl = scorer ( reader ) . explain ( doc ) ; fieldExpl . addDetail ( tfExpl ) ; fieldExpl . addDetail ( idfExpl ) ; Explanation fieldNormExpl = new Explanation ( ) ; byte [ ] fieldNorms = reader . norms ( field ) ; float fieldNorm = fieldNorms != null ? Similarity . decodeNorm ( fieldNorms [ doc ] ) : 0.0f ; fieldNormExpl . setValue ( fieldNorm ) ; fieldNormExpl . setDescription ( "fieldNorm(field=" + field + ", doc=" + doc + ")" ) ; fieldExpl . addDetail ( fieldNormExpl ) ; fieldExpl . setValue ( tfExpl . getValue ( ) * idfExpl . getValue ( ) * fieldNormExpl . getValue ( ) ) ; result . addDetail ( fieldExpl ) ; result . setValue ( queryExpl . getValue ( ) * fieldExpl . getValue ( ) ) ; if ( queryExpl . getValue ( ) == 1.0f ) return fieldExpl ; return result ; } } protected Weight createWeight ( Searcher searcher ) throws IOException { if ( terms . size ( ) == 1 ) { Term term = ( Term ) terms . elementAt ( 0 ) ; Query termQuery = new TermQuery ( term ) ; termQuery . setBoost ( getBoost ( ) ) ; return termQuery . createWeight ( searcher ) ; } return new PhraseWeight ( searcher ) ; } public void extractTerms ( Set queryTerms ) { queryTerms . addAll ( terms ) ; } public String toString ( String f ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! field . equals ( f ) ) { buffer . append ( field ) ; buffer . append ( ":" ) ; } buffer . append ( "\"" ) ; for ( int i = 0 ; i < terms . size ( ) ; i ++ ) { buffer . append ( ( ( Term ) terms . elementAt ( i ) ) . text ( ) ) ; if ( i != terms . size ( ) - 1 ) buffer . append ( " " ) ; } buffer . append ( "\"" ) ; if ( slop != 0 ) { buffer . append ( "~" ) ; buffer . append ( slop ) ; } buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof PhraseQuery ) ) return false ; PhraseQuery other = ( PhraseQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && ( this . slop == other . slop ) && this . terms . equals ( other . terms ) && this . positions . equals ( other . positions ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ slop ^ terms . hashCode ( ) ^ positions . hashCode ( ) ; } } 	0	['15', '2', '0', '9', '43', '0', '3', '7', '11', '0.589285714', '302', '1', '0', '0.461538462', '0.191666667', '2', '3', '18.86666667', '6', '1.8', '0']
package org . apache . lucene . queryParser ; public class Token { public int kind ; public int beginLine , beginColumn , endLine , endColumn ; public String image ; public Token next ; public Token specialToken ; public String toString ( ) { return image ; } public static final Token newToken ( int ofKind ) { switch ( ofKind ) { default : return new Token ( ) ; } } } 	0	['3', '1', '0', '4', '4', '3', '4', '0', '3', '1.4375', '23', '0', '2', '0', '0.5', '0', '0', '4', '2', '1', '0']
package org . apache . lucene . util ; public abstract class StringHelper { public static final int stringDifference ( String s1 , String s2 ) { int len1 = s1 . length ( ) ; int len2 = s2 . length ( ) ; int len = len1 < len2 ? len1 : len2 ; for ( int i = 0 ; i < len ; i ++ ) { if ( s1 . charAt ( i ) != s2 . charAt ( i ) ) { return i ; } } return len ; } private StringHelper ( ) { } } 	0	['2', '1', '0', '2', '5', '1', '2', '0', '1', '2', '36', '0', '0', '0', '0.5', '0', '0', '17', '4', '2', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; public abstract class Analyzer { public abstract TokenStream tokenStream ( String fieldName , Reader reader ) ; public int getPositionIncrementGap ( String fieldName ) { return 0 ; } } 	0	['3', '1', '6', '13', '4', '3', '12', '1', '3', '2', '8', '0', '0', '0', '0.666666667', '0', '0', '1.666666667', '1', '0.6667', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexInput ; final class TermBuffer implements Cloneable { private static final char [ ] NO_CHARS = new char [ 0 ] ; private String field ; private char [ ] text = NO_CHARS ; private int textLength ; private Term term ; public final int compareTo ( TermBuffer other ) { if ( field == other . field ) return compareChars ( text , textLength , other . text , other . textLength ) ; else return field . compareTo ( other . field ) ; } private static final int compareChars ( char [ ] v1 , int len1 , char [ ] v2 , int len2 ) { int end = Math . min ( len1 , len2 ) ; for ( int k = 0 ; k < end ; k ++ ) { char c1 = v1 [ k ] ; char c2 = v2 [ k ] ; if ( c1 != c2 ) { return c1 - c2 ; } } return len1 - len2 ; } private final void setTextLength ( int newLength ) { if ( text . length < newLength ) { char [ ] newText = new char [ newLength ] ; System . arraycopy ( text , 0 , newText , 0 , textLength ) ; text = newText ; } textLength = newLength ; } public final void read ( IndexInput input , FieldInfos fieldInfos ) throws IOException { this . term = null ; int start = input . readVInt ( ) ; int length = input . readVInt ( ) ; int totalLength = start + length ; setTextLength ( totalLength ) ; input . readChars ( this . text , start , length ) ; this . field = fieldInfos . fieldName ( input . readVInt ( ) ) ; } public final void set ( Term term ) { if ( term == null ) { reset ( ) ; return ; } setTextLength ( term . text ( ) . length ( ) ) ; term . text ( ) . getChars ( 0 , term . text ( ) . length ( ) , text , 0 ) ; this . field = term . field ( ) ; this . term = term ; } public final void set ( TermBuffer other ) { setTextLength ( other . textLength ) ; System . arraycopy ( other . text , 0 , text , 0 , textLength ) ; this . field = other . field ; this . term = other . term ; } public void reset ( ) { this . field = null ; this . textLength = 0 ; this . term = null ; } public Term toTerm ( ) { if ( field == null ) return null ; if ( term == null ) term = new Term ( field , new String ( text , 0 , textLength ) , false ) ; return term ; } protected Object clone ( ) { TermBuffer clone = null ; try { clone = ( TermBuffer ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . text = new char [ text . length ] ; System . arraycopy ( text , 0 , clone . text , 0 , textLength ) ; return clone ; } } 	0	['11', '1', '0', '4', '25', '0', '1', '3', '6', '0.52', '241', '1', '1', '0', '0.242857143', '0', '0', '20.45454545', '3', '1.4545', '0']
package org . apache . lucene . index ; public class TermVectorOffsetInfo { public static final TermVectorOffsetInfo [ ] EMPTY_OFFSET_INFO = new TermVectorOffsetInfo [ 0 ] ; private int startOffset ; private int endOffset ; public TermVectorOffsetInfo ( ) { } public TermVectorOffsetInfo ( int startOffset , int endOffset ) { this . endOffset = endOffset ; this . startOffset = startOffset ; } public int getEndOffset ( ) { return endOffset ; } public void setEndOffset ( int endOffset ) { this . endOffset = endOffset ; } public int getStartOffset ( ) { return startOffset ; } public void setStartOffset ( int startOffset ) { this . startOffset = startOffset ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof TermVectorOffsetInfo ) ) return false ; final TermVectorOffsetInfo termVectorOffsetInfo = ( TermVectorOffsetInfo ) o ; if ( endOffset != termVectorOffsetInfo . endOffset ) return false ; if ( startOffset != termVectorOffsetInfo . startOffset ) return false ; return true ; } public int hashCode ( ) { int result ; result = startOffset ; result = 29 * result + endOffset ; return result ; } } 	0	['9', '1', '0', '7', '10', '2', '7', '0', '8', '0.666666667', '83', '0.666666667', '1', '0', '0.5', '1', '1', '7.888888889', '5', '1.1111', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; public class KeywordAnalyzer extends Analyzer { public TokenStream tokenStream ( String fieldName , final Reader reader ) { return new KeywordTokenizer ( reader ) ; } } 	0	['2', '2', '0', '3', '4', '1', '0', '3', '2', '2', '10', '0', '0', '0.666666667', '0.666666667', '0', '0', '4', '1', '0.5', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Query ; import org . apache . lucene . search . Weight ; import org . apache . lucene . search . Searcher ; public abstract class SpanQuery extends Query { public abstract Spans getSpans ( IndexReader reader ) throws IOException ; public abstract String getField ( ) ; public abstract Collection getTerms ( ) ; protected Weight createWeight ( Searcher searcher ) throws IOException { return new SpanWeight ( this , searcher ) ; } } 	0	['5', '2', '5', '17', '7', '10', '12', '6', '4', '2', '14', '0', '0', '0.75', '0.466666667', '1', '1', '1.8', '1', '0.8', '0']
package org . apache . lucene . analysis . standard ; public interface CharStream { char readChar ( ) throws java . io . IOException ; int getColumn ( ) ; int getLine ( ) ; int getEndColumn ( ) ; int getEndLine ( ) ; int getBeginColumn ( ) ; int getBeginLine ( ) ; void backup ( int amount ) ; char BeginToken ( ) throws java . io . IOException ; String GetImage ( ) ; char [ ] GetSuffix ( int len ) ; void Done ( ) ; } 	0	['12', '1', '0', '3', '12', '66', '3', '0', '12', '2', '12', '0', '0', '0', '0.583333333', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . analysis . standard ; import java . io . * ; public final class FastCharStream implements CharStream { char [ ] buffer = null ; int bufferLength = 0 ; int bufferPosition = 0 ; int tokenStart = 0 ; int bufferStart = 0 ; Reader input ; public FastCharStream ( Reader r ) { input = r ; } public final char readChar ( ) throws IOException { if ( bufferPosition >= bufferLength ) refill ( ) ; return buffer [ bufferPosition ++ ] ; } private final void refill ( ) throws IOException { int newPosition = bufferLength - tokenStart ; if ( tokenStart == 0 ) { if ( buffer == null ) { buffer = new char [ 2048 ] ; } else if ( bufferLength == buffer . length ) { char [ ] newBuffer = new char [ buffer . length * 2 ] ; System . arraycopy ( buffer , 0 , newBuffer , 0 , bufferLength ) ; buffer = newBuffer ; } } else { System . arraycopy ( buffer , tokenStart , buffer , 0 , newPosition ) ; } bufferLength = newPosition ; bufferPosition = newPosition ; bufferStart += tokenStart ; tokenStart = 0 ; int charsRead = input . read ( buffer , newPosition , buffer . length - newPosition ) ; if ( charsRead == - 1 ) throw new IOException ( "read past eof" ) ; else bufferLength += charsRead ; } public final char BeginToken ( ) throws IOException { tokenStart = bufferPosition ; return readChar ( ) ; } public final void backup ( int amount ) { bufferPosition -= amount ; } public final String GetImage ( ) { return new String ( buffer , tokenStart , bufferPosition - tokenStart ) ; } public final char [ ] GetSuffix ( int len ) { char [ ] value = new char [ len ] ; System . arraycopy ( buffer , bufferPosition - len , value , 0 , len ) ; return value ; } public final void Done ( ) { try { input . close ( ) ; } catch ( IOException e ) { System . err . println ( "Caught: " + e + "; ignoring." ) ; } } public final int getColumn ( ) { return bufferStart + bufferPosition ; } public final int getLine ( ) { return 1 ; } public final int getEndColumn ( ) { return bufferStart + bufferPosition ; } public final int getEndLine ( ) { return 1 ; } public final int getBeginColumn ( ) { return bufferStart + tokenStart ; } public final int getBeginLine ( ) { return 1 ; } } 	0	['14', '1', '0', '2', '25', '3', '1', '1', '13', '0.602564103', '237', '0', '0', '0', '0.404761905', '0', '0', '15.5', '1', '0.9286', '0']
package org . apache . lucene . document ; public class LoadFirstFieldSelector implements FieldSelector { public FieldSelectorResult accept ( String fieldName ) { return FieldSelectorResult . LOAD_AND_BREAK ; } } 	0	['2', '1', '0', '2', '3', '1', '0', '2', '2', '2', '7', '0', '0', '0', '0.75', '0', '0', '2.5', '1', '0.5', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import org . apache . lucene . store . IndexInput ; class DefaultSkipListReader extends MultiLevelSkipListReader { private boolean currentFieldStoresPayloads ; private long freqPointer [ ] ; private long proxPointer [ ] ; private int payloadLength [ ] ; private long lastFreqPointer ; private long lastProxPointer ; private int lastPayloadLength ; DefaultSkipListReader ( IndexInput skipStream , int maxSkipLevels , int skipInterval ) { super ( skipStream , maxSkipLevels , skipInterval ) ; freqPointer = new long [ maxSkipLevels ] ; proxPointer = new long [ maxSkipLevels ] ; payloadLength = new int [ maxSkipLevels ] ; } void init ( long skipPointer , long freqBasePointer , long proxBasePointer , int df , boolean storesPayloads ) { super . init ( skipPointer , df ) ; this . currentFieldStoresPayloads = storesPayloads ; lastFreqPointer = freqBasePointer ; lastProxPointer = proxBasePointer ; Arrays . fill ( freqPointer , freqBasePointer ) ; Arrays . fill ( proxPointer , proxBasePointer ) ; Arrays . fill ( payloadLength , 0 ) ; } long getFreqPointer ( ) { return lastFreqPointer ; } long getProxPointer ( ) { return lastProxPointer ; } int getPayloadLength ( ) { return lastPayloadLength ; } protected void seekChild ( int level ) throws IOException { super . seekChild ( level ) ; freqPointer [ level ] = lastFreqPointer ; proxPointer [ level ] = lastProxPointer ; payloadLength [ level ] = lastPayloadLength ; } protected void setLastSkipData ( int level ) { super . setLastSkipData ( level ) ; lastFreqPointer = freqPointer [ level ] ; lastProxPointer = proxPointer [ level ] ; lastPayloadLength = payloadLength [ level ] ; } protected int readSkipData ( int level , IndexInput skipStream ) throws IOException { int delta ; if ( currentFieldStoresPayloads ) { delta = skipStream . readVInt ( ) ; if ( ( delta & 1 ) != 0 ) { payloadLength [ level ] = skipStream . readVInt ( ) ; } delta >>>= 1 ; } else { delta = skipStream . readVInt ( ) ; } freqPointer [ level ] += skipStream . readVInt ( ) ; proxPointer [ level ] += skipStream . readVInt ( ) ; return delta ; } } 	0	['8', '2', '0', '3', '15', '0', '1', '2', '0', '0.571428571', '158', '1', '0', '0.5625', '0.425', '1', '3', '17.875', '1', '0.875', '0']
package org . apache . lucene . search ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . FieldSelector ; import org . apache . lucene . index . CorruptIndexException ; import org . apache . lucene . index . Term ; import java . io . IOException ; import java . util . HashMap ; import java . util . HashSet ; import java . util . Map ; import java . util . Set ; public class MultiSearcher extends Searcher { private static class CachedDfSource extends Searcher { private Map dfMap ; private int maxDoc ; public CachedDfSource ( Map dfMap , int maxDoc , Similarity similarity ) { this . dfMap = dfMap ; this . maxDoc = maxDoc ; setSimilarity ( similarity ) ; } public int docFreq ( Term term ) { int df ; try { df = ( ( Integer ) dfMap . get ( term ) ) . intValue ( ) ; } catch ( NullPointerException e ) { throw new IllegalArgumentException ( "df for term " + term . text ( ) + " not available" ) ; } return df ; } public int [ ] docFreqs ( Term [ ] terms ) { int [ ] result = new int [ terms . length ] ; for ( int i = 0 ; i < terms . length ; i ++ ) { result [ i ] = docFreq ( terms [ i ] ) ; } return result ; } public int maxDoc ( ) { return maxDoc ; } public Query rewrite ( Query query ) { return query ; } public void close ( ) { throw new UnsupportedOperationException ( ) ; } public Document doc ( int i ) { throw new UnsupportedOperationException ( ) ; } public Document doc ( int i , FieldSelector fieldSelector ) { throw new UnsupportedOperationException ( ) ; } public Explanation explain ( Weight weight , int doc ) { throw new UnsupportedOperationException ( ) ; } public void search ( Weight weight , Filter filter , HitCollector results ) { throw new UnsupportedOperationException ( ) ; } public TopDocs search ( Weight weight , Filter filter , int n ) { throw new UnsupportedOperationException ( ) ; } public TopFieldDocs search ( Weight weight , Filter filter , int n , Sort sort ) { throw new UnsupportedOperationException ( ) ; } } private Searchable [ ] searchables ; private int [ ] starts ; private int maxDoc = 0 ; public MultiSearcher ( Searchable [ ] searchables ) throws IOException { this . searchables = searchables ; starts = new int [ searchables . length + 1 ] ; for ( int i = 0 ; i < searchables . length ; i ++ ) { starts [ i ] = maxDoc ; maxDoc += searchables [ i ] . maxDoc ( ) ; } starts [ searchables . length ] = maxDoc ; } public Searchable [ ] getSearchables ( ) { return searchables ; } protected int [ ] getStarts ( ) { return starts ; } public void close ( ) throws IOException { for ( int i = 0 ; i < searchables . length ; i ++ ) searchables [ i ] . close ( ) ; } public int docFreq ( Term term ) throws IOException { int docFreq = 0 ; for ( int i = 0 ; i < searchables . length ; i ++ ) docFreq += searchables [ i ] . docFreq ( term ) ; return docFreq ; } public Document doc ( int n ) throws CorruptIndexException , IOException { int i = subSearcher ( n ) ; return searchables [ i ] . doc ( n - starts [ i ] ) ; } public Document doc ( int n , FieldSelector fieldSelector ) throws CorruptIndexException , IOException { int i = subSearcher ( n ) ; return searchables [ i ] . doc ( n - starts [ i ] , fieldSelector ) ; } public int subSearcher ( int n ) { int lo = 0 ; int hi = searchables . length - 1 ; while ( hi >= lo ) { int mid = ( lo + hi ) > > 1 ; int midValue = starts [ mid ] ; if ( n < midValue ) hi = mid - 1 ; else if ( n > midValue ) lo = mid + 1 ; else { while ( mid + 1 < searchables . length && starts [ mid + 1 ] == midValue ) { mid ++ ; } return mid ; } } return hi ; } public int subDoc ( int n ) { return n - starts [ subSearcher ( n ) ] ; } public int maxDoc ( ) throws IOException { return maxDoc ; } public TopDocs search ( Weight weight , Filter filter , int nDocs ) throws IOException { HitQueue hq = new HitQueue ( nDocs ) ; int totalHits = 0 ; for ( int i = 0 ; i < searchables . length ; i ++ ) { TopDocs docs = searchables [ i ] . search ( weight , filter , nDocs ) ; totalHits += docs . totalHits ; ScoreDoc [ ] scoreDocs = docs . scoreDocs ; for ( int j = 0 ; j < scoreDocs . length ; j ++ ) { ScoreDoc scoreDoc = scoreDocs [ j ] ; scoreDoc . doc += starts [ i ] ; if ( ! hq . insert ( scoreDoc ) ) break ; } } ScoreDoc [ ] scoreDocs = new ScoreDoc [ hq . size ( ) ] ; for ( int i = hq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = ( ScoreDoc ) hq . pop ( ) ; float maxScore = ( totalHits == 0 ) ? Float . NEGATIVE_INFINITY : scoreDocs [ 0 ] . score ; return new TopDocs ( totalHits , scoreDocs , maxScore ) ; } public TopFieldDocs search ( Weight weight , Filter filter , int n , Sort sort ) throws IOException { FieldDocSortedHitQueue hq = null ; int totalHits = 0 ; float maxScore = Float . NEGATIVE_INFINITY ; for ( int i = 0 ; i < searchables . length ; i ++ ) { TopFieldDocs docs = searchables [ i ] . search ( weight , filter , n , sort ) ; if ( hq == null ) hq = new FieldDocSortedHitQueue ( docs . fields , n ) ; totalHits += docs . totalHits ; maxScore = Math . max ( maxScore , docs . getMaxScore ( ) ) ; ScoreDoc [ ] scoreDocs = docs . scoreDocs ; for ( int j = 0 ; j < scoreDocs . length ; j ++ ) { ScoreDoc scoreDoc = scoreDocs [ j ] ; scoreDoc . doc += starts [ i ] ; if ( ! hq . insert ( scoreDoc ) ) break ; } } ScoreDoc [ ] scoreDocs = new ScoreDoc [ hq . size ( ) ] ; for ( int i = hq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = ( ScoreDoc ) hq . pop ( ) ; return new TopFieldDocs ( totalHits , scoreDocs , hq . getFields ( ) , maxScore ) ; } public void search ( Weight weight , Filter filter , final HitCollector results ) throws IOException { for ( int i = 0 ; i < searchables . length ; i ++ ) { final int start = starts [ i ] ; searchables [ i ] . search ( weight , filter , new HitCollector ( ) { public void collect ( int doc , float score ) { results . collect ( doc + start , score ) ; } } ) ; } } public Query rewrite ( Query original ) throws IOException { Query [ ] queries = new Query [ searchables . length ] ; for ( int i = 0 ; i < searchables . length ; i ++ ) { queries [ i ] = searchables [ i ] . rewrite ( original ) ; } return queries [ 0 ] . combine ( queries ) ; } public Explanation explain ( Weight weight , int doc ) throws IOException { int i = subSearcher ( doc ) ; return searchables [ i ] . explain ( weight , doc - starts [ i ] ) ; } protected Weight createWeight ( Query original ) throws IOException { Query rewrittenQuery = rewrite ( original ) ; Set terms = new HashSet ( ) ; rewrittenQuery . extractTerms ( terms ) ; Term [ ] allTermsArray = new Term [ terms . size ( ) ] ; terms . toArray ( allTermsArray ) ; int [ ] aggregatedDfs = new int [ terms . size ( ) ] ; for ( int i = 0 ; i < searchables . length ; i ++ ) { int [ ] dfs = searchables [ i ] . docFreqs ( allTermsArray ) ; for ( int j = 0 ; j < aggregatedDfs . length ; j ++ ) { aggregatedDfs [ j ] += dfs [ j ] ; } } HashMap dfMap = new HashMap ( ) ; for ( int i = 0 ; i < allTermsArray . length ; i ++ ) { dfMap . put ( allTermsArray [ i ] , new Integer ( aggregatedDfs [ i ] ) ) ; } int numDocs = maxDoc ( ) ; CachedDfSource cacheSim = new CachedDfSource ( dfMap , numDocs , getSimilarity ( ) ) ; return rewrittenQuery . weight ( cacheSim ) ; } } 	0	['16', '2', '1', '22', '53', '0', '2', '21', '14', '0.466666667', '577', '1', '1', '0.594594595', '0.23125', '1', '5', '34.875', '6', '1.25', '0']
package org . apache . lucene . analysis ; import java . io . BufferedReader ; import java . io . File ; import java . io . FileReader ; import java . io . IOException ; import java . io . Reader ; import java . util . HashMap ; import java . util . HashSet ; public class WordlistLoader { public static HashSet getWordSet ( File wordfile ) throws IOException { HashSet result = new HashSet ( ) ; FileReader reader = null ; try { reader = new FileReader ( wordfile ) ; result = getWordSet ( reader ) ; } finally { if ( reader != null ) reader . close ( ) ; } return result ; } public static HashSet getWordSet ( Reader reader ) throws IOException { HashSet result = new HashSet ( ) ; BufferedReader br = null ; try { if ( reader instanceof BufferedReader ) { br = ( BufferedReader ) reader ; } else { br = new BufferedReader ( reader ) ; } String word = null ; while ( ( word = br . readLine ( ) ) != null ) { result . add ( word . trim ( ) ) ; } } finally { if ( br != null ) br . close ( ) ; } return result ; } public static HashMap getStemDict ( File wordstemfile ) throws IOException { if ( wordstemfile == null ) throw new NullPointerException ( "wordstemfile may not be null" ) ; HashMap result = new HashMap ( ) ; BufferedReader br = null ; FileReader fr = null ; try { fr = new FileReader ( wordstemfile ) ; br = new BufferedReader ( fr ) ; String line ; while ( ( line = br . readLine ( ) ) != null ) { String [ ] wordstem = line . split ( "\t" , 2 ) ; result . put ( wordstem [ 0 ] , wordstem [ 1 ] ) ; } } finally { if ( fr != null ) fr . close ( ) ; if ( br != null ) br . close ( ) ; } return result ; } } 	0	['4', '1', '0', '2', '17', '6', '2', '0', '4', '2', '147', '0', '0', '0', '0.333333333', '0', '0', '35.75', '1', '0.75', '0']
package org . apache . lucene . search . spans ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermPositions ; import java . io . IOException ; public class TermSpans implements Spans { protected TermPositions positions ; protected Term term ; protected int doc ; protected int freq ; protected int count ; protected int position ; public TermSpans ( TermPositions positions , Term term ) throws IOException { this . positions = positions ; this . term = term ; doc = - 1 ; } public boolean next ( ) throws IOException { if ( count == freq ) { if ( ! positions . next ( ) ) { doc = Integer . MAX_VALUE ; return false ; } doc = positions . doc ( ) ; freq = positions . freq ( ) ; count = 0 ; } position = positions . nextPosition ( ) ; count ++ ; return true ; } public boolean skipTo ( int target ) throws IOException { if ( doc >= target ) { return true ; } if ( ! positions . skipTo ( target ) ) { doc = Integer . MAX_VALUE ; return false ; } doc = positions . doc ( ) ; freq = positions . freq ( ) ; count = 0 ; position = positions . nextPosition ( ) ; count ++ ; return true ; } public int doc ( ) { return doc ; } public int start ( ) { return position ; } public int end ( ) { return position + 1 ; } public String toString ( ) { return "spans(" + term . toString ( ) + ")@" + ( doc == - 1 ? "START" : ( doc == Integer . MAX_VALUE ) ? "END" : doc + "-" + position ) ; } public TermPositions getPositions ( ) { return positions ; } } 	0	['8', '1', '0', '6', '19', '0', '3', '3', '8', '0.666666667', '160', '1', '2', '0', '0.34375', '0', '0', '18.25', '3', '1.125', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . List ; import java . util . ArrayList ; import java . util . Iterator ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Query ; import org . apache . lucene . util . ToStringUtils ; public class SpanNearQuery extends SpanQuery { private List clauses ; private int slop ; private boolean inOrder ; private String field ; public SpanNearQuery ( SpanQuery [ ] clauses , int slop , boolean inOrder ) { this . clauses = new ArrayList ( clauses . length ) ; for ( int i = 0 ; i < clauses . length ; i ++ ) { SpanQuery clause = clauses [ i ] ; if ( i == 0 ) { field = clause . getField ( ) ; } else if ( ! clause . getField ( ) . equals ( field ) ) { throw new IllegalArgumentException ( "Clauses must have same field." ) ; } this . clauses . add ( clause ) ; } this . slop = slop ; this . inOrder = inOrder ; } public SpanQuery [ ] getClauses ( ) { return ( SpanQuery [ ] ) clauses . toArray ( new SpanQuery [ clauses . size ( ) ] ) ; } public int getSlop ( ) { return slop ; } public boolean isInOrder ( ) { return inOrder ; } public String getField ( ) { return field ; } public Collection getTerms ( ) { Collection terms = new ArrayList ( ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; terms . addAll ( clause . getTerms ( ) ) ; } return terms ; } public void extractTerms ( Set terms ) { Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; clause . extractTerms ( terms ) ; } } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "spanNear([" ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; buffer . append ( clause . toString ( field ) ) ; if ( i . hasNext ( ) ) { buffer . append ( ", " ) ; } } buffer . append ( "], " ) ; buffer . append ( slop ) ; buffer . append ( ", " ) ; buffer . append ( inOrder ) ; buffer . append ( ")" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public Spans getSpans ( final IndexReader reader ) throws IOException { if ( clauses . size ( ) == 0 ) return new SpanOrQuery ( getClauses ( ) ) . getSpans ( reader ) ; if ( clauses . size ( ) == 1 ) return ( ( SpanQuery ) clauses . get ( 0 ) ) . getSpans ( reader ) ; return inOrder ? ( Spans ) new NearSpansOrdered ( this , reader ) : ( Spans ) new NearSpansUnordered ( this , reader ) ; } public Query rewrite ( IndexReader reader ) throws IOException { SpanNearQuery clone = null ; for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { SpanQuery c = ( SpanQuery ) clauses . get ( i ) ; SpanQuery query = ( SpanQuery ) c . rewrite ( reader ) ; if ( query != c ) { if ( clone == null ) clone = ( SpanNearQuery ) this . clone ( ) ; clone . clauses . set ( i , query ) ; } } if ( clone != null ) { return clone ; } else { return this ; } } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof SpanNearQuery ) ) return false ; final SpanNearQuery spanNearQuery = ( SpanNearQuery ) o ; if ( inOrder != spanNearQuery . inOrder ) return false ; if ( slop != spanNearQuery . slop ) return false ; if ( ! clauses . equals ( spanNearQuery . clauses ) ) return false ; return getBoost ( ) == spanNearQuery . getBoost ( ) ; } public int hashCode ( ) { int result ; result = clauses . hashCode ( ) ; result ^= ( result << 14 ) | ( result > > > 19 ) ; result += Float . floatToRawIntBits ( getBoost ( ) ) ; result += slop ; result ^= ( inOrder ? 0x99AFD3BD : 0 ) ; return result ; } } 	0	['12', '3', '0', '8', '47', '0', '2', '8', '12', '0.590909091', '353', '1', '0', '0.592592593', '0.208333333', '2', '2', '28.08333333', '7', '1.75', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . * ; final class PhrasePositions { int doc ; int position ; int count ; int offset ; TermPositions tp ; PhrasePositions next ; boolean repeats ; PhrasePositions ( TermPositions t , int o ) { tp = t ; offset = o ; } final boolean next ( ) throws IOException { if ( ! tp . next ( ) ) { tp . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } doc = tp . doc ( ) ; position = 0 ; return true ; } final boolean skipTo ( int target ) throws IOException { if ( ! tp . skipTo ( target ) ) { tp . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } doc = tp . doc ( ) ; position = 0 ; return true ; } final void firstPosition ( ) throws IOException { count = tp . freq ( ) ; nextPosition ( ) ; } final boolean nextPosition ( ) throws IOException { if ( count -- > 0 ) { position = tp . nextPosition ( ) - offset ; return true ; } else return false ; } } 	0	['5', '1', '0', '6', '12', '0', '5', '1', '0', '0.678571429', '95', '0', '2', '0', '0.533333333', '0', '0', '16.6', '1', '0.8', '0']
package org . apache . lucene . document ; import java . io . Serializable ; public interface FieldSelector extends Serializable { FieldSelectorResult accept ( String fieldName ) ; } 	0	['1', '1', '0', '18', '1', '0', '17', '1', '1', '2', '1', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search . function ; public class FieldScoreQuery extends ValueSourceQuery { public static class Type { public static final Type BYTE = new Type ( "byte" ) ; public static final Type SHORT = new Type ( "short" ) ; public static final Type INT = new Type ( "int" ) ; public static final Type FLOAT = new Type ( "float" ) ; private String typeName ; private Type ( String name ) { this . typeName = name ; } public String toString ( ) { return getClass ( ) . getName ( ) + "::" + typeName ; } } public FieldScoreQuery ( String field , Type type ) { super ( getValueSource ( field , type ) ) ; } private static ValueSource getValueSource ( String field , Type type ) { if ( type == Type . BYTE ) { return new ByteFieldSource ( field ) ; } if ( type == Type . SHORT ) { return new ShortFieldSource ( field ) ; } if ( type == Type . INT ) { return new IntFieldSource ( field ) ; } if ( type == Type . FLOAT ) { return new FloatFieldSource ( field ) ; } throw new IllegalArgumentException ( type + " is not a known Field Score Query Type!" ) ; } } 	0	['2', '3', '0', '7', '12', '1', '0', '7', '1', '2', '52', '0', '0', '0.947368421', '0.833333333', '0', '0', '25', '5', '2.5', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . HashSet ; import java . util . Iterator ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; public abstract class Query implements java . io . Serializable , Cloneable { private float boost = 1.0f ; public void setBoost ( float b ) { boost = b ; } public float getBoost ( ) { return boost ; } public abstract String toString ( String field ) ; public String toString ( ) { return toString ( "" ) ; } protected Weight createWeight ( Searcher searcher ) throws IOException { throw new UnsupportedOperationException ( ) ; } public Weight weight ( Searcher searcher ) throws IOException { Query query = searcher . rewrite ( this ) ; Weight weight = query . createWeight ( searcher ) ; float sum = weight . sumOfSquaredWeights ( ) ; float norm = getSimilarity ( searcher ) . queryNorm ( sum ) ; weight . normalize ( norm ) ; return weight ; } public Query rewrite ( IndexReader reader ) throws IOException { return this ; } public Query combine ( Query [ ] queries ) { HashSet uniques = new HashSet ( ) ; for ( int i = 0 ; i < queries . length ; i ++ ) { Query query = queries [ i ] ; BooleanClause [ ] clauses = null ; boolean splittable = ( query instanceof BooleanQuery ) ; if ( splittable ) { BooleanQuery bq = ( BooleanQuery ) query ; splittable = bq . isCoordDisabled ( ) ; clauses = bq . getClauses ( ) ; for ( int j = 0 ; splittable && j < clauses . length ; j ++ ) { splittable = ( clauses [ j ] . getOccur ( ) == BooleanClause . Occur . SHOULD ) ; } } if ( splittable ) { for ( int j = 0 ; j < clauses . length ; j ++ ) { uniques . add ( clauses [ j ] . getQuery ( ) ) ; } } else { uniques . add ( query ) ; } } if ( uniques . size ( ) == 1 ) { return ( Query ) uniques . iterator ( ) . next ( ) ; } Iterator it = uniques . iterator ( ) ; BooleanQuery result = new BooleanQuery ( true ) ; while ( it . hasNext ( ) ) result . add ( ( Query ) it . next ( ) , BooleanClause . Occur . SHOULD ) ; return result ; } public void extractTerms ( Set terms ) { throw new UnsupportedOperationException ( ) ; } public static Query mergeBooleanQueries ( Query [ ] queries ) { HashSet allClauses = new HashSet ( ) ; for ( int i = 0 ; i < queries . length ; i ++ ) { BooleanClause [ ] clauses = ( ( BooleanQuery ) queries [ i ] ) . getClauses ( ) ; for ( int j = 0 ; j < clauses . length ; j ++ ) { allClauses . add ( clauses [ j ] ) ; } } boolean coordDisabled = queries . length == 0 ? false : ( ( BooleanQuery ) queries [ 0 ] ) . isCoordDisabled ( ) ; BooleanQuery result = new BooleanQuery ( coordDisabled ) ; Iterator i = allClauses . iterator ( ) ; while ( i . hasNext ( ) ) { result . add ( ( BooleanClause ) i . next ( ) ) ; } return result ; } public Similarity getSimilarity ( Searcher searcher ) { return searcher . getSimilarity ( ) ; } public Object clone ( ) { try { return ( Query ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { throw new RuntimeException ( "Clone not supported: " + e . getMessage ( ) ) ; } } } 	0	['13', '1', '15', '51', '39', '72', '48', '7', '12', '0.833333333', '249', '1', '0', '0', '0.230769231', '0', '0', '18.07692308', '9', '1.8462', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; public final class SimpleAnalyzer extends Analyzer { public TokenStream tokenStream ( String fieldName , Reader reader ) { return new LowerCaseTokenizer ( reader ) ; } } 	0	['2', '2', '0', '4', '4', '1', '1', '3', '2', '2', '10', '0', '0', '0.666666667', '0.666666667', '0', '0', '4', '1', '0.5', '0']
package org . apache . lucene . index ; final class TermInfo { int docFreq = 0 ; long freqPointer = 0 ; long proxPointer = 0 ; int skipOffset ; TermInfo ( ) { } TermInfo ( int df , long fp , long pp ) { docFreq = df ; freqPointer = fp ; proxPointer = pp ; } TermInfo ( TermInfo ti ) { docFreq = ti . docFreq ; freqPointer = ti . freqPointer ; proxPointer = ti . proxPointer ; skipOffset = ti . skipOffset ; } final void set ( int docFreq , long freqPointer , long proxPointer , int skipOffset ) { this . docFreq = docFreq ; this . freqPointer = freqPointer ; this . proxPointer = proxPointer ; this . skipOffset = skipOffset ; } final void set ( TermInfo ti ) { docFreq = ti . docFreq ; freqPointer = ti . freqPointer ; proxPointer = ti . proxPointer ; skipOffset = ti . skipOffset ; } } 	0	['5', '1', '0', '8', '6', '0', '8', '0', '0', '0.125', '100', '0', '0', '0', '0.55', '0', '0', '18.2', '1', '0.4', '0']
package org . apache . lucene . index ; import java . io . IOException ; public class CorruptIndexException extends IOException { public CorruptIndexException ( String message ) { super ( message ) ; } } 	0	['1', '4', '0', '28', '2', '0', '28', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . util . PriorityQueue ; final class SegmentMergeQueue extends PriorityQueue { SegmentMergeQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object a , Object b ) { SegmentMergeInfo stiA = ( SegmentMergeInfo ) a ; SegmentMergeInfo stiB = ( SegmentMergeInfo ) b ; int comparison = stiA . term . compareTo ( stiB . term ) ; if ( comparison == 0 ) return stiA . base < stiB . base ; else return comparison < 0 ; } final void close ( ) throws IOException { while ( top ( ) != null ) ( ( SegmentMergeInfo ) pop ( ) ) . close ( ) ; } } 	0	['3', '2', '0', '5', '9', '3', '2', '3', '0', '2', '47', '0', '0', '0.846153846', '0.555555556', '1', '3', '14.66666667', '4', '1.6667', '0']
package org . apache . lucene . search ; import java . util . List ; import java . util . Iterator ; import java . io . IOException ; import org . apache . lucene . util . ScorerDocQueue ; class DisjunctionSumScorer extends Scorer { private final int nrScorers ; protected final List subScorers ; private final int minimumNrMatchers ; private ScorerDocQueue scorerDocQueue = null ; private int queueSize = - 1 ; private int currentDoc = - 1 ; protected int nrMatchers = - 1 ; private float currentScore = Float . NaN ; public DisjunctionSumScorer ( List subScorers , int minimumNrMatchers ) { super ( null ) ; nrScorers = subScorers . size ( ) ; if ( minimumNrMatchers <= 0 ) { throw new IllegalArgumentException ( "Minimum nr of matchers must be positive" ) ; } if ( nrScorers <= 1 ) { throw new IllegalArgumentException ( "There must be at least 2 subScorers" ) ; } this . minimumNrMatchers = minimumNrMatchers ; this . subScorers = subScorers ; } public DisjunctionSumScorer ( List subScorers ) { this ( subScorers , 1 ) ; } private void initScorerDocQueue ( ) throws IOException { Iterator si = subScorers . iterator ( ) ; scorerDocQueue = new ScorerDocQueue ( nrScorers ) ; queueSize = 0 ; while ( si . hasNext ( ) ) { Scorer se = ( Scorer ) si . next ( ) ; if ( se . next ( ) ) { if ( scorerDocQueue . insert ( se ) ) { queueSize ++ ; } } } } public void score ( HitCollector hc ) throws IOException { while ( next ( ) ) { hc . collect ( currentDoc , currentScore ) ; } } protected boolean score ( HitCollector hc , int max ) throws IOException { while ( currentDoc < max ) { hc . collect ( currentDoc , currentScore ) ; if ( ! next ( ) ) { return false ; } } return true ; } public boolean next ( ) throws IOException { if ( scorerDocQueue == null ) { initScorerDocQueue ( ) ; } return ( scorerDocQueue . size ( ) >= minimumNrMatchers ) && advanceAfterCurrent ( ) ; } protected boolean advanceAfterCurrent ( ) throws IOException { do { currentDoc = scorerDocQueue . topDoc ( ) ; currentScore = scorerDocQueue . topScore ( ) ; nrMatchers = 1 ; do { if ( ! scorerDocQueue . topNextAndAdjustElsePop ( ) ) { if ( -- queueSize == 0 ) { break ; } } if ( scorerDocQueue . topDoc ( ) != currentDoc ) { break ; } currentScore += scorerDocQueue . topScore ( ) ; nrMatchers ++ ; } while ( true ) ; if ( nrMatchers >= minimumNrMatchers ) { return true ; } else if ( queueSize < minimumNrMatchers ) { return false ; } } while ( true ) ; } public float score ( ) throws IOException { return currentScore ; } public int doc ( ) { return currentDoc ; } public int nrMatchers ( ) { return nrMatchers ; } public boolean skipTo ( int target ) throws IOException { if ( scorerDocQueue == null ) { initScorerDocQueue ( ) ; } if ( queueSize < minimumNrMatchers ) { return false ; } if ( target <= currentDoc ) { return true ; } do { if ( scorerDocQueue . topDoc ( ) >= target ) { return advanceAfterCurrent ( ) ; } else if ( ! scorerDocQueue . topSkipToAndAdjustElsePop ( target ) ) { if ( -- queueSize < minimumNrMatchers ) { return false ; } } } while ( true ) ; } public Explanation explain ( int doc ) throws IOException { Explanation res = new Explanation ( ) ; Iterator ssi = subScorers . iterator ( ) ; float sumScore = 0.0f ; int nrMatches = 0 ; while ( ssi . hasNext ( ) ) { Explanation es = ( ( Scorer ) ssi . next ( ) ) . explain ( doc ) ; if ( es . getValue ( ) > 0.0f ) { sumScore += es . getValue ( ) ; nrMatches ++ ; } res . addDetail ( es ) ; } if ( nrMatchers >= minimumNrMatchers ) { res . setValue ( sumScore ) ; res . setDescription ( "sum over at least " + minimumNrMatchers + " of " + subScorers . size ( ) + ":" ) ; } else { res . setValue ( 0.0f ) ; res . setDescription ( nrMatches + " match(es) but at least " + minimumNrMatchers + " of " + subScorers . size ( ) + " needed" ) ; } return res ; } } 	0	['12', '2', '1', '7', '37', '0', '2', '5', '9', '0.454545455', '357', '1', '1', '0.444444444', '0.416666667', '1', '3', '28.08333333', '1', '0.8333', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexInput ; final class TermBuffer implements Cloneable { private static final char [ ] NO_CHARS = new char [ 0 ] ; private String field ; private char [ ] text = NO_CHARS ; private int textLength ; private Term term ; public final int compareTo ( TermBuffer other ) { if ( field == other . field ) return compareChars ( text , textLength , other . text , other . textLength ) ; else return field . compareTo ( other . field ) ; } private static final int compareChars ( char [ ] v1 , int len1 , char [ ] v2 , int len2 ) { int end = Math . min ( len1 , len2 ) ; for ( int k = 0 ; k < end ; k ++ ) { char c1 = v1 [ k ] ; char c2 = v2 [ k ] ; if ( c1 != c2 ) { return c1 - c2 ; } } return len1 - len2 ; } private final void setTextLength ( int newLength ) { if ( text . length < newLength ) { char [ ] newText = new char [ newLength ] ; System . arraycopy ( text , 0 , newText , 0 , textLength ) ; text = newText ; } textLength = newLength ; } public final void read ( IndexInput input , FieldInfos fieldInfos ) throws IOException { this . term = null ; int start = input . readVInt ( ) ; int length = input . readVInt ( ) ; int totalLength = start + length ; setTextLength ( totalLength ) ; input . readChars ( this . text , start , length ) ; this . field = fieldInfos . fieldName ( input . readVInt ( ) ) ; } public final void set ( Term term ) { if ( term == null ) { reset ( ) ; return ; } setTextLength ( term . text ( ) . length ( ) ) ; term . text ( ) . getChars ( 0 , term . text ( ) . length ( ) , text , 0 ) ; this . field = term . field ( ) ; this . term = term ; } public final void set ( TermBuffer other ) { setTextLength ( other . textLength ) ; System . arraycopy ( other . text , 0 , text , 0 , textLength ) ; this . field = other . field ; this . term = other . term ; } public void reset ( ) { this . field = null ; this . textLength = 0 ; this . term = null ; } public Term toTerm ( ) { if ( field == null ) return null ; if ( term == null ) term = new Term ( field , new String ( text , 0 , textLength ) , false ) ; return term ; } protected Object clone ( ) { TermBuffer clone = null ; try { clone = ( TermBuffer ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . text = new char [ text . length ] ; System . arraycopy ( text , 0 , clone . text , 0 , textLength ) ; return clone ; } } 	0	['11', '1', '0', '4', '25', '0', '1', '3', '6', '0.52', '241', '1', '1', '0', '0.242857143', '0', '0', '20.45454545', '3', '1.4545', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class BufferedIndexOutput extends IndexOutput { static final int BUFFER_SIZE = 16384 ; private final byte [ ] buffer = new byte [ BUFFER_SIZE ] ; private long bufferStart = 0 ; private int bufferPosition = 0 ; public void writeByte ( byte b ) throws IOException { if ( bufferPosition >= BUFFER_SIZE ) flush ( ) ; buffer [ bufferPosition ++ ] = b ; } public void writeBytes ( byte [ ] b , int offset , int length ) throws IOException { int bytesLeft = BUFFER_SIZE - bufferPosition ; if ( bytesLeft >= length ) { System . arraycopy ( b , offset , buffer , bufferPosition , length ) ; bufferPosition += length ; if ( BUFFER_SIZE - bufferPosition == 0 ) flush ( ) ; } else { if ( length > BUFFER_SIZE ) { if ( bufferPosition > 0 ) flush ( ) ; flushBuffer ( b , offset , length ) ; bufferStart += length ; } else { int pos = 0 ; int pieceLength ; while ( pos < length ) { pieceLength = ( length - pos < bytesLeft ) ? length - pos : bytesLeft ; System . arraycopy ( b , pos + offset , buffer , bufferPosition , pieceLength ) ; pos += pieceLength ; bufferPosition += pieceLength ; bytesLeft = BUFFER_SIZE - bufferPosition ; if ( bytesLeft == 0 ) { flush ( ) ; bytesLeft = BUFFER_SIZE ; } } } } } public void flush ( ) throws IOException { flushBuffer ( buffer , bufferPosition ) ; bufferStart += bufferPosition ; bufferPosition = 0 ; } private void flushBuffer ( byte [ ] b , int len ) throws IOException { flushBuffer ( b , 0 , len ) ; } protected abstract void flushBuffer ( byte [ ] b , int offset , int len ) throws IOException ; public void close ( ) throws IOException { flush ( ) ; } public long getFilePointer ( ) { return bufferStart + bufferPosition ; } public void seek ( long pos ) throws IOException { flush ( ) ; bufferStart = pos ; } public abstract long length ( ) throws IOException ; } 	0	['10', '2', '1', '2', '12', '17', '1', '1', '8', '0.555555556', '185', '0.75', '0', '0.608695652', '0.36', '1', '3', '17.1', '1', '0.9', '0']
package org . apache . lucene . index ; public class TermVectorOffsetInfo { public static final TermVectorOffsetInfo [ ] EMPTY_OFFSET_INFO = new TermVectorOffsetInfo [ 0 ] ; private int startOffset ; private int endOffset ; public TermVectorOffsetInfo ( ) { } public TermVectorOffsetInfo ( int startOffset , int endOffset ) { this . endOffset = endOffset ; this . startOffset = startOffset ; } public int getEndOffset ( ) { return endOffset ; } public void setEndOffset ( int endOffset ) { this . endOffset = endOffset ; } public int getStartOffset ( ) { return startOffset ; } public void setStartOffset ( int startOffset ) { this . startOffset = startOffset ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof TermVectorOffsetInfo ) ) return false ; final TermVectorOffsetInfo termVectorOffsetInfo = ( TermVectorOffsetInfo ) o ; if ( endOffset != termVectorOffsetInfo . endOffset ) return false ; if ( startOffset != termVectorOffsetInfo . startOffset ) return false ; return true ; } public int hashCode ( ) { int result ; result = startOffset ; result = 29 * result + endOffset ; return result ; } } 	0	['9', '1', '0', '7', '10', '2', '7', '0', '8', '0.666666667', '83', '0.666666667', '1', '0', '0.5', '1', '1', '7.888888889', '5', '1.1111', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . RAMOutputStream ; abstract class MultiLevelSkipListWriter { private int numberOfSkipLevels ; private int skipInterval ; private RAMOutputStream [ ] skipBuffer ; protected MultiLevelSkipListWriter ( int skipInterval , int maxSkipLevels , int df ) { this . skipInterval = skipInterval ; numberOfSkipLevels = df == 0 ? 0 : ( int ) Math . floor ( Math . log ( df ) / Math . log ( skipInterval ) ) ; if ( numberOfSkipLevels > maxSkipLevels ) { numberOfSkipLevels = maxSkipLevels ; } } protected void init ( ) { skipBuffer = new RAMOutputStream [ numberOfSkipLevels ] ; for ( int i = 0 ; i < numberOfSkipLevels ; i ++ ) { skipBuffer [ i ] = new RAMOutputStream ( ) ; } } protected void resetSkip ( ) { if ( skipBuffer == null ) { init ( ) ; } else { for ( int i = 0 ; i < skipBuffer . length ; i ++ ) { skipBuffer [ i ] . reset ( ) ; } } } protected abstract void writeSkipData ( int level , IndexOutput skipBuffer ) throws IOException ; void bufferSkip ( int df ) throws IOException { int numLevels ; for ( numLevels = 0 ; ( df % skipInterval ) == 0 && numLevels < numberOfSkipLevels ; df /= skipInterval ) { numLevels ++ ; } long childPointer = 0 ; for ( int level = 0 ; level < numLevels ; level ++ ) { writeSkipData ( level , skipBuffer [ level ] ) ; long newChildPointer = skipBuffer [ level ] . getFilePointer ( ) ; if ( level != 0 ) { skipBuffer [ level ] . writeVLong ( childPointer ) ; } childPointer = newChildPointer ; } } long writeSkip ( IndexOutput output ) throws IOException { long skipPointer = output . getFilePointer ( ) ; if ( skipBuffer == null || skipBuffer . length == 0 ) return skipPointer ; for ( int level = numberOfSkipLevels - 1 ; level > 0 ; level -- ) { long length = skipBuffer [ level ] . getFilePointer ( ) ; if ( length > 0 ) { output . writeVLong ( length ) ; skipBuffer [ level ] . writeTo ( output ) ; } } skipBuffer [ 0 ] . writeTo ( output ) ; return skipPointer ; } } 	0	['6', '1', '1', '3', '16', '0', '1', '2', '0', '0.466666667', '178', '1', '1', '0', '0.611111111', '0', '0', '28.16666667', '3', '1.3333', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; public class LetterTokenizer extends CharTokenizer { public LetterTokenizer ( Reader in ) { super ( in ) ; } protected boolean isTokenChar ( char c ) { return Character . isLetter ( c ) ; } } 	0	['2', '4', '1', '2', '4', '1', '1', '1', '1', '2', '9', '0', '0', '0.875', '0.666666667', '1', '1', '3.5', '1', '0.5', '0']
package org . apache . lucene . search ; import org . apache . lucene . util . Parameter ; public class BooleanClause implements java . io . Serializable { public static final class Occur extends Parameter implements java . io . Serializable { private Occur ( String name ) { super ( name ) ; } public String toString ( ) { if ( this == MUST ) return "+" ; if ( this == MUST_NOT ) return "-" ; return "" ; } public static final Occur MUST = new Occur ( "MUST" ) ; public static final Occur SHOULD = new Occur ( "SHOULD" ) ; public static final Occur MUST_NOT = new Occur ( "MUST_NOT" ) ; } private Query query ; private Occur occur ; public BooleanClause ( Query query , Occur occur ) { this . query = query ; this . occur = occur ; } public Occur getOccur ( ) { return occur ; } public void setOccur ( Occur occur ) { this . occur = occur ; } public Query getQuery ( ) { return query ; } public void setQuery ( Query query ) { this . query = query ; } public boolean isProhibited ( ) { return Occur . MUST_NOT . equals ( occur ) ; } public boolean isRequired ( ) { return Occur . MUST . equals ( occur ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof BooleanClause ) ) return false ; BooleanClause other = ( BooleanClause ) o ; return this . query . equals ( other . query ) && this . occur . equals ( other . occur ) ; } public int hashCode ( ) { return query . hashCode ( ) ^ ( Occur . MUST . equals ( occur ) ? 1 : 0 ) ^ ( Occur . MUST_NOT . equals ( occur ) ? 2 : 0 ) ; } public String toString ( ) { return occur . toString ( ) + query . toString ( ) ; } } 	0	['10', '1', '0', '6', '18', '0', '5', '2', '10', '0.333333333', '104', '1', '2', '0', '0.375', '1', '1', '9.2', '4', '1.4', '0']
package org . apache . lucene . util ; public class ToStringUtils { public static String boost ( float boost ) { if ( boost != 1.0f ) { return "^" + Float . toString ( boost ) ; } else return "" ; } } 	0	['2', '1', '0', '17', '7', '1', '17', '0', '2', '2', '21', '0', '0', '0', '0.5', '0', '0', '9.5', '2', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import java . io . IOException ; public class WildcardQuery extends MultiTermQuery { private boolean termContainsWildcard ; public WildcardQuery ( Term term ) { super ( term ) ; this . termContainsWildcard = ( term . text ( ) . indexOf ( '*' ) != - 1 ) || ( term . text ( ) . indexOf ( '?' ) != - 1 ) ; } protected FilteredTermEnum getEnum ( IndexReader reader ) throws IOException { return new WildcardTermEnum ( reader , getTerm ( ) ) ; } public boolean equals ( Object o ) { if ( o instanceof WildcardQuery ) return super . equals ( o ) ; return false ; } public Query rewrite ( IndexReader reader ) throws IOException { if ( this . termContainsWildcard ) { return super . rewrite ( reader ) ; } return new TermQuery ( getTerm ( ) ) ; } } 	0	['4', '3', '0', '8', '12', '4', '1', '7', '3', '0.666666667', '55', '1', '0', '0.857142857', '0.5', '1', '1', '12.5', '2', '1', '0']
package org . apache . lucene . document ; import org . apache . lucene . search . PrefixQuery ; import org . apache . lucene . search . RangeQuery ; import java . util . Date ; public class DateField { private DateField ( ) { } private static int DATE_LEN = Long . toString ( 1000L * 365 * 24 * 60 * 60 * 1000 , Character . MAX_RADIX ) . length ( ) ; public static String MIN_DATE_STRING ( ) { return timeToString ( 0 ) ; } public static String MAX_DATE_STRING ( ) { char [ ] buffer = new char [ DATE_LEN ] ; char c = Character . forDigit ( Character . MAX_RADIX - 1 , Character . MAX_RADIX ) ; for ( int i = 0 ; i < DATE_LEN ; i ++ ) buffer [ i ] = c ; return new String ( buffer ) ; } public static String dateToString ( Date date ) { return timeToString ( date . getTime ( ) ) ; } public static String timeToString ( long time ) { if ( time < 0 ) throw new RuntimeException ( "time '" + time + "' is too early, must be >= 0" ) ; String s = Long . toString ( time , Character . MAX_RADIX ) ; if ( s . length ( ) > DATE_LEN ) throw new RuntimeException ( "time '" + time + "' is too late, length of string " + "representation must be <= " + DATE_LEN ) ; if ( s . length ( ) < DATE_LEN ) { StringBuffer sb = new StringBuffer ( s ) ; while ( sb . length ( ) < DATE_LEN ) sb . insert ( 0 , 0 ) ; s = sb . toString ( ) ; } return s ; } public static long stringToTime ( String s ) { return Long . parseLong ( s , Character . MAX_RADIX ) ; } public static Date stringToDate ( String s ) { return new Date ( stringToTime ( s ) ) ; } } 	0	['8', '1', '0', '1', '25', '22', '1', '0', '6', '0.428571429', '126', '1', '0', '0', '0.178571429', '0', '0', '14.625', '5', '1.375', '0']
package org . apache . lucene . document ; import java . io . Serializable ; public final class FieldSelectorResult implements Serializable { public transient static final FieldSelectorResult LOAD = new FieldSelectorResult ( 0 ) ; public transient static final FieldSelectorResult LAZY_LOAD = new FieldSelectorResult ( 1 ) ; public transient static final FieldSelectorResult NO_LOAD = new FieldSelectorResult ( 2 ) ; public transient static final FieldSelectorResult LOAD_AND_BREAK = new FieldSelectorResult ( 3 ) ; public transient static final FieldSelectorResult LOAD_FOR_MERGE = new FieldSelectorResult ( 4 ) ; public transient static final FieldSelectorResult SIZE = new FieldSelectorResult ( 5 ) ; public transient static final FieldSelectorResult SIZE_AND_BREAK = new FieldSelectorResult ( 6 ) ; private int id ; private FieldSelectorResult ( int id ) { this . id = id ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( o == null || getClass ( ) != o . getClass ( ) ) return false ; final FieldSelectorResult that = ( FieldSelectorResult ) o ; if ( id != that . id ) return false ; return true ; } public int hashCode ( ) { return id ; } } 	0	['4', '1', '0', '7', '6', '0', '7', '0', '2', '0.875', '83', '0.125', '7', '0', '0.555555556', '1', '1', '17.75', '5', '1.5', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; import java . io . Serializable ; public interface SortComparatorSource extends Serializable { ScoreDocComparator newComparator ( IndexReader reader , String fieldname ) throws IOException ; } 	0	['1', '1', '0', '6', '1', '0', '4', '2', '1', '2', '1', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search ; import java . io . IOException ; public class ReqOptSumScorer extends Scorer { private Scorer reqScorer ; private Scorer optScorer ; public ReqOptSumScorer ( Scorer reqScorer , Scorer optScorer ) { super ( null ) ; this . reqScorer = reqScorer ; this . optScorer = optScorer ; } private boolean firstTimeOptScorer = true ; public boolean next ( ) throws IOException { return reqScorer . next ( ) ; } public boolean skipTo ( int target ) throws IOException { return reqScorer . skipTo ( target ) ; } public int doc ( ) { return reqScorer . doc ( ) ; } public float score ( ) throws IOException { int curDoc = reqScorer . doc ( ) ; float reqScore = reqScorer . score ( ) ; if ( firstTimeOptScorer ) { firstTimeOptScorer = false ; if ( ! optScorer . skipTo ( curDoc ) ) { optScorer = null ; return reqScore ; } } else if ( optScorer == null ) { return reqScore ; } else if ( ( optScorer . doc ( ) < curDoc ) && ( ! optScorer . skipTo ( curDoc ) ) ) { optScorer = null ; return reqScore ; } return ( optScorer . doc ( ) == curDoc ) ? reqScore + optScorer . score ( ) : reqScore ; } public Explanation explain ( int doc ) throws IOException { Explanation res = new Explanation ( ) ; res . setDescription ( "required, optional" ) ; res . addDetail ( reqScorer . explain ( doc ) ) ; res . addDetail ( optScorer . explain ( doc ) ) ; return res ; } } 	0	['6', '2', '0', '4', '15', '0', '1', '3', '6', '0.466666667', '113', '1', '2', '0.615384615', '0.5', '1', '3', '17.33333333', '1', '0.8333', '0']
package org . apache . lucene . document ; public class NumberTools { private static final int RADIX = 36 ; private static final char NEGATIVE_PREFIX = '-' ; private static final char POSITIVE_PREFIX = '0' ; public static final String MIN_STRING_VALUE = NEGATIVE_PREFIX + "0000000000000" ; public static final String MAX_STRING_VALUE = POSITIVE_PREFIX + "1y2p0ij32e8e7" ; public static final int STR_SIZE = MIN_STRING_VALUE . length ( ) ; public static String longToString ( long l ) { if ( l == Long . MIN_VALUE ) { return MIN_STRING_VALUE ; } StringBuffer buf = new StringBuffer ( STR_SIZE ) ; if ( l < 0 ) { buf . append ( NEGATIVE_PREFIX ) ; l = Long . MAX_VALUE + l + 1 ; } else { buf . append ( POSITIVE_PREFIX ) ; } String num = Long . toString ( l , RADIX ) ; int padLen = STR_SIZE - num . length ( ) - buf . length ( ) ; while ( padLen -- > 0 ) { buf . append ( '0' ) ; } buf . append ( num ) ; return buf . toString ( ) ; } public static long stringToLong ( String str ) { if ( str == null ) { throw new NullPointerException ( "string cannot be null" ) ; } if ( str . length ( ) != STR_SIZE ) { throw new NumberFormatException ( "string is the wrong size" ) ; } if ( str . equals ( MIN_STRING_VALUE ) ) { return Long . MIN_VALUE ; } char prefix = str . charAt ( 0 ) ; long l = Long . parseLong ( str . substring ( 1 ) , RADIX ) ; if ( prefix == POSITIVE_PREFIX ) { } else if ( prefix == NEGATIVE_PREFIX ) { l = l - Long . MAX_VALUE - 1 ; } else { throw new NumberFormatException ( "string does not begin with the correct prefix" ) ; } return l ; } } 	0	['4', '1', '0', '0', '18', '0', '0', '0', '3', '1.166666667', '130', '0.5', '0', '0', '0.333333333', '0', '0', '30', '6', '2.5', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import java . io . IOException ; public final class FuzzyTermEnum extends FilteredTermEnum { private static final int TYPICAL_LONGEST_WORD_IN_INDEX = 19 ; private int [ ] [ ] d ; private float similarity ; private boolean endEnum = false ; private Term searchTerm = null ; private final String field ; private final String text ; private final String prefix ; private final float minimumSimilarity ; private final float scale_factor ; private final int [ ] maxDistances = new int [ TYPICAL_LONGEST_WORD_IN_INDEX ] ; public FuzzyTermEnum ( IndexReader reader , Term term ) throws IOException { this ( reader , term , FuzzyQuery . defaultMinSimilarity , FuzzyQuery . defaultPrefixLength ) ; } public FuzzyTermEnum ( IndexReader reader , Term term , float minSimilarity ) throws IOException { this ( reader , term , minSimilarity , FuzzyQuery . defaultPrefixLength ) ; } public FuzzyTermEnum ( IndexReader reader , Term term , final float minSimilarity , final int prefixLength ) throws IOException { super ( ) ; if ( minSimilarity >= 1.0f ) throw new IllegalArgumentException ( "minimumSimilarity cannot be greater than or equal to 1" ) ; else if ( minSimilarity < 0.0f ) throw new IllegalArgumentException ( "minimumSimilarity cannot be less than 0" ) ; if ( prefixLength < 0 ) throw new IllegalArgumentException ( "prefixLength cannot be less than 0" ) ; this . minimumSimilarity = minSimilarity ; this . scale_factor = 1.0f / ( 1.0f - minimumSimilarity ) ; this . searchTerm = term ; this . field = searchTerm . field ( ) ; final int fullSearchTermLength = searchTerm . text ( ) . length ( ) ; final int realPrefixLength = prefixLength > fullSearchTermLength ? fullSearchTermLength : prefixLength ; this . text = searchTerm . text ( ) . substring ( realPrefixLength ) ; this . prefix = searchTerm . text ( ) . substring ( 0 , realPrefixLength ) ; initializeMaxDistances ( ) ; this . d = initDistanceArray ( ) ; setEnum ( reader . terms ( new Term ( searchTerm . field ( ) , prefix ) ) ) ; } protected final boolean termCompare ( Term term ) { if ( field == term . field ( ) && term . text ( ) . startsWith ( prefix ) ) { final String target = term . text ( ) . substring ( prefix . length ( ) ) ; this . similarity = similarity ( target ) ; return ( similarity > minimumSimilarity ) ; } endEnum = true ; return false ; } public final float difference ( ) { return ( float ) ( ( similarity - minimumSimilarity ) * scale_factor ) ; } public final boolean endEnum ( ) { return endEnum ; } private static final int min ( int a , int b , int c ) { final int t = ( a < b ) ? a : b ; return ( t < c ) ? t : c ; } private final int [ ] [ ] initDistanceArray ( ) { return new int [ this . text . length ( ) + 1 ] [ TYPICAL_LONGEST_WORD_IN_INDEX ] ; } private synchronized final float similarity ( final String target ) { final int m = target . length ( ) ; final int n = text . length ( ) ; if ( n == 0 ) { return prefix . length ( ) == 0 ? 0.0f : 1.0f - ( ( float ) m / prefix . length ( ) ) ; } if ( m == 0 ) { return prefix . length ( ) == 0 ? 0.0f : 1.0f - ( ( float ) n / prefix . length ( ) ) ; } final int maxDistance = getMaxDistance ( m ) ; if ( maxDistance < Math . abs ( m - n ) ) { return 0.0f ; } if ( d [ 0 ] . length <= m ) { growDistanceArray ( m ) ; } for ( int i = 0 ; i <= n ; i ++ ) d [ i ] [ 0 ] = i ; for ( int j = 0 ; j <= m ; j ++ ) d [ 0 ] [ j ] = j ; for ( int i = 1 ; i <= n ; i ++ ) { int bestPossibleEditDistance = m ; final char s_i = text . charAt ( i - 1 ) ; for ( int j = 1 ; j <= m ; j ++ ) { if ( s_i != target . charAt ( j - 1 ) ) { d [ i ] [ j ] = min ( d [ i - 1 ] [ j ] , d [ i ] [ j - 1 ] , d [ i - 1 ] [ j - 1 ] ) + 1 ; } else { d [ i ] [ j ] = min ( d [ i - 1 ] [ j ] + 1 , d [ i ] [ j - 1 ] + 1 , d [ i - 1 ] [ j - 1 ] ) ; } bestPossibleEditDistance = Math . min ( bestPossibleEditDistance , d [ i ] [ j ] ) ; } if ( i > maxDistance && bestPossibleEditDistance > maxDistance ) { return 0.0f ; } } return 1.0f - ( ( float ) d [ n ] [ m ] / ( float ) ( prefix . length ( ) + Math . min ( n , m ) ) ) ; } private void growDistanceArray ( int m ) { for ( int i = 0 ; i < d . length ; i ++ ) { d [ i ] = new int [ m + 1 ] ; } } private final int getMaxDistance ( int m ) { return ( m < maxDistances . length ) ? maxDistances [ m ] : calculateMaxDistance ( m ) ; } private void initializeMaxDistances ( ) { for ( int i = 0 ; i < maxDistances . length ; i ++ ) { maxDistances [ i ] = calculateMaxDistance ( i ) ; } } private int calculateMaxDistance ( int m ) { return ( int ) ( ( 1 - minimumSimilarity ) * ( Math . min ( text . length ( ) , m ) + prefix . length ( ) ) ) ; } public void close ( ) throws IOException { super . close ( ) ; } } 	0	['14', '3', '0', '5', '29', '53', '1', '4', '6', '0.692307692', '514', '1', '1', '0.541666667', '0.333333333', '1', '4', '34.92857143', '14', '2.2857', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Query ; import org . apache . lucene . util . ToStringUtils ; public class SpanFirstQuery extends SpanQuery { private SpanQuery match ; private int end ; public SpanFirstQuery ( SpanQuery match , int end ) { this . match = match ; this . end = end ; } public SpanQuery getMatch ( ) { return match ; } public int getEnd ( ) { return end ; } public String getField ( ) { return match . getField ( ) ; } public Collection getTerms ( ) { return match . getTerms ( ) ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "spanFirst(" ) ; buffer . append ( match . toString ( field ) ) ; buffer . append ( ", " ) ; buffer . append ( end ) ; buffer . append ( ")" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public void extractTerms ( Set terms ) { match . extractTerms ( terms ) ; } public Spans getSpans ( final IndexReader reader ) throws IOException { return new Spans ( ) { private Spans spans = match . getSpans ( reader ) ; public boolean next ( ) throws IOException { while ( spans . next ( ) ) { if ( end ( ) <= end ) return true ; } return false ; } public boolean skipTo ( int target ) throws IOException { if ( ! spans . skipTo ( target ) ) return false ; if ( spans . end ( ) <= end ) return true ; return next ( ) ; } public int doc ( ) { return spans . doc ( ) ; } public int start ( ) { return spans . start ( ) ; } public int end ( ) { return spans . end ( ) ; } public String toString ( ) { return "spans(" + SpanFirstQuery . this . toString ( ) + ")" ; } } ; } public Query rewrite ( IndexReader reader ) throws IOException { SpanFirstQuery clone = null ; SpanQuery rewritten = ( SpanQuery ) match . rewrite ( reader ) ; if ( rewritten != match ) { clone = ( SpanFirstQuery ) this . clone ( ) ; clone . match = rewritten ; } if ( clone != null ) { return clone ; } else { return this ; } } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof SpanFirstQuery ) ) return false ; SpanFirstQuery other = ( SpanFirstQuery ) o ; return this . end == other . end && this . match . equals ( other . match ) && this . getBoost ( ) == other . getBoost ( ) ; } public int hashCode ( ) { int h = match . hashCode ( ) ; h ^= ( h << 8 ) | ( h > > > 25 ) ; h ^= Float . floatToRawIntBits ( getBoost ( ) ) ^ end ; return h ; } } 	0	['13', '3', '0', '6', '30', '0', '1', '6', '11', '0.416666667', '176', '1', '1', '0.571428571', '0.192307692', '2', '2', '12.38461538', '6', '1.3077', '0']
package org . apache . lucene ; public final class LucenePackage { private LucenePackage ( ) { } public static Package get ( ) { return LucenePackage . class . getPackage ( ) ; } } 	0	['3', '1', '0', '0', '8', '3', '0', '0', '1', '1', '27', '0', '0', '0', '0.333333333', '0', '0', '7.666666667', '2', '1', '0']
package org . apache . lucene . util ; import java . io . ObjectStreamException ; import java . io . Serializable ; import java . io . StreamCorruptedException ; import java . util . HashMap ; import java . util . Map ; public abstract class Parameter implements Serializable { static Map allParameters = new HashMap ( ) ; private String name ; private Parameter ( ) { } protected Parameter ( String name ) { this . name = name ; String key = makeKey ( name ) ; if ( allParameters . containsKey ( key ) ) throw new IllegalArgumentException ( "Parameter name " + key + " already used!" ) ; allParameters . put ( key , this ) ; } private String makeKey ( String name ) { return getClass ( ) + " " + name ; } public String toString ( ) { return name ; } protected Object readResolve ( ) throws ObjectStreamException { Object par = allParameters . get ( makeKey ( name ) ) ; if ( par == null ) throw new StreamCorruptedException ( "Unknown parameter value: " + name ) ; return par ; } } 	0	['6', '1', '5', '5', '18', '5', '5', '0', '1', '0.6', '88', '0.5', '0', '0', '0.7', '0', '0', '13.33333333', '1', '0.5', '0']
package org . apache . lucene . search . spans ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . util . ToStringUtils ; import java . io . IOException ; import java . util . ArrayList ; import java . util . Collection ; import java . util . Set ; public class SpanTermQuery extends SpanQuery { protected Term term ; public SpanTermQuery ( Term term ) { this . term = term ; } public Term getTerm ( ) { return term ; } public String getField ( ) { return term . field ( ) ; } public Collection getTerms ( ) { Collection terms = new ArrayList ( ) ; terms . add ( term ) ; return terms ; } public void extractTerms ( Set terms ) { terms . add ( term ) ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( term . field ( ) . equals ( field ) ) buffer . append ( term . text ( ) ) ; else buffer . append ( term . toString ( ) ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof SpanTermQuery ) ) return false ; SpanTermQuery other = ( SpanTermQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && this . term . equals ( other . term ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ term . hashCode ( ) ^ 0xD23FE494 ; } public Spans getSpans ( final IndexReader reader ) throws IOException { return new TermSpans ( reader . termPositions ( term ) , term ) ; } } 	0	['9', '3', '1', '8', '27', '0', '1', '7', '9', '0', '116', '1', '1', '0.666666667', '0.259259259', '2', '2', '11.77777778', '4', '1.3333', '0']
package org . apache . lucene . util ; public abstract class StringHelper { public static final int stringDifference ( String s1 , String s2 ) { int len1 = s1 . length ( ) ; int len2 = s2 . length ( ) ; int len = len1 < len2 ? len1 : len2 ; for ( int i = 0 ; i < len ; i ++ ) { if ( s1 . charAt ( i ) != s2 . charAt ( i ) ) { return i ; } } return len ; } private StringHelper ( ) { } } 	0	['2', '1', '0', '2', '5', '1', '2', '0', '1', '2', '36', '0', '0', '0', '0.5', '0', '0', '17', '4', '2', '0']
package org . apache . lucene . index ; public interface TermPositionVector extends TermFreqVector { public int [ ] getTermPositions ( int index ) ; public TermVectorOffsetInfo [ ] getOffsets ( int index ) ; } 	0	['2', '1', '0', '4', '2', '1', '2', '2', '2', '2', '2', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . util ; import java . io . IOException ; import org . apache . lucene . search . Scorer ; public class ScorerDocQueue { private final HeapedScorerDoc [ ] heap ; private final int maxSize ; private int size ; private class HeapedScorerDoc { Scorer scorer ; int doc ; HeapedScorerDoc ( Scorer s ) { this ( s , s . doc ( ) ) ; } HeapedScorerDoc ( Scorer scorer , int doc ) { this . scorer = scorer ; this . doc = doc ; } void adjust ( ) { doc = scorer . doc ( ) ; } } private HeapedScorerDoc topHSD ; public ScorerDocQueue ( int maxSize ) { size = 0 ; int heapSize = maxSize + 1 ; heap = new HeapedScorerDoc [ heapSize ] ; this . maxSize = maxSize ; topHSD = heap [ 1 ] ; } public final void put ( Scorer scorer ) { size ++ ; heap [ size ] = new HeapedScorerDoc ( scorer ) ; upHeap ( ) ; } public boolean insert ( Scorer scorer ) { if ( size < maxSize ) { put ( scorer ) ; return true ; } else { int docNr = scorer . doc ( ) ; if ( ( size > 0 ) && ( ! ( docNr < topHSD . doc ) ) ) { heap [ 1 ] = new HeapedScorerDoc ( scorer , docNr ) ; downHeap ( ) ; return true ; } else { return false ; } } } public final Scorer top ( ) { return topHSD . scorer ; } public final int topDoc ( ) { return topHSD . doc ; } public final float topScore ( ) throws IOException { return topHSD . scorer . score ( ) ; } public final boolean topNextAndAdjustElsePop ( ) throws IOException { return checkAdjustElsePop ( topHSD . scorer . next ( ) ) ; } public final boolean topSkipToAndAdjustElsePop ( int target ) throws IOException { return checkAdjustElsePop ( topHSD . scorer . skipTo ( target ) ) ; } private boolean checkAdjustElsePop ( boolean cond ) { if ( cond ) { topHSD . doc = topHSD . scorer . doc ( ) ; } else { heap [ 1 ] = heap [ size ] ; heap [ size ] = null ; size -- ; } downHeap ( ) ; return cond ; } public final Scorer pop ( ) { Scorer result = topHSD . scorer ; popNoResult ( ) ; return result ; } private final void popNoResult ( ) { heap [ 1 ] = heap [ size ] ; heap [ size ] = null ; size -- ; downHeap ( ) ; } public final void adjustTop ( ) { topHSD . adjust ( ) ; downHeap ( ) ; } public final int size ( ) { return size ; } public final void clear ( ) { for ( int i = 0 ; i <= size ; i ++ ) { heap [ i ] = null ; } size = 0 ; } private final void upHeap ( ) { int i = size ; HeapedScorerDoc node = heap [ i ] ; int j = i > > > 1 ; while ( ( j > 0 ) && ( node . doc < heap [ j ] . doc ) ) { heap [ i ] = heap [ j ] ; i = j ; j = j > > > 1 ; } heap [ i ] = node ; topHSD = heap [ 1 ] ; } private final void downHeap ( ) { int i = 1 ; HeapedScorerDoc node = heap [ i ] ; int j = i << 1 ; int k = j + 1 ; if ( ( k <= size ) && ( heap [ k ] . doc < heap [ j ] . doc ) ) { j = k ; } while ( ( j <= size ) && ( heap [ j ] . doc < node . doc ) ) { heap [ i ] = heap [ j ] ; i = j ; j = i << 1 ; k = j + 1 ; if ( k <= size && ( heap [ k ] . doc < heap [ j ] . doc ) ) { j = k ; } } heap [ i ] = node ; topHSD = heap [ 1 ] ; } } 	0	['16', '1', '0', '3', '24', '0', '2', '2', '12', '0.383333333', '361', '1', '2', '0', '0.328125', '0', '0', '21.3125', '7', '1.75', '0']
package org . apache . lucene . search ; public class DefaultSimilarity extends Similarity { public float lengthNorm ( String fieldName , int numTerms ) { return ( float ) ( 1.0 / Math . sqrt ( numTerms ) ) ; } public float queryNorm ( float sumOfSquaredWeights ) { return ( float ) ( 1.0 / Math . sqrt ( sumOfSquaredWeights ) ) ; } public float tf ( float freq ) { return ( float ) Math . sqrt ( freq ) ; } public float sloppyFreq ( int distance ) { return 1.0f / ( distance + 1 ) ; } public float idf ( int docFreq , int numDocs ) { return ( float ) ( Math . log ( numDocs / ( double ) ( docFreq + 1 ) ) + 1.0 ) ; } public float coord ( int overlap , int maxOverlap ) { return overlap / ( float ) maxOverlap ; } } 	0	['7', '2', '0', '3', '10', '21', '3', '1', '7', '2', '54', '0', '0', '0.714285714', '0.5', '1', '2', '6.714285714', '1', '0.8571', '0']
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; final class PhraseQueue extends PriorityQueue { PhraseQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object o1 , Object o2 ) { PhrasePositions pp1 = ( PhrasePositions ) o1 ; PhrasePositions pp2 = ( PhrasePositions ) o2 ; if ( pp1 . doc == pp2 . doc ) if ( pp1 . position == pp2 . position ) return pp1 . offset < pp2 . offset ; else return pp1 . position < pp2 . position ; else return pp1 . doc < pp2 . doc ; } } 	0	['2', '2', '0', '5', '4', '1', '3', '2', '0', '2', '51', '0', '0', '0.916666667', '0.666666667', '1', '3', '24.5', '6', '3', '0']
package org . apache . lucene . store ; public class AlreadyClosedException extends IllegalStateException { public AlreadyClosedException ( String message ) { super ( message ) ; } } 	0	['1', '5', '0', '4', '2', '0', '4', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; public interface Spans { boolean next ( ) throws IOException ; boolean skipTo ( int target ) throws IOException ; int doc ( ) ; int start ( ) ; int end ( ) ; } 	0	['5', '1', '0', '20', '5', '10', '20', '0', '5', '2', '5', '0', '0', '0', '0.6', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . index ; import java . io . IOException ; final class SegmentMergeInfo { Term term ; int base ; TermEnum termEnum ; IndexReader reader ; private TermPositions postings ; private int [ ] docMap ; SegmentMergeInfo ( int b , TermEnum te , IndexReader r ) throws IOException { base = b ; reader = r ; termEnum = te ; term = te . term ( ) ; } int [ ] getDocMap ( ) { if ( docMap == null ) { if ( reader . hasDeletions ( ) ) { int maxDoc = reader . maxDoc ( ) ; docMap = new int [ maxDoc ] ; int j = 0 ; for ( int i = 0 ; i < maxDoc ; i ++ ) { if ( reader . isDeleted ( i ) ) docMap [ i ] = - 1 ; else docMap [ i ] = j ++ ; } } } return docMap ; } TermPositions getPositions ( ) throws IOException { if ( postings == null ) { postings = reader . termPositions ( ) ; } return postings ; } final boolean next ( ) throws IOException { if ( termEnum . next ( ) ) { term = termEnum . term ( ) ; return true ; } else { term = null ; return false ; } } final void close ( ) throws IOException { termEnum . close ( ) ; if ( postings != null ) { postings . close ( ) ; } } } 	0	['5', '1', '0', '7', '14', '0', '3', '4', '0', '0.75', '108', '0.333333333', '4', '0', '0.4', '0', '0', '19.4', '5', '1.6', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . util . BitVector ; import org . apache . lucene . store . IndexInput ; class SegmentTermDocs implements TermDocs { protected SegmentReader parent ; protected IndexInput freqStream ; protected int count ; protected int df ; protected BitVector deletedDocs ; int doc = 0 ; int freq ; private int skipInterval ; private int maxSkipLevels ; private DefaultSkipListReader skipListReader ; private long freqBasePointer ; private long proxBasePointer ; private long skipPointer ; private boolean haveSkipped ; protected boolean currentFieldStoresPayloads ; protected SegmentTermDocs ( SegmentReader parent ) { this . parent = parent ; this . freqStream = ( IndexInput ) parent . freqStream . clone ( ) ; this . deletedDocs = parent . deletedDocs ; this . skipInterval = parent . tis . getSkipInterval ( ) ; this . maxSkipLevels = parent . tis . getMaxSkipLevels ( ) ; } public void seek ( Term term ) throws IOException { TermInfo ti = parent . tis . get ( term ) ; seek ( ti , term ) ; } public void seek ( TermEnum termEnum ) throws IOException { TermInfo ti ; Term term ; if ( termEnum instanceof SegmentTermEnum && ( ( SegmentTermEnum ) termEnum ) . fieldInfos == parent . fieldInfos ) { SegmentTermEnum segmentTermEnum = ( ( SegmentTermEnum ) termEnum ) ; term = segmentTermEnum . term ( ) ; ti = segmentTermEnum . termInfo ( ) ; } else { term = termEnum . term ( ) ; ti = parent . tis . get ( term ) ; } seek ( ti , term ) ; } void seek ( TermInfo ti , Term term ) throws IOException { count = 0 ; FieldInfo fi = parent . fieldInfos . fieldInfo ( term . field ) ; currentFieldStoresPayloads = ( fi != null ) ? fi . storePayloads : false ; if ( ti == null ) { df = 0 ; } else { df = ti . docFreq ; doc = 0 ; freqBasePointer = ti . freqPointer ; proxBasePointer = ti . proxPointer ; skipPointer = freqBasePointer + ti . skipOffset ; freqStream . seek ( freqBasePointer ) ; haveSkipped = false ; } } public void close ( ) throws IOException { freqStream . close ( ) ; if ( skipListReader != null ) skipListReader . close ( ) ; } public final int doc ( ) { return doc ; } public final int freq ( ) { return freq ; } protected void skippingDoc ( ) throws IOException { } public boolean next ( ) throws IOException { while ( true ) { if ( count == df ) return false ; int docCode = freqStream . readVInt ( ) ; doc += docCode > > > 1 ; if ( ( docCode & 1 ) != 0 ) freq = 1 ; else freq = freqStream . readVInt ( ) ; count ++ ; if ( deletedDocs == null || ! deletedDocs . get ( doc ) ) break ; skippingDoc ( ) ; } return true ; } public int read ( final int [ ] docs , final int [ ] freqs ) throws IOException { final int length = docs . length ; int i = 0 ; while ( i < length && count < df ) { final int docCode = freqStream . readVInt ( ) ; doc += docCode > > > 1 ; if ( ( docCode & 1 ) != 0 ) freq = 1 ; else freq = freqStream . readVInt ( ) ; count ++ ; if ( deletedDocs == null || ! deletedDocs . get ( doc ) ) { docs [ i ] = doc ; freqs [ i ] = freq ; ++ i ; } } return i ; } protected void skipProx ( long proxPointer , int payloadLength ) throws IOException { } public boolean skipTo ( int target ) throws IOException { if ( df >= skipInterval ) { if ( skipListReader == null ) skipListReader = new DefaultSkipListReader ( ( IndexInput ) freqStream . clone ( ) , maxSkipLevels , skipInterval ) ; if ( ! haveSkipped ) { skipListReader . init ( skipPointer , freqBasePointer , proxBasePointer , df , currentFieldStoresPayloads ) ; haveSkipped = true ; } int newCount = skipListReader . skipTo ( target ) ; if ( newCount > count ) { freqStream . seek ( skipListReader . getFreqPointer ( ) ) ; skipProx ( skipListReader . getProxPointer ( ) , skipListReader . getPayloadLength ( ) ) ; doc = skipListReader . getDoc ( ) ; count = newCount ; } } do { if ( ! next ( ) ) return false ; } while ( target > doc ) ; return true ; } } 	0	['12', '1', '1', '13', '33', '12', '2', '12', '8', '0.690909091', '377', '0.866666667', '4', '0', '0.21875', '0', '0', '29.16666667', '1', '0.9167', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; public final class WhitespaceAnalyzer extends Analyzer { public TokenStream tokenStream ( String fieldName , Reader reader ) { return new WhitespaceTokenizer ( reader ) ; } } 	0	['2', '2', '0', '3', '4', '1', '0', '3', '2', '2', '10', '0', '0', '0.666666667', '0.666666667', '0', '0', '4', '1', '0.5', '0']
package org . apache . lucene . analysis . standard ; public class TokenMgrError extends Error { static final int LEXICAL_ERROR = 0 ; static final int STATIC_LEXER_ERROR = 1 ; static final int INVALID_LEXICAL_STATE = 2 ; static final int LOOP_DETECTED = 3 ; int errorCode ; protected static final String addEscapes ( String str ) { StringBuffer retval = new StringBuffer ( ) ; char ch ; for ( int i = 0 ; i < str . length ( ) ; i ++ ) { switch ( str . charAt ( i ) ) { case 0 : continue ; case '\b' : retval . append ( "\\b" ) ; continue ; case '\t' : retval . append ( "\\t" ) ; continue ; case '\n' : retval . append ( "\\n" ) ; continue ; case '\f' : retval . append ( "\\f" ) ; continue ; case '\r' : retval . append ( "\\r" ) ; continue ; case '\"' : retval . append ( "\\\"" ) ; continue ; case '\'' : retval . append ( "\\\'" ) ; continue ; case '\\' : retval . append ( "\\\\" ) ; continue ; default : if ( ( ch = str . charAt ( i ) ) < 0x20 || ch > 0x7e ) { String s = "0000" + Integer . toString ( ch , 16 ) ; retval . append ( "\\u" + s . substring ( s . length ( ) - 4 , s . length ( ) ) ) ; } else { retval . append ( ch ) ; } continue ; } } return retval . toString ( ) ; } protected static String LexicalError ( boolean EOFSeen , int lexState , int errorLine , int errorColumn , String errorAfter , char curChar ) { return ( "Lexical error at line " + errorLine + ", column " + errorColumn + ".  Encountered: " + ( EOFSeen ? "<EOF> " : ( "\"" + addEscapes ( String . valueOf ( curChar ) ) + "\"" ) + " (" + ( int ) curChar + "), " ) + "after : \"" + addEscapes ( errorAfter ) + "\"" ) ; } public String getMessage ( ) { return super . getMessage ( ) ; } public TokenMgrError ( ) { } public TokenMgrError ( String message , int reason ) { super ( message ) ; errorCode = reason ; } public TokenMgrError ( boolean EOFSeen , int lexState , int errorLine , int errorColumn , String errorAfter , char curChar , int reason ) { this ( LexicalError ( EOFSeen , lexState , errorLine , errorColumn , errorAfter , curChar ) , reason ) ; } } 	0	['6', '3', '0', '1', '19', '15', '1', '0', '4', '1.12', '184', '0', '0', '0.8125', '0.5', '1', '1', '28.83333333', '14', '2.8333', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . * ; final class ExactPhraseScorer extends PhraseScorer { ExactPhraseScorer ( Weight weight , TermPositions [ ] tps , int [ ] offsets , Similarity similarity , byte [ ] norms ) { super ( weight , tps , offsets , similarity , norms ) ; } protected final float phraseFreq ( ) throws IOException { pq . clear ( ) ; for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) { pp . firstPosition ( ) ; pq . put ( pp ) ; } pqToList ( ) ; int freq = 0 ; do { while ( first . position < last . position ) { do { if ( ! first . nextPosition ( ) ) return ( float ) freq ; } while ( first . position < last . position ) ; firstToLast ( ) ; } freq ++ ; } while ( last . nextPosition ( ) ) ; return ( float ) freq ; } } 	0	['2', '3', '0', '8', '9', '1', '2', '6', '0', '2', '64', '0', '0', '0.952380952', '0.583333333', '1', '1', '31', '1', '0.5', '0']
package org . apache . lucene . search ; import java . io . IOException ; public class ReqExclScorer extends Scorer { private Scorer reqScorer , exclScorer ; public ReqExclScorer ( Scorer reqScorer , Scorer exclScorer ) { super ( null ) ; this . reqScorer = reqScorer ; this . exclScorer = exclScorer ; } private boolean firstTime = true ; public boolean next ( ) throws IOException { if ( firstTime ) { if ( ! exclScorer . next ( ) ) { exclScorer = null ; } firstTime = false ; } if ( reqScorer == null ) { return false ; } if ( ! reqScorer . next ( ) ) { reqScorer = null ; return false ; } if ( exclScorer == null ) { return true ; } return toNonExcluded ( ) ; } private boolean toNonExcluded ( ) throws IOException { int exclDoc = exclScorer . doc ( ) ; do { int reqDoc = reqScorer . doc ( ) ; if ( reqDoc < exclDoc ) { return true ; } else if ( reqDoc > exclDoc ) { if ( ! exclScorer . skipTo ( reqDoc ) ) { exclScorer = null ; return true ; } exclDoc = exclScorer . doc ( ) ; if ( exclDoc > reqDoc ) { return true ; } } } while ( reqScorer . next ( ) ) ; reqScorer = null ; return false ; } public int doc ( ) { return reqScorer . doc ( ) ; } public float score ( ) throws IOException { return reqScorer . score ( ) ; } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { firstTime = false ; if ( ! exclScorer . skipTo ( target ) ) { exclScorer = null ; } } if ( reqScorer == null ) { return false ; } if ( exclScorer == null ) { return reqScorer . skipTo ( target ) ; } if ( ! reqScorer . skipTo ( target ) ) { reqScorer = null ; return false ; } return toNonExcluded ( ) ; } public Explanation explain ( int doc ) throws IOException { Explanation res = new Explanation ( ) ; if ( exclScorer . skipTo ( doc ) && ( exclScorer . doc ( ) == doc ) ) { res . setDescription ( "excluded" ) ; } else { res . setDescription ( "not excluded" ) ; res . addDetail ( reqScorer . explain ( doc ) ) ; } return res ; } } 	0	['7', '2', '0', '4', '16', '0', '1', '3', '6', '0.333333333', '179', '1', '2', '0.571428571', '0.476190476', '1', '3', '24.14285714', '1', '0.8571', '0']
package org . apache . lucene . analysis . standard ; public class ParseException extends java . io . IOException { public ParseException ( Token currentTokenVal , int [ ] [ ] expectedTokenSequencesVal , String [ ] tokenImageVal ) { super ( "" ) ; specialConstructor = true ; currentToken = currentTokenVal ; expectedTokenSequences = expectedTokenSequencesVal ; tokenImage = tokenImageVal ; } public ParseException ( ) { super ( ) ; specialConstructor = false ; } public ParseException ( String message ) { super ( message ) ; specialConstructor = false ; } protected boolean specialConstructor ; public Token currentToken ; public int [ ] [ ] expectedTokenSequences ; public String [ ] tokenImage ; public String getMessage ( ) { if ( ! specialConstructor ) { return super . getMessage ( ) ; } String expected = "" ; int maxSize = 0 ; for ( int i = 0 ; i < expectedTokenSequences . length ; i ++ ) { if ( maxSize < expectedTokenSequences [ i ] . length ) { maxSize = expectedTokenSequences [ i ] . length ; } for ( int j = 0 ; j < expectedTokenSequences [ i ] . length ; j ++ ) { expected += tokenImage [ expectedTokenSequences [ i ] [ j ] ] + " " ; } if ( expectedTokenSequences [ i ] [ expectedTokenSequences [ i ] . length - 1 ] != 0 ) { expected += "..." ; } expected += eol + "    " ; } String retval = "Encountered \"" ; Token tok = currentToken . next ; for ( int i = 0 ; i < maxSize ; i ++ ) { if ( i != 0 ) retval += " " ; if ( tok . kind == 0 ) { retval += tokenImage [ 0 ] ; break ; } retval += add_escapes ( tok . image ) ; tok = tok . next ; } retval += "\" at line " + currentToken . next . beginLine + ", column " + currentToken . next . beginColumn + "." + eol ; if ( expectedTokenSequences . length == 1 ) { retval += "Was expecting:" + eol + "    " ; } else { retval += "Was expecting one of:" + eol + "    " ; } retval += expected ; return retval ; } protected String eol = System . getProperty ( "line.separator" , "\n" ) ; protected String add_escapes ( String str ) { StringBuffer retval = new StringBuffer ( ) ; char ch ; for ( int i = 0 ; i < str . length ( ) ; i ++ ) { switch ( str . charAt ( i ) ) { case 0 : continue ; case '\b' : retval . append ( "\\b" ) ; continue ; case '\t' : retval . append ( "\\t" ) ; continue ; case '\n' : retval . append ( "\\n" ) ; continue ; case '\f' : retval . append ( "\\f" ) ; continue ; case '\r' : retval . append ( "\\r" ) ; continue ; case '\"' : retval . append ( "\\\"" ) ; continue ; case '\'' : retval . append ( "\\\'" ) ; continue ; case '\\' : retval . append ( "\\\\" ) ; continue ; default : if ( ( ch = str . charAt ( i ) ) < 0x20 || ch > 0x7e ) { String s = "0000" + Integer . toString ( ch , 16 ) ; retval . append ( "\\u" + s . substring ( s . length ( ) - 4 , s . length ( ) ) ) ; } else { retval . append ( ch ) ; } continue ; } } return retval . toString ( ) ; } } 	0	['5', '4', '0', '2', '18', '0', '1', '1', '4', '0.55', '380', '0.4', '1', '0.866666667', '0.4', '1', '1', '74', '14', '4.8', '0']
package org . apache . lucene . index ; class SegmentTermPositionVector extends SegmentTermVector implements TermPositionVector { protected int [ ] [ ] positions ; protected TermVectorOffsetInfo [ ] [ ] offsets ; public static final int [ ] EMPTY_TERM_POS = new int [ 0 ] ; public SegmentTermPositionVector ( String field , String terms [ ] , int termFreqs [ ] , int [ ] [ ] positions , TermVectorOffsetInfo [ ] [ ] offsets ) { super ( field , terms , termFreqs ) ; this . offsets = offsets ; this . positions = positions ; } public TermVectorOffsetInfo [ ] getOffsets ( int index ) { TermVectorOffsetInfo [ ] result = TermVectorOffsetInfo . EMPTY_OFFSET_INFO ; if ( offsets == null ) return null ; if ( index >= 0 && index < offsets . length ) { result = offsets [ index ] ; } return result ; } public int [ ] getTermPositions ( int index ) { int [ ] result = EMPTY_TERM_POS ; if ( positions == null ) return null ; if ( index >= 0 && index < positions . length ) { result = positions [ index ] ; } return result ; } } 	0	['4', '2', '0', '4', '5', '0', '1', '3', '3', '0.666666667', '65', '0.666666667', '1', '0.777777778', '0.476190476', '0', '0', '14.5', '4', '2', '0']
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; final class HitQueue extends PriorityQueue { HitQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object a , Object b ) { ScoreDoc hitA = ( ScoreDoc ) a ; ScoreDoc hitB = ( ScoreDoc ) b ; if ( hitA . score == hitB . score ) return hitA . doc > hitB . doc ; else return hitA . score < hitB . score ; } } 	0	['2', '2', '0', '6', '4', '1', '4', '2', '0', '2', '39', '0', '0', '0.916666667', '0.666666667', '1', '3', '18.5', '4', '2', '0']
package org . apache . lucene . search ; import java . io . IOException ; class NonMatchingScorer extends Scorer { public NonMatchingScorer ( ) { super ( null ) ; } public int doc ( ) { throw new UnsupportedOperationException ( ) ; } public boolean next ( ) throws IOException { return false ; } public float score ( ) { throw new UnsupportedOperationException ( ) ; } public boolean skipTo ( int target ) { return false ; } public Explanation explain ( int doc ) { Explanation e = new Explanation ( ) ; e . setDescription ( "No document matches." ) ; return e ; } } 	0	['6', '2', '0', '4', '10', '15', '1', '3', '6', '2', '31', '0', '0', '0.615384615', '0.666666667', '1', '3', '4.166666667', '1', '0.8333', '0']
package org . apache . lucene . index ; import java . io . IOException ; public class StaleReaderException extends IOException { public StaleReaderException ( String message ) { super ( message ) ; } } 	0	['1', '4', '0', '2', '2', '0', '2', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . search . spans ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . search . * ; import java . io . IOException ; import java . util . HashSet ; import java . util . Iterator ; import java . util . Set ; public class SpanWeight implements Weight { protected Similarity similarity ; protected float value ; protected float idf ; protected float queryNorm ; protected float queryWeight ; protected Set terms ; protected SpanQuery query ; public SpanWeight ( SpanQuery query , Searcher searcher ) throws IOException { this . similarity = query . getSimilarity ( searcher ) ; this . query = query ; terms = new HashSet ( ) ; query . extractTerms ( terms ) ; idf = this . query . getSimilarity ( searcher ) . idf ( terms , searcher ) ; } public Query getQuery ( ) { return query ; } public float getValue ( ) { return value ; } public float sumOfSquaredWeights ( ) throws IOException { queryWeight = idf * query . getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float queryNorm ) { this . queryNorm = queryNorm ; queryWeight *= queryNorm ; value = queryWeight * idf ; } public Scorer scorer ( IndexReader reader ) throws IOException { return new SpanScorer ( query . getSpans ( reader ) , this , similarity , reader . norms ( query . getField ( ) ) ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { ComplexExplanation result = new ComplexExplanation ( ) ; result . setDescription ( "weight(" + getQuery ( ) + " in " + doc + "), product of:" ) ; String field = ( ( SpanQuery ) getQuery ( ) ) . getField ( ) ; StringBuffer docFreqs = new StringBuffer ( ) ; Iterator i = terms . iterator ( ) ; while ( i . hasNext ( ) ) { Term term = ( Term ) i . next ( ) ; docFreqs . append ( term . text ( ) ) ; docFreqs . append ( "=" ) ; docFreqs . append ( reader . docFreq ( term ) ) ; if ( i . hasNext ( ) ) { docFreqs . append ( " " ) ; } } Explanation idfExpl = new Explanation ( idf , "idf(" + field + ": " + docFreqs + ")" ) ; Explanation queryExpl = new Explanation ( ) ; queryExpl . setDescription ( "queryWeight(" + getQuery ( ) + "), product of:" ) ; Explanation boostExpl = new Explanation ( getQuery ( ) . getBoost ( ) , "boost" ) ; if ( getQuery ( ) . getBoost ( ) != 1.0f ) queryExpl . addDetail ( boostExpl ) ; queryExpl . addDetail ( idfExpl ) ; Explanation queryNormExpl = new Explanation ( queryNorm , "queryNorm" ) ; queryExpl . addDetail ( queryNormExpl ) ; queryExpl . setValue ( boostExpl . getValue ( ) * idfExpl . getValue ( ) * queryNormExpl . getValue ( ) ) ; result . addDetail ( queryExpl ) ; ComplexExplanation fieldExpl = new ComplexExplanation ( ) ; fieldExpl . setDescription ( "fieldWeight(" + field + ":" + query . toString ( field ) + " in " + doc + "), product of:" ) ; Explanation tfExpl = scorer ( reader ) . explain ( doc ) ; fieldExpl . addDetail ( tfExpl ) ; fieldExpl . addDetail ( idfExpl ) ; Explanation fieldNormExpl = new Explanation ( ) ; byte [ ] fieldNorms = reader . norms ( field ) ; float fieldNorm = fieldNorms != null ? Similarity . decodeNorm ( fieldNorms [ doc ] ) : 0.0f ; fieldNormExpl . setValue ( fieldNorm ) ; fieldNormExpl . setDescription ( "fieldNorm(field=" + field + ", doc=" + doc + ")" ) ; fieldExpl . addDetail ( fieldNormExpl ) ; fieldExpl . setMatch ( Boolean . valueOf ( tfExpl . isMatch ( ) ) ) ; fieldExpl . setValue ( tfExpl . getValue ( ) * idfExpl . getValue ( ) * fieldNormExpl . getValue ( ) ) ; result . addDetail ( fieldExpl ) ; result . setMatch ( fieldExpl . getMatch ( ) ) ; result . setValue ( queryExpl . getValue ( ) * fieldExpl . getValue ( ) ) ; if ( queryExpl . getValue ( ) == 1.0f ) return fieldExpl ; return result ; } } 	0	['7', '1', '1', '13', '46', '0', '2', '12', '7', '0.69047619', '357', '1', '2', '0', '0.30952381', '0', '0', '49', '1', '0.8571', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import org . apache . lucene . store . IndexOutput ; class DefaultSkipListWriter extends MultiLevelSkipListWriter { private int [ ] lastSkipDoc ; private int [ ] lastSkipPayloadLength ; private long [ ] lastSkipFreqPointer ; private long [ ] lastSkipProxPointer ; private IndexOutput freqOutput ; private IndexOutput proxOutput ; private int curDoc ; private boolean curStorePayloads ; private int curPayloadLength ; private long curFreqPointer ; private long curProxPointer ; DefaultSkipListWriter ( int skipInterval , int numberOfSkipLevels , int docCount , IndexOutput freqOutput , IndexOutput proxOutput ) { super ( skipInterval , numberOfSkipLevels , docCount ) ; this . freqOutput = freqOutput ; this . proxOutput = proxOutput ; lastSkipDoc = new int [ numberOfSkipLevels ] ; lastSkipPayloadLength = new int [ numberOfSkipLevels ] ; lastSkipFreqPointer = new long [ numberOfSkipLevels ] ; lastSkipProxPointer = new long [ numberOfSkipLevels ] ; } void setSkipData ( int doc , boolean storePayloads , int payloadLength ) { this . curDoc = doc ; this . curStorePayloads = storePayloads ; this . curPayloadLength = payloadLength ; this . curFreqPointer = freqOutput . getFilePointer ( ) ; this . curProxPointer = proxOutput . getFilePointer ( ) ; } protected void resetSkip ( ) { super . resetSkip ( ) ; Arrays . fill ( lastSkipDoc , 0 ) ; Arrays . fill ( lastSkipPayloadLength , - 1 ) ; Arrays . fill ( lastSkipFreqPointer , freqOutput . getFilePointer ( ) ) ; Arrays . fill ( lastSkipProxPointer , proxOutput . getFilePointer ( ) ) ; } protected void writeSkipData ( int level , IndexOutput skipBuffer ) throws IOException { if ( curStorePayloads ) { int delta = curDoc - lastSkipDoc [ level ] ; if ( curPayloadLength == lastSkipPayloadLength [ level ] ) { skipBuffer . writeVInt ( delta * 2 ) ; } else { skipBuffer . writeVInt ( delta * 2 + 1 ) ; skipBuffer . writeVInt ( curPayloadLength ) ; lastSkipPayloadLength [ level ] = curPayloadLength ; } } else { skipBuffer . writeVInt ( curDoc - lastSkipDoc [ level ] ) ; } skipBuffer . writeVInt ( ( int ) ( curFreqPointer - lastSkipFreqPointer [ level ] ) ) ; skipBuffer . writeVInt ( ( int ) ( curProxPointer - lastSkipProxPointer [ level ] ) ) ; lastSkipDoc [ level ] = curDoc ; lastSkipFreqPointer [ level ] = curFreqPointer ; lastSkipProxPointer [ level ] = curProxPointer ; } } 	0	['4', '2', '0', '3', '10', '0', '1', '2', '0', '0.484848485', '176', '1', '2', '0.625', '0.625', '1', '1', '40.25', '1', '0.75', '0']
package org . apache . lucene . analysis . standard ; public interface StandardTokenizerConstants { int EOF = 0 ; int ALPHANUM = 1 ; int APOSTROPHE = 2 ; int ACRONYM = 3 ; int COMPANY = 4 ; int EMAIL = 5 ; int HOST = 6 ; int NUM = 7 ; int P = 8 ; int HAS_DIGIT = 9 ; int ALPHA = 10 ; int LETTER = 11 ; int CJ = 12 ; int KOREAN = 13 ; int DIGIT = 14 ; int NOISE = 15 ; int DEFAULT = 0 ; String [ ] tokenImage = { "<EOF>" , "<ALPHANUM>" , "<APOSTROPHE>" , "<ACRONYM>" , "<COMPANY>" , "<EMAIL>" , "<HOST>" , "<NUM>" , "<P>" , "<HAS_DIGIT>" , "<ALPHA>" , "<LETTER>" , "<CJ>" , "<KOREAN>" , "<DIGIT>" , "<NOISE>" , } ; } 	0	['1', '1', '0', '3', '1', '0', '3', '0', '0', '2', '87', '0', '0', '0', '0', '0', '0', '68', '0', '0', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; public abstract class SortComparator implements SortComparatorSource { public ScoreDocComparator newComparator ( final IndexReader reader , final String fieldname ) throws IOException { final String field = fieldname . intern ( ) ; final Comparable [ ] cachedValues = FieldCache . DEFAULT . getCustom ( reader , field , SortComparator . this ) ; return new ScoreDocComparator ( ) { public int compare ( ScoreDoc i , ScoreDoc j ) { return cachedValues [ i . doc ] . compareTo ( cachedValues [ j . doc ] ) ; } public Comparable sortValue ( ScoreDoc i ) { return cachedValues [ i . doc ] ; } public int sortType ( ) { return SortField . CUSTOM ; } } ; } protected abstract Comparable getComparable ( String termtext ) ; } 	0	['3', '1', '0', '7', '7', '3', '4', '5', '2', '2', '21', '0', '0', '0', '0.666666667', '0', '0', '6', '1', '0.6667', '0']
package org . apache . lucene . index ; import java . io . IOException ; public abstract class TermEnum { public abstract boolean next ( ) throws IOException ; public abstract Term term ( ) ; public abstract int docFreq ( ) ; public abstract void close ( ) throws IOException ; public boolean skipTo ( Term target ) throws IOException { do { if ( ! next ( ) ) return false ; } while ( target . compareTo ( term ( ) ) > 0 ) ; return true ; } } 	0	['6', '1', '5', '33', '8', '15', '32', '1', '6', '2', '21', '0', '0', '0', '0.583333333', '0', '0', '2.5', '1', '0.8333', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . TermDocs ; final class TermScorer extends Scorer { private Weight weight ; private TermDocs termDocs ; private byte [ ] norms ; private float weightValue ; private int doc ; private final int [ ] docs = new int [ 32 ] ; private final int [ ] freqs = new int [ 32 ] ; private int pointer ; private int pointerMax ; private static final int SCORE_CACHE_SIZE = 32 ; private float [ ] scoreCache = new float [ SCORE_CACHE_SIZE ] ; TermScorer ( Weight weight , TermDocs td , Similarity similarity , byte [ ] norms ) { super ( similarity ) ; this . weight = weight ; this . termDocs = td ; this . norms = norms ; this . weightValue = weight . getValue ( ) ; for ( int i = 0 ; i < SCORE_CACHE_SIZE ; i ++ ) scoreCache [ i ] = getSimilarity ( ) . tf ( i ) * weightValue ; } public void score ( HitCollector hc ) throws IOException { next ( ) ; score ( hc , Integer . MAX_VALUE ) ; } protected boolean score ( HitCollector c , int end ) throws IOException { Similarity similarity = getSimilarity ( ) ; float [ ] normDecoder = Similarity . getNormDecoder ( ) ; while ( doc < end ) { int f = freqs [ pointer ] ; float score = f < SCORE_CACHE_SIZE ? scoreCache [ f ] : similarity . tf ( f ) * weightValue ; score *= normDecoder [ norms [ doc ] & 0xFF ] ; c . collect ( doc , score ) ; if ( ++ pointer >= pointerMax ) { pointerMax = termDocs . read ( docs , freqs ) ; if ( pointerMax != 0 ) { pointer = 0 ; } else { termDocs . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } } doc = docs [ pointer ] ; } return true ; } public int doc ( ) { return doc ; } public boolean next ( ) throws IOException { pointer ++ ; if ( pointer >= pointerMax ) { pointerMax = termDocs . read ( docs , freqs ) ; if ( pointerMax != 0 ) { pointer = 0 ; } else { termDocs . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } } doc = docs [ pointer ] ; return true ; } public float score ( ) { int f = freqs [ pointer ] ; float raw = f < SCORE_CACHE_SIZE ? scoreCache [ f ] : getSimilarity ( ) . tf ( f ) * weightValue ; return raw * Similarity . decodeNorm ( norms [ doc ] ) ; } public boolean skipTo ( int target ) throws IOException { for ( pointer ++ ; pointer < pointerMax ; pointer ++ ) { if ( docs [ pointer ] >= target ) { doc = docs [ pointer ] ; return true ; } } boolean result = termDocs . skipTo ( target ) ; if ( result ) { pointerMax = 1 ; pointer = 0 ; docs [ pointer ] = doc = termDocs . doc ( ) ; freqs [ pointer ] = termDocs . freq ( ) ; } else { doc = Integer . MAX_VALUE ; } return result ; } public Explanation explain ( int doc ) throws IOException { TermQuery query = ( TermQuery ) weight . getQuery ( ) ; Explanation tfExplanation = new Explanation ( ) ; int tf = 0 ; while ( pointer < pointerMax ) { if ( docs [ pointer ] == doc ) tf = freqs [ pointer ] ; pointer ++ ; } if ( tf == 0 ) { if ( termDocs . skipTo ( doc ) ) { if ( termDocs . doc ( ) == doc ) { tf = termDocs . freq ( ) ; } } } termDocs . close ( ) ; tfExplanation . setValue ( getSimilarity ( ) . tf ( tf ) ) ; tfExplanation . setDescription ( "tf(termFreq(" + query . getTerm ( ) + ")=" + tf + ")" ) ; return tfExplanation ; } public String toString ( ) { return "scorer(" + weight + ")" ; } } 	0	['9', '2', '0', '10', '31', '0', '1', '9', '7', '0.545454545', '409', '1', '2', '0.5', '0.285714286', '1', '3', '43.22222222', '2', '1', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermEnum ; public abstract class FilteredTermEnum extends TermEnum { private Term currentTerm = null ; private TermEnum actualEnum = null ; public FilteredTermEnum ( ) { } protected abstract boolean termCompare ( Term term ) ; public abstract float difference ( ) ; protected abstract boolean endEnum ( ) ; protected void setEnum ( TermEnum actualEnum ) throws IOException { this . actualEnum = actualEnum ; Term term = actualEnum . term ( ) ; if ( term != null && termCompare ( term ) ) currentTerm = term ; else next ( ) ; } public int docFreq ( ) { if ( actualEnum == null ) return - 1 ; return actualEnum . docFreq ( ) ; } public boolean next ( ) throws IOException { if ( actualEnum == null ) return false ; currentTerm = null ; while ( currentTerm == null ) { if ( endEnum ( ) ) return false ; if ( actualEnum . next ( ) ) { Term term = actualEnum . term ( ) ; if ( termCompare ( term ) ) { currentTerm = term ; return true ; } } else return false ; } currentTerm = null ; return false ; } public Term term ( ) { return currentTerm ; } public void close ( ) throws IOException { actualEnum . close ( ) ; currentTerm = null ; actualEnum = null ; } } 	0	['9', '2', '2', '7', '14', '8', '5', '2', '6', '0.5', '103', '1', '2', '0.384615385', '0.407407407', '1', '2', '10.22222222', '2', '1', '0']
package org . apache . lucene . index ; import java . io . IOException ; public interface TermDocs { void seek ( Term term ) throws IOException ; void seek ( TermEnum termEnum ) throws IOException ; int doc ( ) ; int freq ( ) ; boolean next ( ) throws IOException ; int read ( int [ ] docs , int [ ] freqs ) throws IOException ; boolean skipTo ( int target ) throws IOException ; void close ( ) throws IOException ; } 	0	['8', '1', '0', '27', '8', '28', '25', '2', '8', '2', '8', '0', '0', '0', '0.3', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; import java . util . Map ; import java . util . HashMap ; public class PerFieldAnalyzerWrapper extends Analyzer { private Analyzer defaultAnalyzer ; private Map analyzerMap = new HashMap ( ) ; public PerFieldAnalyzerWrapper ( Analyzer defaultAnalyzer ) { this . defaultAnalyzer = defaultAnalyzer ; } public void addAnalyzer ( String fieldName , Analyzer analyzer ) { analyzerMap . put ( fieldName , analyzer ) ; } public TokenStream tokenStream ( String fieldName , Reader reader ) { Analyzer analyzer = ( Analyzer ) analyzerMap . get ( fieldName ) ; if ( analyzer == null ) { analyzer = defaultAnalyzer ; } return analyzer . tokenStream ( fieldName , reader ) ; } public int getPositionIncrementGap ( String fieldName ) { Analyzer analyzer = ( Analyzer ) analyzerMap . get ( fieldName ) ; if ( analyzer == null ) analyzer = defaultAnalyzer ; return analyzer . getPositionIncrementGap ( fieldName ) ; } public String toString ( ) { return "PerFieldAnalyzerWrapper(" + analyzerMap + ", default=" + defaultAnalyzer + ")" ; } } 	0	['5', '2', '0', '2', '15', '0', '0', '2', '5', '0.125', '73', '1', '1', '0.333333333', '0.55', '0', '0', '13.2', '2', '1.2', '0']
package org . apache . lucene . search ; public abstract class HitCollector { public abstract void collect ( int doc , float score ) ; } 	0	['2', '1', '6', '22', '3', '1', '22', '0', '2', '2', '5', '0', '0', '0', '0.666666667', '0', '0', '1.5', '1', '0.5', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; public class WhitespaceTokenizer extends CharTokenizer { public WhitespaceTokenizer ( Reader in ) { super ( in ) ; } protected boolean isTokenChar ( char c ) { return ! Character . isWhitespace ( c ) ; } } 	0	['2', '4', '0', '2', '4', '1', '1', '1', '1', '2', '13', '0', '0', '0.875', '0.666666667', '1', '1', '5.5', '2', '1', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Query ; import org . apache . lucene . util . ToStringUtils ; public class SpanNotQuery extends SpanQuery { private SpanQuery include ; private SpanQuery exclude ; public SpanNotQuery ( SpanQuery include , SpanQuery exclude ) { this . include = include ; this . exclude = exclude ; if ( ! include . getField ( ) . equals ( exclude . getField ( ) ) ) throw new IllegalArgumentException ( "Clauses must have same field." ) ; } public SpanQuery getInclude ( ) { return include ; } public SpanQuery getExclude ( ) { return exclude ; } public String getField ( ) { return include . getField ( ) ; } public Collection getTerms ( ) { return include . getTerms ( ) ; } public void extractTerms ( Set terms ) { include . extractTerms ( terms ) ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "spanNot(" ) ; buffer . append ( include . toString ( field ) ) ; buffer . append ( ", " ) ; buffer . append ( exclude . toString ( field ) ) ; buffer . append ( ")" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public Spans getSpans ( final IndexReader reader ) throws IOException { return new Spans ( ) { private Spans includeSpans = include . getSpans ( reader ) ; private boolean moreInclude = true ; private Spans excludeSpans = exclude . getSpans ( reader ) ; private boolean moreExclude = excludeSpans . next ( ) ; public boolean next ( ) throws IOException { if ( moreInclude ) moreInclude = includeSpans . next ( ) ; while ( moreInclude && moreExclude ) { if ( includeSpans . doc ( ) > excludeSpans . doc ( ) ) moreExclude = excludeSpans . skipTo ( includeSpans . doc ( ) ) ; while ( moreExclude && includeSpans . doc ( ) == excludeSpans . doc ( ) && excludeSpans . end ( ) <= includeSpans . start ( ) ) { moreExclude = excludeSpans . next ( ) ; } if ( ! moreExclude || includeSpans . doc ( ) != excludeSpans . doc ( ) || includeSpans . end ( ) <= excludeSpans . start ( ) ) break ; moreInclude = includeSpans . next ( ) ; } return moreInclude ; } public boolean skipTo ( int target ) throws IOException { if ( moreInclude ) moreInclude = includeSpans . skipTo ( target ) ; if ( ! moreInclude ) return false ; if ( moreExclude && includeSpans . doc ( ) > excludeSpans . doc ( ) ) moreExclude = excludeSpans . skipTo ( includeSpans . doc ( ) ) ; while ( moreExclude && includeSpans . doc ( ) == excludeSpans . doc ( ) && excludeSpans . end ( ) <= includeSpans . start ( ) ) { moreExclude = excludeSpans . next ( ) ; } if ( ! moreExclude || includeSpans . doc ( ) != excludeSpans . doc ( ) || includeSpans . end ( ) <= excludeSpans . start ( ) ) return true ; return next ( ) ; } public int doc ( ) { return includeSpans . doc ( ) ; } public int start ( ) { return includeSpans . start ( ) ; } public int end ( ) { return includeSpans . end ( ) ; } public String toString ( ) { return "spans(" + SpanNotQuery . this . toString ( ) + ")" ; } } ; } public Query rewrite ( IndexReader reader ) throws IOException { SpanNotQuery clone = null ; SpanQuery rewrittenInclude = ( SpanQuery ) include . rewrite ( reader ) ; if ( rewrittenInclude != include ) { clone = ( SpanNotQuery ) this . clone ( ) ; clone . include = rewrittenInclude ; } SpanQuery rewrittenExclude = ( SpanQuery ) exclude . rewrite ( reader ) ; if ( rewrittenExclude != exclude ) { if ( clone == null ) clone = ( SpanNotQuery ) this . clone ( ) ; clone . exclude = rewrittenExclude ; } if ( clone != null ) { return clone ; } else { return this ; } } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof SpanNotQuery ) ) return false ; SpanNotQuery other = ( SpanNotQuery ) o ; return this . include . equals ( other . include ) && this . exclude . equals ( other . exclude ) && this . getBoost ( ) == other . getBoost ( ) ; } public int hashCode ( ) { int h = include . hashCode ( ) ; h = ( h << 1 ) | ( h > > > 31 ) ; h ^= exclude . hashCode ( ) ; h = ( h << 1 ) | ( h > > > 31 ) ; h ^= Float . floatToRawIntBits ( getBoost ( ) ) ; return h ; } } 	0	['13', '3', '0', '6', '31', '0', '1', '6', '11', '0.375', '218', '1', '2', '0.571428571', '0.208791209', '2', '2', '15.61538462', '6', '1.3077', '0']
package org . apache . lucene . search ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . FieldSelector ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . CorruptIndexException ; import java . io . IOException ; import java . rmi . Naming ; import java . rmi . RMISecurityManager ; import java . rmi . RemoteException ; import java . rmi . server . UnicastRemoteObject ; public class RemoteSearchable extends UnicastRemoteObject implements Searchable { private Searchable local ; public RemoteSearchable ( Searchable local ) throws RemoteException { super ( ) ; this . local = local ; } public void search ( Weight weight , Filter filter , HitCollector results ) throws IOException { local . search ( weight , filter , results ) ; } public void close ( ) throws IOException { local . close ( ) ; } public int docFreq ( Term term ) throws IOException { return local . docFreq ( term ) ; } public int [ ] docFreqs ( Term [ ] terms ) throws IOException { return local . docFreqs ( terms ) ; } public int maxDoc ( ) throws IOException { return local . maxDoc ( ) ; } public TopDocs search ( Weight weight , Filter filter , int n ) throws IOException { return local . search ( weight , filter , n ) ; } public TopFieldDocs search ( Weight weight , Filter filter , int n , Sort sort ) throws IOException { return local . search ( weight , filter , n , sort ) ; } public Document doc ( int i ) throws CorruptIndexException , IOException { return local . doc ( i ) ; } public Document doc ( int i , FieldSelector fieldSelector ) throws CorruptIndexException , IOException { return local . doc ( i , fieldSelector ) ; } public Query rewrite ( Query original ) throws IOException { return local . rewrite ( original ) ; } public Explanation explain ( Weight weight , int doc ) throws IOException { return local . explain ( weight , doc ) ; } public static void main ( String args [ ] ) throws Exception { String indexName = null ; if ( args != null && args . length == 1 ) indexName = args [ 0 ] ; if ( indexName == null ) { System . out . println ( "Usage: org.apache.lucene.search.RemoteSearchable <index>" ) ; return ; } if ( System . getSecurityManager ( ) == null ) { System . setSecurityManager ( new RMISecurityManager ( ) ) ; } Searchable local = new IndexSearcher ( indexName ) ; RemoteSearchable impl = new RemoteSearchable ( local ) ; Naming . rebind ( "//localhost/Searchable" , impl ) ; } } 	0	['13', '4', '0', '14', '31', '0', '0', '14', '13', '0', '120', '1', '1', '0.6', '0.205128205', '0', '0', '8.153846154', '1', '0.9231', '0']
package org . apache . lucene . index ; public interface TermFreqVector { public String getField ( ) ; public int size ( ) ; public String [ ] getTerms ( ) ; public int [ ] getTermFrequencies ( ) ; public int indexOf ( String term ) ; public int [ ] indexesOf ( String [ ] terms , int start , int len ) ; } 	0	['6', '1', '0', '11', '6', '15', '11', '0', '6', '2', '6', '0', '0', '0', '0.375', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . List ; import java . util . Collection ; import java . util . ArrayList ; import java . util . Iterator ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . PriorityQueue ; import org . apache . lucene . util . ToStringUtils ; import org . apache . lucene . search . Query ; public class SpanOrQuery extends SpanQuery { private List clauses ; private String field ; public SpanOrQuery ( SpanQuery [ ] clauses ) { this . clauses = new ArrayList ( clauses . length ) ; for ( int i = 0 ; i < clauses . length ; i ++ ) { SpanQuery clause = clauses [ i ] ; if ( i == 0 ) { field = clause . getField ( ) ; } else if ( ! clause . getField ( ) . equals ( field ) ) { throw new IllegalArgumentException ( "Clauses must have same field." ) ; } this . clauses . add ( clause ) ; } } public SpanQuery [ ] getClauses ( ) { return ( SpanQuery [ ] ) clauses . toArray ( new SpanQuery [ clauses . size ( ) ] ) ; } public String getField ( ) { return field ; } public Collection getTerms ( ) { Collection terms = new ArrayList ( ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; terms . addAll ( clause . getTerms ( ) ) ; } return terms ; } public void extractTerms ( Set terms ) { Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; clause . extractTerms ( terms ) ; } } public Query rewrite ( IndexReader reader ) throws IOException { SpanOrQuery clone = null ; for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { SpanQuery c = ( SpanQuery ) clauses . get ( i ) ; SpanQuery query = ( SpanQuery ) c . rewrite ( reader ) ; if ( query != c ) { if ( clone == null ) clone = ( SpanOrQuery ) this . clone ( ) ; clone . clauses . set ( i , query ) ; } } if ( clone != null ) { return clone ; } else { return this ; } } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "spanOr([" ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; buffer . append ( clause . toString ( field ) ) ; if ( i . hasNext ( ) ) { buffer . append ( ", " ) ; } } buffer . append ( "])" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( o == null || getClass ( ) != o . getClass ( ) ) return false ; final SpanOrQuery that = ( SpanOrQuery ) o ; if ( ! clauses . equals ( that . clauses ) ) return false ; if ( ! field . equals ( that . field ) ) return false ; return getBoost ( ) == that . getBoost ( ) ; } public int hashCode ( ) { int h = clauses . hashCode ( ) ; h ^= ( h << 10 ) | ( h > > > 23 ) ; h ^= Float . floatToRawIntBits ( getBoost ( ) ) ; return h ; } private class SpanQueue extends PriorityQueue { public SpanQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object o1 , Object o2 ) { Spans spans1 = ( Spans ) o1 ; Spans spans2 = ( Spans ) o2 ; if ( spans1 . doc ( ) == spans2 . doc ( ) ) { if ( spans1 . start ( ) == spans2 . start ( ) ) { return spans1 . end ( ) < spans2 . end ( ) ; } else { return spans1 . start ( ) < spans2 . start ( ) ; } } else { return spans1 . doc ( ) < spans2 . doc ( ) ; } } } public Spans getSpans ( final IndexReader reader ) throws IOException { if ( clauses . size ( ) == 1 ) return ( ( SpanQuery ) clauses . get ( 0 ) ) . getSpans ( reader ) ; return new Spans ( ) { private SpanQueue queue = null ; private boolean initSpanQueue ( int target ) throws IOException { queue = new SpanQueue ( clauses . size ( ) ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { Spans spans = ( ( SpanQuery ) i . next ( ) ) . getSpans ( reader ) ; if ( ( ( target == - 1 ) && spans . next ( ) ) || ( ( target != - 1 ) && spans . skipTo ( target ) ) ) { queue . put ( spans ) ; } } return queue . size ( ) != 0 ; } public boolean next ( ) throws IOException { if ( queue == null ) { return initSpanQueue ( - 1 ) ; } if ( queue . size ( ) == 0 ) { return false ; } if ( top ( ) . next ( ) ) { queue . adjustTop ( ) ; return true ; } queue . pop ( ) ; return queue . size ( ) != 0 ; } private Spans top ( ) { return ( Spans ) queue . top ( ) ; } public boolean skipTo ( int target ) throws IOException { if ( queue == null ) { return initSpanQueue ( target ) ; } while ( queue . size ( ) != 0 && top ( ) . doc ( ) < target ) { if ( top ( ) . skipTo ( target ) ) { queue . adjustTop ( ) ; } else { queue . pop ( ) ; } } return queue . size ( ) != 0 ; } public int doc ( ) { return top ( ) . doc ( ) ; } public int start ( ) { return top ( ) . start ( ) ; } public int end ( ) { return top ( ) . end ( ) ; } public String toString ( ) { return "spans(" + SpanOrQuery . this + ")@" + ( ( queue == null ) ? "START" : ( queue . size ( ) > 0 ? ( doc ( ) + ":" + start ( ) + "-" + end ( ) ) : "END" ) ) ; } } ; } } 	0	['11', '3', '0', '8', '42', '0', '3', '6', '10', '0.45', '286', '1', '0', '0.615384615', '0.220779221', '2', '2', '24.81818182', '7', '1.7273', '0']
package org . apache . lucene . store ; import java . io . IOException ; public class NoLockFactory extends LockFactory { private static NoLock singletonLock = new NoLock ( ) ; private static NoLockFactory singleton = new NoLockFactory ( ) ; public static NoLockFactory getNoLockFactory ( ) { return singleton ; } public Lock makeLock ( String lockName ) { return singletonLock ; } public void clearLock ( String lockName ) { } ; } ; class NoLock extends Lock { public boolean obtain ( ) throws IOException { return true ; } public void release ( ) { } public boolean isLocked ( ) { return false ; } public String toString ( ) { return "NoLock" ; } } 	0	['5', '2', '0', '4', '7', '6', '1', '3', '4', '0.75', '24', '1', '2', '0.571428571', '0.625', '0', '0', '3.4', '1', '0.6', '0']
package org . apache . lucene . search ; public class ScoreDoc implements java . io . Serializable { public float score ; public int doc ; public ScoreDoc ( int doc , float score ) { this . doc = doc ; this . score = score ; } } 	0	['1', '1', '1', '19', '2', '0', '19', '0', '1', '2', '12', '0', '0', '0', '1', '0', '0', '9', '0', '0', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Arrays ; import java . util . Comparator ; import org . apache . lucene . index . IndexReader ; class NearSpansOrdered implements Spans { private final int allowedSlop ; private boolean firstTime = true ; private boolean more = false ; private final Spans [ ] subSpans ; private boolean inSameDoc = false ; private int matchDoc = - 1 ; private int matchStart = - 1 ; private int matchEnd = - 1 ; private final Spans [ ] subSpansByDoc ; private final Comparator spanDocComparator = new Comparator ( ) { public int compare ( Object o1 , Object o2 ) { return ( ( Spans ) o1 ) . doc ( ) - ( ( Spans ) o2 ) . doc ( ) ; } } ; private SpanNearQuery query ; public NearSpansOrdered ( SpanNearQuery spanNearQuery , IndexReader reader ) throws IOException { if ( spanNearQuery . getClauses ( ) . length < 2 ) { throw new IllegalArgumentException ( "Less than 2 clauses: " + spanNearQuery ) ; } allowedSlop = spanNearQuery . getSlop ( ) ; SpanQuery [ ] clauses = spanNearQuery . getClauses ( ) ; subSpans = new Spans [ clauses . length ] ; subSpansByDoc = new Spans [ clauses . length ] ; for ( int i = 0 ; i < clauses . length ; i ++ ) { subSpans [ i ] = clauses [ i ] . getSpans ( reader ) ; subSpansByDoc [ i ] = subSpans [ i ] ; } query = spanNearQuery ; } public int doc ( ) { return matchDoc ; } public int start ( ) { return matchStart ; } public int end ( ) { return matchEnd ; } public boolean next ( ) throws IOException { if ( firstTime ) { firstTime = false ; for ( int i = 0 ; i < subSpans . length ; i ++ ) { if ( ! subSpans [ i ] . next ( ) ) { more = false ; return false ; } } more = true ; } return advanceAfterOrdered ( ) ; } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { firstTime = false ; for ( int i = 0 ; i < subSpans . length ; i ++ ) { if ( ! subSpans [ i ] . skipTo ( target ) ) { more = false ; return false ; } } more = true ; } else if ( more && ( subSpans [ 0 ] . doc ( ) < target ) ) { if ( subSpans [ 0 ] . skipTo ( target ) ) { inSameDoc = false ; } else { more = false ; return false ; } } return advanceAfterOrdered ( ) ; } private boolean advanceAfterOrdered ( ) throws IOException { while ( more && ( inSameDoc || toSameDoc ( ) ) ) { if ( stretchToOrder ( ) && shrinkToAfterShortestMatch ( ) ) { return true ; } } return false ; } private boolean toSameDoc ( ) throws IOException { Arrays . sort ( subSpansByDoc , spanDocComparator ) ; int firstIndex = 0 ; int maxDoc = subSpansByDoc [ subSpansByDoc . length - 1 ] . doc ( ) ; while ( subSpansByDoc [ firstIndex ] . doc ( ) != maxDoc ) { if ( ! subSpansByDoc [ firstIndex ] . skipTo ( maxDoc ) ) { more = false ; inSameDoc = false ; return false ; } maxDoc = subSpansByDoc [ firstIndex ] . doc ( ) ; if ( ++ firstIndex == subSpansByDoc . length ) { firstIndex = 0 ; } } for ( int i = 0 ; i < subSpansByDoc . length ; i ++ ) { assert ( subSpansByDoc [ i ] . doc ( ) == maxDoc ) : " NearSpansOrdered.toSameDoc() spans " + subSpansByDoc [ 0 ] + "\n at doc " + subSpansByDoc [ i ] . doc ( ) + ", but should be at " + maxDoc ; } inSameDoc = true ; return true ; } static final boolean docSpansOrdered ( Spans spans1 , Spans spans2 ) { assert spans1 . doc ( ) == spans2 . doc ( ) : "doc1 " + spans1 . doc ( ) + " != doc2 " + spans2 . doc ( ) ; int start1 = spans1 . start ( ) ; int start2 = spans2 . start ( ) ; return ( start1 == start2 ) ? ( spans1 . end ( ) < spans2 . end ( ) ) : ( start1 < start2 ) ; } private static final boolean docSpansOrdered ( int start1 , int end1 , int start2 , int end2 ) { return ( start1 == start2 ) ? ( end1 < end2 ) : ( start1 < start2 ) ; } private boolean stretchToOrder ( ) throws IOException { matchDoc = subSpans [ 0 ] . doc ( ) ; for ( int i = 1 ; inSameDoc && ( i < subSpans . length ) ; i ++ ) { while ( ! docSpansOrdered ( subSpans [ i - 1 ] , subSpans [ i ] ) ) { if ( ! subSpans [ i ] . next ( ) ) { inSameDoc = false ; more = false ; break ; } else if ( matchDoc != subSpans [ i ] . doc ( ) ) { inSameDoc = false ; break ; } } } return inSameDoc ; } private boolean shrinkToAfterShortestMatch ( ) throws IOException { matchStart = subSpans [ subSpans . length - 1 ] . start ( ) ; matchEnd = subSpans [ subSpans . length - 1 ] . end ( ) ; int matchSlop = 0 ; int lastStart = matchStart ; int lastEnd = matchEnd ; for ( int i = subSpans . length - 2 ; i >= 0 ; i -- ) { Spans prevSpans = subSpans [ i ] ; int prevStart = prevSpans . start ( ) ; int prevEnd = prevSpans . end ( ) ; while ( true ) { if ( ! prevSpans . next ( ) ) { inSameDoc = false ; more = false ; break ; } else if ( matchDoc != prevSpans . doc ( ) ) { inSameDoc = false ; break ; } else { int ppStart = prevSpans . start ( ) ; int ppEnd = prevSpans . end ( ) ; if ( ! docSpansOrdered ( ppStart , ppEnd , lastStart , lastEnd ) ) { break ; } else { prevStart = ppStart ; prevEnd = ppEnd ; } } } assert prevStart <= matchStart ; if ( matchStart > prevEnd ) { matchSlop += ( matchStart - prevEnd ) ; } matchStart = prevStart ; lastStart = prevStart ; lastEnd = prevEnd ; } return matchSlop <= allowedSlop ; } public String toString ( ) { return getClass ( ) . getName ( ) + "(" + query . toString ( ) + ")@" + ( firstTime ? "START" : ( more ? ( doc ( ) + ":" + start ( ) + "-" + end ( ) ) : "END" ) ) ; } } 	0	['15', '1', '0', '6', '41', '25', '3', '5', '7', '0.747252747', '661', '0.846153846', '3', '0', '0.202380952', '0', '0', '42.2', '6', '1.5333', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; public interface Weight extends java . io . Serializable { Query getQuery ( ) ; float getValue ( ) ; float sumOfSquaredWeights ( ) throws IOException ; void normalize ( float norm ) ; Scorer scorer ( IndexReader reader ) throws IOException ; Explanation explain ( IndexReader reader , int doc ) throws IOException ; } 	0	['6', '1', '0', '47', '6', '15', '44', '4', '6', '2', '6', '0', '0', '0', '0.416666667', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . analysis ; import java . io . File ; import java . io . IOException ; import java . io . Reader ; import java . util . Set ; public final class StopAnalyzer extends Analyzer { private Set stopWords ; public static final String [ ] ENGLISH_STOP_WORDS = { "a" , "an" , "and" , "are" , "as" , "at" , "be" , "but" , "by" , "for" , "if" , "in" , "into" , "is" , "it" , "no" , "not" , "of" , "on" , "or" , "such" , "that" , "the" , "their" , "then" , "there" , "these" , "they" , "this" , "to" , "was" , "will" , "with" } ; public StopAnalyzer ( ) { stopWords = StopFilter . makeStopSet ( ENGLISH_STOP_WORDS ) ; } public StopAnalyzer ( Set stopWords ) { this . stopWords = stopWords ; } public StopAnalyzer ( String [ ] stopWords ) { this . stopWords = StopFilter . makeStopSet ( stopWords ) ; } public StopAnalyzer ( File stopwordsFile ) throws IOException { stopWords = WordlistLoader . getWordSet ( stopwordsFile ) ; } public StopAnalyzer ( Reader stopwords ) throws IOException { stopWords = WordlistLoader . getWordSet ( stopwords ) ; } public TokenStream tokenStream ( String fieldName , Reader reader ) { return new StopFilter ( new LowerCaseTokenizer ( reader ) , stopWords ) ; } } 	0	['7', '2', '0', '6', '13', '0', '1', '5', '6', '0.5', '189', '0.5', '0', '0.666666667', '0.333333333', '0', '0', '25.71428571', '1', '0.1429', '0']
package org . apache . lucene . analysis . standard ; import java . io . * ; public class StandardTokenizerTokenManager implements StandardTokenizerConstants { public java . io . PrintStream debugStream = System . out ; public void setDebugStream ( java . io . PrintStream ds ) { debugStream = ds ; } private final int jjMoveStringLiteralDfa0_0 ( ) { return jjMoveNfa_0 ( 0 , 0 ) ; } private final void jjCheckNAdd ( int state ) { if ( jjrounds [ state ] != jjround ) { jjstateSet [ jjnewStateCnt ++ ] = state ; jjrounds [ state ] = jjround ; } } private final void jjAddStates ( int start , int end ) { do { jjstateSet [ jjnewStateCnt ++ ] = jjnextStates [ start ] ; } while ( start ++ != end ) ; } private final void jjCheckNAddTwoStates ( int state1 , int state2 ) { jjCheckNAdd ( state1 ) ; jjCheckNAdd ( state2 ) ; } private final void jjCheckNAddStates ( int start , int end ) { do { jjCheckNAdd ( jjnextStates [ start ] ) ; } while ( start ++ != end ) ; } private final void jjCheckNAddStates ( int start ) { jjCheckNAdd ( jjnextStates [ start ] ) ; jjCheckNAdd ( jjnextStates [ start + 1 ] ) ; } static final long [ ] jjbitVec0 = { 0xfff0000000000000L , 0xffffffffffffdfffL , 0xffffffffL , 0x600000000000000L } ; static final long [ ] jjbitVec2 = { 0x0L , 0xffffffffffffffffL , 0xffffffffffffffffL , 0xffffffffffffffffL } ; static final long [ ] jjbitVec3 = { 0xffffffffffffffffL , 0xffffffffffffffffL , 0xffffL , 0xffff000000000000L } ; static final long [ ] jjbitVec4 = { 0xffffffffffffffffL , 0xffffffffffffffffL , 0x0L , 0x0L } ; static final long [ ] jjbitVec5 = { 0xffffffffffffffffL , 0xffffffffffffffffL , 0xffffffffffffffffL , 0x0L } ; static final long [ ] jjbitVec6 = { 0x0L , 0xffffffe000000000L , 0xffffffffL , 0x0L } ; static final long [ ] jjbitVec7 = { 0x20000L , 0x0L , 0xfffff00000000000L , 0x7fffffL } ; static final long [ ] jjbitVec8 = { 0xffffffffffffffffL , 0xffffffffffffffffL , 0xffffffffffffL , 0x0L } ; static final long [ ] jjbitVec9 = { 0xfffffffeL , 0x0L , 0x0L , 0x0L } ; static final long [ ] jjbitVec10 = { 0x0L , 0x0L , 0x0L , 0xff7fffffff7fffffL } ; static final long [ ] jjbitVec11 = { 0x0L , 0x0L , 0xffffffff00000000L , 0x1fffffffL } ; static final long [ ] jjbitVec12 = { 0x1600L , 0x0L , 0x0L , 0x0L } ; static final long [ ] jjbitVec13 = { 0x0L , 0xffc000000000L , 0x0L , 0xffc000000000L } ; static final long [ ] jjbitVec14 = { 0x0L , 0x3ff00000000L , 0x0L , 0x3ff000000000000L } ; static final long [ ] jjbitVec15 = { 0x0L , 0xffc000000000L , 0x0L , 0xff8000000000L } ; static final long [ ] jjbitVec16 = { 0x0L , 0xffc000000000L , 0x0L , 0x0L } ; static final long [ ] jjbitVec17 = { 0x0L , 0x3ff0000L , 0x0L , 0x3ff0000L } ; static final long [ ] jjbitVec18 = { 0x0L , 0x3ffL , 0x0L , 0x0L } ; static final long [ ] jjbitVec19 = { 0xfffffffeL , 0x0L , 0xfffff00000000000L , 0x7fffffL } ; private final int jjMoveNfa_0 ( int startState , int curPos ) { int [ ] nextStates ; int startsAt = 0 ; jjnewStateCnt = 75 ; int i = 1 ; jjstateSet [ 0 ] = startState ; int j , kind = 0x7fffffff ; for ( ; ; ) { if ( ++ jjround == 0x7fffffff ) ReInitRounds ( ) ; if ( curChar < 64 ) { long l = 1L << curChar ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( ( 0x3ff000000000000L & l ) != 0L ) { if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; } if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 12 , 17 ) ; if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 18 , 23 ) ; break ; case 2 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 18 , 23 ) ; break ; case 3 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 3 , 4 ) ; break ; case 4 : case 5 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 5 , 6 ) ; break ; case 6 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 7 ) ; break ; case 7 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAdd ( 7 ) ; break ; case 8 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 8 , 9 ) ; break ; case 9 : case 10 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 10 , 11 ) ; break ; case 11 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 12 ) ; break ; case 12 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 12 , 13 ) ; break ; case 13 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 14 , 15 ) ; break ; case 14 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 14 , 15 ) ; break ; case 15 : case 16 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 11 , 16 ) ; break ; case 17 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 17 , 18 ) ; break ; case 18 : case 19 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 19 , 20 ) ; break ; case 20 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 21 ) ; break ; case 21 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 21 , 22 ) ; break ; case 22 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 23 , 24 ) ; break ; case 23 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 23 , 24 ) ; break ; case 24 : case 25 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 26 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 27 ) ; break ; case 27 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 22 , 27 ) ; break ; case 28 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 12 , 17 ) ; break ; case 29 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; break ; case 30 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAdd ( 30 ) ; break ; case 31 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 24 , 26 ) ; break ; case 32 : if ( ( 0x600000000000L & l ) != 0L ) jjCheckNAdd ( 33 ) ; break ; case 33 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 27 , 29 ) ; break ; case 35 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 35 , 36 ) ; break ; case 36 : if ( ( 0x600000000000L & l ) != 0L ) jjCheckNAdd ( 37 ) ; break ; case 37 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 5 ) kind = 5 ; jjCheckNAddTwoStates ( 36 , 37 ) ; break ; case 38 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 38 , 39 ) ; break ; case 39 : if ( curChar == 46 ) jjCheckNAdd ( 40 ) ; break ; case 40 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 6 ) kind = 6 ; jjCheckNAddTwoStates ( 39 , 40 ) ; break ; case 41 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 41 , 42 ) ; break ; case 42 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 43 , 44 ) ; break ; case 43 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 43 , 44 ) ; break ; case 44 : case 45 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAdd ( 45 ) ; break ; case 46 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 46 , 47 ) ; break ; case 47 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 48 , 49 ) ; break ; case 48 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 48 , 49 ) ; break ; case 49 : case 50 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 50 , 51 ) ; break ; case 51 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 52 ) ; break ; case 52 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 47 , 52 ) ; break ; case 53 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 53 , 54 ) ; break ; case 54 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 55 , 56 ) ; break ; case 55 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 55 , 56 ) ; break ; case 56 : case 57 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 57 , 58 ) ; break ; case 58 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 59 ) ; break ; case 59 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 59 , 60 ) ; break ; case 60 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 61 , 62 ) ; break ; case 61 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 61 , 62 ) ; break ; case 62 : case 63 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 58 , 63 ) ; break ; case 66 : if ( curChar == 39 ) jjstateSet [ jjnewStateCnt ++ ] = 67 ; break ; case 69 : if ( curChar == 46 ) jjCheckNAdd ( 70 ) ; break ; case 71 : if ( curChar != 46 ) break ; if ( kind > 3 ) kind = 3 ; jjCheckNAdd ( 70 ) ; break ; case 73 : if ( curChar == 38 ) jjstateSet [ jjnewStateCnt ++ ] = 74 ; break ; default : break ; } } while ( i != startsAt ) ; } else if ( curChar < 128 ) { long l = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 30 , 35 ) ; if ( ( 0x7fffffe07fffffeL & l ) != 0L ) { if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; } if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 18 , 23 ) ; break ; case 2 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 18 , 23 ) ; break ; case 3 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 3 , 4 ) ; break ; case 5 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjAddStates ( 36 , 37 ) ; break ; case 6 : if ( curChar == 95 ) jjCheckNAdd ( 7 ) ; break ; case 7 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAdd ( 7 ) ; break ; case 8 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 8 , 9 ) ; break ; case 10 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 10 , 11 ) ; break ; case 11 : if ( curChar == 95 ) jjCheckNAdd ( 12 ) ; break ; case 12 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 12 , 13 ) ; break ; case 13 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 14 , 15 ) ; break ; case 14 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 14 , 15 ) ; break ; case 16 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 11 , 16 ) ; break ; case 17 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 17 , 18 ) ; break ; case 19 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjAddStates ( 38 , 39 ) ; break ; case 20 : if ( curChar == 95 ) jjCheckNAdd ( 21 ) ; break ; case 21 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 21 , 22 ) ; break ; case 22 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 23 , 24 ) ; break ; case 23 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 23 , 24 ) ; break ; case 25 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjAddStates ( 40 , 41 ) ; break ; case 26 : if ( curChar == 95 ) jjCheckNAdd ( 27 ) ; break ; case 27 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 22 , 27 ) ; break ; case 29 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; break ; case 30 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAdd ( 30 ) ; break ; case 31 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 24 , 26 ) ; break ; case 32 : if ( curChar == 95 ) jjCheckNAdd ( 33 ) ; break ; case 33 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 27 , 29 ) ; break ; case 34 : if ( curChar == 64 ) jjCheckNAdd ( 35 ) ; break ; case 35 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 35 , 36 ) ; break ; case 37 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 5 ) kind = 5 ; jjCheckNAddTwoStates ( 36 , 37 ) ; break ; case 38 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 38 , 39 ) ; break ; case 40 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 6 ) kind = 6 ; jjCheckNAddTwoStates ( 39 , 40 ) ; break ; case 41 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 41 , 42 ) ; break ; case 42 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 43 , 44 ) ; break ; case 43 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 43 , 44 ) ; break ; case 45 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjstateSet [ jjnewStateCnt ++ ] = 45 ; break ; case 46 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 46 , 47 ) ; break ; case 47 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 48 , 49 ) ; break ; case 48 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 48 , 49 ) ; break ; case 50 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjAddStates ( 42 , 43 ) ; break ; case 51 : if ( curChar == 95 ) jjCheckNAdd ( 52 ) ; break ; case 52 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 47 , 52 ) ; break ; case 53 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 53 , 54 ) ; break ; case 54 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 55 , 56 ) ; break ; case 55 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 55 , 56 ) ; break ; case 57 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 57 , 58 ) ; break ; case 58 : if ( curChar == 95 ) jjCheckNAdd ( 59 ) ; break ; case 59 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 59 , 60 ) ; break ; case 60 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 61 , 62 ) ; break ; case 61 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 61 , 62 ) ; break ; case 63 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 58 , 63 ) ; break ; case 64 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 30 , 35 ) ; break ; case 65 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 65 , 66 ) ; break ; case 67 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 2 ) kind = 2 ; jjCheckNAddTwoStates ( 66 , 67 ) ; break ; case 68 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 68 , 69 ) ; break ; case 70 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjAddStates ( 44 , 45 ) ; break ; case 72 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 72 , 73 ) ; break ; case 73 : if ( curChar == 64 ) jjCheckNAdd ( 74 ) ; break ; case 74 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 4 ) kind = 4 ; jjCheckNAdd ( 74 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else { int hiByte = ( int ) ( curChar > > 8 ) ; int i1 = hiByte > > 6 ; long l1 = 1L << ( hiByte & 077 ) ; int i2 = ( curChar & 0xff ) > > 6 ; long l2 = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) { if ( kind > 12 ) kind = 12 ; } if ( jjCanMove_1 ( hiByte , i1 , i2 , l1 , l2 ) ) { if ( kind > 13 ) kind = 13 ; } if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 18 , 23 ) ; if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 12 , 17 ) ; if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) { if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; } if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 30 , 35 ) ; break ; case 1 : if ( jjCanMove_1 ( hiByte , i1 , i2 , l1 , l2 ) && kind > 13 ) kind = 13 ; break ; case 2 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 18 , 23 ) ; break ; case 3 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 3 , 4 ) ; break ; case 4 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 5 , 6 ) ; break ; case 5 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 5 , 6 ) ; break ; case 7 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjstateSet [ jjnewStateCnt ++ ] = 7 ; break ; case 8 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 8 , 9 ) ; break ; case 9 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 10 , 11 ) ; break ; case 10 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 10 , 11 ) ; break ; case 12 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 46 , 47 ) ; break ; case 14 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 48 , 49 ) ; break ; case 15 : if ( ! jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 11 , 16 ) ; break ; case 16 : if ( ! jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 11 , 16 ) ; break ; case 17 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 17 , 18 ) ; break ; case 18 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 19 , 20 ) ; break ; case 19 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 19 , 20 ) ; break ; case 21 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 21 , 22 ) ; break ; case 23 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 50 , 51 ) ; break ; case 24 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 25 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 27 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 22 , 27 ) ; break ; case 28 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 12 , 17 ) ; break ; case 29 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; break ; case 30 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAdd ( 30 ) ; break ; case 31 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 24 , 26 ) ; break ; case 33 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 27 , 29 ) ; break ; case 35 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 35 , 36 ) ; break ; case 37 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 5 ) kind = 5 ; jjCheckNAddTwoStates ( 36 , 37 ) ; break ; case 38 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 38 , 39 ) ; break ; case 40 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 6 ) kind = 6 ; jjCheckNAddTwoStates ( 39 , 40 ) ; break ; case 41 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 41 , 42 ) ; break ; case 43 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 52 , 53 ) ; break ; case 44 : if ( ! jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAdd ( 45 ) ; break ; case 45 : if ( ! jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAdd ( 45 ) ; break ; case 46 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 46 , 47 ) ; break ; case 48 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 54 , 55 ) ; break ; case 49 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 50 , 51 ) ; break ; case 50 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 50 , 51 ) ; break ; case 52 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 47 , 52 ) ; break ; case 53 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 53 , 54 ) ; break ; case 55 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 56 , 57 ) ; break ; case 56 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 57 , 58 ) ; break ; case 57 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 57 , 58 ) ; break ; case 59 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 58 , 59 ) ; break ; case 61 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 60 , 61 ) ; break ; case 62 : if ( ! jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 58 , 63 ) ; break ; case 63 : if ( ! jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 58 , 63 ) ; break ; case 64 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 30 , 35 ) ; break ; case 65 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 65 , 66 ) ; break ; case 67 : if ( ! jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 2 ) kind = 2 ; jjCheckNAddTwoStates ( 66 , 67 ) ; break ; case 68 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 68 , 69 ) ; break ; case 70 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 44 , 45 ) ; break ; case 72 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 72 , 73 ) ; break ; case 74 : if ( ! jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 4 ) kind = 4 ; jjstateSet [ jjnewStateCnt ++ ] = 74 ; break ; default : break ; } } while ( i != startsAt ) ; } if ( kind != 0x7fffffff ) { jjmatchedKind = kind ; jjmatchedPos = curPos ; kind = 0x7fffffff ; } ++ curPos ; if ( ( i = jjnewStateCnt ) == ( startsAt = 75 - ( jjnewStateCnt = startsAt ) ) ) return curPos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return curPos ; } } } static final int [ ] jjnextStates = { 30 , 31 , 32 , 34 , 38 , 39 , 41 , 42 , 46 , 47 , 53 , 54 , 5 , 6 , 10 , 11 , 19 , 20 , 3 , 4 , 8 , 9 , 17 , 18 , 31 , 32 , 34 , 32 , 33 , 34 , 65 , 66 , 68 , 69 , 72 , 73 , 5 , 6 , 19 , 20 , 25 , 26 , 50 , 51 , 70 , 71 , 12 , 13 , 14 , 15 , 23 , 24 , 43 , 44 , 48 , 49 , 55 , 56 , 59 , 60 , 61 , 62 , } ; private static final boolean jjCanMove_0 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 48 : return ( ( jjbitVec2 [ i2 ] & l2 ) != 0L ) ; case 49 : return ( ( jjbitVec3 [ i2 ] & l2 ) != 0L ) ; case 51 : return ( ( jjbitVec4 [ i2 ] & l2 ) != 0L ) ; case 77 : return ( ( jjbitVec5 [ i2 ] & l2 ) != 0L ) ; case 255 : return ( ( jjbitVec6 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec0 [ i1 ] & l1 ) != 0L ) return true ; return false ; } } private static final boolean jjCanMove_1 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 215 : return ( ( jjbitVec8 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec7 [ i1 ] & l1 ) != 0L ) return true ; return false ; } } private static final boolean jjCanMove_2 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 0 : return ( ( jjbitVec10 [ i2 ] & l2 ) != 0L ) ; case 255 : return ( ( jjbitVec11 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec9 [ i1 ] & l1 ) != 0L ) return true ; return false ; } } private static final boolean jjCanMove_3 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 6 : return ( ( jjbitVec14 [ i2 ] & l2 ) != 0L ) ; case 11 : return ( ( jjbitVec15 [ i2 ] & l2 ) != 0L ) ; case 13 : return ( ( jjbitVec16 [ i2 ] & l2 ) != 0L ) ; case 14 : return ( ( jjbitVec17 [ i2 ] & l2 ) != 0L ) ; case 16 : return ( ( jjbitVec18 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec12 [ i1 ] & l1 ) != 0L ) if ( ( jjbitVec13 [ i2 ] & l2 ) == 0L ) return false ; else return true ; return false ; } } private static final boolean jjCanMove_4 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 0 : return ( ( jjbitVec10 [ i2 ] & l2 ) != 0L ) ; case 215 : return ( ( jjbitVec8 [ i2 ] & l2 ) != 0L ) ; case 255 : return ( ( jjbitVec11 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec19 [ i1 ] & l1 ) != 0L ) return true ; return false ; } } public static final String [ ] jjstrLiteralImages = { "" , null , null , null , null , null , null , null , null , null , null , null , null , null , null , null , } ; public static final String [ ] lexStateNames = { "DEFAULT" , } ; static final long [ ] jjtoToken = { 0x30ffL , } ; static final long [ ] jjtoSkip = { 0x8000L , } ; protected CharStream input_stream ; private final int [ ] jjrounds = new int [ 75 ] ; private final int [ ] jjstateSet = new int [ 150 ] ; protected char curChar ; public StandardTokenizerTokenManager ( CharStream stream ) { input_stream = stream ; } public StandardTokenizerTokenManager ( CharStream stream , int lexState ) { this ( stream ) ; SwitchTo ( lexState ) ; } public void ReInit ( CharStream stream ) { jjmatchedPos = jjnewStateCnt = 0 ; curLexState = defaultLexState ; input_stream = stream ; ReInitRounds ( ) ; } private final void ReInitRounds ( ) { int i ; jjround = 0x80000001 ; for ( i = 75 ; i -- > 0 ; ) jjrounds [ i ] = 0x80000000 ; } public void ReInit ( CharStream stream , int lexState ) { ReInit ( stream ) ; SwitchTo ( lexState ) ; } public void SwitchTo ( int lexState ) { if ( lexState >= 1 || lexState < 0 ) throw new TokenMgrError ( "Error: Ignoring invalid lexical state : " + lexState + ". State unchanged." , TokenMgrError . INVALID_LEXICAL_STATE ) ; else curLexState = lexState ; } protected Token jjFillToken ( ) { Token t = Token . newToken ( jjmatchedKind ) ; t . kind = jjmatchedKind ; String im = jjstrLiteralImages [ jjmatchedKind ] ; t . image = ( im == null ) ? input_stream . GetImage ( ) : im ; t . beginLine = input_stream . getBeginLine ( ) ; t . beginColumn = input_stream . getBeginColumn ( ) ; t . endLine = input_stream . getEndLine ( ) ; t . endColumn = input_stream . getEndColumn ( ) ; return t ; } int curLexState = 0 ; int defaultLexState = 0 ; int jjnewStateCnt ; int jjround ; int jjmatchedPos ; int jjmatchedKind ; public Token getNextToken ( ) { int kind ; Token specialToken = null ; Token matchedToken ; int curPos = 0 ; EOFLoop : for ( ; ; ) { try { curChar = input_stream . BeginToken ( ) ; } catch ( java . io . IOException e ) { jjmatchedKind = 0 ; matchedToken = jjFillToken ( ) ; return matchedToken ; } jjmatchedKind = 0x7fffffff ; jjmatchedPos = 0 ; curPos = jjMoveStringLiteralDfa0_0 ( ) ; if ( jjmatchedPos == 0 && jjmatchedKind > 15 ) { jjmatchedKind = 15 ; } if ( jjmatchedKind != 0x7fffffff ) { if ( jjmatchedPos + 1 < curPos ) input_stream . backup ( curPos - jjmatchedPos - 1 ) ; if ( ( jjtoToken [ jjmatchedKind > > 6 ] & ( 1L << ( jjmatchedKind & 077 ) ) ) != 0L ) { matchedToken = jjFillToken ( ) ; return matchedToken ; } else { continue EOFLoop ; } } int error_line = input_stream . getEndLine ( ) ; int error_column = input_stream . getEndColumn ( ) ; String error_after = null ; boolean EOFSeen = false ; try { input_stream . readChar ( ) ; input_stream . backup ( 1 ) ; } catch ( java . io . IOException e1 ) { EOFSeen = true ; error_after = curPos <= 1 ? "" : input_stream . GetImage ( ) ; if ( curChar == '\n' || curChar == '\r' ) { error_line ++ ; error_column = 0 ; } else error_column ++ ; } if ( ! EOFSeen ) { input_stream . backup ( 1 ) ; error_after = curPos <= 1 ? "" : input_stream . GetImage ( ) ; } throw new TokenMgrError ( EOFSeen , curLexState , error_line , error_column , error_after , curChar , TokenMgrError . LEXICAL_ERROR ) ; } } } 	0	['22', '1', '0', '5', '38', '153', '1', '4', '7', '0.850340136', '3785', '0.114285714', '1', '0', '0.380952381', '0', '0', '169.4545455', '236', '14.0455', '0']
package org . apache . lucene . analysis . standard ; public class Token { public int kind ; public int beginLine , beginColumn , endLine , endColumn ; public String image ; public Token next ; public Token specialToken ; public String toString ( ) { return image ; } public static final Token newToken ( int ofKind ) { switch ( ofKind ) { default : return new Token ( ) ; } } } 	0	['3', '1', '0', '3', '4', '3', '3', '0', '3', '1.4375', '23', '0', '2', '0', '0.5', '0', '0', '4', '2', '1', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermEnum ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; public class PrefixQuery extends Query { private Term prefix ; public PrefixQuery ( Term prefix ) { this . prefix = prefix ; } public Term getPrefix ( ) { return prefix ; } public Query rewrite ( IndexReader reader ) throws IOException { BooleanQuery query = new BooleanQuery ( true ) ; TermEnum enumerator = reader . terms ( prefix ) ; try { String prefixText = prefix . text ( ) ; String prefixField = prefix . field ( ) ; do { Term term = enumerator . term ( ) ; if ( term != null && term . text ( ) . startsWith ( prefixText ) && term . field ( ) == prefixField ) { TermQuery tq = new TermQuery ( term ) ; tq . setBoost ( getBoost ( ) ) ; query . add ( tq , BooleanClause . Occur . SHOULD ) ; } else { break ; } } while ( enumerator . next ( ) ) ; } finally { enumerator . close ( ) ; } return query ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! prefix . field ( ) . equals ( field ) ) { buffer . append ( prefix . field ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( prefix . text ( ) ) ; buffer . append ( '*' ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof PrefixQuery ) ) return false ; PrefixQuery other = ( PrefixQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && this . prefix . equals ( other . prefix ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ prefix . hashCode ( ) ^ 0x6634D93C ; } } 	0	['6', '2', '0', '9', '28', '0', '1', '8', '6', '0', '147', '1', '1', '0.705882353', '0.333333333', '2', '3', '23.33333333', '4', '1.5', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; public final class LowerCaseTokenizer extends LetterTokenizer { public LowerCaseTokenizer ( Reader in ) { super ( in ) ; } protected char normalize ( char c ) { return Character . toLowerCase ( c ) ; } } 	0	['2', '5', '0', '3', '4', '1', '2', '1', '1', '2', '9', '0', '0', '0.888888889', '0.666666667', '1', '1', '3.5', '1', '0.5', '0']
package org . apache . lucene . util ; import java . io . IOException ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . IndexOutput ; public final class BitVector { private byte [ ] bits ; private int size ; private int count = - 1 ; public BitVector ( int n ) { size = n ; bits = new byte [ ( size > > 3 ) + 1 ] ; } public final void set ( int bit ) { if ( bit >= size ) { throw new ArrayIndexOutOfBoundsException ( bit ) ; } bits [ bit > > 3 ] |= 1 << ( bit & 7 ) ; count = - 1 ; } public final void clear ( int bit ) { if ( bit >= size ) { throw new ArrayIndexOutOfBoundsException ( bit ) ; } bits [ bit > > 3 ] &= ~ ( 1 << ( bit & 7 ) ) ; count = - 1 ; } public final boolean get ( int bit ) { if ( bit >= size ) { throw new ArrayIndexOutOfBoundsException ( bit ) ; } return ( bits [ bit > > 3 ] & ( 1 << ( bit & 7 ) ) ) != 0 ; } public final int size ( ) { return size ; } public final int count ( ) { if ( count == - 1 ) { int c = 0 ; int end = bits . length ; for ( int i = 0 ; i < end ; i ++ ) c += BYTE_COUNTS [ bits [ i ] & 0xFF ] ; count = c ; } return count ; } private static final byte [ ] BYTE_COUNTS = { 0 , 1 , 1 , 2 , 1 , 2 , 2 , 3 , 1 , 2 , 2 , 3 , 2 , 3 , 3 , 4 , 1 , 2 , 2 , 3 , 2 , 3 , 3 , 4 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 1 , 2 , 2 , 3 , 2 , 3 , 3 , 4 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 1 , 2 , 2 , 3 , 2 , 3 , 3 , 4 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 4 , 5 , 5 , 6 , 5 , 6 , 6 , 7 , 1 , 2 , 2 , 3 , 2 , 3 , 3 , 4 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 4 , 5 , 5 , 6 , 5 , 6 , 6 , 7 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 4 , 5 , 5 , 6 , 5 , 6 , 6 , 7 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 4 , 5 , 5 , 6 , 5 , 6 , 6 , 7 , 4 , 5 , 5 , 6 , 5 , 6 , 6 , 7 , 5 , 6 , 6 , 7 , 6 , 7 , 7 , 8 } ; public final void write ( Directory d , String name ) throws IOException { IndexOutput output = d . createOutput ( name ) ; try { if ( isSparse ( ) ) { writeDgaps ( output ) ; } else { writeBits ( output ) ; } } finally { output . close ( ) ; } } private void writeBits ( IndexOutput output ) throws IOException { output . writeInt ( size ( ) ) ; output . writeInt ( count ( ) ) ; output . writeBytes ( bits , bits . length ) ; } private void writeDgaps ( IndexOutput output ) throws IOException { output . writeInt ( - 1 ) ; output . writeInt ( size ( ) ) ; output . writeInt ( count ( ) ) ; int last = 0 ; int n = count ( ) ; int m = bits . length ; for ( int i = 0 ; i < m && n > 0 ; i ++ ) { if ( bits [ i ] != 0 ) { output . writeVInt ( i - last ) ; output . writeByte ( bits [ i ] ) ; last = i ; n -= BYTE_COUNTS [ bits [ i ] & 0xFF ] ; } } } private boolean isSparse ( ) { int factor = 10 ; if ( bits . length < ( 1 << 7 ) ) return factor * ( 4 + ( 8 + 8 ) * count ( ) ) < size ( ) ; if ( bits . length < ( 1 << 14 ) ) return factor * ( 4 + ( 8 + 16 ) * count ( ) ) < size ( ) ; if ( bits . length < ( 1 << 21 ) ) return factor * ( 4 + ( 8 + 24 ) * count ( ) ) < size ( ) ; if ( bits . length < ( 1 << 28 ) ) return factor * ( 4 + ( 8 + 32 ) * count ( ) ) < size ( ) ; return factor * ( 4 + ( 8 + 40 ) * count ( ) ) < size ( ) ; } public BitVector ( Directory d , String name ) throws IOException { IndexInput input = d . openInput ( name ) ; try { size = input . readInt ( ) ; if ( size == - 1 ) { readDgaps ( input ) ; } else { readBits ( input ) ; } } finally { input . close ( ) ; } } private void readBits ( IndexInput input ) throws IOException { count = input . readInt ( ) ; bits = new byte [ ( size > > 3 ) + 1 ] ; input . readBytes ( bits , 0 , bits . length ) ; } private void readDgaps ( IndexInput input ) throws IOException { size = input . readInt ( ) ; count = input . readInt ( ) ; bits = new byte [ ( size > > 3 ) + 1 ] ; int last = 0 ; int n = count ( ) ; while ( n > 0 ) { last += input . readVInt ( ) ; bits [ last ] = input . readByte ( ) ; n -= BYTE_COUNTS [ bits [ last ] & 0xFF ] ; } } } 	0	['14', '1', '0', '5', '28', '0', '2', '3', '8', '0.288461538', '1483', '1', '0', '0', '0.320512821', '0', '0', '104.6428571', '10', '1.8571', '0']
package org . apache . lucene . index ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . analysis . TokenStream ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . search . Similarity ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; import java . io . IOException ; import java . io . PrintStream ; import java . io . Reader ; import java . io . StringReader ; import java . util . Arrays ; import java . util . BitSet ; import java . util . Enumeration ; import java . util . Hashtable ; import java . util . Iterator ; import java . util . LinkedList ; import java . util . List ; final class DocumentWriter { private Analyzer analyzer ; private Directory directory ; private Similarity similarity ; private FieldInfos fieldInfos ; private int maxFieldLength ; private int termIndexInterval = IndexWriter . DEFAULT_TERM_INDEX_INTERVAL ; private PrintStream infoStream ; DocumentWriter ( Directory directory , Analyzer analyzer , Similarity similarity , int maxFieldLength ) { this . directory = directory ; this . analyzer = analyzer ; this . similarity = similarity ; this . maxFieldLength = maxFieldLength ; } DocumentWriter ( Directory directory , Analyzer analyzer , IndexWriter writer ) { this . directory = directory ; this . analyzer = analyzer ; this . similarity = writer . getSimilarity ( ) ; this . maxFieldLength = writer . getMaxFieldLength ( ) ; this . termIndexInterval = writer . getTermIndexInterval ( ) ; } final void addDocument ( String segment , Document doc ) throws CorruptIndexException , IOException { fieldInfos = new FieldInfos ( ) ; fieldInfos . add ( doc ) ; postingTable . clear ( ) ; fieldLengths = new int [ fieldInfos . size ( ) ] ; fieldPositions = new int [ fieldInfos . size ( ) ] ; fieldOffsets = new int [ fieldInfos . size ( ) ] ; fieldStoresPayloads = new BitSet ( fieldInfos . size ( ) ) ; fieldBoosts = new float [ fieldInfos . size ( ) ] ; Arrays . fill ( fieldBoosts , doc . getBoost ( ) ) ; try { invertDocument ( doc ) ; Posting [ ] postings = sortPostingTable ( ) ; fieldInfos . write ( directory , segment + ".fnm" ) ; FieldsWriter fieldsWriter = new FieldsWriter ( directory , segment , fieldInfos ) ; try { fieldsWriter . addDocument ( doc ) ; } finally { fieldsWriter . close ( ) ; } writePostings ( postings , segment ) ; writeNorms ( segment ) ; } finally { IOException ex = null ; Iterator it = openTokenStreams . iterator ( ) ; while ( it . hasNext ( ) ) { try { ( ( TokenStream ) it . next ( ) ) . close ( ) ; } catch ( IOException e ) { if ( ex != null ) { ex = e ; } } } openTokenStreams . clear ( ) ; if ( ex != null ) { throw ex ; } } } private final Hashtable postingTable = new Hashtable ( ) ; private int [ ] fieldLengths ; private int [ ] fieldPositions ; private int [ ] fieldOffsets ; private float [ ] fieldBoosts ; private BitSet fieldStoresPayloads ; private List openTokenStreams = new LinkedList ( ) ; private final void invertDocument ( Document doc ) throws IOException { Iterator fieldIterator = doc . getFields ( ) . iterator ( ) ; while ( fieldIterator . hasNext ( ) ) { Fieldable field = ( Fieldable ) fieldIterator . next ( ) ; String fieldName = field . name ( ) ; int fieldNumber = fieldInfos . fieldNumber ( fieldName ) ; int length = fieldLengths [ fieldNumber ] ; int position = fieldPositions [ fieldNumber ] ; if ( length > 0 ) position += analyzer . getPositionIncrementGap ( fieldName ) ; int offset = fieldOffsets [ fieldNumber ] ; if ( field . isIndexed ( ) ) { if ( ! field . isTokenized ( ) ) { String stringValue = field . stringValue ( ) ; if ( field . isStoreOffsetWithTermVector ( ) ) addPosition ( fieldName , stringValue , position ++ , null , new TermVectorOffsetInfo ( offset , offset + stringValue . length ( ) ) ) ; else addPosition ( fieldName , stringValue , position ++ , null , null ) ; offset += stringValue . length ( ) ; length ++ ; } else { TokenStream stream = field . tokenStreamValue ( ) ; if ( stream == null ) { Reader reader ; if ( field . readerValue ( ) != null ) reader = field . readerValue ( ) ; else if ( field . stringValue ( ) != null ) reader = new StringReader ( field . stringValue ( ) ) ; else throw new IllegalArgumentException ( "field must have either String or Reader value" ) ; stream = analyzer . tokenStream ( fieldName , reader ) ; } openTokenStreams . add ( stream ) ; stream . reset ( ) ; Token lastToken = null ; for ( Token t = stream . next ( ) ; t != null ; t = stream . next ( ) ) { position += ( t . getPositionIncrement ( ) - 1 ) ; Payload payload = t . getPayload ( ) ; if ( payload != null ) { fieldStoresPayloads . set ( fieldNumber ) ; } TermVectorOffsetInfo termVectorOffsetInfo ; if ( field . isStoreOffsetWithTermVector ( ) ) { termVectorOffsetInfo = new TermVectorOffsetInfo ( offset + t . startOffset ( ) , offset + t . endOffset ( ) ) ; } else { termVectorOffsetInfo = null ; } addPosition ( fieldName , t . termText ( ) , position ++ , payload , termVectorOffsetInfo ) ; lastToken = t ; if ( ++ length >= maxFieldLength ) { if ( infoStream != null ) infoStream . println ( "maxFieldLength " + maxFieldLength + " reached, ignoring following tokens" ) ; break ; } } if ( lastToken != null ) offset += lastToken . endOffset ( ) + 1 ; } fieldLengths [ fieldNumber ] = length ; fieldPositions [ fieldNumber ] = position ; fieldBoosts [ fieldNumber ] *= field . getBoost ( ) ; fieldOffsets [ fieldNumber ] = offset ; } } for ( int i = fieldStoresPayloads . nextSetBit ( 0 ) ; i >= 0 ; i = fieldStoresPayloads . nextSetBit ( i + 1 ) ) { fieldInfos . fieldInfo ( i ) . storePayloads = true ; } } private final Term termBuffer = new Term ( "" , "" ) ; private final void addPosition ( String field , String text , int position , Payload payload , TermVectorOffsetInfo offset ) { termBuffer . set ( field , text ) ; Posting ti = ( Posting ) postingTable . get ( termBuffer ) ; if ( ti != null ) { int freq = ti . freq ; if ( ti . positions . length == freq ) { int [ ] newPositions = new int [ freq * 2 ] ; int [ ] positions = ti . positions ; System . arraycopy ( positions , 0 , newPositions , 0 , freq ) ; ti . positions = newPositions ; if ( ti . payloads != null ) { Payload [ ] newPayloads = new Payload [ freq * 2 ] ; Payload [ ] payloads = ti . payloads ; System . arraycopy ( payloads , 0 , newPayloads , 0 , payloads . length ) ; ti . payloads = newPayloads ; } } ti . positions [ freq ] = position ; if ( payload != null ) { if ( ti . payloads == null ) { ti . payloads = new Payload [ ti . positions . length ] ; } ti . payloads [ freq ] = payload ; } if ( offset != null ) { if ( ti . offsets . length == freq ) { TermVectorOffsetInfo [ ] newOffsets = new TermVectorOffsetInfo [ freq * 2 ] ; TermVectorOffsetInfo [ ] offsets = ti . offsets ; System . arraycopy ( offsets , 0 , newOffsets , 0 , freq ) ; ti . offsets = newOffsets ; } ti . offsets [ freq ] = offset ; } ti . freq = freq + 1 ; } else { Term term = new Term ( field , text , false ) ; postingTable . put ( term , new Posting ( term , position , payload , offset ) ) ; } } private final Posting [ ] sortPostingTable ( ) { Posting [ ] array = new Posting [ postingTable . size ( ) ] ; Enumeration postings = postingTable . elements ( ) ; for ( int i = 0 ; postings . hasMoreElements ( ) ; i ++ ) array [ i ] = ( Posting ) postings . nextElement ( ) ; quickSort ( array , 0 , array . length - 1 ) ; return array ; } private static final void quickSort ( Posting [ ] postings , int lo , int hi ) { if ( lo >= hi ) return ; int mid = ( lo + hi ) / 2 ; if ( postings [ lo ] . term . compareTo ( postings [ mid ] . term ) > 0 ) { Posting tmp = postings [ lo ] ; postings [ lo ] = postings [ mid ] ; postings [ mid ] = tmp ; } if ( postings [ mid ] . term . compareTo ( postings [ hi ] . term ) > 0 ) { Posting tmp = postings [ mid ] ; postings [ mid ] = postings [ hi ] ; postings [ hi ] = tmp ; if ( postings [ lo ] . term . compareTo ( postings [ mid ] . term ) > 0 ) { Posting tmp2 = postings [ lo ] ; postings [ lo ] = postings [ mid ] ; postings [ mid ] = tmp2 ; } } int left = lo + 1 ; int right = hi - 1 ; if ( left >= right ) return ; Term partition = postings [ mid ] . term ; for ( ; ; ) { while ( postings [ right ] . term . compareTo ( partition ) > 0 ) -- right ; while ( left < right && postings [ left ] . term . compareTo ( partition ) <= 0 ) ++ left ; if ( left < right ) { Posting tmp = postings [ left ] ; postings [ left ] = postings [ right ] ; postings [ right ] = tmp ; -- right ; } else { break ; } } quickSort ( postings , lo , left ) ; quickSort ( postings , left + 1 , hi ) ; } private final void writePostings ( Posting [ ] postings , String segment ) throws CorruptIndexException , IOException { IndexOutput freq = null , prox = null ; TermInfosWriter tis = null ; TermVectorsWriter termVectorWriter = null ; try { freq = directory . createOutput ( segment + ".frq" ) ; prox = directory . createOutput ( segment + ".prx" ) ; tis = new TermInfosWriter ( directory , segment , fieldInfos , termIndexInterval ) ; TermInfo ti = new TermInfo ( ) ; String currentField = null ; boolean currentFieldHasPayloads = false ; for ( int i = 0 ; i < postings . length ; i ++ ) { Posting posting = postings [ i ] ; String termField = posting . term . field ( ) ; if ( currentField != termField ) { currentField = termField ; FieldInfo fi = fieldInfos . fieldInfo ( currentField ) ; currentFieldHasPayloads = fi . storePayloads ; if ( fi . storeTermVector ) { if ( termVectorWriter == null ) { termVectorWriter = new TermVectorsWriter ( directory , segment , fieldInfos ) ; termVectorWriter . openDocument ( ) ; } termVectorWriter . openField ( currentField ) ; } else if ( termVectorWriter != null ) { termVectorWriter . closeField ( ) ; } } ti . set ( 1 , freq . getFilePointer ( ) , prox . getFilePointer ( ) , - 1 ) ; tis . add ( posting . term , ti ) ; int postingFreq = posting . freq ; if ( postingFreq == 1 ) freq . writeVInt ( 1 ) ; else { freq . writeVInt ( 0 ) ; freq . writeVInt ( postingFreq ) ; } int lastPosition = 0 ; int [ ] positions = posting . positions ; Payload [ ] payloads = posting . payloads ; int lastPayloadLength = - 1 ; for ( int j = 0 ; j < postingFreq ; j ++ ) { int position = positions [ j ] ; int delta = position - lastPosition ; if ( currentFieldHasPayloads ) { int payloadLength = 0 ; Payload payload = null ; if ( payloads != null ) { payload = payloads [ j ] ; if ( payload != null ) { payloadLength = payload . length ; } } if ( payloadLength == lastPayloadLength ) { prox . writeVInt ( delta * 2 ) ; } else { prox . writeVInt ( delta * 2 + 1 ) ; prox . writeVInt ( payloadLength ) ; lastPayloadLength = payloadLength ; } if ( payloadLength > 0 ) { prox . writeBytes ( payload . data , payload . offset , payload . length ) ; } } else { prox . writeVInt ( delta ) ; } lastPosition = position ; } if ( termVectorWriter != null && termVectorWriter . isFieldOpen ( ) ) { termVectorWriter . addTerm ( posting . term . text ( ) , postingFreq , posting . positions , posting . offsets ) ; } } if ( termVectorWriter != null ) termVectorWriter . closeDocument ( ) ; } finally { IOException keep = null ; if ( freq != null ) try { freq . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( prox != null ) try { prox . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tis != null ) try { tis . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( termVectorWriter != null ) try { termVectorWriter . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( keep != null ) throw ( IOException ) keep . fillInStackTrace ( ) ; } } private final void writeNorms ( String segment ) throws IOException { for ( int n = 0 ; n < fieldInfos . size ( ) ; n ++ ) { FieldInfo fi = fieldInfos . fieldInfo ( n ) ; if ( fi . isIndexed && ! fi . omitNorms ) { float norm = fieldBoosts [ n ] * similarity . lengthNorm ( fi . name , fieldLengths [ n ] ) ; IndexOutput norms = directory . createOutput ( segment + ".f" + n ) ; try { norms . writeByte ( Similarity . encodeNorm ( norm ) ) ; } finally { norms . close ( ) ; } } } } void setInfoStream ( PrintStream infoStream ) { this . infoStream = infoStream ; } int getNumFields ( ) { return fieldInfos . size ( ) ; } } final class Posting { Term term ; int freq ; int [ ] positions ; Payload [ ] payloads ; TermVectorOffsetInfo [ ] offsets ; Posting ( Term t , int position , Payload payload , TermVectorOffsetInfo offset ) { term = t ; freq = 1 ; positions = new int [ 1 ] ; positions [ 0 ] = position ; if ( payload != null ) { payloads = new Payload [ 1 ] ; payloads [ 0 ] = payload ; } else payloads = null ; if ( offset != null ) { offsets = new TermVectorOffsetInfo [ 1 ] ; offsets [ 0 ] = offset ; } else offsets = null ; } } 	0	['11', '1', '0', '20', '102', '1', '1', '20', '0', '0.733333333', '1192', '1', '5', '0', '0.227272727', '0', '0', '106', '10', '2.3636', '0']
package org . apache . lucene . util ; public final class Constants { private Constants ( ) { } public static final String JAVA_VERSION = System . getProperty ( "java.version" ) ; public static final boolean JAVA_1_1 = JAVA_VERSION . startsWith ( "1.1." ) ; public static final boolean JAVA_1_2 = JAVA_VERSION . startsWith ( "1.2." ) ; public static final boolean JAVA_1_3 = JAVA_VERSION . startsWith ( "1.3." ) ; public static final String OS_NAME = System . getProperty ( "os.name" ) ; public static final boolean LINUX = OS_NAME . startsWith ( "Linux" ) ; public static final boolean WINDOWS = OS_NAME . startsWith ( "Windows" ) ; public static final boolean SUN_OS = OS_NAME . startsWith ( "SunOS" ) ; } 	0	['2', '1', '0', '0', '5', '1', '0', '0', '0', '1', '44', '0', '0', '0', '1', '0', '0', '17', '0', '0', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . util . ToStringUtils ; public abstract class MultiTermQuery extends Query { private Term term ; public MultiTermQuery ( Term term ) { this . term = term ; } public Term getTerm ( ) { return term ; } protected abstract FilteredTermEnum getEnum ( IndexReader reader ) throws IOException ; public Query rewrite ( IndexReader reader ) throws IOException { FilteredTermEnum enumerator = getEnum ( reader ) ; BooleanQuery query = new BooleanQuery ( true ) ; try { do { Term t = enumerator . term ( ) ; if ( t != null ) { TermQuery tq = new TermQuery ( t ) ; tq . setBoost ( getBoost ( ) * enumerator . difference ( ) ) ; query . add ( tq , BooleanClause . Occur . SHOULD ) ; } } while ( enumerator . next ( ) ) ; } finally { enumerator . close ( ) ; } return query ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! term . field ( ) . equals ( field ) ) { buffer . append ( term . field ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( term . text ( ) ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof MultiTermQuery ) ) return false ; final MultiTermQuery multiTermQuery = ( MultiTermQuery ) o ; if ( ! term . equals ( multiTermQuery . term ) ) return false ; return getBoost ( ) == multiTermQuery . getBoost ( ) ; } public int hashCode ( ) { return term . hashCode ( ) + Float . floatToRawIntBits ( getBoost ( ) ) ; } } 	0	['7', '2', '2', '10', '27', '1', '2', '8', '6', '0.333333333', '134', '1', '1', '0.666666667', '0.342857143', '2', '3', '18', '5', '1.5714', '0']
package org . apache . lucene . search ; import java . io . IOException ; final class BooleanScorer extends Scorer { private SubScorer scorers = null ; private BucketTable bucketTable = new BucketTable ( ) ; private int maxCoord = 1 ; private float [ ] coordFactors = null ; private int requiredMask = 0 ; private int prohibitedMask = 0 ; private int nextMask = 1 ; private final int minNrShouldMatch ; BooleanScorer ( Similarity similarity ) { this ( similarity , 1 ) ; } BooleanScorer ( Similarity similarity , int minNrShouldMatch ) { super ( similarity ) ; this . minNrShouldMatch = minNrShouldMatch ; } static final class SubScorer { public Scorer scorer ; public boolean done ; public boolean required = false ; public boolean prohibited = false ; public HitCollector collector ; public SubScorer next ; public SubScorer ( Scorer scorer , boolean required , boolean prohibited , HitCollector collector , SubScorer next ) throws IOException { this . scorer = scorer ; this . done = ! scorer . next ( ) ; this . required = required ; this . prohibited = prohibited ; this . collector = collector ; this . next = next ; } } final void add ( Scorer scorer , boolean required , boolean prohibited ) throws IOException { int mask = 0 ; if ( required || prohibited ) { if ( nextMask == 0 ) throw new IndexOutOfBoundsException ( "More than 32 required/prohibited clauses in query." ) ; mask = nextMask ; nextMask = nextMask << 1 ; } else mask = 0 ; if ( ! prohibited ) maxCoord ++ ; if ( prohibited ) prohibitedMask |= mask ; else if ( required ) requiredMask |= mask ; scorers = new SubScorer ( scorer , required , prohibited , bucketTable . newCollector ( mask ) , scorers ) ; } private final void computeCoordFactors ( ) { coordFactors = new float [ maxCoord ] ; for ( int i = 0 ; i < maxCoord ; i ++ ) coordFactors [ i ] = getSimilarity ( ) . coord ( i , maxCoord - 1 ) ; } private int end ; private Bucket current ; public void score ( HitCollector hc ) throws IOException { next ( ) ; score ( hc , Integer . MAX_VALUE ) ; } protected boolean score ( HitCollector hc , int max ) throws IOException { if ( coordFactors == null ) computeCoordFactors ( ) ; boolean more ; Bucket tmp ; do { bucketTable . first = null ; while ( current != null ) { if ( ( current . bits & prohibitedMask ) == 0 && ( current . bits & requiredMask ) == requiredMask ) { if ( current . doc >= max ) { tmp = current ; current = current . next ; tmp . next = bucketTable . first ; bucketTable . first = tmp ; continue ; } if ( current . coord >= minNrShouldMatch ) { hc . collect ( current . doc , current . score * coordFactors [ current . coord ] ) ; } } current = current . next ; } if ( bucketTable . first != null ) { current = bucketTable . first ; bucketTable . first = current . next ; return true ; } more = false ; end += BucketTable . SIZE ; for ( SubScorer sub = scorers ; sub != null ; sub = sub . next ) { if ( ! sub . done ) { sub . done = ! sub . scorer . score ( sub . collector , end ) ; if ( ! sub . done ) more = true ; } } current = bucketTable . first ; } while ( current != null || more ) ; return false ; } public int doc ( ) { return current . doc ; } public boolean next ( ) throws IOException { boolean more ; do { while ( bucketTable . first != null ) { current = bucketTable . first ; bucketTable . first = current . next ; if ( ( current . bits & prohibitedMask ) == 0 && ( current . bits & requiredMask ) == requiredMask && current . coord >= minNrShouldMatch ) { return true ; } } more = false ; end += BucketTable . SIZE ; for ( SubScorer sub = scorers ; sub != null ; sub = sub . next ) { Scorer scorer = sub . scorer ; while ( ! sub . done && scorer . doc ( ) < end ) { sub . collector . collect ( scorer . doc ( ) , scorer . score ( ) ) ; sub . done = ! scorer . next ( ) ; } if ( ! sub . done ) { more = true ; } } } while ( bucketTable . first != null || more ) ; return false ; } public float score ( ) { if ( coordFactors == null ) computeCoordFactors ( ) ; return current . score * coordFactors [ current . coord ] ; } static final class Bucket { int doc = - 1 ; float score ; int bits ; int coord ; Bucket next ; } static final class BucketTable { public static final int SIZE = 1 << 11 ; public static final int MASK = SIZE - 1 ; final Bucket [ ] buckets = new Bucket [ SIZE ] ; Bucket first = null ; public BucketTable ( ) { } public final int size ( ) { return SIZE ; } public HitCollector newCollector ( int mask ) { return new Collector ( mask , this ) ; } } static final class Collector extends HitCollector { private BucketTable bucketTable ; private int mask ; public Collector ( int mask , BucketTable bucketTable ) { this . mask = mask ; this . bucketTable = bucketTable ; } public final void collect ( final int doc , final float score ) { final BucketTable table = bucketTable ; final int i = doc & BucketTable . MASK ; Bucket bucket = table . buckets [ i ] ; if ( bucket == null ) table . buckets [ i ] = bucket = new Bucket ( ) ; if ( bucket . doc != doc ) { bucket . doc = doc ; bucket . score = score ; bucket . bits = mask ; bucket . coord = 1 ; bucket . next = table . first ; table . first = bucket ; } else { bucket . score += score ; bucket . bits |= mask ; bucket . coord ++ ; } } } public boolean skipTo ( int target ) { throw new UnsupportedOperationException ( ) ; } public Explanation explain ( int doc ) { throw new UnsupportedOperationException ( ) ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "boolean(" ) ; for ( SubScorer sub = scorers ; sub != null ; sub = sub . next ) { buffer . append ( sub . scorer . toString ( ) ) ; buffer . append ( " " ) ; } buffer . append ( ")" ) ; return buffer . toString ( ) ; } } 	0	['12', '2', '0', '8', '29', '26', '1', '7', '7', '0.609090909', '461', '1', '3', '0.444444444', '0.305555556', '1', '3', '36.58333333', '2', '1.0833', '0']
package org . apache . lucene . search ; public class QueryFilter extends CachingWrapperFilter { public QueryFilter ( Query query ) { super ( new QueryWrapperFilter ( query ) ) ; } public boolean equals ( Object o ) { return super . equals ( ( QueryFilter ) o ) ; } public int hashCode ( ) { return super . hashCode ( ) ^ 0x923F64B9 ; } } 	0	['3', '3', '0', '4', '7', '3', '0', '4', '3', '2', '20', '0', '0', '0.714285714', '0.555555556', '2', '3', '5.666666667', '1', '0.6667', '0']
package org . apache . lucene . queryParser ; import java . io . * ; public final class FastCharStream implements CharStream { char [ ] buffer = null ; int bufferLength = 0 ; int bufferPosition = 0 ; int tokenStart = 0 ; int bufferStart = 0 ; Reader input ; public FastCharStream ( Reader r ) { input = r ; } public final char readChar ( ) throws IOException { if ( bufferPosition >= bufferLength ) refill ( ) ; return buffer [ bufferPosition ++ ] ; } private final void refill ( ) throws IOException { int newPosition = bufferLength - tokenStart ; if ( tokenStart == 0 ) { if ( buffer == null ) { buffer = new char [ 2048 ] ; } else if ( bufferLength == buffer . length ) { char [ ] newBuffer = new char [ buffer . length * 2 ] ; System . arraycopy ( buffer , 0 , newBuffer , 0 , bufferLength ) ; buffer = newBuffer ; } } else { System . arraycopy ( buffer , tokenStart , buffer , 0 , newPosition ) ; } bufferLength = newPosition ; bufferPosition = newPosition ; bufferStart += tokenStart ; tokenStart = 0 ; int charsRead = input . read ( buffer , newPosition , buffer . length - newPosition ) ; if ( charsRead == - 1 ) throw new IOException ( "read past eof" ) ; else bufferLength += charsRead ; } public final char BeginToken ( ) throws IOException { tokenStart = bufferPosition ; return readChar ( ) ; } public final void backup ( int amount ) { bufferPosition -= amount ; } public final String GetImage ( ) { return new String ( buffer , tokenStart , bufferPosition - tokenStart ) ; } public final char [ ] GetSuffix ( int len ) { char [ ] value = new char [ len ] ; System . arraycopy ( buffer , bufferPosition - len , value , 0 , len ) ; return value ; } public final void Done ( ) { try { input . close ( ) ; } catch ( IOException e ) { System . err . println ( "Caught: " + e + "; ignoring." ) ; } } public final int getColumn ( ) { return bufferStart + bufferPosition ; } public final int getLine ( ) { return 1 ; } public final int getEndColumn ( ) { return bufferStart + bufferPosition ; } public final int getEndLine ( ) { return 1 ; } public final int getBeginColumn ( ) { return bufferStart + tokenStart ; } public final int getBeginLine ( ) { return 1 ; } } 	0	['14', '1', '0', '2', '25', '3', '1', '1', '13', '0.602564103', '237', '0', '0', '0', '0.404761905', '0', '0', '15.5', '1', '0.9286', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; public class WildcardTermEnum extends FilteredTermEnum { Term searchTerm ; String field = "" ; String text = "" ; String pre = "" ; int preLen = 0 ; boolean endEnum = false ; public WildcardTermEnum ( IndexReader reader , Term term ) throws IOException { super ( ) ; searchTerm = term ; field = searchTerm . field ( ) ; text = searchTerm . text ( ) ; int sidx = text . indexOf ( WILDCARD_STRING ) ; int cidx = text . indexOf ( WILDCARD_CHAR ) ; int idx = sidx ; if ( idx == - 1 ) { idx = cidx ; } else if ( cidx >= 0 ) { idx = Math . min ( idx , cidx ) ; } pre = searchTerm . text ( ) . substring ( 0 , idx ) ; preLen = pre . length ( ) ; text = text . substring ( preLen ) ; setEnum ( reader . terms ( new Term ( searchTerm . field ( ) , pre ) ) ) ; } protected final boolean termCompare ( Term term ) { if ( field == term . field ( ) ) { String searchText = term . text ( ) ; if ( searchText . startsWith ( pre ) ) { return wildcardEquals ( text , 0 , searchText , preLen ) ; } } endEnum = true ; return false ; } public final float difference ( ) { return 1.0f ; } public final boolean endEnum ( ) { return endEnum ; } public static final char WILDCARD_STRING = '*' ; public static final char WILDCARD_CHAR = '?' ; public static final boolean wildcardEquals ( String pattern , int patternIdx , String string , int stringIdx ) { int p = patternIdx ; for ( int s = stringIdx ; ; ++ p , ++ s ) { boolean sEnd = ( s >= string . length ( ) ) ; boolean pEnd = ( p >= pattern . length ( ) ) ; if ( sEnd ) { boolean justWildcardsLeft = true ; int wildcardSearchPos = p ; while ( wildcardSearchPos < pattern . length ( ) && justWildcardsLeft ) { char wildchar = pattern . charAt ( wildcardSearchPos ) ; if ( wildchar != WILDCARD_CHAR && wildchar != WILDCARD_STRING ) { justWildcardsLeft = false ; } else { if ( wildchar == WILDCARD_CHAR ) { return false ; } wildcardSearchPos ++ ; } } if ( justWildcardsLeft ) { return true ; } } if ( sEnd || pEnd ) { break ; } if ( pattern . charAt ( p ) == WILDCARD_CHAR ) { continue ; } if ( pattern . charAt ( p ) == WILDCARD_STRING ) { ++ p ; for ( int i = string . length ( ) ; i >= s ; -- i ) { if ( wildcardEquals ( pattern , p , string , i ) ) { return true ; } } break ; } if ( pattern . charAt ( p ) != string . charAt ( s ) ) { break ; } } return false ; } public void close ( ) throws IOException { super . close ( ) ; searchTerm = null ; field = null ; text = null ; } } 	0	['6', '3', '0', '5', '20', '5', '1', '4', '5', '0.825', '247', '0', '1', '0.722222222', '0.333333333', '1', '4', '38.83333333', '16', '3.6667', '0']
package org . apache . lucene . store ; import java . io . IOException ; public class LockObtainFailedException extends IOException { public LockObtainFailedException ( String message ) { super ( message ) ; } } 	0	['1', '4', '0', '5', '2', '0', '5', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . * ; abstract class PhraseScorer extends Scorer { private Weight weight ; protected byte [ ] norms ; protected float value ; private boolean firstTime = true ; private boolean more = true ; protected PhraseQueue pq ; protected PhrasePositions first , last ; private float freq ; PhraseScorer ( Weight weight , TermPositions [ ] tps , int [ ] offsets , Similarity similarity , byte [ ] norms ) { super ( similarity ) ; this . norms = norms ; this . weight = weight ; this . value = weight . getValue ( ) ; for ( int i = 0 ; i < tps . length ; i ++ ) { PhrasePositions pp = new PhrasePositions ( tps [ i ] , offsets [ i ] ) ; if ( last != null ) { last . next = pp ; } else first = pp ; last = pp ; } pq = new PhraseQueue ( tps . length ) ; } public int doc ( ) { return first . doc ; } public boolean next ( ) throws IOException { if ( firstTime ) { init ( ) ; firstTime = false ; } else if ( more ) { more = last . next ( ) ; } return doNext ( ) ; } private boolean doNext ( ) throws IOException { while ( more ) { while ( more && first . doc < last . doc ) { more = first . skipTo ( last . doc ) ; firstToLast ( ) ; } if ( more ) { freq = phraseFreq ( ) ; if ( freq == 0.0f ) more = last . next ( ) ; else return true ; } } return false ; } public float score ( ) throws IOException { float raw = getSimilarity ( ) . tf ( freq ) * value ; return raw * Similarity . decodeNorm ( norms [ first . doc ] ) ; } public boolean skipTo ( int target ) throws IOException { firstTime = false ; for ( PhrasePositions pp = first ; more && pp != null ; pp = pp . next ) { more = pp . skipTo ( target ) ; } if ( more ) sort ( ) ; return doNext ( ) ; } protected abstract float phraseFreq ( ) throws IOException ; private void init ( ) throws IOException { for ( PhrasePositions pp = first ; more && pp != null ; pp = pp . next ) more = pp . next ( ) ; if ( more ) sort ( ) ; } private void sort ( ) { pq . clear ( ) ; for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) pq . put ( pp ) ; pqToList ( ) ; } protected final void pqToList ( ) { last = first = null ; while ( pq . top ( ) != null ) { PhrasePositions pp = ( PhrasePositions ) pq . pop ( ) ; if ( last != null ) { last . next = pp ; } else first = pp ; last = pp ; pp . next = null ; } } protected final void firstToLast ( ) { last . next = first ; last = first ; first = first . next ; last . next = null ; } public Explanation explain ( final int doc ) throws IOException { Explanation tfExplanation = new Explanation ( ) ; while ( next ( ) && doc ( ) < doc ) { } float phraseFreq = ( doc ( ) == doc ) ? freq : 0.0f ; tfExplanation . setValue ( getSimilarity ( ) . tf ( phraseFreq ) ) ; tfExplanation . setDescription ( "tf(phraseFreq=" + phraseFreq + ")" ) ; return tfExplanation ; } public String toString ( ) { return "scorer(" + weight + ")" ; } } 	0	['13', '2', '2', '9', '34', '0', '2', '7', '6', '0.666666667', '345', '1', '4', '0.4', '0.21978022', '1', '3', '24.84615385', '3', '1.1538', '0']
package org . apache . lucene . document ; import java . util . Set ; public class SetBasedFieldSelector implements FieldSelector { private Set fieldsToLoad ; private Set lazyFieldsToLoad ; public SetBasedFieldSelector ( Set fieldsToLoad , Set lazyFieldsToLoad ) { this . fieldsToLoad = fieldsToLoad ; this . lazyFieldsToLoad = lazyFieldsToLoad ; } public FieldSelectorResult accept ( String fieldName ) { FieldSelectorResult result = FieldSelectorResult . NO_LOAD ; if ( fieldsToLoad . contains ( fieldName ) == true ) { result = FieldSelectorResult . LOAD ; } if ( lazyFieldsToLoad . contains ( fieldName ) == true ) { result = FieldSelectorResult . LAZY_LOAD ; } return result ; } } 	0	['2', '1', '0', '2', '4', '0', '0', '2', '2', '0', '33', '1', '0', '0', '0.666666667', '0', '0', '14.5', '3', '1.5', '0']
package org . apache . lucene . search ; import java . util . ArrayList ; public class Explanation implements java . io . Serializable { private float value ; private String description ; private ArrayList details ; public Explanation ( ) { } public Explanation ( float value , String description ) { this . value = value ; this . description = description ; } public boolean isMatch ( ) { return ( 0.0f < getValue ( ) ) ; } public float getValue ( ) { return value ; } public void setValue ( float value ) { this . value = value ; } public String getDescription ( ) { return description ; } public void setDescription ( String description ) { this . description = description ; } protected String getSummary ( ) { return getValue ( ) + " = " + getDescription ( ) ; } public Explanation [ ] getDetails ( ) { if ( details == null ) return null ; return ( Explanation [ ] ) details . toArray ( new Explanation [ 0 ] ) ; } public void addDetail ( Explanation detail ) { if ( details == null ) details = new ArrayList ( ) ; details . add ( detail ) ; } public String toString ( ) { return toString ( 0 ) ; } protected String toString ( int depth ) { StringBuffer buffer = new StringBuffer ( ) ; for ( int i = 0 ; i < depth ; i ++ ) { buffer . append ( "  " ) ; } buffer . append ( getSummary ( ) ) ; buffer . append ( "\n" ) ; Explanation [ ] details = getDetails ( ) ; if ( details != null ) { for ( int i = 0 ; i < details . length ; i ++ ) { buffer . append ( details [ i ] . toString ( depth + 1 ) ) ; } } return buffer . toString ( ) ; } public String toHtml ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "<ul>\n" ) ; buffer . append ( "<li>" ) ; buffer . append ( getSummary ( ) ) ; buffer . append ( "<br />\n" ) ; Explanation [ ] details = getDetails ( ) ; if ( details != null ) { for ( int i = 0 ; i < details . length ; i ++ ) { buffer . append ( details [ i ] . toHtml ( ) ) ; } } buffer . append ( "</li>\n" ) ; buffer . append ( "</ul>\n" ) ; return buffer . toString ( ) ; } } 	0	['13', '1', '1', '41', '21', '64', '41', '0', '11', '0.611111111', '197', '1', '0', '0', '0.292307692', '0', '0', '13.92307692', '4', '1.4615', '0']
package org . apache . lucene . index ; public class FieldReaderException extends RuntimeException { public FieldReaderException ( ) { } public FieldReaderException ( Throwable cause ) { super ( cause ) ; } public FieldReaderException ( String message ) { super ( message ) ; } public FieldReaderException ( String message , Throwable cause ) { super ( message , cause ) ; } } 	0	['4', '4', '0', '1', '8', '6', '1', '0', '4', '2', '20', '0', '0', '1', '0.666666667', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . io . StringReader ; import java . util . ArrayList ; import java . util . Arrays ; import java . util . HashMap ; import java . util . Iterator ; import java . util . List ; import java . util . Map ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . analysis . TokenStream ; import org . apache . lucene . index . TermFreqVector ; public class QueryTermVector implements TermFreqVector { private String [ ] terms = new String [ 0 ] ; private int [ ] termFreqs = new int [ 0 ] ; public String getField ( ) { return null ; } public QueryTermVector ( String [ ] queryTerms ) { processTerms ( queryTerms ) ; } public QueryTermVector ( String queryString , Analyzer analyzer ) { if ( analyzer != null ) { TokenStream stream = analyzer . tokenStream ( "" , new StringReader ( queryString ) ) ; if ( stream != null ) { List terms = new ArrayList ( ) ; try { final Token reusableToken = new Token ( ) ; for ( Token nextToken = stream . next ( reusableToken ) ; nextToken != null ; nextToken = stream . next ( reusableToken ) ) { terms . add ( nextToken . term ( ) ) ; } processTerms ( ( String [ ] ) terms . toArray ( new String [ terms . size ( ) ] ) ) ; } catch ( IOException e ) { } } } } private void processTerms ( String [ ] queryTerms ) { if ( queryTerms != null ) { Arrays . sort ( queryTerms ) ; Map tmpSet = new HashMap ( queryTerms . length ) ; List tmpList = new ArrayList ( queryTerms . length ) ; List tmpFreqs = new ArrayList ( queryTerms . length ) ; int j = 0 ; for ( int i = 0 ; i < queryTerms . length ; i ++ ) { String term = queryTerms [ i ] ; Integer position = ( Integer ) tmpSet . get ( term ) ; if ( position == null ) { tmpSet . put ( term , new Integer ( j ++ ) ) ; tmpList . add ( term ) ; tmpFreqs . add ( new Integer ( 1 ) ) ; } else { Integer integer = ( Integer ) tmpFreqs . get ( position . intValue ( ) ) ; tmpFreqs . set ( position . intValue ( ) , new Integer ( integer . intValue ( ) + 1 ) ) ; } } terms = ( String [ ] ) tmpList . toArray ( terms ) ; termFreqs = new int [ tmpFreqs . size ( ) ] ; int i = 0 ; for ( Iterator iter = tmpFreqs . iterator ( ) ; iter . hasNext ( ) ; ) { Integer integer = ( Integer ) iter . next ( ) ; termFreqs [ i ++ ] = integer . intValue ( ) ; } } } public final String toString ( ) { StringBuffer sb = new StringBuffer ( ) ; sb . append ( '{' ) ; for ( int i = 0 ; i < terms . length ; i ++ ) { if ( i > 0 ) sb . append ( ", " ) ; sb . append ( terms [ i ] ) . append ( '/' ) . append ( termFreqs [ i ] ) ; } sb . append ( '}' ) ; return sb . toString ( ) ; } public int size ( ) { return terms . length ; } public String [ ] getTerms ( ) { return terms ; } public int [ ] getTermFrequencies ( ) { return termFreqs ; } public int indexOf ( String term ) { int res = Arrays . binarySearch ( terms , term ) ; return res >= 0 ? res : - 1 ; } public int [ ] indexesOf ( String [ ] terms , int start , int len ) { int res [ ] = new int [ len ] ; for ( int i = 0 ; i < len ; i ++ ) { res [ i ] = indexOf ( terms [ i ] ) ; } return res ; } } 	0	['10', '1', '0', '4', '38', '0', '0', '4', '9', '0.388888889', '287', '1', '0', '0', '0.34', '0', '0', '27.5', '5', '1.6', '0']
package org . apache . lucene . index ; class ReadOnlySegmentReader extends SegmentReader { static void noWrite ( ) { throw new UnsupportedOperationException ( "This IndexReader cannot make any changes to the index (it was opened with readOnly = true)" ) ; } protected void acquireWriteLock ( ) { noWrite ( ) ; } public boolean isDeleted ( int n ) { return deletedDocs != null && deletedDocs . get ( n ) ; } } 	0	['4', '4', '0', '3', '7', '6', '2', '2', '1', '2', '26', '0', '0', '0.98125', '0.5', '2', '3', '5.5', '3', '1.25', '0']
package org . apache . lucene . index ; import java . io . IOException ; abstract class DocFieldConsumerPerThread { abstract void startDocument ( ) throws IOException ; abstract DocumentsWriter . DocWriter finishDocument ( ) throws IOException ; abstract DocFieldConsumerPerField addField ( FieldInfo fi ) ; abstract void abort ( ) ; } 	0	['5', '1', '3', '13', '6', '10', '10', '3', '0', '2', '8', '0', '0', '0', '0.6', '0', '0', '0.6', '1', '0.8', '0']
package org . apache . lucene . index ; import org . apache . lucene . util . PriorityQueue ; import java . io . IOException ; import java . util . Arrays ; import java . util . Iterator ; import java . util . LinkedList ; import java . util . List ; public class MultipleTermPositions implements TermPositions { private static final class TermPositionsQueue extends PriorityQueue { TermPositionsQueue ( List termPositions ) throws IOException { initialize ( termPositions . size ( ) ) ; Iterator i = termPositions . iterator ( ) ; while ( i . hasNext ( ) ) { TermPositions tp = ( TermPositions ) i . next ( ) ; if ( tp . next ( ) ) put ( tp ) ; } } final TermPositions peek ( ) { return ( TermPositions ) top ( ) ; } public final boolean lessThan ( Object a , Object b ) { return ( ( TermPositions ) a ) . doc ( ) < ( ( TermPositions ) b ) . doc ( ) ; } } private static final class IntQueue { private int _arraySize = 16 ; private int _index = 0 ; private int _lastIndex = 0 ; private int [ ] _array = new int [ _arraySize ] ; final void add ( int i ) { if ( _lastIndex == _arraySize ) growArray ( ) ; _array [ _lastIndex ++ ] = i ; } final int next ( ) { return _array [ _index ++ ] ; } final void sort ( ) { Arrays . sort ( _array , _index , _lastIndex ) ; } final void clear ( ) { _index = 0 ; _lastIndex = 0 ; } final int size ( ) { return ( _lastIndex - _index ) ; } private void growArray ( ) { int [ ] newArray = new int [ _arraySize * 2 ] ; System . arraycopy ( _array , 0 , newArray , 0 , _arraySize ) ; _array = newArray ; _arraySize *= 2 ; } } private int _doc ; private int _freq ; private TermPositionsQueue _termPositionsQueue ; private IntQueue _posList ; public MultipleTermPositions ( IndexReader indexReader , Term [ ] terms ) throws IOException { List termPositions = new LinkedList ( ) ; for ( int i = 0 ; i < terms . length ; i ++ ) termPositions . add ( indexReader . termPositions ( terms [ i ] ) ) ; _termPositionsQueue = new TermPositionsQueue ( termPositions ) ; _posList = new IntQueue ( ) ; } public final boolean next ( ) throws IOException { if ( _termPositionsQueue . size ( ) == 0 ) return false ; _posList . clear ( ) ; _doc = _termPositionsQueue . peek ( ) . doc ( ) ; TermPositions tp ; do { tp = _termPositionsQueue . peek ( ) ; for ( int i = 0 ; i < tp . freq ( ) ; i ++ ) _posList . add ( tp . nextPosition ( ) ) ; if ( tp . next ( ) ) _termPositionsQueue . adjustTop ( ) ; else { _termPositionsQueue . pop ( ) ; tp . close ( ) ; } } while ( _termPositionsQueue . size ( ) > 0 && _termPositionsQueue . peek ( ) . doc ( ) == _doc ) ; _posList . sort ( ) ; _freq = _posList . size ( ) ; return true ; } public final int nextPosition ( ) { return _posList . next ( ) ; } public final boolean skipTo ( int target ) throws IOException { while ( _termPositionsQueue . peek ( ) != null && target > _termPositionsQueue . peek ( ) . doc ( ) ) { TermPositions tp = ( TermPositions ) _termPositionsQueue . pop ( ) ; if ( tp . skipTo ( target ) ) _termPositionsQueue . put ( tp ) ; else tp . close ( ) ; } return next ( ) ; } public final int doc ( ) { return _doc ; } public final int freq ( ) { return _freq ; } public final void close ( ) throws IOException { while ( _termPositionsQueue . size ( ) > 0 ) ( ( TermPositions ) _termPositionsQueue . pop ( ) ) . close ( ) ; } public void seek ( Term arg0 ) throws IOException { throw new UnsupportedOperationException ( ) ; } public void seek ( TermEnum termEnum ) throws IOException { throw new UnsupportedOperationException ( ) ; } public int read ( int [ ] arg0 , int [ ] arg1 ) throws IOException { throw new UnsupportedOperationException ( ) ; } public int getPayloadLength ( ) { throw new UnsupportedOperationException ( ) ; } public byte [ ] getPayload ( byte [ ] data , int offset ) throws IOException { throw new UnsupportedOperationException ( ) ; } public boolean isPayloadAvailable ( ) { return false ; } } 	0	['13', '1', '0', '8', '36', '58', '1', '7', '13', '0.791666667', '191', '1', '2', '0', '0.201923077', '0', '0', '13.38461538', '1', '0.9231', '0']
package org . apache . lucene . index ; import java . io . IOException ; public interface TermDocs { void seek ( Term term ) throws IOException ; void seek ( TermEnum termEnum ) throws IOException ; int doc ( ) ; int freq ( ) ; boolean next ( ) throws IOException ; int read ( int [ ] docs , int [ ] freqs ) throws IOException ; boolean skipTo ( int target ) throws IOException ; void close ( ) throws IOException ; } 	0	['8', '1', '0', '30', '8', '28', '28', '2', '8', '2', '8', '0', '0', '0', '0.3', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . index ; final class IntBlockPool { public int [ ] [ ] buffers = new int [ 10 ] [ ] ; int bufferUpto = - 1 ; public int intUpto = DocumentsWriter . INT_BLOCK_SIZE ; public int [ ] buffer ; public int intOffset = - DocumentsWriter . INT_BLOCK_SIZE ; final private DocumentsWriter docWriter ; final boolean trackAllocations ; public IntBlockPool ( DocumentsWriter docWriter , boolean trackAllocations ) { this . docWriter = docWriter ; this . trackAllocations = trackAllocations ; } public void reset ( ) { if ( bufferUpto != - 1 ) { if ( bufferUpto > 0 ) docWriter . recycleIntBlocks ( buffers , 1 , 1 + bufferUpto ) ; bufferUpto = 0 ; intUpto = 0 ; intOffset = 0 ; buffer = buffers [ 0 ] ; } } public void nextBuffer ( ) { if ( 1 + bufferUpto == buffers . length ) { int [ ] [ ] newBuffers = new int [ ( int ) ( buffers . length * 1.5 ) ] [ ] ; System . arraycopy ( buffers , 0 , newBuffers , 0 , buffers . length ) ; buffers = newBuffers ; } buffer = buffers [ 1 + bufferUpto ] = docWriter . getIntBlock ( trackAllocations ) ; bufferUpto ++ ; intUpto = 0 ; intOffset += DocumentsWriter . INT_BLOCK_SIZE ; } } 	0	['3', '1', '0', '3', '7', '0', '2', '1', '3', '0.142857143', '125', '0.142857143', '1', '0', '0.555555556', '0', '0', '38.33333333', '3', '1.6667', '0']
package org . apache . lucene . store ; import java . io . IOException ; import java . util . zip . CRC32 ; import java . util . zip . Checksum ; public class ChecksumIndexInput extends IndexInput { IndexInput main ; Checksum digest ; public ChecksumIndexInput ( IndexInput main ) { this . main = main ; digest = new CRC32 ( ) ; } public byte readByte ( ) throws IOException { final byte b = main . readByte ( ) ; digest . update ( b ) ; return b ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { main . readBytes ( b , offset , len ) ; digest . update ( b , offset , len ) ; } public long getChecksum ( ) { return digest . getValue ( ) ; } public void close ( ) throws IOException { main . close ( ) ; } public long getFilePointer ( ) { return main . getFilePointer ( ) ; } public void seek ( long pos ) { throw new RuntimeException ( "not allowed" ) ; } public long length ( ) { return main . length ( ) ; } } 	0	['8', '2', '0', '2', '19', '0', '1', '1', '8', '0.428571429', '65', '0', '1', '0.708333333', '0.3', '1', '4', '6.875', '1', '0.875', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Collection ; abstract class DocConsumer { abstract DocConsumerPerThread addThread ( DocumentsWriterThreadState perThread ) throws IOException ; abstract void flush ( final Collection threads , final DocumentsWriter . FlushState state ) throws IOException ; abstract void closeDocStore ( final DocumentsWriter . FlushState state ) throws IOException ; abstract void abort ( ) ; abstract boolean freeRAM ( ) ; } 	0	['6', '1', '1', '5', '7', '15', '3', '3', '0', '2', '9', '0', '0', '0', '0.416666667', '0', '0', '0.5', '1', '0.8333', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . analysis . Token ; abstract class TermsHashConsumerPerField { abstract boolean start ( Fieldable [ ] fields , int count ) throws IOException ; abstract void finish ( ) throws IOException ; abstract void skippingLongTerm ( Token t ) throws IOException ; abstract void newTerm ( Token t , RawPostingList p ) throws IOException ; abstract void addTerm ( Token t , RawPostingList p ) throws IOException ; abstract int getStreamCount ( ) ; } 	0	['7', '1', '2', '10', '8', '21', '7', '3', '0', '2', '10', '0', '0', '0', '0.4', '0', '0', '0.428571429', '1', '0.8571', '0']
package org . apache . lucene . index ; final class TermInfo { int docFreq = 0 ; long freqPointer = 0 ; long proxPointer = 0 ; int skipOffset ; TermInfo ( ) { } TermInfo ( int df , long fp , long pp ) { docFreq = df ; freqPointer = fp ; proxPointer = pp ; } TermInfo ( TermInfo ti ) { docFreq = ti . docFreq ; freqPointer = ti . freqPointer ; proxPointer = ti . proxPointer ; skipOffset = ti . skipOffset ; } final void set ( int docFreq , long freqPointer , long proxPointer , int skipOffset ) { this . docFreq = docFreq ; this . freqPointer = freqPointer ; this . proxPointer = proxPointer ; this . skipOffset = skipOffset ; } final void set ( TermInfo ti ) { docFreq = ti . docFreq ; freqPointer = ti . freqPointer ; proxPointer = ti . proxPointer ; skipOffset = ti . skipOffset ; } } 	0	['5', '1', '0', '8', '6', '0', '8', '0', '0', '0.125', '100', '0', '0', '0', '0.55', '0', '0', '18.2', '1', '0.4', '0']
package org . apache . lucene . index ; import java . io . IOException ; public class CorruptIndexException extends IOException { public CorruptIndexException ( String message ) { super ( message ) ; } } 	0	['1', '4', '0', '36', '2', '0', '36', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . queryParser ; import java . util . ArrayList ; import java . util . List ; import java . util . Map ; import java . util . Vector ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . search . BooleanClause ; import org . apache . lucene . search . BooleanQuery ; import org . apache . lucene . search . MultiPhraseQuery ; import org . apache . lucene . search . PhraseQuery ; import org . apache . lucene . search . Query ; public class MultiFieldQueryParser extends QueryParser { protected String [ ] fields ; protected Map boosts ; public MultiFieldQueryParser ( String [ ] fields , Analyzer analyzer , Map boosts ) { this ( fields , analyzer ) ; this . boosts = boosts ; } public MultiFieldQueryParser ( String [ ] fields , Analyzer analyzer ) { super ( null , analyzer ) ; this . fields = fields ; } protected Query getFieldQuery ( String field , String queryText , int slop ) throws ParseException { if ( field == null ) { List clauses = new ArrayList ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { Query q = super . getFieldQuery ( fields [ i ] , queryText ) ; if ( q != null ) { if ( boosts != null ) { Float boost = ( Float ) boosts . get ( fields [ i ] ) ; if ( boost != null ) { q . setBoost ( boost . floatValue ( ) ) ; } } applySlop ( q , slop ) ; clauses . add ( new BooleanClause ( q , BooleanClause . Occur . SHOULD ) ) ; } } if ( clauses . size ( ) == 0 ) return null ; return getBooleanQuery ( clauses , true ) ; } Query q = super . getFieldQuery ( field , queryText ) ; applySlop ( q , slop ) ; return q ; } private void applySlop ( Query q , int slop ) { if ( q instanceof PhraseQuery ) { ( ( PhraseQuery ) q ) . setSlop ( slop ) ; } else if ( q instanceof MultiPhraseQuery ) { ( ( MultiPhraseQuery ) q ) . setSlop ( slop ) ; } } protected Query getFieldQuery ( String field , String queryText ) throws ParseException { return getFieldQuery ( field , queryText , 0 ) ; } protected Query getFuzzyQuery ( String field , String termStr , float minSimilarity ) throws ParseException { if ( field == null ) { List clauses = new ArrayList ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( getFuzzyQuery ( fields [ i ] , termStr , minSimilarity ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getFuzzyQuery ( field , termStr , minSimilarity ) ; } protected Query getPrefixQuery ( String field , String termStr ) throws ParseException { if ( field == null ) { List clauses = new ArrayList ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( getPrefixQuery ( fields [ i ] , termStr ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getPrefixQuery ( field , termStr ) ; } protected Query getWildcardQuery ( String field , String termStr ) throws ParseException { if ( field == null ) { List clauses = new ArrayList ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( getWildcardQuery ( fields [ i ] , termStr ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getWildcardQuery ( field , termStr ) ; } protected Query getRangeQuery ( String field , String part1 , String part2 , boolean inclusive ) throws ParseException { if ( field == null ) { List clauses = new ArrayList ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( getRangeQuery ( fields [ i ] , part1 , part2 , inclusive ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getRangeQuery ( field , part1 , part2 , inclusive ) ; } public static Query parse ( String [ ] queries , String [ ] fields , Analyzer analyzer ) throws ParseException { if ( queries . length != fields . length ) throw new IllegalArgumentException ( "queries.length != fields.length" ) ; BooleanQuery bQuery = new BooleanQuery ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { QueryParser qp = new QueryParser ( fields [ i ] , analyzer ) ; Query q = qp . parse ( queries [ i ] ) ; if ( q != null && ( ! ( q instanceof BooleanQuery ) || ( ( BooleanQuery ) q ) . getClauses ( ) . length > 0 ) ) { bQuery . add ( q , BooleanClause . Occur . SHOULD ) ; } } return bQuery ; } public static Query parse ( String query , String [ ] fields , BooleanClause . Occur [ ] flags , Analyzer analyzer ) throws ParseException { if ( fields . length != flags . length ) throw new IllegalArgumentException ( "fields.length != flags.length" ) ; BooleanQuery bQuery = new BooleanQuery ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { QueryParser qp = new QueryParser ( fields [ i ] , analyzer ) ; Query q = qp . parse ( query ) ; if ( q != null && ( ! ( q instanceof BooleanQuery ) || ( ( BooleanQuery ) q ) . getClauses ( ) . length > 0 ) ) { bQuery . add ( q , flags [ i ] ) ; } } return bQuery ; } public static Query parse ( String [ ] queries , String [ ] fields , BooleanClause . Occur [ ] flags , Analyzer analyzer ) throws ParseException { if ( ! ( queries . length == fields . length && queries . length == flags . length ) ) throw new IllegalArgumentException ( "queries, fields, and flags array have have different length" ) ; BooleanQuery bQuery = new BooleanQuery ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { QueryParser qp = new QueryParser ( fields [ i ] , analyzer ) ; Query q = qp . parse ( queries [ i ] ) ; if ( q != null && ( ! ( q instanceof BooleanQuery ) || ( ( BooleanQuery ) q ) . getClauses ( ) . length > 0 ) ) { bQuery . add ( q , flags [ i ] ) ; } } return bQuery ; } } 	0	['12', '2', '0', '9', '33', '34', '0', '9', '5', '0.590909091', '453', '1', '0', '0.885057471', '0.283333333', '1', '5', '36.58333333', '3', '1', '0']
package org . apache . lucene . search ; public interface ScoreDocComparator { static final ScoreDocComparator RELEVANCE = new ScoreDocComparator ( ) { public int compare ( ScoreDoc i , ScoreDoc j ) { if ( i . score > j . score ) return - 1 ; if ( i . score < j . score ) return 1 ; return 0 ; } public Comparable sortValue ( ScoreDoc i ) { return new Float ( i . score ) ; } public int sortType ( ) { return SortField . SCORE ; } } ; static final ScoreDocComparator INDEXORDER = new ScoreDocComparator ( ) { public int compare ( ScoreDoc i , ScoreDoc j ) { if ( i . doc < j . doc ) return - 1 ; if ( i . doc > j . doc ) return 1 ; return 0 ; } public Comparable sortValue ( ScoreDoc i ) { return new Integer ( i . doc ) ; } public int sortType ( ) { return SortField . DOC ; } } ; int compare ( ScoreDoc i , ScoreDoc j ) ; Comparable sortValue ( ScoreDoc i ) ; int sortType ( ) ; } 	0	['4', '1', '0', '16', '6', '6', '15', '3', '3', '1', '15', '0', '2', '0', '0.833333333', '0', '0', '2.25', '1', '0.75', '0']
package org . apache . lucene . index ; import java . io . IOException ; abstract class TermsHashConsumerPerThread { abstract void startDocument ( ) throws IOException ; abstract DocumentsWriter . DocWriter finishDocument ( ) throws IOException ; abstract public TermsHashConsumerPerField addField ( TermsHashPerField termsHashPerField , FieldInfo fieldInfo ) ; abstract public void abort ( ) ; } 	0	['5', '1', '2', '11', '6', '10', '8', '4', '2', '2', '8', '0', '0', '0', '0.466666667', '0', '0', '0.6', '1', '0.8', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; public final class LowerCaseFilter extends TokenFilter { public LowerCaseFilter ( TokenStream in ) { super ( in ) ; } public final Token next ( final Token reusableToken ) throws IOException { assert reusableToken != null ; Token nextToken = input . next ( reusableToken ) ; if ( nextToken != null ) { final char [ ] buffer = nextToken . termBuffer ( ) ; final int length = nextToken . termLength ( ) ; for ( int i = 0 ; i < length ; i ++ ) buffer [ i ] = Character . toLowerCase ( buffer [ i ] ) ; return nextToken ; } else return null ; } } 	0	['4', '3', '0', '4', '14', '4', '1', '3', '2', '0.833333333', '74', '0', '0', '0.777777778', '0.416666667', '1', '2', '17', '1', '0.5', '0']
package org . apache . lucene . index ; import org . apache . lucene . util . ArrayUtil ; import org . apache . lucene . search . Similarity ; final class NormsWriterPerField extends InvertedDocEndConsumerPerField implements Comparable { final NormsWriterPerThread perThread ; final FieldInfo fieldInfo ; final DocumentsWriter . DocState docState ; int [ ] docIDs = new int [ 1 ] ; byte [ ] norms = new byte [ 1 ] ; int upto ; final DocInverter . FieldInvertState fieldState ; public void reset ( ) { docIDs = ArrayUtil . shrink ( docIDs , upto ) ; norms = ArrayUtil . shrink ( norms , upto ) ; upto = 0 ; } public NormsWriterPerField ( final DocInverterPerField docInverterPerField , final NormsWriterPerThread perThread , final FieldInfo fieldInfo ) { this . perThread = perThread ; this . fieldInfo = fieldInfo ; docState = perThread . docState ; fieldState = docInverterPerField . fieldState ; } void abort ( ) { upto = 0 ; } public int compareTo ( Object other ) { return fieldInfo . name . compareTo ( ( ( NormsWriterPerField ) other ) . fieldInfo . name ) ; } void finish ( ) { assert docIDs . length == norms . length ; if ( fieldInfo . isIndexed && ! fieldInfo . omitNorms ) { if ( docIDs . length <= upto ) { assert docIDs . length == upto ; docIDs = ArrayUtil . grow ( docIDs , 1 + upto ) ; norms = ArrayUtil . grow ( norms , 1 + upto ) ; } final float norm = fieldState . boost * docState . similarity . lengthNorm ( fieldInfo . name , fieldState . length ) ; norms [ upto ] = Similarity . encodeNorm ( norm ) ; docIDs [ upto ] = docState . docID ; upto ++ ; } } } 	0	['7', '2', '0', '9', '20', '5', '2', '8', '3', '0.796296296', '191', '0', '4', '0.285714286', '0.277777778', '0', '0', '25', '8', '1.7143', '0']
package org . apache . lucene . index ; import java . io . IOException ; public class SerialMergeScheduler extends MergeScheduler { synchronized public void merge ( IndexWriter writer ) throws CorruptIndexException , IOException { while ( true ) { MergePolicy . OneMerge merge = writer . getNextMerge ( ) ; if ( merge == null ) break ; writer . merge ( merge ) ; } } public void close ( ) { } } 	0	['3', '2', '0', '5', '6', '3', '1', '4', '3', '2', '18', '0', '0', '0.5', '0.666666667', '0', '0', '5', '1', '0.6667', '0']
package org . apache . lucene . search ; public class TopFieldDocs extends TopDocs { public SortField [ ] fields ; TopFieldDocs ( int totalHits , ScoreDoc [ ] scoreDocs , SortField [ ] fields , float maxScore ) { super ( totalHits , scoreDocs , maxScore ) ; this . fields = fields ; } } 	0	['1', '2', '0', '14', '2', '0', '11', '3', '0', '2', '11', '0', '1', '1', '1', '0', '0', '9', '0', '0', '0']
package org . apache . lucene . document ; import java . util . * ; import org . apache . lucene . search . ScoreDoc ; import org . apache . lucene . search . Searcher ; import org . apache . lucene . index . IndexReader ; public final class Document implements java . io . Serializable { List fields = new ArrayList ( ) ; private float boost = 1.0f ; public Document ( ) { } public void setBoost ( float boost ) { this . boost = boost ; } public float getBoost ( ) { return boost ; } public final void add ( Fieldable field ) { fields . add ( field ) ; } public final void removeField ( String name ) { Iterator it = fields . iterator ( ) ; while ( it . hasNext ( ) ) { Fieldable field = ( Fieldable ) it . next ( ) ; if ( field . name ( ) . equals ( name ) ) { it . remove ( ) ; return ; } } } public final void removeFields ( String name ) { Iterator it = fields . iterator ( ) ; while ( it . hasNext ( ) ) { Fieldable field = ( Fieldable ) it . next ( ) ; if ( field . name ( ) . equals ( name ) ) { it . remove ( ) ; } } } public final Field getField ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Field field = ( Field ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) return field ; } return null ; } public Fieldable getFieldable ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) return field ; } return null ; } public final String get ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( ! field . isBinary ( ) ) ) return field . stringValue ( ) ; } return null ; } public final Enumeration fields ( ) { return new Enumeration ( ) { final Iterator iter = fields . iterator ( ) ; public boolean hasMoreElements ( ) { return iter . hasNext ( ) ; } public Object nextElement ( ) { return iter . next ( ) ; } } ; } public final List getFields ( ) { return fields ; } private final static Field [ ] NO_FIELDS = new Field [ 0 ] ; public final Field [ ] getFields ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Field field = ( Field ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) { result . add ( field ) ; } } if ( result . size ( ) == 0 ) return NO_FIELDS ; return ( Field [ ] ) result . toArray ( new Field [ result . size ( ) ] ) ; } private final static Fieldable [ ] NO_FIELDABLES = new Fieldable [ 0 ] ; public Fieldable [ ] getFieldables ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) { result . add ( field ) ; } } if ( result . size ( ) == 0 ) return NO_FIELDABLES ; return ( Fieldable [ ] ) result . toArray ( new Fieldable [ result . size ( ) ] ) ; } private final static String [ ] NO_STRINGS = new String [ 0 ] ; public final String [ ] getValues ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( ! field . isBinary ( ) ) ) result . add ( field . stringValue ( ) ) ; } if ( result . size ( ) == 0 ) return NO_STRINGS ; return ( String [ ] ) result . toArray ( new String [ result . size ( ) ] ) ; } private final static byte [ ] [ ] NO_BYTES = new byte [ 0 ] [ ] ; public final byte [ ] [ ] getBinaryValues ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( field . isBinary ( ) ) ) result . add ( field . binaryValue ( ) ) ; } if ( result . size ( ) == 0 ) return NO_BYTES ; return ( byte [ ] [ ] ) result . toArray ( new byte [ result . size ( ) ] [ ] ) ; } public final byte [ ] getBinaryValue ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( field . isBinary ( ) ) ) return field . binaryValue ( ) ; } return null ; } public final String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "Document<" ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; buffer . append ( field . toString ( ) ) ; if ( i != fields . size ( ) - 1 ) buffer . append ( " " ) ; } buffer . append ( ">" ) ; return buffer . toString ( ) ; } } 	0	['18', '1', '0', '30', '39', '0', '28', '3', '17', '0.81372549', '432', '0.833333333', '2', '0', '0.426470588', '0', '0', '22.66666667', '5', '2.4444', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . * ; final class PhrasePositions { int doc ; int position ; int count ; int offset ; TermPositions tp ; PhrasePositions next ; boolean repeats ; PhrasePositions ( TermPositions t , int o ) { tp = t ; offset = o ; } final boolean next ( ) throws IOException { if ( ! tp . next ( ) ) { tp . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } doc = tp . doc ( ) ; position = 0 ; return true ; } final boolean skipTo ( int target ) throws IOException { if ( ! tp . skipTo ( target ) ) { tp . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } doc = tp . doc ( ) ; position = 0 ; return true ; } final void firstPosition ( ) throws IOException { count = tp . freq ( ) ; nextPosition ( ) ; } final boolean nextPosition ( ) throws IOException { if ( count -- > 0 ) { position = tp . nextPosition ( ) - offset ; return true ; } else return false ; } } 	0	['5', '1', '0', '5', '12', '0', '4', '1', '0', '0.678571429', '95', '0', '2', '0', '0.533333333', '0', '0', '16.6', '1', '0.8', '0']
package org . apache . lucene . document ; import java . io . Serializable ; public interface FieldSelector extends Serializable { FieldSelectorResult accept ( String fieldName ) ; } 	0	['1', '1', '0', '19', '1', '0', '18', '1', '1', '2', '1', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . IndexInput ; import java . util . LinkedList ; import java . util . HashSet ; import java . util . Iterator ; import java . io . IOException ; final class CompoundFileWriter { private static final class FileEntry { String file ; long directoryOffset ; long dataOffset ; } private Directory directory ; private String fileName ; private HashSet ids ; private LinkedList entries ; private boolean merged = false ; private SegmentMerger . CheckAbort checkAbort ; public CompoundFileWriter ( Directory dir , String name ) { this ( dir , name , null ) ; } CompoundFileWriter ( Directory dir , String name , SegmentMerger . CheckAbort checkAbort ) { if ( dir == null ) throw new NullPointerException ( "directory cannot be null" ) ; if ( name == null ) throw new NullPointerException ( "name cannot be null" ) ; this . checkAbort = checkAbort ; directory = dir ; fileName = name ; ids = new HashSet ( ) ; entries = new LinkedList ( ) ; } public Directory getDirectory ( ) { return directory ; } public String getName ( ) { return fileName ; } public void addFile ( String file ) { if ( merged ) throw new IllegalStateException ( "Can't add extensions after merge has been called" ) ; if ( file == null ) throw new NullPointerException ( "file cannot be null" ) ; if ( ! ids . add ( file ) ) throw new IllegalArgumentException ( "File " + file + " already added" ) ; FileEntry entry = new FileEntry ( ) ; entry . file = file ; entries . add ( entry ) ; } public void close ( ) throws IOException { if ( merged ) throw new IllegalStateException ( "Merge already performed" ) ; if ( entries . isEmpty ( ) ) throw new IllegalStateException ( "No entries to merge have been defined" ) ; merged = true ; IndexOutput os = null ; try { os = directory . createOutput ( fileName ) ; os . writeVInt ( entries . size ( ) ) ; Iterator it = entries . iterator ( ) ; long totalSize = 0 ; while ( it . hasNext ( ) ) { FileEntry fe = ( FileEntry ) it . next ( ) ; fe . directoryOffset = os . getFilePointer ( ) ; os . writeLong ( 0 ) ; os . writeString ( fe . file ) ; totalSize += directory . fileLength ( fe . file ) ; } final long finalLength = totalSize + os . getFilePointer ( ) ; os . setLength ( finalLength ) ; byte buffer [ ] = new byte [ 16384 ] ; it = entries . iterator ( ) ; while ( it . hasNext ( ) ) { FileEntry fe = ( FileEntry ) it . next ( ) ; fe . dataOffset = os . getFilePointer ( ) ; copyFile ( fe , os , buffer ) ; } it = entries . iterator ( ) ; while ( it . hasNext ( ) ) { FileEntry fe = ( FileEntry ) it . next ( ) ; os . seek ( fe . directoryOffset ) ; os . writeLong ( fe . dataOffset ) ; } assert finalLength == os . length ( ) ; IndexOutput tmp = os ; os = null ; tmp . close ( ) ; } finally { if ( os != null ) try { os . close ( ) ; } catch ( IOException e ) { } } } private void copyFile ( FileEntry source , IndexOutput os , byte buffer [ ] ) throws IOException { IndexInput is = null ; try { long startPtr = os . getFilePointer ( ) ; is = directory . openInput ( source . file ) ; long length = is . length ( ) ; long remainder = length ; int chunk = buffer . length ; while ( remainder > 0 ) { int len = ( int ) Math . min ( chunk , remainder ) ; is . readBytes ( buffer , 0 , len , false ) ; os . writeBytes ( buffer , len ) ; remainder -= len ; if ( checkAbort != null ) checkAbort . work ( 80 ) ; } if ( remainder != 0 ) throw new IOException ( "Non-zero remainder length after copying: " + remainder + " (id: " + source . file + ", length: " + length + ", buffer size: " + chunk + ")" ) ; long endPtr = os . getFilePointer ( ) ; long diff = endPtr - startPtr ; if ( diff != length ) throw new IOException ( "Difference in the output file offsets " + diff + " does not match the original file length " + length ) ; } finally { if ( is != null ) is . close ( ) ; } } } 	0	['9', '1', '0', '9', '51', '14', '3', '6', '5', '0.703125', '414', '0.75', '2', '0', '0.303571429', '0', '0', '44.11111111', '4', '1', '0']
package org . apache . lucene . index ; import org . apache . lucene . document . Fieldable ; final class DocFieldProcessorPerField { final DocFieldConsumerPerField consumer ; final FieldInfo fieldInfo ; DocFieldProcessorPerField next ; int lastGen = - 1 ; int fieldCount ; Fieldable [ ] fields = new Fieldable [ 1 ] ; public DocFieldProcessorPerField ( final DocFieldProcessorPerThread perThread , final FieldInfo fieldInfo ) { this . consumer = perThread . consumer . addField ( fieldInfo ) ; this . fieldInfo = fieldInfo ; } public void abort ( ) { consumer . abort ( ) ; } } 	0	['2', '1', '0', '5', '5', '0', '1', '5', '2', '1.166666667', '31', '0', '4', '0', '0.666666667', '0', '0', '11.5', '1', '0.5', '0']
package org . apache . lucene . index ; import java . util . HashMap ; import java . util . Collection ; import java . util . Iterator ; import java . util . Map ; import java . util . HashSet ; import java . io . IOException ; import org . apache . lucene . util . ArrayUtil ; final class DocFieldConsumers extends DocFieldConsumer { final DocFieldConsumer one ; final DocFieldConsumer two ; public DocFieldConsumers ( DocFieldConsumer one , DocFieldConsumer two ) { this . one = one ; this . two = two ; } void setFieldInfos ( FieldInfos fieldInfos ) { super . setFieldInfos ( fieldInfos ) ; one . setFieldInfos ( fieldInfos ) ; two . setFieldInfos ( fieldInfos ) ; } public void flush ( Map threadsAndFields , DocumentsWriter . FlushState state ) throws IOException { Map oneThreadsAndFields = new HashMap ( ) ; Map twoThreadsAndFields = new HashMap ( ) ; Iterator it = threadsAndFields . entrySet ( ) . iterator ( ) ; while ( it . hasNext ( ) ) { Map . Entry entry = ( Map . Entry ) it . next ( ) ; DocFieldConsumersPerThread perThread = ( DocFieldConsumersPerThread ) entry . getKey ( ) ; Collection fields = ( Collection ) entry . getValue ( ) ; Iterator fieldsIt = fields . iterator ( ) ; Collection oneFields = new HashSet ( ) ; Collection twoFields = new HashSet ( ) ; while ( fieldsIt . hasNext ( ) ) { DocFieldConsumersPerField perField = ( DocFieldConsumersPerField ) fieldsIt . next ( ) ; oneFields . add ( perField . one ) ; twoFields . add ( perField . two ) ; } oneThreadsAndFields . put ( perThread . one , oneFields ) ; twoThreadsAndFields . put ( perThread . two , twoFields ) ; } one . flush ( oneThreadsAndFields , state ) ; two . flush ( twoThreadsAndFields , state ) ; } public void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException { try { one . closeDocStore ( state ) ; } finally { two . closeDocStore ( state ) ; } } public void abort ( ) { try { one . abort ( ) ; } finally { two . abort ( ) ; } } public boolean freeRAM ( ) { boolean any = one . freeRAM ( ) ; any |= two . freeRAM ( ) ; return any ; } public DocFieldConsumerPerThread addThread ( DocFieldProcessorPerThread docFieldProcessorPerThread ) throws IOException { return new DocFieldConsumersPerThread ( docFieldProcessorPerThread , this , one . addThread ( docFieldProcessorPerThread ) , two . addThread ( docFieldProcessorPerThread ) ) ; } PerDoc [ ] docFreeList = new PerDoc [ 1 ] ; int freeCount ; int allocCount ; synchronized PerDoc getPerDoc ( ) { if ( freeCount == 0 ) { allocCount ++ ; if ( allocCount > docFreeList . length ) { assert allocCount == 1 + docFreeList . length ; docFreeList = new PerDoc [ ArrayUtil . getNextSize ( allocCount ) ] ; } return new PerDoc ( ) ; } else return docFreeList [ -- freeCount ] ; } synchronized void freePerDoc ( PerDoc perDoc ) { assert freeCount < docFreeList . length ; docFreeList [ freeCount ++ ] = perDoc ; } class PerDoc extends DocumentsWriter . DocWriter { DocumentsWriter . DocWriter one ; DocumentsWriter . DocWriter two ; public long sizeInBytes ( ) { return one . sizeInBytes ( ) + two . sizeInBytes ( ) ; } public void finish ( ) throws IOException { try { try { one . finish ( ) ; } finally { two . finish ( ) ; } } finally { freePerDoc ( this ) ; } } public void abort ( ) { try { try { one . abort ( ) ; } finally { two . abort ( ) ; } } finally { freePerDoc ( this ) ; } } } } 	0	['11', '2', '0', '11', '37', '3', '3', '10', '6', '0.757142857', '281', '0', '3', '0.4', '0.2125', '0', '0', '23.90909091', '5', '1.5455', '0']
package org . apache . lucene . index ; abstract class InvertedDocEndConsumerPerThread { abstract void startDocument ( ) ; abstract InvertedDocEndConsumerPerField addField ( DocInverterPerField docInverterPerField , FieldInfo fieldInfo ) ; abstract void finishDocument ( ) ; abstract void abort ( ) ; } 	0	['5', '1', '1', '8', '6', '10', '6', '3', '0', '2', '8', '0', '0', '0', '0.466666667', '0', '0', '0.6', '1', '0.8', '0']
package org . apache . lucene . index ; import java . io . IOException ; public abstract class MergeScheduler { abstract void merge ( IndexWriter writer ) throws CorruptIndexException , IOException ; abstract void close ( ) throws CorruptIndexException , IOException ; } 	0	['3', '1', '2', '5', '4', '3', '4', '2', '1', '2', '6', '0', '0', '0', '0.666666667', '0', '0', '1', '1', '0.6667', '0']
package org . apache . lucene . index ; import java . util . Map ; import java . util . HashMap ; import java . util . HashSet ; import java . util . Collection ; import java . util . Iterator ; import java . io . IOException ; final class DocInverter extends DocFieldConsumer { final InvertedDocConsumer consumer ; final InvertedDocEndConsumer endConsumer ; public DocInverter ( InvertedDocConsumer consumer , InvertedDocEndConsumer endConsumer ) { this . consumer = consumer ; this . endConsumer = endConsumer ; } void setFieldInfos ( FieldInfos fieldInfos ) { super . setFieldInfos ( fieldInfos ) ; consumer . setFieldInfos ( fieldInfos ) ; endConsumer . setFieldInfos ( fieldInfos ) ; } void flush ( Map threadsAndFields , DocumentsWriter . FlushState state ) throws IOException { Map childThreadsAndFields = new HashMap ( ) ; Map endChildThreadsAndFields = new HashMap ( ) ; Iterator it = threadsAndFields . entrySet ( ) . iterator ( ) ; while ( it . hasNext ( ) ) { Map . Entry entry = ( Map . Entry ) it . next ( ) ; DocInverterPerThread perThread = ( DocInverterPerThread ) entry . getKey ( ) ; Collection fields = ( Collection ) entry . getValue ( ) ; Iterator fieldsIt = fields . iterator ( ) ; Collection childFields = new HashSet ( ) ; Collection endChildFields = new HashSet ( ) ; while ( fieldsIt . hasNext ( ) ) { DocInverterPerField perField = ( DocInverterPerField ) fieldsIt . next ( ) ; childFields . add ( perField . consumer ) ; endChildFields . add ( perField . endConsumer ) ; } childThreadsAndFields . put ( perThread . consumer , childFields ) ; endChildThreadsAndFields . put ( perThread . endConsumer , endChildFields ) ; } consumer . flush ( childThreadsAndFields , state ) ; endConsumer . flush ( endChildThreadsAndFields , state ) ; } public void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException { consumer . closeDocStore ( state ) ; endConsumer . closeDocStore ( state ) ; } void abort ( ) { consumer . abort ( ) ; endConsumer . abort ( ) ; } public boolean freeRAM ( ) { return consumer . freeRAM ( ) ; } public DocFieldConsumerPerThread addThread ( DocFieldProcessorPerThread docFieldProcessorPerThread ) { return new DocInverterPerThread ( docFieldProcessorPerThread , this ) ; } final static class FieldInvertState { int position ; int length ; int offset ; float boost ; void reset ( float docBoost ) { position = 0 ; length = 0 ; offset = 0 ; boost = docBoost ; } } } 	0	['7', '2', '0', '14', '30', '0', '2', '13', '4', '0.25', '136', '0', '2', '0.5', '0.285714286', '0', '0', '18.14285714', '1', '0.8571', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . document . Fieldable ; final class DocFieldConsumersPerField extends DocFieldConsumerPerField { final DocFieldConsumerPerField one ; final DocFieldConsumerPerField two ; final DocFieldConsumersPerThread perThread ; public DocFieldConsumersPerField ( DocFieldConsumersPerThread perThread , DocFieldConsumerPerField one , DocFieldConsumerPerField two ) { this . perThread = perThread ; this . one = one ; this . two = two ; } public void processFields ( Fieldable [ ] fields , int count ) throws IOException { one . processFields ( fields , count ) ; two . processFields ( fields , count ) ; } public void abort ( ) { try { one . abort ( ) ; } finally { two . abort ( ) ; } } } 	0	['3', '2', '0', '4', '6', '0', '2', '3', '3', '0.333333333', '44', '0', '3', '0.5', '0.466666667', '0', '0', '12.66666667', '3', '1.3333', '0']
package org . apache . lucene . document ; public class LoadFirstFieldSelector implements FieldSelector { public FieldSelectorResult accept ( String fieldName ) { return FieldSelectorResult . LOAD_AND_BREAK ; } } 	0	['2', '1', '0', '2', '3', '1', '0', '2', '2', '2', '7', '0', '0', '0', '0.75', '0', '0', '2.5', '1', '0.5', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class BufferedIndexInput extends IndexInput { public static final int BUFFER_SIZE = 1024 ; private int bufferSize = BUFFER_SIZE ; protected byte [ ] buffer ; private long bufferStart = 0 ; private int bufferLength = 0 ; private int bufferPosition = 0 ; public byte readByte ( ) throws IOException { if ( bufferPosition >= bufferLength ) refill ( ) ; return buffer [ bufferPosition ++ ] ; } public BufferedIndexInput ( ) { } public BufferedIndexInput ( int bufferSize ) { checkBufferSize ( bufferSize ) ; this . bufferSize = bufferSize ; } public void setBufferSize ( int newSize ) { assert buffer == null || bufferSize == buffer . length : "buffer=" + buffer + " bufferSize=" + bufferSize + " buffer.length=" + ( buffer != null ? buffer . length : 0 ) ; if ( newSize != bufferSize ) { checkBufferSize ( newSize ) ; bufferSize = newSize ; if ( buffer != null ) { byte [ ] newBuffer = new byte [ newSize ] ; final int leftInBuffer = bufferLength - bufferPosition ; final int numToCopy ; if ( leftInBuffer > newSize ) numToCopy = newSize ; else numToCopy = leftInBuffer ; System . arraycopy ( buffer , bufferPosition , newBuffer , 0 , numToCopy ) ; bufferStart += bufferPosition ; bufferPosition = 0 ; bufferLength = numToCopy ; newBuffer ( newBuffer ) ; } } } protected void newBuffer ( byte [ ] newBuffer ) { buffer = newBuffer ; } public int getBufferSize ( ) { return bufferSize ; } private void checkBufferSize ( int bufferSize ) { if ( bufferSize <= 0 ) throw new IllegalArgumentException ( "bufferSize must be greater than 0 (got " + bufferSize + ")" ) ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { readBytes ( b , offset , len , true ) ; } public void readBytes ( byte [ ] b , int offset , int len , boolean useBuffer ) throws IOException { if ( len <= ( bufferLength - bufferPosition ) ) { if ( len > 0 ) System . arraycopy ( buffer , bufferPosition , b , offset , len ) ; bufferPosition += len ; } else { int available = bufferLength - bufferPosition ; if ( available > 0 ) { System . arraycopy ( buffer , bufferPosition , b , offset , available ) ; offset += available ; len -= available ; bufferPosition += available ; } if ( useBuffer && len < bufferSize ) { refill ( ) ; if ( bufferLength < len ) { System . arraycopy ( buffer , 0 , b , offset , bufferLength ) ; throw new IOException ( "read past EOF" ) ; } else { System . arraycopy ( buffer , 0 , b , offset , len ) ; bufferPosition = len ; } } else { long after = bufferStart + bufferPosition + len ; if ( after > length ( ) ) throw new IOException ( "read past EOF" ) ; readInternal ( b , offset , len ) ; bufferStart = after ; bufferPosition = 0 ; bufferLength = 0 ; } } } private void refill ( ) throws IOException { long start = bufferStart + bufferPosition ; long end = start + bufferSize ; if ( end > length ( ) ) end = length ( ) ; int newLength = ( int ) ( end - start ) ; if ( newLength <= 0 ) throw new IOException ( "read past EOF" ) ; if ( buffer == null ) { newBuffer ( new byte [ bufferSize ] ) ; seekInternal ( bufferStart ) ; } readInternal ( buffer , 0 , newLength ) ; bufferLength = newLength ; bufferStart = start ; bufferPosition = 0 ; } protected abstract void readInternal ( byte [ ] b , int offset , int length ) throws IOException ; public long getFilePointer ( ) { return bufferStart + bufferPosition ; } public void seek ( long pos ) throws IOException { if ( pos >= bufferStart && pos < ( bufferStart + bufferLength ) ) bufferPosition = ( int ) ( pos - bufferStart ) ; else { bufferStart = pos ; bufferPosition = 0 ; bufferLength = 0 ; seekInternal ( pos ) ; } } protected abstract void seekInternal ( long pos ) throws IOException ; public Object clone ( ) { BufferedIndexInput clone = ( BufferedIndexInput ) super . clone ( ) ; clone . buffer = null ; clone . bufferLength = 0 ; clone . bufferPosition = 0 ; clone . bufferStart = getFilePointer ( ) ; return clone ; } } 	0	['17', '2', '2', '4', '33', '42', '3', '1', '10', '0.6953125', '478', '0.625', '0', '0.548387097', '0.302083333', '1', '5', '26.64705882', '8', '1.2941', '0']
package org . apache . lucene . index ; import java . io . Reader ; final class ReusableStringReader extends Reader { int upto ; int left ; String s ; void init ( String s ) { this . s = s ; left = s . length ( ) ; this . upto = 0 ; } public int read ( char [ ] c ) { return read ( c , 0 , c . length ) ; } public int read ( char [ ] c , int off , int len ) { if ( left > len ) { s . getChars ( upto , upto + len , c , off ) ; upto += len ; left -= len ; return len ; } else if ( 0 == left ) { return - 1 ; } else { s . getChars ( upto , upto + left , c , off ) ; int r = left ; left = 0 ; upto = s . length ( ) ; return r ; } } public void close ( ) { } ; } 	0	['5', '2', '0', '2', '8', '8', '2', '0', '3', '0.5', '90', '0', '0', '0.714285714', '0.45', '1', '2', '16.4', '3', '1.2', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . document . Fieldable ; abstract class DocFieldConsumerPerField { abstract void processFields ( Fieldable [ ] fields , int count ) throws IOException ; abstract void abort ( ) ; } 	0	['3', '1', '3', '11', '4', '3', '10', '1', '0', '2', '6', '0', '0', '0', '0.555555556', '0', '0', '1', '1', '0.6667', '0']
package org . apache . lucene . index ; class SegmentTermPositionVector extends SegmentTermVector implements TermPositionVector { protected int [ ] [ ] positions ; protected TermVectorOffsetInfo [ ] [ ] offsets ; public static final int [ ] EMPTY_TERM_POS = new int [ 0 ] ; public SegmentTermPositionVector ( String field , String terms [ ] , int termFreqs [ ] , int [ ] [ ] positions , TermVectorOffsetInfo [ ] [ ] offsets ) { super ( field , terms , termFreqs ) ; this . offsets = offsets ; this . positions = positions ; } public TermVectorOffsetInfo [ ] getOffsets ( int index ) { TermVectorOffsetInfo [ ] result = TermVectorOffsetInfo . EMPTY_OFFSET_INFO ; if ( offsets == null ) return null ; if ( index >= 0 && index < offsets . length ) { result = offsets [ index ] ; } return result ; } public int [ ] getTermPositions ( int index ) { int [ ] result = EMPTY_TERM_POS ; if ( positions == null ) return null ; if ( index >= 0 && index < positions . length ) { result = positions [ index ] ; } return result ; } } 	0	['4', '2', '0', '4', '5', '0', '1', '3', '3', '0.666666667', '65', '0.666666667', '1', '0.777777778', '0.476190476', '0', '0', '14.5', '4', '2', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . IndexOutput ; import java . io . IOException ; final class ByteSliceReader extends IndexInput { ByteBlockPool pool ; int bufferUpto ; byte [ ] buffer ; public int upto ; int limit ; int level ; public int bufferOffset ; public int endIndex ; public void init ( ByteBlockPool pool , int startIndex , int endIndex ) { assert endIndex - startIndex >= 0 ; assert startIndex >= 0 ; assert endIndex >= 0 ; this . pool = pool ; this . endIndex = endIndex ; level = 0 ; bufferUpto = startIndex / DocumentsWriter . BYTE_BLOCK_SIZE ; bufferOffset = bufferUpto * DocumentsWriter . BYTE_BLOCK_SIZE ; buffer = pool . buffers [ bufferUpto ] ; upto = startIndex & DocumentsWriter . BYTE_BLOCK_MASK ; final int firstSize = ByteBlockPool . levelSizeArray [ 0 ] ; if ( startIndex + firstSize >= endIndex ) { limit = endIndex & DocumentsWriter . BYTE_BLOCK_MASK ; } else limit = upto + firstSize - 4 ; } public boolean eof ( ) { assert upto + bufferOffset <= endIndex ; return upto + bufferOffset == endIndex ; } public byte readByte ( ) { assert ! eof ( ) ; assert upto <= limit ; if ( upto == limit ) nextSlice ( ) ; return buffer [ upto ++ ] ; } public long writeTo ( IndexOutput out ) throws IOException { long size = 0 ; while ( true ) { if ( limit + bufferOffset == endIndex ) { assert endIndex - bufferOffset >= upto ; out . writeBytes ( buffer , upto , limit - upto ) ; size += limit - upto ; break ; } else { out . writeBytes ( buffer , upto , limit - upto ) ; size += limit - upto ; nextSlice ( ) ; } } return size ; } public void nextSlice ( ) { final int nextIndex = ( ( buffer [ limit ] & 0xff ) << 24 ) + ( ( buffer [ 1 + limit ] & 0xff ) << 16 ) + ( ( buffer [ 2 + limit ] & 0xff ) << 8 ) + ( buffer [ 3 + limit ] & 0xff ) ; level = ByteBlockPool . nextLevelArray [ level ] ; final int newSize = ByteBlockPool . levelSizeArray [ level ] ; bufferUpto = nextIndex / DocumentsWriter . BYTE_BLOCK_SIZE ; bufferOffset = bufferUpto * DocumentsWriter . BYTE_BLOCK_SIZE ; buffer = pool . buffers [ bufferUpto ] ; upto = nextIndex & DocumentsWriter . BYTE_BLOCK_MASK ; if ( nextIndex + newSize >= endIndex ) { assert endIndex - nextIndex > 0 ; limit = endIndex - bufferOffset ; } else { limit = upto + newSize - 4 ; } } public void readBytes ( byte [ ] b , int offset , int len ) { while ( len > 0 ) { final int numLeft = limit - upto ; if ( numLeft < len ) { System . arraycopy ( buffer , upto , b , offset , numLeft ) ; offset += numLeft ; len -= numLeft ; nextSlice ( ) ; } else { System . arraycopy ( buffer , upto , b , offset , len ) ; upto += len ; break ; } } } public long getFilePointer ( ) { throw new RuntimeException ( "not implemented" ) ; } public long length ( ) { throw new RuntimeException ( "not implemented" ) ; } public void seek ( long pos ) { throw new RuntimeException ( "not implemented" ) ; } public void close ( ) { throw new RuntimeException ( "not implemented" ) ; } } 	0	['13', '2', '0', '8', '22', '38', '5', '3', '10', '0.658333333', '447', '0', '1', '0.607142857', '0.214285714', '1', '4', '32.61538462', '8', '2.3846', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . util . UnicodeUtil ; final class TermBuffer implements Cloneable { private String field ; private Term term ; private boolean preUTF8Strings ; private boolean dirty ; private UnicodeUtil . UTF16Result text = new UnicodeUtil . UTF16Result ( ) ; private UnicodeUtil . UTF8Result bytes = new UnicodeUtil . UTF8Result ( ) ; public final int compareTo ( TermBuffer other ) { if ( field == other . field ) return compareChars ( text . result , text . length , other . text . result , other . text . length ) ; else return field . compareTo ( other . field ) ; } private static final int compareChars ( char [ ] chars1 , int len1 , char [ ] chars2 , int len2 ) { final int end = len1 < len2 ? len1 : len2 ; for ( int k = 0 ; k < end ; k ++ ) { char c1 = chars1 [ k ] ; char c2 = chars2 [ k ] ; if ( c1 != c2 ) { return c1 - c2 ; } } return len1 - len2 ; } void setPreUTF8Strings ( ) { preUTF8Strings = true ; } public final void read ( IndexInput input , FieldInfos fieldInfos ) throws IOException { this . term = null ; int start = input . readVInt ( ) ; int length = input . readVInt ( ) ; int totalLength = start + length ; if ( preUTF8Strings ) { text . setLength ( totalLength ) ; input . readChars ( text . result , start , length ) ; } else { if ( dirty ) { UnicodeUtil . UTF16toUTF8 ( text . result , 0 , text . length , bytes ) ; bytes . setLength ( totalLength ) ; input . readBytes ( bytes . result , start , length ) ; UnicodeUtil . UTF8toUTF16 ( bytes . result , 0 , totalLength , text ) ; dirty = false ; } else { bytes . setLength ( totalLength ) ; input . readBytes ( bytes . result , start , length ) ; UnicodeUtil . UTF8toUTF16 ( bytes . result , start , length , text ) ; } } this . field = fieldInfos . fieldName ( input . readVInt ( ) ) ; } public final void set ( Term term ) { if ( term == null ) { reset ( ) ; return ; } final String termText = term . text ( ) ; final int termLen = termText . length ( ) ; text . setLength ( termLen ) ; termText . getChars ( 0 , termLen , text . result , 0 ) ; dirty = true ; field = term . field ( ) ; this . term = term ; } public final void set ( TermBuffer other ) { text . copyText ( other . text ) ; dirty = true ; field = other . field ; term = other . term ; } public void reset ( ) { field = null ; text . setLength ( 0 ) ; term = null ; dirty = true ; } public Term toTerm ( ) { if ( field == null ) return null ; if ( term == null ) term = new Term ( field , new String ( text . result , 0 , text . length ) , false ) ; return term ; } protected Object clone ( ) { TermBuffer clone = null ; try { clone = ( TermBuffer ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . dirty = true ; clone . bytes = new UnicodeUtil . UTF8Result ( ) ; clone . text = new UnicodeUtil . UTF16Result ( ) ; clone . text . copyText ( text ) ; return clone ; } } 	0	['10', '1', '0', '7', '30', '0', '1', '6', '6', '0.574074074', '303', '1', '3', '0', '0.228571429', '0', '0', '28.7', '4', '1.6', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Map ; abstract class DocFieldConsumer { FieldInfos fieldInfos ; abstract void flush ( Map threadsAndFields , DocumentsWriter . FlushState state ) throws IOException ; abstract void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException ; abstract void abort ( ) ; abstract DocFieldConsumerPerThread addThread ( DocFieldProcessorPerThread docFieldProcessorPerThread ) throws IOException ; abstract boolean freeRAM ( ) ; void setFieldInfos ( FieldInfos fieldInfos ) { this . fieldInfos = fieldInfos ; } } 	0	['7', '1', '3', '9', '8', '21', '6', '4', '0', '1', '15', '0', '1', '0', '0.342857143', '0', '0', '1', '1', '0.8571', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class BufferedIndexOutput extends IndexOutput { static final int BUFFER_SIZE = 16384 ; private final byte [ ] buffer = new byte [ BUFFER_SIZE ] ; private long bufferStart = 0 ; private int bufferPosition = 0 ; public void writeByte ( byte b ) throws IOException { if ( bufferPosition >= BUFFER_SIZE ) flush ( ) ; buffer [ bufferPosition ++ ] = b ; } public void writeBytes ( byte [ ] b , int offset , int length ) throws IOException { int bytesLeft = BUFFER_SIZE - bufferPosition ; if ( bytesLeft >= length ) { System . arraycopy ( b , offset , buffer , bufferPosition , length ) ; bufferPosition += length ; if ( BUFFER_SIZE - bufferPosition == 0 ) flush ( ) ; } else { if ( length > BUFFER_SIZE ) { if ( bufferPosition > 0 ) flush ( ) ; flushBuffer ( b , offset , length ) ; bufferStart += length ; } else { int pos = 0 ; int pieceLength ; while ( pos < length ) { pieceLength = ( length - pos < bytesLeft ) ? length - pos : bytesLeft ; System . arraycopy ( b , pos + offset , buffer , bufferPosition , pieceLength ) ; pos += pieceLength ; bufferPosition += pieceLength ; bytesLeft = BUFFER_SIZE - bufferPosition ; if ( bytesLeft == 0 ) { flush ( ) ; bytesLeft = BUFFER_SIZE ; } } } } } public void flush ( ) throws IOException { flushBuffer ( buffer , bufferPosition ) ; bufferStart += bufferPosition ; bufferPosition = 0 ; } private void flushBuffer ( byte [ ] b , int len ) throws IOException { flushBuffer ( b , 0 , len ) ; } protected abstract void flushBuffer ( byte [ ] b , int offset , int len ) throws IOException ; public void close ( ) throws IOException { flush ( ) ; } public long getFilePointer ( ) { return bufferStart + bufferPosition ; } public void seek ( long pos ) throws IOException { flush ( ) ; bufferStart = pos ; } public abstract long length ( ) throws IOException ; } 	0	['10', '2', '1', '2', '12', '17', '1', '1', '8', '0.555555556', '185', '0.75', '0', '0.653846154', '0.36', '1', '5', '17.1', '1', '0.9', '0']
package org . apache . lucene . index ; final class IndexFileNames { static final String SEGMENTS = "segments" ; static final String SEGMENTS_GEN = "segments.gen" ; static final String DELETABLE = "deletable" ; static final String NORMS_EXTENSION = "nrm" ; static final String FREQ_EXTENSION = "frq" ; static final String PROX_EXTENSION = "prx" ; static final String TERMS_EXTENSION = "tis" ; static final String TERMS_INDEX_EXTENSION = "tii" ; static final String FIELDS_INDEX_EXTENSION = "fdx" ; static final String FIELDS_EXTENSION = "fdt" ; static final String VECTORS_FIELDS_EXTENSION = "tvf" ; static final String VECTORS_DOCUMENTS_EXTENSION = "tvd" ; static final String VECTORS_INDEX_EXTENSION = "tvx" ; static final String COMPOUND_FILE_EXTENSION = "cfs" ; static final String COMPOUND_FILE_STORE_EXTENSION = "cfx" ; static final String DELETES_EXTENSION = "del" ; static final String FIELD_INFOS_EXTENSION = "fnm" ; static final String PLAIN_NORMS_EXTENSION = "f" ; static final String SEPARATE_NORMS_EXTENSION = "s" ; static final String GEN_EXTENSION = "gen" ; static final String INDEX_EXTENSIONS [ ] = new String [ ] { COMPOUND_FILE_EXTENSION , FIELD_INFOS_EXTENSION , FIELDS_INDEX_EXTENSION , FIELDS_EXTENSION , TERMS_INDEX_EXTENSION , TERMS_EXTENSION , FREQ_EXTENSION , PROX_EXTENSION , DELETES_EXTENSION , VECTORS_INDEX_EXTENSION , VECTORS_DOCUMENTS_EXTENSION , VECTORS_FIELDS_EXTENSION , GEN_EXTENSION , NORMS_EXTENSION , COMPOUND_FILE_STORE_EXTENSION , } ; static final String [ ] INDEX_EXTENSIONS_IN_COMPOUND_FILE = new String [ ] { FIELD_INFOS_EXTENSION , FIELDS_INDEX_EXTENSION , FIELDS_EXTENSION , TERMS_INDEX_EXTENSION , TERMS_EXTENSION , FREQ_EXTENSION , PROX_EXTENSION , VECTORS_INDEX_EXTENSION , VECTORS_DOCUMENTS_EXTENSION , VECTORS_FIELDS_EXTENSION , NORMS_EXTENSION } ; static final String [ ] STORE_INDEX_EXTENSIONS = new String [ ] { VECTORS_INDEX_EXTENSION , VECTORS_FIELDS_EXTENSION , VECTORS_DOCUMENTS_EXTENSION , FIELDS_INDEX_EXTENSION , FIELDS_EXTENSION } ; static final String [ ] NON_STORE_INDEX_EXTENSIONS = new String [ ] { FIELD_INFOS_EXTENSION , FREQ_EXTENSION , PROX_EXTENSION , TERMS_EXTENSION , TERMS_INDEX_EXTENSION , NORMS_EXTENSION } ; static final String COMPOUND_EXTENSIONS [ ] = new String [ ] { FIELD_INFOS_EXTENSION , FREQ_EXTENSION , PROX_EXTENSION , FIELDS_INDEX_EXTENSION , FIELDS_EXTENSION , TERMS_INDEX_EXTENSION , TERMS_EXTENSION } ; static final String VECTOR_EXTENSIONS [ ] = new String [ ] { VECTORS_INDEX_EXTENSION , VECTORS_DOCUMENTS_EXTENSION , VECTORS_FIELDS_EXTENSION } ; static final String fileNameFromGeneration ( String base , String extension , long gen ) { if ( gen == SegmentInfo . NO ) { return null ; } else if ( gen == SegmentInfo . WITHOUT_GEN ) { return base + extension ; } else { return base + "_" + Long . toString ( gen , Character . MAX_RADIX ) + extension ; } } static final boolean isDocStoreFile ( String fileName ) { if ( fileName . endsWith ( COMPOUND_FILE_STORE_EXTENSION ) ) return true ; for ( int i = 0 ; i < STORE_INDEX_EXTENSIONS . length ; i ++ ) if ( fileName . endsWith ( STORE_INDEX_EXTENSIONS [ i ] ) ) return true ; return false ; } } 	0	['4', '1', '0', '5', '10', '4', '5', '0', '0', '1.243589744', '298', '0', '0', '0', '0.444444444', '0', '0', '67', '4', '1.75', '0']
package org . apache . lucene . index ; import java . util . List ; import java . io . IOException ; public interface IndexDeletionPolicy { public void onInit ( List commits ) throws IOException ; public void onCommit ( List commits ) throws IOException ; } 	0	['2', '1', '0', '8', '2', '1', '8', '0', '2', '2', '2', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; public class LetterTokenizer extends CharTokenizer { public LetterTokenizer ( Reader in ) { super ( in ) ; } protected boolean isTokenChar ( char c ) { return Character . isLetter ( c ) ; } } 	0	['2', '4', '1', '2', '4', '1', '1', '1', '1', '2', '9', '0', '0', '0.923076923', '0.666666667', '1', '1', '3.5', '1', '0.5', '0']
package org . apache . lucene . index ; import org . apache . lucene . util . UnicodeUtil ; final class TermVectorsTermsWriterPerThread extends TermsHashConsumerPerThread { final TermVectorsTermsWriter termsWriter ; final TermsHashPerThread termsHashPerThread ; final DocumentsWriter . DocState docState ; TermVectorsTermsWriter . PerDoc doc ; public TermVectorsTermsWriterPerThread ( TermsHashPerThread termsHashPerThread , TermVectorsTermsWriter termsWriter ) { this . termsWriter = termsWriter ; this . termsHashPerThread = termsHashPerThread ; docState = termsHashPerThread . docState ; } final ByteSliceReader vectorSliceReader = new ByteSliceReader ( ) ; final UnicodeUtil . UTF8Result utf8Results [ ] = { new UnicodeUtil . UTF8Result ( ) , new UnicodeUtil . UTF8Result ( ) } ; public void startDocument ( ) { assert clearLastVectorFieldName ( ) ; if ( doc != null ) { doc . reset ( ) ; doc . docID = docState . docID ; } } public DocumentsWriter . DocWriter finishDocument ( ) { try { return doc ; } finally { doc = null ; } } public TermsHashConsumerPerField addField ( TermsHashPerField termsHashPerField , FieldInfo fieldInfo ) { return new TermVectorsTermsWriterPerField ( termsHashPerField , this , fieldInfo ) ; } public void abort ( ) { if ( doc != null ) { doc . abort ( ) ; doc = null ; } } final boolean clearLastVectorFieldName ( ) { lastVectorFieldName = null ; return true ; } String lastVectorFieldName ; final boolean vectorFieldsInOrder ( FieldInfo fi ) { try { if ( lastVectorFieldName != null ) return lastVectorFieldName . compareTo ( fi . name ) < 0 ; else return true ; } finally { lastVectorFieldName = fi . name ; } } } 	0	['9', '2', '0', '12', '21', '24', '2', '12', '5', '0.916666667', '167', '0', '6', '0.363636364', '0.270833333', '0', '0', '16.55555556', '6', '2', '0']
package org . apache . lucene . index ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . analysis . Token ; import java . io . IOException ; abstract class InvertedDocConsumerPerField { abstract boolean start ( Fieldable [ ] fields , int count ) throws IOException ; abstract void add ( Token token ) throws IOException ; abstract void finish ( ) throws IOException ; abstract void abort ( ) ; } 	0	['5', '1', '1', '7', '6', '10', '5', '2', '0', '2', '8', '0', '0', '0', '0.4', '0', '0', '0.6', '1', '0.8', '0']
package org . apache . lucene . analysis . standard ; import org . apache . lucene . analysis . Token ; class StandardTokenizerImpl { public static final int YYEOF = - 1 ; private static final int ZZ_BUFFERSIZE = 16384 ; public static final int YYINITIAL = 0 ; private static final String ZZ_CMAP_PACKED = "\11\0\1\0\1\15\1\0\1\0\1\14\22\0\1\0\5\0\1\5" + "\1\3\4\0\1\11\1\7\1\4\1\11\12\2\6\0\1\6\32\12" + "\4\0\1\10\1\0\32\12\57\0\1\12\12\0\1\12\4\0\1\12" + "\5\0\27\12\1\0\37\12\1\0\12\2\0\22\12\34\0\136\12" + "\2\0\11\12\2\0\7\12\16\0\2\12\16\0\5\12\11\0\1\12" + "\213\0\1\12\13\0\1\12\1\0\3\12\1\0\1\12\1\0\24\12" + "\1\0\54\12\1\0\10\12\2\0\32\12\14\0\202\12\12\0\71\12" + "\2\0\2\12\2\0\2\12\3\0\46\12\2\0\2\12\67\0\46\12" + "\2\0\1\12\7\0\47\12\110\0\33\12\5\0\3\12\56\0\32\12" + "\5\0\13\12\25\0\12\2\7\0\143\12\1\0\1\12\17\0\2\12" + "\11\0\12\2\3\12\23\0\1\12\1\0\33\12\123\0\46\12\0" + "\65\12\3\0\1\12\22\0\1\12\7\0\12\12\4\0\12\2\25\0" + "\10\12\2\0\2\12\2\0\26\12\1\0\7\12\1\0\1\12\3\0" + "\4\12\42\0\2\12\1\0\3\12\4\0\12\2\2\12\23\0\6\12" + "\4\0\2\12\2\0\26\12\1\0\7\12\1\0\2\12\1\0\2\12" + "\1\0\2\12\37\0\4\12\1\0\1\12\7\0\12\2\2\0\3\12" + "\20\0\7\12\1\0\1\12\1\0\3\12\1\0\26\12\1\0\7\12" + "\1\0\2\12\1\0\5\12\3\0\1\12\22\0\1\12\17\0\1\12" + "\5\0\12\2\25\0\10\12\2\0\2\12\2\0\26\12\1\0\7\12" + "\1\0\2\12\2\0\4\12\3\0\1\12\36\0\2\12\1\0\3\12" + "\4\0\12\2\25\0\6\12\3\0\3\12\1\0\4\12\3\0\2\12" + "\1\0\1\12\1\0\2\12\3\0\2\12\3\0\3\12\3\0\10\12" + "\1\0\3\12\55\0\11\2\25\0\10\12\1\0\3\12\1\0\27\12" + "\1\0\12\12\1\0\5\12\46\0\2\12\4\0\12\2\25\0\10\12" + "\1\0\3\12\1\0\27\12\1\0\12\12\1\0\5\12\44\0\1\12" + "\1\0\2\12\4\0\12\2\25\0\10\12\1\0\3\12\1\0\27\12" + "\1\0\20\12\46\0\2\12\4\0\12\2\25\0\22\12\3\0\30\12" + "\1\0\11\12\1\0\1\12\2\0\7\12\71\0\1\1\60\12\1\1" + "\2\12\14\1\7\12\11\1\12\2\47\0\2\12\1\0\1\12\2\0" + "\2\12\1\0\1\12\2\0\1\12\6\0\4\12\1\0\7\12\1\0" + "\3\12\1\0\1\12\1\0\1\12\2\0\2\12\1\0\4\12\1\0" + "\2\12\11\0\1\12\2\0\5\12\1\0\1\12\11\0\12\2\2\0" + "\2\12\42\0\1\12\37\0\12\2\26\0\10\12\1\0\42\12\35\0" + "\4\12\164\0\42\12\1\0\5\12\1\0\2\12\25\0\12\2\6\0" + "\6\12\112\0\46\12\12\0\47\12\11\0\132\12\5\0\104\12\5\0" + "\122\12\6\0\7\12\1\0\77\12\1\0\1\12\1\0\4\12\2\0" + "\7\12\1\0\1\12\1\0\4\12\2\0\47\12\1\0\1\12\1\0" + "\4\12\2\0\37\12\1\0\1\12\1\0\4\12\2\0\7\12\1\0" + "\1\12\1\0\4\12\2\0\7\12\1\0\7\12\1\0\27\12\1\0" + "\37\12\1\0\1\12\1\0\4\12\2\0\7\12\1\0\47\12\1\0" + "\23\12\16\0\11\2\56\0\125\12\14\0\12\2\0\10\12\12\0" + "\32\12\5\0\113\12\225\0\64\12\54\0\12\2\46\0\12\2\6\0" + "\130\12\10\0\51\12\0\234\12\4\0\132\12\6\0\26\12\2\0" + "\6\12\2\0\46\12\2\0\6\12\2\0\10\12\1\0\1\12\1\0" + "\1\12\1\0\1\12\1\0\37\12\2\0\65\12\1\0\7\12\1\0" + "\1\12\3\0\3\12\1\0\7\12\3\0\4\12\2\0\6\12\4\0" + "\15\12\5\0\3\12\1\0\7\12\202\0\1\12\202\0\1\12\4\0" + "\1\12\2\0\12\12\1\0\1\12\3\0\5\12\6\0\1\12\1\0" + "\1\12\1\0\1\12\1\0\4\12\1\0\3\12\1\0\7\12\0" + "\2\12\52\0\5\12\12\0\1\13\124\13\10\13\2\13\2\13\132\13" + "\1\13\3\13\6\13\50\13\3\13\1\0\136\12\21\0\30\12\70\0" + "\20\13\0\200\13\200\0\13\12\13\100\0\13\132\13\12" + "\0\12\0\13\322\13\7\12\14\0\5\12\5\0\1\12" + "\1\0\12\12\1\0\15\12\1\0\5\12\1\0\1\12\1\0\2\12" + "\1\0\2\12\1\0\154\12\41\0\12\22\0\100\12\2\0\66\12" + "\50\0\14\12\164\0\3\12\1\0\1\12\1\0\207\12\23\0\12\2" + "\7\0\32\12\6\0\32\12\12\0\1\13\72\13\37\12\3\0\6\12" + "\2\0\6\12\2\0\6\12\2\0\3\12\43\0" ; private static final char [ ] ZZ_CMAP = zzUnpackCMap ( ZZ_CMAP_PACKED ) ; private static final int [ ] ZZ_ACTION = zzUnpackAction ( ) ; private static final String ZZ_ACTION_PACKED_0 = "\1\0\1\1\3\2\1\3\1\1\13\0\1\2\3\4" + "\2\0\1\5\1\0\1\5\3\4\6\5\1\6\1\4" + "\2\7\1\10\1\0\1\10\3\0\2\10\1\11\1\12" + "\1\4" ; private static int [ ] zzUnpackAction ( ) { int [ ] result = new int [ 51 ] ; int offset = 0 ; offset = zzUnpackAction ( ZZ_ACTION_PACKED_0 , offset , result ) ; return result ; } private static int zzUnpackAction ( String packed , int offset , int [ ] result ) { int i = 0 ; int j = offset ; int l = packed . length ( ) ; while ( i < l ) { int count = packed . charAt ( i ++ ) ; int value = packed . charAt ( i ++ ) ; do result [ j ++ ] = value ; while ( -- count > 0 ) ; } return j ; } private static final int [ ] ZZ_ROWMAP = zzUnpackRowMap ( ) ; private static final String ZZ_ROWMAP_PACKED_0 = "\0\0\0\16\0\34\0\52\0\70\0\16\0\106\0\124" + "\0\142\0\160\0\176\0\214\0\232\0\250\0\266\0\304" + "\0\322\0\340\0\356\0\374\0\0\0\0" + "\0\0\0\0\0\0\0\0" + "\0\0\0\0\0\0\0\322\0" + "\0\0\0\0\0\0\0\124\0\214" + "\0\0\0" ; private static int [ ] zzUnpackRowMap ( ) { int [ ] result = new int [ 51 ] ; int offset = 0 ; offset = zzUnpackRowMap ( ZZ_ROWMAP_PACKED_0 , offset , result ) ; return result ; } private static int zzUnpackRowMap ( String packed , int offset , int [ ] result ) { int i = 0 ; int j = offset ; int l = packed . length ( ) ; while ( i < l ) { int high = packed . charAt ( i ++ ) << 16 ; result [ j ++ ] = high | packed . charAt ( i ++ ) ; } return j ; } private static final int [ ] ZZ_TRANS = zzUnpackTrans ( ) ; private static final String ZZ_TRANS_PACKED_0 = "\1\2\1\3\1\4\7\2\1\5\1\6\1\7\1\2" + "\17\0\2\3\1\0\1\10\1\0\1\11\2\12\1\13" + "\1\3\4\0\1\3\1\4\1\0\1\14\1\0\1\11" + "\2\15\1\16\1\4\4\0\1\3\1\4\1\17\1\20" + "\1\21\1\22\2\12\1\13\1\23\20\0\1\2\1\0" + "\1\24\1\25\7\0\1\26\4\0\2\27\7\0\1\27" + "\4\0\1\30\1\31\7\0\1\32\5\0\1\33\7\0" + "\1\13\4\0\1\34\1\35\7\0\1\36\4\0\1\37" + "\1\40\7\0\1\41\4\0\1\42\1\43\7\0\1\44" + "\15\0\1\45\4\0\1\24\1\25\7\0\1\46\15\0" + "\1\47\4\0\2\27\7\0\1\50\4\0\1\3\1\4" + "\1\17\1\10\1\21\1\22\2\12\1\13\1\23\4\0" + "\2\24\1\0\1\51\1\0\1\11\2\52\1\0\1\24" + "\4\0\1\24\1\25\1\0\1\53\1\0\1\11\2\54" + "\1\55\1\25\4\0\1\24\1\25\1\0\1\51\1\0" + "\1\11\2\52\1\0\1\26\4\0\2\27\1\0\1\56" + "\2\0\1\56\2\0\1\27\4\0\2\30\1\0\1\52" + "\1\0\1\11\2\52\1\0\1\30\4\0\1\30\1\31" + "\1\0\1\54\1\0\1\11\2\54\1\55\1\31\4\0" + "\1\30\1\31\1\0\1\52\1\0\1\11\2\52\1\0" + "\1\32\5\0\1\33\1\0\1\55\2\0\3\55\1\33" + "\4\0\2\34\1\0\1\57\1\0\1\11\2\12\1\13" + "\1\34\4\0\1\34\1\35\1\0\1\60\1\0\1\11" + "\2\15\1\16\1\35\4\0\1\34\1\35\1\0\1\57" + "\1\0\1\11\2\12\1\13\1\36\4\0\2\37\1\0" + "\1\12\1\0\1\11\2\12\1\13\1\37\4\0\1\37" + "\1\40\1\0\1\15\1\0\1\11\2\15\1\16\1\40" + "\4\0\1\37\1\40\1\0\1\12\1\0\1\11\2\12" + "\1\13\1\41\4\0\2\42\1\0\1\13\2\0\3\13" + "\1\42\4\0\1\42\1\43\1\0\1\16\2\0\3\16" + "\1\43\4\0\1\42\1\43\1\0\1\13\2\0\3\13" + "\1\44\6\0\1\17\6\0\1\45\4\0\1\24\1\25" + "\1\0\1\61\1\0\1\11\2\52\1\0\1\26\4\0" + "\2\27\1\0\1\56\2\0\1\56\2\0\1\50\4\0" + "\2\24\7\0\1\24\4\0\2\30\7\0\1\30\4\0" + "\2\34\7\0\1\34\4\0\2\37\7\0\1\37\4\0" + "\2\42\7\0\1\42\4\0\2\62\7\0\1\62\4\0" + "\2\24\7\0\1\63\4\0\2\62\1\0\1\56\2\0" + "\1\56\2\0\1\62\4\0\2\24\1\0\1\61\1\0" + "\1\11\2\52\1\0\1\24\3\0" ; private static int [ ] zzUnpackTrans ( ) { int [ ] result = new int [ 658 ] ; int offset = 0 ; offset = zzUnpackTrans ( ZZ_TRANS_PACKED_0 , offset , result ) ; return result ; } private static int zzUnpackTrans ( String packed , int offset , int [ ] result ) { int i = 0 ; int j = offset ; int l = packed . length ( ) ; while ( i < l ) { int count = packed . charAt ( i ++ ) ; int value = packed . charAt ( i ++ ) ; value -- ; do result [ j ++ ] = value ; while ( -- count > 0 ) ; } return j ; } private static final int ZZ_UNKNOWN_ERROR = 0 ; private static final int ZZ_NO_MATCH = 1 ; private static final int ZZ_PUSHBACK_2BIG = 2 ; private static final String ZZ_ERROR_MSG [ ] = { "Unkown internal scanner error" , "Error: could not match input" , "Error: pushback value was too large" } ; private static final int [ ] ZZ_ATTRIBUTE = zzUnpackAttribute ( ) ; private static final String ZZ_ATTRIBUTE_PACKED_0 = "\1\0\1\11\3\1\1\11\1\1\13\0\4\1\2\0" + "\1\1\1\0\17\1\1\0\1\1\3\0\5\1" ; private static int [ ] zzUnpackAttribute ( ) { int [ ] result = new int [ 51 ] ; int offset = 0 ; offset = zzUnpackAttribute ( ZZ_ATTRIBUTE_PACKED_0 , offset , result ) ; return result ; } private static int zzUnpackAttribute ( String packed , int offset , int [ ] result ) { int i = 0 ; int j = offset ; int l = packed . length ( ) ; while ( i < l ) { int count = packed . charAt ( i ++ ) ; int value = packed . charAt ( i ++ ) ; do result [ j ++ ] = value ; while ( -- count > 0 ) ; } return j ; } private java . io . Reader zzReader ; private int zzState ; private int zzLexicalState = YYINITIAL ; private char zzBuffer [ ] = new char [ ZZ_BUFFERSIZE ] ; private int zzMarkedPos ; private int zzPushbackPos ; private int zzCurrentPos ; private int zzStartRead ; private int zzEndRead ; private int yyline ; private int yychar ; private int yycolumn ; private boolean zzAtBOL = true ; private boolean zzAtEOF ; public static final int ALPHANUM = StandardTokenizer . ALPHANUM ; public static final int APOSTROPHE = StandardTokenizer . APOSTROPHE ; public static final int ACRONYM = StandardTokenizer . ACRONYM ; public static final int COMPANY = StandardTokenizer . COMPANY ; public static final int EMAIL = StandardTokenizer . EMAIL ; public static final int HOST = StandardTokenizer . HOST ; public static final int NUM = StandardTokenizer . NUM ; public static final int CJ = StandardTokenizer . CJ ; public static final int ACRONYM_DEP = StandardTokenizer . ACRONYM_DEP ; public static final String [ ] TOKEN_TYPES = StandardTokenizer . TOKEN_TYPES ; public final int yychar ( ) { return yychar ; } final void getText ( Token t ) { t . setTermBuffer ( zzBuffer , zzStartRead , zzMarkedPos - zzStartRead ) ; } StandardTokenizerImpl ( java . io . Reader in ) { this . zzReader = in ; } StandardTokenizerImpl ( java . io . InputStream in ) { this ( new java . io . InputStreamReader ( in ) ) ; } private static char [ ] zzUnpackCMap ( String packed ) { char [ ] map = new char [ 0x10000 ] ; int i = 0 ; int j = 0 ; while ( i < 1154 ) { int count = packed . charAt ( i ++ ) ; char value = packed . charAt ( i ++ ) ; do map [ j ++ ] = value ; while ( -- count > 0 ) ; } return map ; } private boolean zzRefill ( ) throws java . io . IOException { if ( zzStartRead > 0 ) { System . arraycopy ( zzBuffer , zzStartRead , zzBuffer , 0 , zzEndRead - zzStartRead ) ; zzEndRead -= zzStartRead ; zzCurrentPos -= zzStartRead ; zzMarkedPos -= zzStartRead ; zzPushbackPos -= zzStartRead ; zzStartRead = 0 ; } if ( zzCurrentPos >= zzBuffer . length ) { char newBuffer [ ] = new char [ zzCurrentPos * 2 ] ; System . arraycopy ( zzBuffer , 0 , newBuffer , 0 , zzBuffer . length ) ; zzBuffer = newBuffer ; } int numRead = zzReader . read ( zzBuffer , zzEndRead , zzBuffer . length - zzEndRead ) ; if ( numRead < 0 ) { return true ; } else { zzEndRead += numRead ; return false ; } } public final void yyclose ( ) throws java . io . IOException { zzAtEOF = true ; zzEndRead = zzStartRead ; if ( zzReader != null ) zzReader . close ( ) ; } public final void yyreset ( java . io . Reader reader ) { zzReader = reader ; zzAtBOL = true ; zzAtEOF = false ; zzEndRead = zzStartRead = 0 ; zzCurrentPos = zzMarkedPos = zzPushbackPos = 0 ; yyline = yychar = yycolumn = 0 ; zzLexicalState = YYINITIAL ; } public final int yystate ( ) { return zzLexicalState ; } public final void yybegin ( int newState ) { zzLexicalState = newState ; } public final String yytext ( ) { return new String ( zzBuffer , zzStartRead , zzMarkedPos - zzStartRead ) ; } public final char yycharat ( int pos ) { return zzBuffer [ zzStartRead + pos ] ; } public final int yylength ( ) { return zzMarkedPos - zzStartRead ; } private void zzScanError ( int errorCode ) { String message ; try { message = ZZ_ERROR_MSG [ errorCode ] ; } catch ( ArrayIndexOutOfBoundsException e ) { message = ZZ_ERROR_MSG [ ZZ_UNKNOWN_ERROR ] ; } throw new Error ( message ) ; } public void yypushback ( int number ) { if ( number > yylength ( ) ) zzScanError ( ZZ_PUSHBACK_2BIG ) ; zzMarkedPos -= number ; } public int getNextToken ( ) throws java . io . IOException { int zzInput ; int zzAction ; int zzCurrentPosL ; int zzMarkedPosL ; int zzEndReadL = zzEndRead ; char [ ] zzBufferL = zzBuffer ; char [ ] zzCMapL = ZZ_CMAP ; int [ ] zzTransL = ZZ_TRANS ; int [ ] zzRowMapL = ZZ_ROWMAP ; int [ ] zzAttrL = ZZ_ATTRIBUTE ; while ( true ) { zzMarkedPosL = zzMarkedPos ; yychar += zzMarkedPosL - zzStartRead ; zzAction = - 1 ; zzCurrentPosL = zzCurrentPos = zzStartRead = zzMarkedPosL ; zzState = zzLexicalState ; zzForAction : { while ( true ) { if ( zzCurrentPosL < zzEndReadL ) zzInput = zzBufferL [ zzCurrentPosL ++ ] ; else if ( zzAtEOF ) { zzInput = YYEOF ; break zzForAction ; } else { zzCurrentPos = zzCurrentPosL ; zzMarkedPos = zzMarkedPosL ; boolean eof = zzRefill ( ) ; zzCurrentPosL = zzCurrentPos ; zzMarkedPosL = zzMarkedPos ; zzBufferL = zzBuffer ; zzEndReadL = zzEndRead ; if ( eof ) { zzInput = YYEOF ; break zzForAction ; } else { zzInput = zzBufferL [ zzCurrentPosL ++ ] ; } } int zzNext = zzTransL [ zzRowMapL [ zzState ] + zzCMapL [ zzInput ] ] ; if ( zzNext == - 1 ) break zzForAction ; zzState = zzNext ; int zzAttributes = zzAttrL [ zzState ] ; if ( ( zzAttributes & 1 ) == 1 ) { zzAction = zzState ; zzMarkedPosL = zzCurrentPosL ; if ( ( zzAttributes & 8 ) == 8 ) break zzForAction ; } } } zzMarkedPos = zzMarkedPosL ; switch ( zzAction < 0 ? zzAction : ZZ_ACTION [ zzAction ] ) { case 4 : { return HOST ; } case 11 : break ; case 9 : { return ACRONYM ; } case 12 : break ; case 8 : { return ACRONYM_DEP ; } case 13 : break ; case 1 : { } case 14 : break ; case 5 : { return NUM ; } case 15 : break ; case 3 : { return CJ ; } case 16 : break ; case 2 : { return ALPHANUM ; } case 17 : break ; case 7 : { return COMPANY ; } case 18 : break ; case 6 : { return APOSTROPHE ; } case 19 : break ; case 10 : { return EMAIL ; } case 20 : break ; default : if ( zzInput == YYEOF && zzStartRead == zzCurrentPos ) { zzAtEOF = true ; return YYEOF ; } else { zzScanError ( ZZ_NO_MATCH ) ; } } } } } 	0	['25', '1', '0', '3', '35', '196', '2', '2', '10', '0.968495935', '729', '0.707317073', '0', '0', '0.214285714', '0', '0', '26.52', '3', '1.28', '0']
package org . apache . lucene . store ; import java . io . IOException ; import java . util . zip . CRC32 ; import java . util . zip . Checksum ; public class ChecksumIndexOutput extends IndexOutput { IndexOutput main ; Checksum digest ; public ChecksumIndexOutput ( IndexOutput main ) { this . main = main ; digest = new CRC32 ( ) ; } public void writeByte ( byte b ) throws IOException { digest . update ( b ) ; main . writeByte ( b ) ; } public void writeBytes ( byte [ ] b , int offset , int length ) throws IOException { digest . update ( b , offset , length ) ; main . writeBytes ( b , offset , length ) ; } public long getChecksum ( ) { return digest . getValue ( ) ; } public void flush ( ) throws IOException { main . flush ( ) ; } public void close ( ) throws IOException { main . close ( ) ; } public long getFilePointer ( ) { return main . getFilePointer ( ) ; } public void seek ( long pos ) { throw new RuntimeException ( "not allowed" ) ; } public void prepareCommit ( ) throws IOException { final long checksum = getChecksum ( ) ; final long pos = main . getFilePointer ( ) ; main . writeLong ( checksum - 1 ) ; main . flush ( ) ; main . seek ( pos ) ; } public void finishCommit ( ) throws IOException { main . writeLong ( getChecksum ( ) ) ; } public long length ( ) throws IOException { return main . length ( ) ; } } 	0	['11', '2', '0', '2', '25', '0', '1', '1', '11', '0.35', '98', '0', '1', '0.62962963', '0.242424242', '1', '5', '7.727272727', '1', '0.9091', '0']
package org . apache . lucene . index ; import java . io . IOException ; abstract class DocConsumerPerThread { abstract DocumentsWriter . DocWriter processDocument ( ) throws IOException ; abstract void abort ( ) ; } 	0	['3', '1', '1', '6', '4', '3', '5', '1', '0', '2', '6', '0', '0', '0', '1', '0', '0', '1', '1', '0.6667', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; import java . util . ArrayList ; import java . util . Iterator ; import java . util . List ; public class SinkTokenizer extends Tokenizer { protected List lst = new ArrayList ( ) ; protected Iterator iter ; public SinkTokenizer ( List input ) { this . lst = input ; if ( this . lst == null ) this . lst = new ArrayList ( ) ; } public SinkTokenizer ( ) { this . lst = new ArrayList ( ) ; } public SinkTokenizer ( int initCap ) { this . lst = new ArrayList ( initCap ) ; } public List getTokens ( ) { return lst ; } public Token next ( final Token reusableToken ) throws IOException { assert reusableToken != null ; if ( iter == null ) iter = lst . iterator ( ) ; if ( iter . hasNext ( ) ) { Token nextToken = ( Token ) iter . next ( ) ; return ( Token ) nextToken . clone ( ) ; } return null ; } public void add ( Token t ) { if ( t == null ) return ; lst . add ( ( Token ) t . clone ( ) ) ; } public void close ( ) throws IOException { input = null ; lst = null ; } public void reset ( ) throws IOException { iter = lst . iterator ( ) ; } } 	0	['10', '3', '0', '3', '23', '0', '1', '2', '8', '0.75', '143', '0.5', '0', '0.538461538', '0.288888889', '2', '2', '12.9', '2', '0.7', '0']
package org . apache . lucene . index ; import java . util . Comparator ; public class TermVectorEntryFreqSortedComparator implements Comparator { public int compare ( Object object , Object object1 ) { int result = 0 ; TermVectorEntry entry = ( TermVectorEntry ) object ; TermVectorEntry entry1 = ( TermVectorEntry ) object1 ; result = entry1 . getFrequency ( ) - entry . getFrequency ( ) ; if ( result == 0 ) { result = entry . getTerm ( ) . compareTo ( entry1 . getTerm ( ) ) ; if ( result == 0 ) { result = entry . getField ( ) . compareTo ( entry1 . getField ( ) ) ; } } return result ; } } 	0	['2', '1', '0', '1', '7', '1', '0', '1', '2', '2', '37', '0', '0', '0', '0.75', '0', '0', '17.5', '3', '1.5', '0']
package org . apache . lucene ; public final class LucenePackage { private LucenePackage ( ) { } public static Package get ( ) { return LucenePackage . class . getPackage ( ) ; } } 	0	['3', '1', '0', '0', '8', '3', '0', '0', '1', '1', '27', '0', '0', '0', '0.333333333', '0', '0', '7.666666667', '2', '1', '0']
package org . apache . lucene . index ; abstract class RawPostingList { final static int BYTES_SIZE = DocumentsWriter . OBJECT_HEADER_BYTES + 3 * DocumentsWriter . INT_NUM_BYTE ; int textStart ; int intStart ; int byteStart ; } 	0	['1', '1', '2', '12', '2', '0', '12', '0', '0', '2', '8', '0', '0', '0', '1', '0', '0', '3', '0', '0', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . document . Document ; import org . apache . lucene . index . CorruptIndexException ; public class Hit implements java . io . Serializable { private Document doc = null ; private boolean resolved = false ; private Hits hits = null ; private int hitNumber ; Hit ( Hits hits , int hitNumber ) { this . hits = hits ; this . hitNumber = hitNumber ; } public Document getDocument ( ) throws CorruptIndexException , IOException { if ( ! resolved ) fetchTheHit ( ) ; return doc ; } public float getScore ( ) throws IOException { return hits . score ( hitNumber ) ; } public int getId ( ) throws IOException { return hits . id ( hitNumber ) ; } private void fetchTheHit ( ) throws CorruptIndexException , IOException { doc = hits . doc ( hitNumber ) ; resolved = true ; } public float getBoost ( ) throws CorruptIndexException , IOException { return getDocument ( ) . getBoost ( ) ; } public String get ( String name ) throws CorruptIndexException , IOException { return getDocument ( ) . get ( name ) ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "Hit<" ) ; buffer . append ( hits . toString ( ) ) ; buffer . append ( " [" ) ; buffer . append ( hitNumber ) ; buffer . append ( "] " ) ; if ( resolved ) { buffer . append ( "resolved" ) ; } else { buffer . append ( "unresolved" ) ; } buffer . append ( ">" ) ; return buffer . toString ( ) ; } } 	0	['8', '1', '0', '4', '19', '2', '1', '3', '6', '0.178571429', '116', '1', '2', '0', '0.34375', '0', '0', '13', '2', '1', '0']
package org . apache . lucene . util ; import java . io . ObjectStreamException ; import java . io . Serializable ; import java . io . StreamCorruptedException ; import java . util . HashMap ; import java . util . Map ; public abstract class Parameter implements Serializable { static Map allParameters = new HashMap ( ) ; private String name ; private Parameter ( ) { } protected Parameter ( String name ) { this . name = name ; String key = makeKey ( name ) ; if ( allParameters . containsKey ( key ) ) throw new IllegalArgumentException ( "Parameter name " + key + " already used!" ) ; allParameters . put ( key , this ) ; } private String makeKey ( String name ) { return getClass ( ) + " " + name ; } public String toString ( ) { return name ; } protected Object readResolve ( ) throws ObjectStreamException { Object par = allParameters . get ( makeKey ( name ) ) ; if ( par == null ) throw new StreamCorruptedException ( "Unknown parameter value: " + name ) ; return par ; } } 	0	['6', '1', '5', '5', '18', '5', '5', '0', '1', '0.6', '88', '0.5', '0', '0', '0.7', '0', '0', '13.33333333', '1', '0.5', '0']
package org . apache . lucene . search ; public class QueryFilter extends CachingWrapperFilter { public QueryFilter ( Query query ) { super ( new QueryWrapperFilter ( query ) ) ; } public boolean equals ( Object o ) { return super . equals ( ( QueryFilter ) o ) ; } public int hashCode ( ) { return super . hashCode ( ) ^ 0x923F64B9 ; } } 	0	['3', '3', '0', '4', '7', '3', '0', '4', '3', '2', '20', '0', '0', '0.777777778', '0.555555556', '2', '3', '5.666666667', '1', '0.6667', '0']
package org . apache . lucene . index ; import java . util . * ; public class FieldSortedTermVectorMapper extends TermVectorMapper { private Map fieldToTerms = new HashMap ( ) ; private SortedSet currentSet ; private String currentField ; private Comparator comparator ; public FieldSortedTermVectorMapper ( Comparator comparator ) { this ( false , false , comparator ) ; } public FieldSortedTermVectorMapper ( boolean ignoringPositions , boolean ignoringOffsets , Comparator comparator ) { super ( ignoringPositions , ignoringOffsets ) ; this . comparator = comparator ; } public void map ( String term , int frequency , TermVectorOffsetInfo [ ] offsets , int [ ] positions ) { TermVectorEntry entry = new TermVectorEntry ( currentField , term , frequency , offsets , positions ) ; currentSet . add ( entry ) ; } public void setExpectations ( String field , int numTerms , boolean storeOffsets , boolean storePositions ) { currentSet = new TreeSet ( comparator ) ; currentField = field ; fieldToTerms . put ( field , currentSet ) ; } public Map getFieldToTerms ( ) { return fieldToTerms ; } public Comparator getComparator ( ) { return comparator ; } } 	0	['6', '2', '0', '3', '12', '3', '0', '3', '6', '0.6', '69', '1', '0', '0.555555556', '0.380952381', '0', '0', '9.833333333', '1', '0.6667', '0']
package org . apache . lucene . util ; public class SmallFloat { public static byte floatToByte ( float f , int numMantissaBits , int zeroExp ) { int fzero = ( 63 - zeroExp ) << numMantissaBits ; int bits = Float . floatToRawIntBits ( f ) ; int smallfloat = bits > > ( 24 - numMantissaBits ) ; if ( smallfloat < fzero ) { return ( bits <= 0 ) ? ( byte ) 0 : ( byte ) 1 ; } else if ( smallfloat >= fzero + 0x100 ) { return - 1 ; } else { return ( byte ) ( smallfloat - fzero ) ; } } public static float byteToFloat ( byte b , int numMantissaBits , int zeroExp ) { if ( b == 0 ) return 0.0f ; int bits = ( b & 0xff ) << ( 24 - numMantissaBits ) ; bits += ( 63 - zeroExp ) << 24 ; return Float . intBitsToFloat ( bits ) ; } public static byte floatToByte315 ( float f ) { int bits = Float . floatToRawIntBits ( f ) ; int smallfloat = bits > > ( 24 - 3 ) ; if ( smallfloat < ( 63 - 15 ) << 3 ) { return ( bits <= 0 ) ? ( byte ) 0 : ( byte ) 1 ; } if ( smallfloat >= ( ( 63 - 15 ) << 3 ) + 0x100 ) { return - 1 ; } return ( byte ) ( smallfloat - ( ( 63 - 15 ) << 3 ) ) ; } public static float byte315ToFloat ( byte b ) { if ( b == 0 ) return 0.0f ; int bits = ( b & 0xff ) << ( 24 - 3 ) ; bits += ( 63 - 15 ) << 24 ; return Float . intBitsToFloat ( bits ) ; } public static byte floatToByte52 ( float f ) { int bits = Float . floatToRawIntBits ( f ) ; int smallfloat = bits > > ( 24 - 5 ) ; if ( smallfloat < ( 63 - 2 ) << 5 ) { return ( bits <= 0 ) ? ( byte ) 0 : ( byte ) 1 ; } if ( smallfloat >= ( ( 63 - 2 ) << 5 ) + 0x100 ) { return - 1 ; } return ( byte ) ( smallfloat - ( ( 63 - 2 ) << 5 ) ) ; } public static float byte52ToFloat ( byte b ) { if ( b == 0 ) return 0.0f ; int bits = ( b & 0xff ) << ( 24 - 5 ) ; bits += ( 63 - 2 ) << 24 ; return Float . intBitsToFloat ( bits ) ; } } 	0	['7', '1', '0', '1', '10', '21', '1', '0', '7', '2', '155', '0', '0', '0', '0.321428571', '0', '0', '21.14285714', '4', '2.5714', '0']
package org . apache . lucene . store ; import java . io . IOException ; import java . io . File ; public class LockStressTest { public static void main ( String [ ] args ) throws Exception { if ( args . length != 6 ) { System . out . println ( "\nUsage: java org.apache.lucene.store.LockStressTest myID verifierHostOrIP verifierPort lockFactoryClassName lockDirName sleepTime\n" + "\n" + "  myID = int from 0 .. 255 (should be unique for test process)\n" + "  verifierHostOrIP = host name or IP address where LockVerifyServer is running\n" + "  verifierPort = port that LockVerifyServer is listening on\n" + "  lockFactoryClassName = primary LockFactory class that we will use\n" + "  lockDirName = path to the lock directory (only set for Simple/NativeFSLockFactory\n" + "  sleepTimeMS = milliseconds to pause betweeen each lock obtain/release\n" + "\n" + "You should run multiple instances of this process, each with its own\n" + "unique ID, and each pointing to the same lock directory, to verify\n" + "that locking is working correctly.\n" + "\n" + "Make sure you are first running LockVerifyServer.\n" + "\n" ) ; System . exit ( 1 ) ; } final int myID = Integer . parseInt ( args [ 0 ] ) ; if ( myID < 0 || myID > 255 ) { System . out . println ( "myID must be a unique int 0..255" ) ; System . exit ( 1 ) ; } final String verifierHost = args [ 1 ] ; final int verifierPort = Integer . parseInt ( args [ 2 ] ) ; final String lockFactoryClassName = args [ 3 ] ; final String lockDirName = args [ 4 ] ; final int sleepTimeMS = Integer . parseInt ( args [ 5 ] ) ; Class c ; try { c = Class . forName ( lockFactoryClassName ) ; } catch ( ClassNotFoundException e ) { throw new IOException ( "unable to find LockClass " + lockFactoryClassName ) ; } LockFactory lockFactory ; try { lockFactory = ( LockFactory ) c . newInstance ( ) ; } catch ( IllegalAccessException e ) { throw new IOException ( "IllegalAccessException when instantiating LockClass " + lockFactoryClassName ) ; } catch ( InstantiationException e ) { throw new IOException ( "InstantiationException when instantiating LockClass " + lockFactoryClassName ) ; } catch ( ClassCastException e ) { throw new IOException ( "unable to cast LockClass " + lockFactoryClassName + " instance to a LockFactory" ) ; } File lockDir = new File ( lockDirName ) ; if ( lockFactory instanceof NativeFSLockFactory ) { ( ( NativeFSLockFactory ) lockFactory ) . setLockDir ( lockDir ) ; } else if ( lockFactory instanceof SimpleFSLockFactory ) { ( ( SimpleFSLockFactory ) lockFactory ) . setLockDir ( lockDir ) ; } lockFactory . setLockPrefix ( "test" ) ; LockFactory verifyLF = new VerifyingLockFactory ( ( byte ) myID , lockFactory , verifierHost , verifierPort ) ; Lock l = verifyLF . makeLock ( "test.lock" ) ; while ( true ) { boolean obtained = false ; try { obtained = l . obtain ( 10 ) ; } catch ( LockObtainFailedException e ) { System . out . print ( "x" ) ; } if ( obtained ) { System . out . print ( "l" ) ; l . release ( ) ; } Thread . sleep ( sleepTimeMS ) ; } } } 	0	['2', '1', '0', '6', '22', '1', '0', '6', '2', '2', '172', '0', '0', '0', '0.5', '0', '0', '85', '1', '0.5', '0']
package org . apache . lucene . index ; final class NormsWriterPerThread extends InvertedDocEndConsumerPerThread { final NormsWriter normsWriter ; final DocumentsWriter . DocState docState ; public NormsWriterPerThread ( DocInverterPerThread docInverterPerThread , NormsWriter normsWriter ) { this . normsWriter = normsWriter ; docState = docInverterPerThread . docState ; } InvertedDocEndConsumerPerField addField ( DocInverterPerField docInverterPerField , final FieldInfo fieldInfo ) { return new NormsWriterPerField ( docInverterPerField , this , fieldInfo ) ; } void abort ( ) { } void startDocument ( ) { } void finishDocument ( ) { } boolean freeRAM ( ) { return false ; } } 	0	['6', '2', '0', '8', '8', '15', '2', '8', '1', '1', '30', '0', '2', '0.444444444', '0.333333333', '0', '0', '3.666666667', '1', '0.8333', '0']
package org . apache . lucene . index ; import java . util . List ; public final class KeepOnlyLastCommitDeletionPolicy implements IndexDeletionPolicy { public void onInit ( List commits ) { onCommit ( commits ) ; } public void onCommit ( List commits ) { int size = commits . size ( ) ; for ( int i = 0 ; i < size - 1 ; i ++ ) { ( ( IndexCommit ) commits . get ( i ) ) . delete ( ) ; } } } 	0	['3', '1', '0', '4', '7', '3', '2', '2', '3', '2', '28', '0', '0', '0', '0.833333333', '0', '0', '8.333333333', '2', '1', '0']
package org . apache . lucene . document ; import java . util . Set ; public class SetBasedFieldSelector implements FieldSelector { private Set fieldsToLoad ; private Set lazyFieldsToLoad ; public SetBasedFieldSelector ( Set fieldsToLoad , Set lazyFieldsToLoad ) { this . fieldsToLoad = fieldsToLoad ; this . lazyFieldsToLoad = lazyFieldsToLoad ; } public FieldSelectorResult accept ( String fieldName ) { FieldSelectorResult result = FieldSelectorResult . NO_LOAD ; if ( fieldsToLoad . contains ( fieldName ) == true ) { result = FieldSelectorResult . LOAD ; } if ( lazyFieldsToLoad . contains ( fieldName ) == true ) { result = FieldSelectorResult . LAZY_LOAD ; } return result ; } } 	0	['2', '1', '0', '2', '4', '0', '0', '2', '2', '0', '33', '1', '0', '0', '0.666666667', '0', '0', '14.5', '3', '1.5', '0']
package org . apache . lucene . index ; import java . util . Arrays ; final class ByteBlockPool { abstract static class Allocator { abstract void recycleByteBlocks ( byte [ ] [ ] blocks , int start , int end ) ; abstract byte [ ] getByteBlock ( boolean trackAllocations ) ; } public byte [ ] [ ] buffers = new byte [ 10 ] [ ] ; int bufferUpto = - 1 ; public int byteUpto = DocumentsWriter . BYTE_BLOCK_SIZE ; public byte [ ] buffer ; public int byteOffset = - DocumentsWriter . BYTE_BLOCK_SIZE ; private final boolean trackAllocations ; private final Allocator allocator ; public ByteBlockPool ( Allocator allocator , boolean trackAllocations ) { this . allocator = allocator ; this . trackAllocations = trackAllocations ; } public void reset ( ) { if ( bufferUpto != - 1 ) { for ( int i = 0 ; i < bufferUpto ; i ++ ) Arrays . fill ( buffers [ i ] , ( byte ) 0 ) ; Arrays . fill ( buffers [ bufferUpto ] , 0 , byteUpto , ( byte ) 0 ) ; if ( bufferUpto > 0 ) allocator . recycleByteBlocks ( buffers , 1 , 1 + bufferUpto ) ; bufferUpto = 0 ; byteUpto = 0 ; byteOffset = 0 ; buffer = buffers [ 0 ] ; } } public void nextBuffer ( ) { if ( 1 + bufferUpto == buffers . length ) { byte [ ] [ ] newBuffers = new byte [ ( int ) ( buffers . length * 1.5 ) ] [ ] ; System . arraycopy ( buffers , 0 , newBuffers , 0 , buffers . length ) ; buffers = newBuffers ; } buffer = buffers [ 1 + bufferUpto ] = allocator . getByteBlock ( trackAllocations ) ; bufferUpto ++ ; byteUpto = 0 ; byteOffset += DocumentsWriter . BYTE_BLOCK_SIZE ; } public int newSlice ( final int size ) { if ( byteUpto > DocumentsWriter . BYTE_BLOCK_SIZE - size ) nextBuffer ( ) ; final int upto = byteUpto ; byteUpto += size ; buffer [ byteUpto - 1 ] = 16 ; return upto ; } final static int [ ] nextLevelArray = { 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 9 } ; final static int [ ] levelSizeArray = { 5 , 14 , 20 , 30 , 40 , 40 , 80 , 80 , 120 , 200 } ; final static int FIRST_LEVEL_SIZE = levelSizeArray [ 0 ] ; public int allocSlice ( final byte [ ] slice , final int upto ) { final int level = slice [ upto ] & 15 ; final int newLevel = nextLevelArray [ level ] ; final int newSize = levelSizeArray [ newLevel ] ; if ( byteUpto > DocumentsWriter . BYTE_BLOCK_SIZE - newSize ) nextBuffer ( ) ; final int newUpto = byteUpto ; final int offset = newUpto + byteOffset ; byteUpto += newSize ; buffer [ newUpto ] = slice [ upto - 3 ] ; buffer [ newUpto + 1 ] = slice [ upto - 2 ] ; buffer [ newUpto + 2 ] = slice [ upto - 1 ] ; slice [ upto - 3 ] = ( byte ) ( offset > > > 24 ) ; slice [ upto - 2 ] = ( byte ) ( offset > > > 16 ) ; slice [ upto - 1 ] = ( byte ) ( offset > > > 8 ) ; slice [ upto ] = ( byte ) offset ; buffer [ byteUpto - 1 ] = ( byte ) ( 16 | newLevel ) ; return newUpto + 3 ; } } 	0	['6', '1', '0', '5', '12', '0', '4', '1', '5', '0.44', '387', '0.2', '1', '0', '0.4', '0', '0', '61.83333333', '4', '1.6667', '0']
package org . apache . lucene . store ; import java . net . ServerSocket ; import java . net . Socket ; import java . io . OutputStream ; import java . io . InputStream ; import java . io . IOException ; public class LockVerifyServer { private static String getTime ( long startTime ) { return "[" + ( ( System . currentTimeMillis ( ) - startTime ) / 1000 ) + "s] " ; } public static void main ( String [ ] args ) throws IOException { if ( args . length != 1 ) { System . out . println ( "\nUsage: java org.apache.lucene.store.LockVerifyServer port\n" ) ; System . exit ( 1 ) ; } final int port = Integer . parseInt ( args [ 0 ] ) ; ServerSocket s = new ServerSocket ( port ) ; s . setReuseAddress ( true ) ; System . out . println ( "\nReady on port " + port + "..." ) ; int lockedID = 0 ; long startTime = System . currentTimeMillis ( ) ; while ( true ) { Socket cs = s . accept ( ) ; OutputStream out = cs . getOutputStream ( ) ; InputStream in = cs . getInputStream ( ) ; int id = in . read ( ) ; int command = in . read ( ) ; boolean err = false ; if ( command == 1 ) { if ( lockedID != 0 ) { err = true ; System . out . println ( getTime ( startTime ) + " ERROR: id " + id + " got lock, but " + lockedID + " already holds the lock" ) ; } lockedID = id ; } else if ( command == 0 ) { if ( lockedID != id ) { err = true ; System . out . println ( getTime ( startTime ) + " ERROR: id " + id + " released the lock, but " + lockedID + " is the one holding the lock" ) ; } lockedID = 0 ; } else throw new RuntimeException ( "unrecognized command " + command ) ; System . out . print ( "." ) ; if ( err ) out . write ( 1 ) ; else out . write ( 0 ) ; out . close ( ) ; in . close ( ) ; cs . close ( ) ; } } } 	0	['3', '1', '0', '0', '25', '3', '0', '0', '2', '2', '165', '0', '0', '0', '0.333333333', '0', '0', '54', '1', '0.6667', '0']
package org . apache . lucene . index ; final class ByteSliceWriter { private byte [ ] slice ; private int upto ; private final ByteBlockPool pool ; int offset0 ; public ByteSliceWriter ( ByteBlockPool pool ) { this . pool = pool ; } public void init ( int address ) { slice = pool . buffers [ address > > DocumentsWriter . BYTE_BLOCK_SHIFT ] ; assert slice != null ; upto = address & DocumentsWriter . BYTE_BLOCK_MASK ; offset0 = address ; assert upto < slice . length ; } public void writeByte ( byte b ) { assert slice != null ; if ( slice [ upto ] != 0 ) { upto = pool . allocSlice ( slice , upto ) ; slice = pool . buffer ; offset0 = pool . byteOffset ; assert slice != null ; } slice [ upto ++ ] = b ; assert upto != slice . length ; } public void writeBytes ( final byte [ ] b , int offset , final int len ) { final int offsetEnd = offset + len ; while ( offset < offsetEnd ) { if ( slice [ upto ] != 0 ) { upto = pool . allocSlice ( slice , upto ) ; slice = pool . buffer ; offset0 = pool . byteOffset ; } slice [ upto ++ ] = b [ offset ++ ] ; assert upto != slice . length ; } } public int getAddress ( ) { return upto + ( offset0 & DocumentsWriter . BYTE_BLOCK_NOT_MASK ) ; } public void writeVInt ( int i ) { while ( ( i & ~ 0x7F ) != 0 ) { writeByte ( ( byte ) ( ( i & 0x7f ) | 0x80 ) ) ; i >>>= 7 ; } writeByte ( ( byte ) i ) ; } } 	0	['8', '1', '0', '1', '15', '4', '0', '1', '6', '0.547619048', '240', '0.5', '1', '0', '0.30952381', '0', '0', '28.25', '8', '2.75', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . BufferedIndexInput ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . Lock ; import java . util . HashMap ; import java . io . IOException ; class CompoundFileReader extends Directory { private int readBufferSize ; private static final class FileEntry { long offset ; long length ; } private Directory directory ; private String fileName ; private IndexInput stream ; private HashMap entries = new HashMap ( ) ; public CompoundFileReader ( Directory dir , String name ) throws IOException { this ( dir , name , BufferedIndexInput . BUFFER_SIZE ) ; } public CompoundFileReader ( Directory dir , String name , int readBufferSize ) throws IOException { directory = dir ; fileName = name ; this . readBufferSize = readBufferSize ; boolean success = false ; try { stream = dir . openInput ( name , readBufferSize ) ; int count = stream . readVInt ( ) ; FileEntry entry = null ; for ( int i = 0 ; i < count ; i ++ ) { long offset = stream . readLong ( ) ; String id = stream . readString ( ) ; if ( entry != null ) { entry . length = offset - entry . offset ; } entry = new FileEntry ( ) ; entry . offset = offset ; entries . put ( id , entry ) ; } if ( entry != null ) { entry . length = stream . length ( ) - entry . offset ; } success = true ; } finally { if ( ! success && ( stream != null ) ) { try { stream . close ( ) ; } catch ( IOException e ) { } } } } public Directory getDirectory ( ) { return directory ; } public String getName ( ) { return fileName ; } public synchronized void close ( ) throws IOException { if ( stream == null ) throw new IOException ( "Already closed" ) ; entries . clear ( ) ; stream . close ( ) ; stream = null ; } public synchronized IndexInput openInput ( String id ) throws IOException { return openInput ( id , readBufferSize ) ; } public synchronized IndexInput openInput ( String id , int readBufferSize ) throws IOException { if ( stream == null ) throw new IOException ( "Stream closed" ) ; FileEntry entry = ( FileEntry ) entries . get ( id ) ; if ( entry == null ) throw new IOException ( "No sub-file with id " + id + " found" ) ; return new CSIndexInput ( stream , entry . offset , entry . length , readBufferSize ) ; } public String [ ] list ( ) { String res [ ] = new String [ entries . size ( ) ] ; return ( String [ ] ) entries . keySet ( ) . toArray ( res ) ; } public boolean fileExists ( String name ) { return entries . containsKey ( name ) ; } public long fileModified ( String name ) throws IOException { return directory . fileModified ( fileName ) ; } public void touchFile ( String name ) throws IOException { directory . touchFile ( fileName ) ; } public void deleteFile ( String name ) { throw new UnsupportedOperationException ( ) ; } public void renameFile ( String from , String to ) { throw new UnsupportedOperationException ( ) ; } public long fileLength ( String name ) throws IOException { FileEntry e = ( FileEntry ) entries . get ( name ) ; if ( e == null ) throw new IOException ( "File " + name + " does not exist" ) ; return e . length ; } public IndexOutput createOutput ( String name ) { throw new UnsupportedOperationException ( ) ; } public Lock makeLock ( String name ) { throw new UnsupportedOperationException ( ) ; } static final class CSIndexInput extends BufferedIndexInput { IndexInput base ; long fileOffset ; long length ; CSIndexInput ( final IndexInput base , final long fileOffset , final long length ) { this ( base , fileOffset , length , BufferedIndexInput . BUFFER_SIZE ) ; } CSIndexInput ( final IndexInput base , final long fileOffset , final long length , int readBufferSize ) { super ( readBufferSize ) ; this . base = ( IndexInput ) base . clone ( ) ; this . fileOffset = fileOffset ; this . length = length ; } public Object clone ( ) { CSIndexInput clone = ( CSIndexInput ) super . clone ( ) ; clone . base = ( IndexInput ) base . clone ( ) ; clone . fileOffset = fileOffset ; clone . length = length ; return clone ; } protected void readInternal ( byte [ ] b , int offset , int len ) throws IOException { long start = getFilePointer ( ) ; if ( start + len > length ) throw new IOException ( "read past EOF" ) ; base . seek ( fileOffset + start ) ; base . readBytes ( b , offset , len , false ) ; } protected void seekInternal ( long pos ) { } public void close ( ) throws IOException { base . close ( ) ; } public long length ( ) { return length ; } } } 	0	['16', '2', '0', '9', '40', '70', '2', '7', '16', '0.72', '267', '1', '2', '0.575757576', '0.5', '1', '5', '15.375', '1', '0.875', '0']
package org . apache . lucene . index ; import java . util . Collection ; import java . util . Map ; import java . util . HashMap ; import java . util . Iterator ; import java . util . HashSet ; import java . util . Arrays ; import java . io . IOException ; import org . apache . lucene . util . ArrayUtil ; final class TermsHash extends InvertedDocConsumer { final TermsHashConsumer consumer ; final TermsHash nextTermsHash ; final int bytesPerPosting ; final int postingsFreeChunk ; final DocumentsWriter docWriter ; private TermsHash primaryTermsHash ; private RawPostingList [ ] postingsFreeList = new RawPostingList [ 1 ] ; private int postingsFreeCount ; private int postingsAllocCount ; boolean trackAllocations ; public TermsHash ( final DocumentsWriter docWriter , boolean trackAllocations , final TermsHashConsumer consumer , final TermsHash nextTermsHash ) { this . docWriter = docWriter ; this . consumer = consumer ; this . nextTermsHash = nextTermsHash ; this . trackAllocations = trackAllocations ; bytesPerPosting = consumer . bytesPerPosting ( ) + 4 * DocumentsWriter . POINTER_NUM_BYTE ; postingsFreeChunk = ( int ) ( DocumentsWriter . BYTE_BLOCK_SIZE / bytesPerPosting ) ; } InvertedDocConsumerPerThread addThread ( DocInverterPerThread docInverterPerThread ) { return new TermsHashPerThread ( docInverterPerThread , this , nextTermsHash , null ) ; } TermsHashPerThread addThread ( DocInverterPerThread docInverterPerThread , TermsHashPerThread primaryPerThread ) { return new TermsHashPerThread ( docInverterPerThread , this , nextTermsHash , primaryPerThread ) ; } void setFieldInfos ( FieldInfos fieldInfos ) { this . fieldInfos = fieldInfos ; consumer . setFieldInfos ( fieldInfos ) ; } synchronized public void abort ( ) { consumer . abort ( ) ; if ( nextTermsHash != null ) nextTermsHash . abort ( ) ; } void shrinkFreePostings ( Map threadsAndFields , DocumentsWriter . FlushState state ) { assert postingsFreeCount == postingsAllocCount : Thread . currentThread ( ) . getName ( ) + ": postingsFreeCount=" + postingsFreeCount + " postingsAllocCount=" + postingsAllocCount + " consumer=" + consumer ; final int newSize = ArrayUtil . getShrinkSize ( postingsFreeList . length , postingsAllocCount ) ; if ( newSize != postingsFreeList . length ) { RawPostingList [ ] newArray = new RawPostingList [ newSize ] ; System . arraycopy ( postingsFreeList , 0 , newArray , 0 , postingsFreeCount ) ; postingsFreeList = newArray ; } } synchronized void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException { consumer . closeDocStore ( state ) ; if ( nextTermsHash != null ) nextTermsHash . closeDocStore ( state ) ; } synchronized void flush ( Map threadsAndFields , final DocumentsWriter . FlushState state ) throws IOException { Map childThreadsAndFields = new HashMap ( ) ; Map nextThreadsAndFields ; if ( nextTermsHash != null ) nextThreadsAndFields = new HashMap ( ) ; else nextThreadsAndFields = null ; Iterator it = threadsAndFields . entrySet ( ) . iterator ( ) ; while ( it . hasNext ( ) ) { Map . Entry entry = ( Map . Entry ) it . next ( ) ; TermsHashPerThread perThread = ( TermsHashPerThread ) entry . getKey ( ) ; Collection fields = ( Collection ) entry . getValue ( ) ; Iterator fieldsIt = fields . iterator ( ) ; Collection childFields = new HashSet ( ) ; Collection nextChildFields ; if ( nextTermsHash != null ) nextChildFields = new HashSet ( ) ; else nextChildFields = null ; while ( fieldsIt . hasNext ( ) ) { TermsHashPerField perField = ( TermsHashPerField ) fieldsIt . next ( ) ; childFields . add ( perField . consumer ) ; if ( nextTermsHash != null ) nextChildFields . add ( perField . nextPerField ) ; } childThreadsAndFields . put ( perThread . consumer , childFields ) ; if ( nextTermsHash != null ) nextThreadsAndFields . put ( perThread . nextPerThread , nextChildFields ) ; } consumer . flush ( childThreadsAndFields , state ) ; shrinkFreePostings ( threadsAndFields , state ) ; if ( nextTermsHash != null ) nextTermsHash . flush ( nextThreadsAndFields , state ) ; } synchronized public boolean freeRAM ( ) { if ( ! trackAllocations ) return false ; boolean any ; final int numToFree ; if ( postingsFreeCount >= postingsFreeChunk ) numToFree = postingsFreeChunk ; else numToFree = postingsFreeCount ; any = numToFree > 0 ; if ( any ) { Arrays . fill ( postingsFreeList , postingsFreeCount - numToFree , postingsFreeCount , null ) ; postingsFreeCount -= numToFree ; postingsAllocCount -= numToFree ; docWriter . bytesAllocated ( - numToFree * bytesPerPosting ) ; any = true ; } if ( nextTermsHash != null ) any |= nextTermsHash . freeRAM ( ) ; return any ; } synchronized public void recyclePostings ( final RawPostingList [ ] postings , final int numPostings ) { assert postings . length >= numPostings ; assert postingsFreeCount + numPostings <= postingsFreeList . length ; System . arraycopy ( postings , 0 , postingsFreeList , postingsFreeCount , numPostings ) ; postingsFreeCount += numPostings ; } synchronized public void getPostings ( final RawPostingList [ ] postings ) { assert docWriter . writer . testPoint ( "TermsHash.getPostings start" ) ; assert postingsFreeCount <= postingsFreeList . length ; assert postingsFreeCount <= postingsAllocCount : "postingsFreeCount=" + postingsFreeCount + " postingsAllocCount=" + postingsAllocCount ; final int numToCopy ; if ( postingsFreeCount < postings . length ) numToCopy = postingsFreeCount ; else numToCopy = postings . length ; final int start = postingsFreeCount - numToCopy ; assert start >= 0 ; assert start + numToCopy <= postingsFreeList . length ; assert numToCopy <= postings . length ; System . arraycopy ( postingsFreeList , start , postings , 0 , numToCopy ) ; if ( numToCopy != postings . length ) { final int extra = postings . length - numToCopy ; final int newPostingsAllocCount = postingsAllocCount + extra ; consumer . createPostings ( postings , numToCopy , extra ) ; assert docWriter . writer . testPoint ( "TermsHash.getPostings after create" ) ; postingsAllocCount += extra ; if ( trackAllocations ) docWriter . bytesAllocated ( extra * bytesPerPosting ) ; if ( newPostingsAllocCount > postingsFreeList . length ) postingsFreeList = new RawPostingList [ ArrayUtil . getNextSize ( newPostingsAllocCount ) ] ; } postingsFreeCount -= numToCopy ; if ( trackAllocations ) docWriter . bytesUsed ( postings . length * bytesPerPosting ) ; } } 	0	['13', '2', '0', '14', '52', '0', '3', '14', '5', '0.763888889', '584', '0.333333333', '5', '0.352941176', '0.179487179', '0', '0', '43', '20', '3.3077', '0']
package org . apache . lucene . index ; import java . util . ArrayList ; import java . util . HashMap ; import java . util . List ; import java . util . Map ; public class PositionBasedTermVectorMapper extends TermVectorMapper { private Map fieldToTerms ; private String currentField ; private Map currentPositions ; private boolean storeOffsets ; public PositionBasedTermVectorMapper ( ) { super ( false , false ) ; } public PositionBasedTermVectorMapper ( boolean ignoringOffsets ) { super ( false , ignoringOffsets ) ; } public boolean isIgnoringPositions ( ) { return false ; } public void map ( String term , int frequency , TermVectorOffsetInfo [ ] offsets , int [ ] positions ) { for ( int i = 0 ; i < positions . length ; i ++ ) { Integer posVal = new Integer ( positions [ i ] ) ; TVPositionInfo pos = ( TVPositionInfo ) currentPositions . get ( posVal ) ; if ( pos == null ) { pos = new TVPositionInfo ( positions [ i ] , storeOffsets ) ; currentPositions . put ( posVal , pos ) ; } pos . addTerm ( term , offsets != null ? offsets [ i ] : null ) ; } } public void setExpectations ( String field , int numTerms , boolean storeOffsets , boolean storePositions ) { if ( storePositions == false ) { throw new RuntimeException ( "You must store positions in order to use this Mapper" ) ; } if ( storeOffsets == true ) { } fieldToTerms = new HashMap ( numTerms ) ; this . storeOffsets = storeOffsets ; currentField = field ; currentPositions = new HashMap ( ) ; fieldToTerms . put ( currentField , currentPositions ) ; } public Map getFieldToTerms ( ) { return fieldToTerms ; } public static class TVPositionInfo { private int position ; private List terms ; private List offsets ; public TVPositionInfo ( int position , boolean storeOffsets ) { this . position = position ; terms = new ArrayList ( ) ; if ( storeOffsets ) { offsets = new ArrayList ( ) ; } } void addTerm ( String term , TermVectorOffsetInfo info ) { terms . add ( term ) ; if ( offsets != null ) { offsets . add ( info ) ; } } public int getPosition ( ) { return position ; } public List getTerms ( ) { return terms ; } public List getOffsets ( ) { return offsets ; } } } 	0	['6', '2', '0', '3', '15', '11', '0', '3', '6', '0.85', '110', '1', '0', '0.555555556', '0.388888889', '0', '0', '16.66666667', '4', '1.5', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . BufferedIndexInput ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import java . io . IOException ; import java . util . Arrays ; class TermVectorsReader implements Cloneable { static final int FORMAT_VERSION = 2 ; static final int FORMAT_VERSION2 = 3 ; static final int FORMAT_UTF8_LENGTH_IN_BYTES = 4 ; static final int FORMAT_CURRENT = FORMAT_UTF8_LENGTH_IN_BYTES ; static final int FORMAT_SIZE = 4 ; static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x1 ; static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x2 ; private FieldInfos fieldInfos ; private IndexInput tvx ; private IndexInput tvd ; private IndexInput tvf ; private int size ; private int numTotalDocs ; private int docStoreOffset ; private final int format ; TermVectorsReader ( Directory d , String segment , FieldInfos fieldInfos ) throws CorruptIndexException , IOException { this ( d , segment , fieldInfos , BufferedIndexInput . BUFFER_SIZE ) ; } TermVectorsReader ( Directory d , String segment , FieldInfos fieldInfos , int readBufferSize ) throws CorruptIndexException , IOException { this ( d , segment , fieldInfos , readBufferSize , - 1 , 0 ) ; } TermVectorsReader ( Directory d , String segment , FieldInfos fieldInfos , int readBufferSize , int docStoreOffset , int size ) throws CorruptIndexException , IOException { boolean success = false ; try { if ( d . fileExists ( segment + "." + IndexFileNames . VECTORS_INDEX_EXTENSION ) ) { tvx = d . openInput ( segment + "." + IndexFileNames . VECTORS_INDEX_EXTENSION , readBufferSize ) ; format = checkValidFormat ( tvx ) ; tvd = d . openInput ( segment + "." + IndexFileNames . VECTORS_DOCUMENTS_EXTENSION , readBufferSize ) ; final int tvdFormat = checkValidFormat ( tvd ) ; tvf = d . openInput ( segment + "." + IndexFileNames . VECTORS_FIELDS_EXTENSION , readBufferSize ) ; final int tvfFormat = checkValidFormat ( tvf ) ; assert format == tvdFormat ; assert format == tvfFormat ; if ( format >= FORMAT_VERSION2 ) { assert ( tvx . length ( ) - FORMAT_SIZE ) % 16 == 0 ; numTotalDocs = ( int ) ( tvx . length ( ) > > 4 ) ; } else { assert ( tvx . length ( ) - FORMAT_SIZE ) % 8 == 0 ; numTotalDocs = ( int ) ( tvx . length ( ) > > 3 ) ; } if ( - 1 == docStoreOffset ) { this . docStoreOffset = 0 ; this . size = numTotalDocs ; assert size == 0 || numTotalDocs == size ; } else { this . docStoreOffset = docStoreOffset ; this . size = size ; assert numTotalDocs >= size + docStoreOffset : "numTotalDocs=" + numTotalDocs + " size=" + size + " docStoreOffset=" + docStoreOffset ; } } else format = 0 ; this . fieldInfos = fieldInfos ; success = true ; } finally { if ( ! success ) { close ( ) ; } } } IndexInput getTvdStream ( ) { return tvd ; } IndexInput getTvfStream ( ) { return tvf ; } final private void seekTvx ( final int docNum ) throws IOException { if ( format < FORMAT_VERSION2 ) tvx . seek ( ( docNum + docStoreOffset ) * 8L + FORMAT_SIZE ) ; else tvx . seek ( ( docNum + docStoreOffset ) * 16L + FORMAT_SIZE ) ; } boolean canReadRawDocs ( ) { return format >= FORMAT_UTF8_LENGTH_IN_BYTES ; } final void rawDocs ( int [ ] tvdLengths , int [ ] tvfLengths , int startDocID , int numDocs ) throws IOException { if ( tvx == null ) { Arrays . fill ( tvdLengths , 0 ) ; Arrays . fill ( tvfLengths , 0 ) ; return ; } if ( format < FORMAT_VERSION2 ) throw new IllegalStateException ( "cannot read raw docs with older term vector formats" ) ; seekTvx ( startDocID ) ; long tvdPosition = tvx . readLong ( ) ; tvd . seek ( tvdPosition ) ; long tvfPosition = tvx . readLong ( ) ; tvf . seek ( tvfPosition ) ; long lastTvdPosition = tvdPosition ; long lastTvfPosition = tvfPosition ; int count = 0 ; while ( count < numDocs ) { final int docID = docStoreOffset + startDocID + count + 1 ; assert docID <= numTotalDocs ; if ( docID < numTotalDocs ) { tvdPosition = tvx . readLong ( ) ; tvfPosition = tvx . readLong ( ) ; } else { tvdPosition = tvd . length ( ) ; tvfPosition = tvf . length ( ) ; assert count == numDocs - 1 ; } tvdLengths [ count ] = ( int ) ( tvdPosition - lastTvdPosition ) ; tvfLengths [ count ] = ( int ) ( tvfPosition - lastTvfPosition ) ; count ++ ; lastTvdPosition = tvdPosition ; lastTvfPosition = tvfPosition ; } } private int checkValidFormat ( IndexInput in ) throws CorruptIndexException , IOException { int format = in . readInt ( ) ; if ( format > FORMAT_CURRENT ) { throw new CorruptIndexException ( "Incompatible format version: " + format + " expected " + FORMAT_CURRENT + " or less" ) ; } return format ; } void close ( ) throws IOException { IOException keep = null ; if ( tvx != null ) try { tvx . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tvd != null ) try { tvd . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tvf != null ) try { tvf . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( keep != null ) throw ( IOException ) keep . fillInStackTrace ( ) ; } int size ( ) { return size ; } public void get ( int docNum , String field , TermVectorMapper mapper ) throws IOException { if ( tvx != null ) { int fieldNumber = fieldInfos . fieldNumber ( field ) ; seekTvx ( docNum ) ; long tvdPosition = tvx . readLong ( ) ; tvd . seek ( tvdPosition ) ; int fieldCount = tvd . readVInt ( ) ; int number = 0 ; int found = - 1 ; for ( int i = 0 ; i < fieldCount ; i ++ ) { if ( format >= FORMAT_VERSION ) number = tvd . readVInt ( ) ; else number += tvd . readVInt ( ) ; if ( number == fieldNumber ) found = i ; } if ( found != - 1 ) { long position ; if ( format >= FORMAT_VERSION2 ) position = tvx . readLong ( ) ; else position = tvd . readVLong ( ) ; for ( int i = 1 ; i <= found ; i ++ ) position += tvd . readVLong ( ) ; mapper . setDocumentNumber ( docNum ) ; readTermVector ( field , position , mapper ) ; } else { } } else { } } TermFreqVector get ( int docNum , String field ) throws IOException { ParallelArrayTermVectorMapper mapper = new ParallelArrayTermVectorMapper ( ) ; get ( docNum , field , mapper ) ; return mapper . materializeVector ( ) ; } final private String [ ] readFields ( int fieldCount ) throws IOException { int number = 0 ; String [ ] fields = new String [ fieldCount ] ; for ( int i = 0 ; i < fieldCount ; i ++ ) { if ( format >= FORMAT_VERSION ) number = tvd . readVInt ( ) ; else number += tvd . readVInt ( ) ; fields [ i ] = fieldInfos . fieldName ( number ) ; } return fields ; } final private long [ ] readTvfPointers ( int fieldCount ) throws IOException { long position ; if ( format >= FORMAT_VERSION2 ) position = tvx . readLong ( ) ; else position = tvd . readVLong ( ) ; long [ ] tvfPointers = new long [ fieldCount ] ; tvfPointers [ 0 ] = position ; for ( int i = 1 ; i < fieldCount ; i ++ ) { position += tvd . readVLong ( ) ; tvfPointers [ i ] = position ; } return tvfPointers ; } TermFreqVector [ ] get ( int docNum ) throws IOException { TermFreqVector [ ] result = null ; if ( tvx != null ) { seekTvx ( docNum ) ; long tvdPosition = tvx . readLong ( ) ; tvd . seek ( tvdPosition ) ; int fieldCount = tvd . readVInt ( ) ; if ( fieldCount != 0 ) { final String [ ] fields = readFields ( fieldCount ) ; final long [ ] tvfPointers = readTvfPointers ( fieldCount ) ; result = readTermVectors ( docNum , fields , tvfPointers ) ; } } else { } return result ; } public void get ( int docNumber , TermVectorMapper mapper ) throws IOException { if ( tvx != null ) { seekTvx ( docNumber ) ; long tvdPosition = tvx . readLong ( ) ; tvd . seek ( tvdPosition ) ; int fieldCount = tvd . readVInt ( ) ; if ( fieldCount != 0 ) { final String [ ] fields = readFields ( fieldCount ) ; final long [ ] tvfPointers = readTvfPointers ( fieldCount ) ; mapper . setDocumentNumber ( docNumber ) ; readTermVectors ( fields , tvfPointers , mapper ) ; } } else { } } private SegmentTermVector [ ] readTermVectors ( int docNum , String fields [ ] , long tvfPointers [ ] ) throws IOException { SegmentTermVector res [ ] = new SegmentTermVector [ fields . length ] ; for ( int i = 0 ; i < fields . length ; i ++ ) { ParallelArrayTermVectorMapper mapper = new ParallelArrayTermVectorMapper ( ) ; mapper . setDocumentNumber ( docNum ) ; readTermVector ( fields [ i ] , tvfPointers [ i ] , mapper ) ; res [ i ] = ( SegmentTermVector ) mapper . materializeVector ( ) ; } return res ; } private void readTermVectors ( String fields [ ] , long tvfPointers [ ] , TermVectorMapper mapper ) throws IOException { for ( int i = 0 ; i < fields . length ; i ++ ) { readTermVector ( fields [ i ] , tvfPointers [ i ] , mapper ) ; } } private void readTermVector ( String field , long tvfPointer , TermVectorMapper mapper ) throws IOException { tvf . seek ( tvfPointer ) ; int numTerms = tvf . readVInt ( ) ; if ( numTerms == 0 ) return ; boolean storePositions ; boolean storeOffsets ; if ( format >= FORMAT_VERSION ) { byte bits = tvf . readByte ( ) ; storePositions = ( bits & STORE_POSITIONS_WITH_TERMVECTOR ) != 0 ; storeOffsets = ( bits & STORE_OFFSET_WITH_TERMVECTOR ) != 0 ; } else { tvf . readVInt ( ) ; storePositions = false ; storeOffsets = false ; } mapper . setExpectations ( field , numTerms , storeOffsets , storePositions ) ; int start = 0 ; int deltaLength = 0 ; int totalLength = 0 ; byte [ ] byteBuffer ; char [ ] charBuffer ; final boolean preUTF8 = format < FORMAT_UTF8_LENGTH_IN_BYTES ; if ( preUTF8 ) { charBuffer = new char [ 10 ] ; byteBuffer = null ; } else { charBuffer = null ; byteBuffer = new byte [ 20 ] ; } for ( int i = 0 ; i < numTerms ; i ++ ) { start = tvf . readVInt ( ) ; deltaLength = tvf . readVInt ( ) ; totalLength = start + deltaLength ; final String term ; if ( preUTF8 ) { if ( charBuffer . length < totalLength ) { char [ ] newCharBuffer = new char [ ( int ) ( 1.5 * totalLength ) ] ; System . arraycopy ( charBuffer , 0 , newCharBuffer , 0 , start ) ; charBuffer = newCharBuffer ; } tvf . readChars ( charBuffer , start , deltaLength ) ; term = new String ( charBuffer , 0 , totalLength ) ; } else { if ( byteBuffer . length < totalLength ) { byte [ ] newByteBuffer = new byte [ ( int ) ( 1.5 * totalLength ) ] ; System . arraycopy ( byteBuffer , 0 , newByteBuffer , 0 , start ) ; byteBuffer = newByteBuffer ; } tvf . readBytes ( byteBuffer , start , deltaLength ) ; term = new String ( byteBuffer , 0 , totalLength , "UTF-8" ) ; } int freq = tvf . readVInt ( ) ; int [ ] positions = null ; if ( storePositions ) { if ( mapper . isIgnoringPositions ( ) == false ) { positions = new int [ freq ] ; int prevPosition = 0 ; for ( int j = 0 ; j < freq ; j ++ ) { positions [ j ] = prevPosition + tvf . readVInt ( ) ; prevPosition = positions [ j ] ; } } else { for ( int j = 0 ; j < freq ; j ++ ) { tvf . readVInt ( ) ; } } } TermVectorOffsetInfo [ ] offsets = null ; if ( storeOffsets ) { if ( mapper . isIgnoringOffsets ( ) == false ) { offsets = new TermVectorOffsetInfo [ freq ] ; int prevOffset = 0 ; for ( int j = 0 ; j < freq ; j ++ ) { int startOffset = prevOffset + tvf . readVInt ( ) ; int endOffset = startOffset + tvf . readVInt ( ) ; offsets [ j ] = new TermVectorOffsetInfo ( startOffset , endOffset ) ; prevOffset = endOffset ; } } else { for ( int j = 0 ; j < freq ; j ++ ) { tvf . readVInt ( ) ; tvf . readVInt ( ) ; } } } mapper . map ( term , freq , offsets , positions ) ; } } protected Object clone ( ) throws CloneNotSupportedException { final TermVectorsReader clone = ( TermVectorsReader ) super . clone ( ) ; if ( tvx != null && tvd != null && tvf != null ) { clone . tvx = ( IndexInput ) tvx . clone ( ) ; clone . tvd = ( IndexInput ) tvd . clone ( ) ; clone . tvf = ( IndexInput ) tvf . clone ( ) ; } return clone ; } } class ParallelArrayTermVectorMapper extends TermVectorMapper { private String [ ] terms ; private int [ ] termFreqs ; private int positions [ ] [ ] ; private TermVectorOffsetInfo offsets [ ] [ ] ; private int currentPosition ; private boolean storingOffsets ; private boolean storingPositions ; private String field ; public void setExpectations ( String field , int numTerms , boolean storeOffsets , boolean storePositions ) { this . field = field ; terms = new String [ numTerms ] ; termFreqs = new int [ numTerms ] ; this . storingOffsets = storeOffsets ; this . storingPositions = storePositions ; if ( storePositions ) this . positions = new int [ numTerms ] [ ] ; if ( storeOffsets ) this . offsets = new TermVectorOffsetInfo [ numTerms ] [ ] ; } public void map ( String term , int frequency , TermVectorOffsetInfo [ ] offsets , int [ ] positions ) { terms [ currentPosition ] = term ; termFreqs [ currentPosition ] = frequency ; if ( storingOffsets ) { this . offsets [ currentPosition ] = offsets ; } if ( storingPositions ) { this . positions [ currentPosition ] = positions ; } currentPosition ++ ; } public TermFreqVector materializeVector ( ) { SegmentTermVector tv = null ; if ( field != null && terms != null ) { if ( storingPositions || storingOffsets ) { tv = new SegmentTermPositionVector ( field , terms , termFreqs , positions , offsets ) ; } else { tv = new SegmentTermVector ( field , terms , termFreqs ) ; } } return tv ; } } 	0	['23', '1', '0', '12', '66', '99', '3', '9', '2', '0.847593583', '1164', '0.470588235', '4', '0', '0.231404959', '0', '0', '48.86956522', '2', '0.8696', '0']
package org . apache . lucene . index ; public class TermVectorEntry { private String field ; private String term ; private int frequency ; private TermVectorOffsetInfo [ ] offsets ; int [ ] positions ; public TermVectorEntry ( ) { } public TermVectorEntry ( String field , String term , int frequency , TermVectorOffsetInfo [ ] offsets , int [ ] positions ) { this . field = field ; this . term = term ; this . frequency = frequency ; this . offsets = offsets ; this . positions = positions ; } public String getField ( ) { return field ; } public int getFrequency ( ) { return frequency ; } public TermVectorOffsetInfo [ ] getOffsets ( ) { return offsets ; } public int [ ] getPositions ( ) { return positions ; } public String getTerm ( ) { return term ; } void setFrequency ( int frequency ) { this . frequency = frequency ; } void setOffsets ( TermVectorOffsetInfo [ ] offsets ) { this . offsets = offsets ; } void setPositions ( int [ ] positions ) { this . positions = positions ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( o == null || getClass ( ) != o . getClass ( ) ) return false ; TermVectorEntry that = ( TermVectorEntry ) o ; if ( term != null ? ! term . equals ( that . term ) : that . term != null ) return false ; return true ; } public int hashCode ( ) { return ( term != null ? term . hashCode ( ) : 0 ) ; } public String toString ( ) { return "TermVectorEntry{" + "field='" + field + '\'' + ", term='" + term + '\'' + ", frequency=" + frequency + '}' ; } } 	0	['13', '1', '0', '4', '22', '32', '3', '1', '10', '0.783333333', '135', '0.8', '1', '0', '0.269230769', '1', '1', '9', '7', '1.3846', '0']
package org . apache . lucene . analysis ; public class ISOLatin1AccentFilter extends TokenFilter { public ISOLatin1AccentFilter ( TokenStream input ) { super ( input ) ; } private char [ ] output = new char [ 256 ] ; private int outputPos ; public final Token next ( final Token reusableToken ) throws java . io . IOException { assert reusableToken != null ; Token nextToken = input . next ( reusableToken ) ; if ( nextToken != null ) { final char [ ] buffer = nextToken . termBuffer ( ) ; final int length = nextToken . termLength ( ) ; for ( int i = 0 ; i < length ; i ++ ) { final char c = buffer [ i ] ; if ( c >= '' && c <= '' ) { removeAccents ( buffer , length ) ; nextToken . setTermBuffer ( output , 0 , outputPos ) ; break ; } } return nextToken ; } else return null ; } public final void removeAccents ( char [ ] input , int length ) { final int maxSizeNeeded = 2 * length ; int size = output . length ; while ( size < maxSizeNeeded ) size *= 2 ; if ( size != output . length ) output = new char [ size ] ; outputPos = 0 ; int pos = 0 ; for ( int i = 0 ; i < length ; i ++ , pos ++ ) { final char c = input [ pos ] ; if ( c < '' || c > '' ) output [ outputPos ++ ] = c ; else { switch ( c ) { case '' : case '' : case '' : case '' : case '' : case '' : output [ outputPos ++ ] = 'A' ; break ; case '' : output [ outputPos ++ ] = 'A' ; output [ outputPos ++ ] = 'E' ; break ; case '' : output [ outputPos ++ ] = 'C' ; break ; case '' : case '' : case '' : case '' : output [ outputPos ++ ] = 'E' ; break ; case '' : case '' : case '' : case '' : output [ outputPos ++ ] = 'I' ; break ; case '' : output [ outputPos ++ ] = 'I' ; output [ outputPos ++ ] = 'J' ; break ; case '' : output [ outputPos ++ ] = 'D' ; break ; case '' : output [ outputPos ++ ] = 'N' ; break ; case '' : case '' : case '' : case '' : case '' : case '' : output [ outputPos ++ ] = 'O' ; break ; case '' : output [ outputPos ++ ] = 'O' ; output [ outputPos ++ ] = 'E' ; break ; case '' : output [ outputPos ++ ] = 'T' ; output [ outputPos ++ ] = 'H' ; break ; case '' : case '' : case '' : case '' : output [ outputPos ++ ] = 'U' ; break ; case '' : case '' : output [ outputPos ++ ] = 'Y' ; break ; case '' : case '' : case '' : case '' : case '' : case '' : output [ outputPos ++ ] = 'a' ; break ; case '' : output [ outputPos ++ ] = 'a' ; output [ outputPos ++ ] = 'e' ; break ; case '' : output [ outputPos ++ ] = 'c' ; break ; case '' : case '' : case '' : case '' : output [ outputPos ++ ] = 'e' ; break ; case '' : case '' : case '' : case '' : output [ outputPos ++ ] = 'i' ; break ; case '' : output [ outputPos ++ ] = 'i' ; output [ outputPos ++ ] = 'j' ; break ; case '' : output [ outputPos ++ ] = 'd' ; break ; case '' : output [ outputPos ++ ] = 'n' ; break ; case '' : case '' : case '' : case '' : case '' : case '' : output [ outputPos ++ ] = 'o' ; break ; case '' : output [ outputPos ++ ] = 'o' ; output [ outputPos ++ ] = 'e' ; break ; case '' : output [ outputPos ++ ] = 's' ; output [ outputPos ++ ] = 's' ; break ; case '' : output [ outputPos ++ ] = 't' ; output [ outputPos ++ ] = 'h' ; break ; case '' : case '' : case '' : case '' : output [ outputPos ++ ] = 'u' ; break ; case '' : case '' : output [ outputPos ++ ] = 'y' ; break ; case '' : output [ outputPos ++ ] = 'f' ; output [ outputPos ++ ] = 'f' ; break ; case '' : output [ outputPos ++ ] = 'f' ; output [ outputPos ++ ] = 'i' ; break ; case '' : output [ outputPos ++ ] = 'f' ; output [ outputPos ++ ] = 'l' ; break ; case '' : output [ outputPos ++ ] = 'f' ; output [ outputPos ++ ] = 't' ; break ; case '' : output [ outputPos ++ ] = 's' ; output [ outputPos ++ ] = 't' ; break ; default : output [ outputPos ++ ] = c ; break ; } } } } } 	0	['5', '3', '0', '3', '15', '2', '0', '3', '3', '0.75', '708', '0.5', '0', '0.7', '0.333333333', '1', '2', '139.8', '79', '16.2', '0']
package org . apache . lucene . util . cache ; import java . util . LinkedHashMap ; import java . util . Map ; public class SimpleLRUCache extends SimpleMapCache { private final static float LOADFACTOR = 0.75f ; private int cacheSize ; public SimpleLRUCache ( int cacheSize ) { super ( null ) ; this . cacheSize = cacheSize ; int capacity = ( int ) Math . ceil ( cacheSize / LOADFACTOR ) + 1 ; super . map = new LinkedHashMap ( capacity , LOADFACTOR , true ) { protected boolean removeEldestEntry ( Map . Entry eldest ) { return size ( ) > SimpleLRUCache . this . cacheSize ; } } ; } } 	0	['2', '3', '0', '3', '5', '0', '2', '2', '1', '1', '33', '1', '0', '0.923076923', '0.5', '1', '4', '14.5', '1', '0.5', '0']
package org . apache . lucene . store ; import java . util . ArrayList ; import java . io . Serializable ; class RAMFile implements Serializable { private static final long serialVersionUID = 1l ; private ArrayList buffers = new ArrayList ( ) ; long length ; RAMDirectory directory ; long sizeInBytes ; private long lastModified = System . currentTimeMillis ( ) ; RAMFile ( ) { } RAMFile ( RAMDirectory directory ) { this . directory = directory ; } synchronized long getLength ( ) { return length ; } synchronized void setLength ( long length ) { this . length = length ; } synchronized long getLastModified ( ) { return lastModified ; } synchronized void setLastModified ( long lastModified ) { this . lastModified = lastModified ; } final synchronized byte [ ] addBuffer ( int size ) { byte [ ] buffer = newBuffer ( size ) ; if ( directory != null ) synchronized ( directory ) { buffers . add ( buffer ) ; directory . sizeInBytes += size ; sizeInBytes += size ; } else buffers . add ( buffer ) ; return buffer ; } final synchronized byte [ ] getBuffer ( int index ) { return ( byte [ ] ) buffers . get ( index ) ; } final synchronized int numBuffers ( ) { return buffers . size ( ) ; } byte [ ] newBuffer ( int size ) { return new byte [ size ] ; } long getSizeInBytes ( ) { synchronized ( directory ) { return sizeInBytes ; } } } 	0	['11', '1', '0', '3', '17', '19', '3', '1', '0', '0.833333333', '133', '0.5', '1', '0', '0.386363636', '0', '0', '10.54545455', '2', '0.9091', '0']
package org . apache . lucene . store ; import java . io . IOException ; public class LockObtainFailedException extends IOException { public LockObtainFailedException ( String message ) { super ( message ) ; } } 	0	['1', '4', '0', '8', '2', '0', '8', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import org . apache . lucene . store . BufferedIndexInput ; import org . apache . lucene . store . IndexInput ; abstract class MultiLevelSkipListReader { private int maxNumberOfSkipLevels ; private int numberOfSkipLevels ; private int numberOfLevelsToBuffer = 1 ; private int docCount ; private boolean haveSkipped ; private IndexInput [ ] skipStream ; private long skipPointer [ ] ; private int skipInterval [ ] ; private int [ ] numSkipped ; private int [ ] skipDoc ; private int lastDoc ; private long [ ] childPointer ; private long lastChildPointer ; private boolean inputIsBuffered ; public MultiLevelSkipListReader ( IndexInput skipStream , int maxSkipLevels , int skipInterval ) { this . skipStream = new IndexInput [ maxSkipLevels ] ; this . skipPointer = new long [ maxSkipLevels ] ; this . childPointer = new long [ maxSkipLevels ] ; this . numSkipped = new int [ maxSkipLevels ] ; this . maxNumberOfSkipLevels = maxSkipLevels ; this . skipInterval = new int [ maxSkipLevels ] ; this . skipStream [ 0 ] = skipStream ; this . inputIsBuffered = ( skipStream instanceof BufferedIndexInput ) ; this . skipInterval [ 0 ] = skipInterval ; for ( int i = 1 ; i < maxSkipLevels ; i ++ ) { this . skipInterval [ i ] = this . skipInterval [ i - 1 ] * skipInterval ; } skipDoc = new int [ maxSkipLevels ] ; } int getDoc ( ) { return lastDoc ; } int skipTo ( int target ) throws IOException { if ( ! haveSkipped ) { loadSkipLevels ( ) ; haveSkipped = true ; } int level = 0 ; while ( level < numberOfSkipLevels - 1 && target > skipDoc [ level + 1 ] ) { level ++ ; } while ( level >= 0 ) { if ( target > skipDoc [ level ] ) { if ( ! loadNextSkip ( level ) ) { continue ; } } else { if ( level > 0 && lastChildPointer > skipStream [ level - 1 ] . getFilePointer ( ) ) { seekChild ( level - 1 ) ; } level -- ; } } return numSkipped [ 0 ] - skipInterval [ 0 ] - 1 ; } private boolean loadNextSkip ( int level ) throws IOException { setLastSkipData ( level ) ; numSkipped [ level ] += skipInterval [ level ] ; if ( numSkipped [ level ] > docCount ) { skipDoc [ level ] = Integer . MAX_VALUE ; if ( numberOfSkipLevels > level ) numberOfSkipLevels = level ; return false ; } skipDoc [ level ] += readSkipData ( level , skipStream [ level ] ) ; if ( level != 0 ) { childPointer [ level ] = skipStream [ level ] . readVLong ( ) + skipPointer [ level - 1 ] ; } return true ; } protected void seekChild ( int level ) throws IOException { skipStream [ level ] . seek ( lastChildPointer ) ; numSkipped [ level ] = numSkipped [ level + 1 ] - skipInterval [ level + 1 ] ; skipDoc [ level ] = lastDoc ; if ( level > 0 ) { childPointer [ level ] = skipStream [ level ] . readVLong ( ) + skipPointer [ level - 1 ] ; } } void close ( ) throws IOException { for ( int i = 1 ; i < skipStream . length ; i ++ ) { if ( skipStream [ i ] != null ) { skipStream [ i ] . close ( ) ; } } } void init ( long skipPointer , int df ) { this . skipPointer [ 0 ] = skipPointer ; this . docCount = df ; Arrays . fill ( skipDoc , 0 ) ; Arrays . fill ( numSkipped , 0 ) ; Arrays . fill ( childPointer , 0 ) ; haveSkipped = false ; for ( int i = 1 ; i < numberOfSkipLevels ; i ++ ) { skipStream [ i ] = null ; } } private void loadSkipLevels ( ) throws IOException { numberOfSkipLevels = docCount == 0 ? 0 : ( int ) Math . floor ( Math . log ( docCount ) / Math . log ( skipInterval [ 0 ] ) ) ; if ( numberOfSkipLevels > maxNumberOfSkipLevels ) { numberOfSkipLevels = maxNumberOfSkipLevels ; } skipStream [ 0 ] . seek ( skipPointer [ 0 ] ) ; int toBuffer = numberOfLevelsToBuffer ; for ( int i = numberOfSkipLevels - 1 ; i > 0 ; i -- ) { long length = skipStream [ 0 ] . readVLong ( ) ; skipPointer [ i ] = skipStream [ 0 ] . getFilePointer ( ) ; if ( toBuffer > 0 ) { skipStream [ i ] = new SkipBuffer ( skipStream [ 0 ] , ( int ) length ) ; toBuffer -- ; } else { skipStream [ i ] = ( IndexInput ) skipStream [ 0 ] . clone ( ) ; if ( inputIsBuffered && length < BufferedIndexInput . BUFFER_SIZE ) { ( ( BufferedIndexInput ) skipStream [ i ] ) . setBufferSize ( ( int ) length ) ; } skipStream [ 0 ] . seek ( skipStream [ 0 ] . getFilePointer ( ) + length ) ; } } skipPointer [ 0 ] = skipStream [ 0 ] . getFilePointer ( ) ; } protected abstract int readSkipData ( int level , IndexInput skipStream ) throws IOException ; protected void setLastSkipData ( int level ) { lastDoc = skipDoc [ level ] ; lastChildPointer = childPointer [ level ] ; } private final static class SkipBuffer extends IndexInput { private byte [ ] data ; private long pointer ; private int pos ; SkipBuffer ( IndexInput input , int length ) throws IOException { data = new byte [ length ] ; pointer = input . getFilePointer ( ) ; input . readBytes ( data , 0 , length ) ; } public void close ( ) throws IOException { data = null ; } public long getFilePointer ( ) { return pointer + pos ; } public long length ( ) { return data . length ; } public byte readByte ( ) throws IOException { return data [ pos ++ ] ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { System . arraycopy ( data , pos , b , offset , len ) ; pos += len ; } public void seek ( long pos ) throws IOException { this . pos = ( int ) ( pos - pointer ) ; } } } 	0	['10', '1', '1', '4', '22', '0', '1', '3', '1', '0.611111111', '481', '1', '1', '0', '0.5', '0', '0', '45.7', '2', '1', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Map ; abstract class TermsHashConsumer { abstract int bytesPerPosting ( ) ; abstract void createPostings ( RawPostingList [ ] postings , int start , int count ) ; abstract TermsHashConsumerPerThread addThread ( TermsHashPerThread perThread ) ; abstract void flush ( Map threadsAndFields , final DocumentsWriter . FlushState state ) throws IOException ; abstract void abort ( ) ; abstract void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException ; FieldInfos fieldInfos ; void setFieldInfos ( FieldInfos fieldInfos ) { this . fieldInfos = fieldInfos ; } } 	0	['8', '1', '2', '9', '9', '28', '5', '5', '0', '1', '16', '0', '1', '0', '0.267857143', '0', '0', '0.875', '1', '0.875', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermEnum ; public abstract class FilteredTermEnum extends TermEnum { private Term currentTerm = null ; private TermEnum actualEnum = null ; public FilteredTermEnum ( ) { } protected abstract boolean termCompare ( Term term ) ; public abstract float difference ( ) ; protected abstract boolean endEnum ( ) ; protected void setEnum ( TermEnum actualEnum ) throws IOException { this . actualEnum = actualEnum ; Term term = actualEnum . term ( ) ; if ( term != null && termCompare ( term ) ) currentTerm = term ; else next ( ) ; } public int docFreq ( ) { if ( actualEnum == null ) return - 1 ; return actualEnum . docFreq ( ) ; } public boolean next ( ) throws IOException { if ( actualEnum == null ) return false ; currentTerm = null ; while ( currentTerm == null ) { if ( endEnum ( ) ) return false ; if ( actualEnum . next ( ) ) { Term term = actualEnum . term ( ) ; if ( termCompare ( term ) ) { currentTerm = term ; return true ; } } else return false ; } currentTerm = null ; return false ; } public Term term ( ) { return currentTerm ; } public void close ( ) throws IOException { actualEnum . close ( ) ; currentTerm = null ; actualEnum = null ; } } 	0	['9', '2', '2', '7', '14', '8', '5', '2', '6', '0.5', '103', '1', '2', '0.384615385', '0.407407407', '1', '2', '10.22222222', '2', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; import java . text . Collator ; import java . util . Locale ; class FieldDocSortedHitQueue extends PriorityQueue { volatile SortField [ ] fields ; volatile Collator [ ] collators ; FieldDocSortedHitQueue ( SortField [ ] fields , int size ) { this . fields = fields ; this . collators = hasCollators ( fields ) ; initialize ( size ) ; } synchronized void setFields ( SortField [ ] fields ) { if ( this . fields == null ) { this . fields = fields ; this . collators = hasCollators ( fields ) ; } } SortField [ ] getFields ( ) { return fields ; } private Collator [ ] hasCollators ( final SortField [ ] fields ) { if ( fields == null ) return null ; Collator [ ] ret = new Collator [ fields . length ] ; for ( int i = 0 ; i < fields . length ; ++ i ) { Locale locale = fields [ i ] . getLocale ( ) ; if ( locale != null ) ret [ i ] = Collator . getInstance ( locale ) ; } return ret ; } protected final boolean lessThan ( final Object a , final Object b ) { final FieldDoc docA = ( FieldDoc ) a ; final FieldDoc docB = ( FieldDoc ) b ; final int n = fields . length ; int c = 0 ; for ( int i = 0 ; i < n && c == 0 ; ++ i ) { final int type = fields [ i ] . getType ( ) ; switch ( type ) { case SortField . SCORE : { float r1 = ( ( Float ) docA . fields [ i ] ) . floatValue ( ) ; float r2 = ( ( Float ) docB . fields [ i ] ) . floatValue ( ) ; if ( r1 > r2 ) c = - 1 ; if ( r1 < r2 ) c = 1 ; break ; } case SortField . DOC : case SortField . INT : { int i1 = ( ( Integer ) docA . fields [ i ] ) . intValue ( ) ; int i2 = ( ( Integer ) docB . fields [ i ] ) . intValue ( ) ; if ( i1 < i2 ) c = - 1 ; if ( i1 > i2 ) c = 1 ; break ; } case SortField . LONG : { long l1 = ( ( Long ) docA . fields [ i ] ) . longValue ( ) ; long l2 = ( ( Long ) docB . fields [ i ] ) . longValue ( ) ; if ( l1 < l2 ) c = - 1 ; if ( l1 > l2 ) c = 1 ; break ; } case SortField . STRING : { String s1 = ( String ) docA . fields [ i ] ; String s2 = ( String ) docB . fields [ i ] ; if ( s1 == null ) c = ( s2 == null ) ? 0 : - 1 ; else if ( s2 == null ) c = 1 ; else if ( fields [ i ] . getLocale ( ) == null ) { c = s1 . compareTo ( s2 ) ; } else { c = collators [ i ] . compare ( s1 , s2 ) ; } break ; } case SortField . FLOAT : { float f1 = ( ( Float ) docA . fields [ i ] ) . floatValue ( ) ; float f2 = ( ( Float ) docB . fields [ i ] ) . floatValue ( ) ; if ( f1 < f2 ) c = - 1 ; if ( f1 > f2 ) c = 1 ; break ; } case SortField . DOUBLE : { double d1 = ( ( Double ) docA . fields [ i ] ) . doubleValue ( ) ; double d2 = ( ( Double ) docB . fields [ i ] ) . doubleValue ( ) ; if ( d1 < d2 ) c = - 1 ; if ( d1 > d2 ) c = 1 ; break ; } case SortField . BYTE : { int i1 = ( ( Byte ) docA . fields [ i ] ) . byteValue ( ) ; int i2 = ( ( Byte ) docB . fields [ i ] ) . byteValue ( ) ; if ( i1 < i2 ) c = - 1 ; if ( i1 > i2 ) c = 1 ; break ; } case SortField . SHORT : { int i1 = ( ( Short ) docA . fields [ i ] ) . shortValue ( ) ; int i2 = ( ( Short ) docB . fields [ i ] ) . shortValue ( ) ; if ( i1 < i2 ) c = - 1 ; if ( i1 > i2 ) c = 1 ; break ; } case SortField . CUSTOM : { c = docA . fields [ i ] . compareTo ( docB . fields [ i ] ) ; break ; } case SortField . AUTO : { throw new RuntimeException ( "FieldDocSortedHitQueue cannot use an AUTO SortField" ) ; } default : { throw new RuntimeException ( "invalid SortField type: " + type ) ; } } if ( fields [ i ] . getReverse ( ) ) { c = - c ; } } if ( c == 0 ) return docA . doc > docB . doc ; return c > 0 ; } } 	0	['5', '2', '0', '6', '25', '0', '3', '3', '0', '0.375', '378', '0', '1', '0.75', '0.5', '1', '3', '74.2', '26', '6.6', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . analysis . Token ; final class FreqProxTermsWriterPerField extends TermsHashConsumerPerField implements Comparable { final FreqProxTermsWriterPerThread perThread ; final TermsHashPerField termsHashPerField ; final FieldInfo fieldInfo ; final DocumentsWriter . DocState docState ; final DocInverter . FieldInvertState fieldState ; boolean omitTf ; public FreqProxTermsWriterPerField ( TermsHashPerField termsHashPerField , FreqProxTermsWriterPerThread perThread , FieldInfo fieldInfo ) { this . termsHashPerField = termsHashPerField ; this . perThread = perThread ; this . fieldInfo = fieldInfo ; docState = termsHashPerField . docState ; fieldState = termsHashPerField . fieldState ; omitTf = fieldInfo . omitTf ; } int getStreamCount ( ) { if ( fieldInfo . omitTf ) return 1 ; else return 2 ; } void finish ( ) { } boolean hasPayloads ; void skippingLongTerm ( Token t ) throws IOException { } public int compareTo ( Object other0 ) { FreqProxTermsWriterPerField other = ( FreqProxTermsWriterPerField ) other0 ; return fieldInfo . name . compareTo ( other . fieldInfo . name ) ; } void reset ( ) { omitTf = fieldInfo . omitTf ; } boolean start ( Fieldable [ ] fields , int count ) { for ( int i = 0 ; i < count ; i ++ ) if ( fields [ i ] . isIndexed ( ) ) return true ; return false ; } final void writeProx ( Token t , FreqProxTermsWriter . PostingList p , int proxCode ) { final Payload payload = t . getPayload ( ) ; if ( payload != null && payload . length > 0 ) { termsHashPerField . writeVInt ( 1 , ( proxCode << 1 ) | 1 ) ; termsHashPerField . writeVInt ( 1 , payload . length ) ; termsHashPerField . writeBytes ( 1 , payload . data , payload . offset , payload . length ) ; hasPayloads = true ; } else termsHashPerField . writeVInt ( 1 , proxCode << 1 ) ; p . lastPosition = fieldState . position ; } final void newTerm ( Token t , RawPostingList p0 ) { assert docState . testPoint ( "FreqProxTermsWriterPerField.newTerm start" ) ; FreqProxTermsWriter . PostingList p = ( FreqProxTermsWriter . PostingList ) p0 ; p . lastDocID = docState . docID ; if ( omitTf ) { p . lastDocCode = docState . docID ; } else { p . lastDocCode = docState . docID << 1 ; p . docFreq = 1 ; writeProx ( t , p , fieldState . position ) ; } } final void addTerm ( Token t , RawPostingList p0 ) { assert docState . testPoint ( "FreqProxTermsWriterPerField.addTerm start" ) ; FreqProxTermsWriter . PostingList p = ( FreqProxTermsWriter . PostingList ) p0 ; assert omitTf || p . docFreq > 0 ; if ( omitTf ) { if ( docState . docID != p . lastDocID ) { assert docState . docID > p . lastDocID ; termsHashPerField . writeVInt ( 0 , p . lastDocCode ) ; p . lastDocCode = docState . docID - p . lastDocID ; p . lastDocID = docState . docID ; } } else { if ( docState . docID != p . lastDocID ) { assert docState . docID > p . lastDocID ; if ( 1 == p . docFreq ) termsHashPerField . writeVInt ( 0 , p . lastDocCode | 1 ) ; else { termsHashPerField . writeVInt ( 0 , p . lastDocCode ) ; termsHashPerField . writeVInt ( 0 , p . docFreq ) ; } p . docFreq = 1 ; p . lastDocCode = ( docState . docID - p . lastDocID ) << 1 ; p . lastDocID = docState . docID ; writeProx ( t , p , fieldState . position ) ; } else { p . docFreq ++ ; writeProx ( t , p , fieldState . position - p . lastPosition ) ; } } } public void abort ( ) { } } 	0	['13', '2', '0', '13', '25', '46', '3', '11', '3', '0.833333333', '364', '0', '5', '0.352941176', '0.196969697', '0', '0', '26.30769231', '14', '2.4615', '0']
package org . apache . lucene . util . cache ; public abstract class Cache { static class SynchronizedCache extends Cache { Object mutex ; Cache cache ; SynchronizedCache ( Cache cache ) { this . cache = cache ; this . mutex = this ; } SynchronizedCache ( Cache cache , Object mutex ) { this . cache = cache ; this . mutex = mutex ; } public void put ( Object key , Object value ) { synchronized ( mutex ) { cache . put ( key , value ) ; } } public Object get ( Object key ) { synchronized ( mutex ) { return cache . get ( key ) ; } } public boolean containsKey ( Object key ) { synchronized ( mutex ) { return cache . containsKey ( key ) ; } } public void close ( ) { synchronized ( mutex ) { cache . close ( ) ; } } Cache getSynchronizedCache ( ) { return this ; } } public static Cache synchronizedCache ( Cache cache ) { return cache . getSynchronizedCache ( ) ; } Cache getSynchronizedCache ( ) { return new SynchronizedCache ( this ) ; } public abstract void put ( Object key , Object value ) ; public abstract Object get ( Object key ) ; public abstract boolean containsKey ( Object key ) ; public abstract void close ( ) ; } 	0	['7', '1', '2', '5', '9', '21', '5', '1', '6', '2', '18', '0', '0', '0', '0.476190476', '0', '0', '1.571428571', '1', '0.8571', '0']
package org . apache . lucene . analysis . standard ; import org . apache . lucene . analysis . TokenFilter ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . analysis . TokenStream ; public final class StandardFilter extends TokenFilter { public StandardFilter ( TokenStream in ) { super ( in ) ; } private static final String APOSTROPHE_TYPE = StandardTokenizerImpl . TOKEN_TYPES [ StandardTokenizerImpl . APOSTROPHE ] ; private static final String ACRONYM_TYPE = StandardTokenizerImpl . TOKEN_TYPES [ StandardTokenizerImpl . ACRONYM ] ; public final Token next ( final Token reusableToken ) throws java . io . IOException { assert reusableToken != null ; Token nextToken = input . next ( reusableToken ) ; if ( nextToken == null ) return null ; char [ ] buffer = nextToken . termBuffer ( ) ; final int bufferLength = nextToken . termLength ( ) ; final String type = nextToken . type ( ) ; if ( type == APOSTROPHE_TYPE && bufferLength >= 2 && buffer [ bufferLength - 2 ] == '\'' && ( buffer [ bufferLength - 1 ] == 's' || buffer [ bufferLength - 1 ] == 'S' ) ) { nextToken . setTermLength ( bufferLength - 2 ) ; } else if ( type == ACRONYM_TYPE ) { int upto = 0 ; for ( int i = 0 ; i < bufferLength ; i ++ ) { char c = buffer [ i ] ; if ( c != '.' ) buffer [ upto ++ ] = c ; } nextToken . setTermLength ( upto ) ; } return nextToken ; } } 	0	['4', '3', '0', '5', '15', '4', '1', '4', '2', '0.75', '133', '0.5', '0', '0.777777778', '0.416666667', '1', '2', '31.25', '1', '0.5', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . util . PriorityQueue ; final class SegmentMergeQueue extends PriorityQueue { SegmentMergeQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object a , Object b ) { SegmentMergeInfo stiA = ( SegmentMergeInfo ) a ; SegmentMergeInfo stiB = ( SegmentMergeInfo ) b ; int comparison = stiA . term . compareTo ( stiB . term ) ; if ( comparison == 0 ) return stiA . base < stiB . base ; else return comparison < 0 ; } final void close ( ) throws IOException { while ( top ( ) != null ) ( ( SegmentMergeInfo ) pop ( ) ) . close ( ) ; } } 	0	['3', '2', '0', '5', '9', '3', '2', '3', '0', '2', '47', '0', '0', '0.857142857', '0.555555556', '1', '3', '14.66666667', '4', '1.6667', '0']
package org . apache . lucene . index ; import java . io . Serializable ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . analysis . TokenStream ; import org . apache . lucene . util . ArrayUtil ; public class Payload implements Serializable , Cloneable { protected byte [ ] data ; protected int offset ; protected int length ; public Payload ( ) { } public Payload ( byte [ ] data ) { this ( data , 0 , data . length ) ; } public Payload ( byte [ ] data , int offset , int length ) { if ( offset < 0 || offset + length > data . length ) { throw new IllegalArgumentException ( ) ; } this . data = data ; this . offset = offset ; this . length = length ; } public void setData ( byte [ ] data ) { setData ( data , 0 , data . length ) ; } public void setData ( byte [ ] data , int offset , int length ) { this . data = data ; this . offset = offset ; this . length = length ; } public byte [ ] getData ( ) { return this . data ; } public int getOffset ( ) { return this . offset ; } public int length ( ) { return this . length ; } public byte byteAt ( int index ) { if ( 0 <= index && index < this . length ) { return this . data [ this . offset + index ] ; } throw new ArrayIndexOutOfBoundsException ( index ) ; } public byte [ ] toByteArray ( ) { byte [ ] retArray = new byte [ this . length ] ; System . arraycopy ( this . data , this . offset , retArray , 0 , this . length ) ; return retArray ; } public void copyTo ( byte [ ] target , int targetOffset ) { if ( this . length > target . length + targetOffset ) { throw new ArrayIndexOutOfBoundsException ( ) ; } System . arraycopy ( this . data , this . offset , target , targetOffset , this . length ) ; } public Object clone ( ) { try { Payload clone = ( Payload ) super . clone ( ) ; if ( offset == 0 && length == data . length ) { clone . data = ( byte [ ] ) data . clone ( ) ; } else { clone . data = this . toByteArray ( ) ; clone . offset = 0 ; } return clone ; } catch ( CloneNotSupportedException e ) { throw new RuntimeException ( e ) ; } } public boolean equals ( Object obj ) { if ( obj == this ) return true ; if ( obj instanceof Payload ) { Payload other = ( Payload ) obj ; if ( length == other . length ) { for ( int i = 0 ; i < length ; i ++ ) if ( data [ offset + i ] != other . data [ other . offset + i ] ) return false ; return true ; } else return false ; } else return false ; } public int hashCode ( ) { return ArrayUtil . hashCode ( data , offset , offset + length ) ; } } 	0	['14', '1', '0', '4', '22', '0', '3', '1', '14', '0.230769231', '227', '1', '0', '0', '0.428571429', '1', '1', '15', '6', '1.5', '0']
package org . apache . lucene . store ; import java . net . Socket ; import java . io . IOException ; import java . io . InputStream ; import java . io . OutputStream ; public class VerifyingLockFactory extends LockFactory { LockFactory lf ; byte id ; String host ; int port ; private class CheckedLock extends Lock { private Lock lock ; public CheckedLock ( Lock lock ) { this . lock = lock ; } private void verify ( byte message ) { try { Socket s = new Socket ( host , port ) ; OutputStream out = s . getOutputStream ( ) ; out . write ( id ) ; out . write ( message ) ; InputStream in = s . getInputStream ( ) ; int result = in . read ( ) ; in . close ( ) ; out . close ( ) ; s . close ( ) ; if ( result != 0 ) throw new RuntimeException ( "lock was double acquired" ) ; } catch ( Exception e ) { throw new RuntimeException ( e ) ; } } public synchronized boolean obtain ( long lockWaitTimeout ) throws LockObtainFailedException , IOException { boolean obtained = lock . obtain ( lockWaitTimeout ) ; if ( obtained ) verify ( ( byte ) 1 ) ; return obtained ; } public synchronized boolean obtain ( ) throws LockObtainFailedException , IOException { return lock . obtain ( ) ; } public synchronized boolean isLocked ( ) { return lock . isLocked ( ) ; } public synchronized void release ( ) throws IOException { if ( isLocked ( ) ) { verify ( ( byte ) 0 ) ; lock . release ( ) ; } } } public VerifyingLockFactory ( byte id , LockFactory lf , String host , int port ) throws IOException { this . id = id ; this . lf = lf ; this . host = host ; this . port = port ; } public synchronized Lock makeLock ( String lockName ) { return new CheckedLock ( lf . makeLock ( lockName ) ) ; } public synchronized void clearLock ( String lockName ) throws IOException { lf . clearLock ( lockName ) ; } } 	0	['3', '2', '0', '4', '7', '0', '2', '3', '3', '0.75', '36', '0', '1', '0.666666667', '0.6', '0', '0', '9.666666667', '1', '0.6667', '0']
package org . apache . lucene . index ; final class FreqProxTermsWriterPerThread extends TermsHashConsumerPerThread { final TermsHashPerThread termsHashPerThread ; final DocumentsWriter . DocState docState ; public FreqProxTermsWriterPerThread ( TermsHashPerThread perThread ) { docState = perThread . docState ; termsHashPerThread = perThread ; } public TermsHashConsumerPerField addField ( TermsHashPerField termsHashPerField , FieldInfo fieldInfo ) { return new FreqProxTermsWriterPerField ( termsHashPerField , this , fieldInfo ) ; } void startDocument ( ) { } DocumentsWriter . DocWriter finishDocument ( ) { return null ; } public void abort ( ) { } } 	0	['5', '2', '0', '10', '7', '10', '3', '8', '3', '1', '28', '0', '2', '0.5', '0.4', '0', '0', '4.2', '1', '0.8', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . util . UnicodeUtil ; final class TermsHashPerField extends InvertedDocConsumerPerField { final TermsHashConsumerPerField consumer ; final TermsHashPerField nextPerField ; final TermsHashPerThread perThread ; final DocumentsWriter . DocState docState ; final DocInverter . FieldInvertState fieldState ; final CharBlockPool charPool ; final IntBlockPool intPool ; final ByteBlockPool bytePool ; final int streamCount ; final int numPostingInt ; final FieldInfo fieldInfo ; boolean postingsCompacted ; int numPostings ; private int postingsHashSize = 4 ; private int postingsHashHalfSize = postingsHashSize / 2 ; private int postingsHashMask = postingsHashSize - 1 ; private RawPostingList [ ] postingsHash = new RawPostingList [ postingsHashSize ] ; private RawPostingList p ; public TermsHashPerField ( DocInverterPerField docInverterPerField , final TermsHashPerThread perThread , final TermsHashPerThread nextPerThread , final FieldInfo fieldInfo ) { this . perThread = perThread ; intPool = perThread . intPool ; charPool = perThread . charPool ; bytePool = perThread . bytePool ; docState = perThread . docState ; fieldState = docInverterPerField . fieldState ; this . consumer = perThread . consumer . addField ( this , fieldInfo ) ; streamCount = consumer . getStreamCount ( ) ; numPostingInt = 2 * streamCount ; this . fieldInfo = fieldInfo ; if ( nextPerThread != null ) nextPerField = ( TermsHashPerField ) nextPerThread . addField ( docInverterPerField , fieldInfo ) ; else nextPerField = null ; } void shrinkHash ( int targetSize ) { assert postingsCompacted || numPostings == 0 ; int newSize = postingsHash . length ; while ( newSize >= 8 && newSize / 4 > targetSize ) { newSize /= 2 ; } if ( newSize != postingsHash . length ) { postingsHash = new RawPostingList [ newSize ] ; postingsHashSize = newSize ; postingsHashHalfSize = newSize / 2 ; postingsHashMask = newSize - 1 ; } } public void reset ( ) { if ( ! postingsCompacted ) compactPostings ( ) ; assert numPostings <= postingsHash . length ; if ( numPostings > 0 ) { perThread . termsHash . recyclePostings ( postingsHash , numPostings ) ; Arrays . fill ( postingsHash , 0 , numPostings , null ) ; numPostings = 0 ; } postingsCompacted = false ; if ( nextPerField != null ) nextPerField . reset ( ) ; } synchronized public void abort ( ) { reset ( ) ; if ( nextPerField != null ) nextPerField . abort ( ) ; } public void initReader ( ByteSliceReader reader , RawPostingList p , int stream ) { assert stream < streamCount ; final int [ ] ints = intPool . buffers [ p . intStart > > DocumentsWriter . INT_BLOCK_SHIFT ] ; final int upto = p . intStart & DocumentsWriter . INT_BLOCK_MASK ; reader . init ( bytePool , p . byteStart + stream * ByteBlockPool . FIRST_LEVEL_SIZE , ints [ upto + stream ] ) ; } private synchronized void compactPostings ( ) { int upto = 0 ; for ( int i = 0 ; i < postingsHashSize ; i ++ ) { if ( postingsHash [ i ] != null ) { if ( upto < i ) { postingsHash [ upto ] = postingsHash [ i ] ; postingsHash [ i ] = null ; } upto ++ ; } } assert upto == numPostings ; postingsCompacted = true ; } public RawPostingList [ ] sortPostings ( ) { compactPostings ( ) ; quickSort ( postingsHash , 0 , numPostings - 1 ) ; return postingsHash ; } void quickSort ( RawPostingList [ ] postings , int lo , int hi ) { if ( lo >= hi ) return ; else if ( hi == 1 + lo ) { if ( comparePostings ( postings [ lo ] , postings [ hi ] ) > 0 ) { final RawPostingList tmp = postings [ lo ] ; postings [ lo ] = postings [ hi ] ; postings [ hi ] = tmp ; } return ; } int mid = ( lo + hi ) > > > 1 ; if ( comparePostings ( postings [ lo ] , postings [ mid ] ) > 0 ) { RawPostingList tmp = postings [ lo ] ; postings [ lo ] = postings [ mid ] ; postings [ mid ] = tmp ; } if ( comparePostings ( postings [ mid ] , postings [ hi ] ) > 0 ) { RawPostingList tmp = postings [ mid ] ; postings [ mid ] = postings [ hi ] ; postings [ hi ] = tmp ; if ( comparePostings ( postings [ lo ] , postings [ mid ] ) > 0 ) { RawPostingList tmp2 = postings [ lo ] ; postings [ lo ] = postings [ mid ] ; postings [ mid ] = tmp2 ; } } int left = lo + 1 ; int right = hi - 1 ; if ( left >= right ) return ; RawPostingList partition = postings [ mid ] ; for ( ; ; ) { while ( comparePostings ( postings [ right ] , partition ) > 0 ) -- right ; while ( left < right && comparePostings ( postings [ left ] , partition ) <= 0 ) ++ left ; if ( left < right ) { RawPostingList tmp = postings [ left ] ; postings [ left ] = postings [ right ] ; postings [ right ] = tmp ; -- right ; } else { break ; } } quickSort ( postings , lo , left ) ; quickSort ( postings , left + 1 , hi ) ; } int comparePostings ( RawPostingList p1 , RawPostingList p2 ) { if ( p1 == p2 ) return 0 ; final char [ ] text1 = charPool . buffers [ p1 . textStart > > DocumentsWriter . CHAR_BLOCK_SHIFT ] ; int pos1 = p1 . textStart & DocumentsWriter . CHAR_BLOCK_MASK ; final char [ ] text2 = charPool . buffers [ p2 . textStart > > DocumentsWriter . CHAR_BLOCK_SHIFT ] ; int pos2 = p2 . textStart & DocumentsWriter . CHAR_BLOCK_MASK ; assert text1 != text2 || pos1 != pos2 ; while ( true ) { final char c1 = text1 [ pos1 ++ ] ; final char c2 = text2 [ pos2 ++ ] ; if ( c1 != c2 ) { if ( 0xffff == c2 ) return 1 ; else if ( 0xffff == c1 ) return - 1 ; else return c1 - c2 ; } else assert c1 != 0xffff ; } } private boolean postingEquals ( final char [ ] tokenText , final int tokenTextLen ) { final char [ ] text = perThread . charPool . buffers [ p . textStart > > DocumentsWriter . CHAR_BLOCK_SHIFT ] ; assert text != null ; int pos = p . textStart & DocumentsWriter . CHAR_BLOCK_MASK ; int tokenPos = 0 ; for ( ; tokenPos < tokenTextLen ; pos ++ , tokenPos ++ ) if ( tokenText [ tokenPos ] != text [ pos ] ) return false ; return 0xffff == text [ pos ] ; } private boolean doCall ; private boolean doNextCall ; boolean start ( Fieldable [ ] fields , int count ) throws IOException { doCall = consumer . start ( fields , count ) ; if ( nextPerField != null ) doNextCall = nextPerField . start ( fields , count ) ; return doCall || doNextCall ; } public void add ( Token token , int textStart ) throws IOException { int code = textStart ; int hashPos = code & postingsHashMask ; assert ! postingsCompacted ; p = postingsHash [ hashPos ] ; if ( p != null && p . textStart != textStart ) { final int inc = ( ( code > > 8 ) + code ) | 1 ; do { code += inc ; hashPos = code & postingsHashMask ; p = postingsHash [ hashPos ] ; } while ( p != null && p . textStart != textStart ) ; } if ( p == null ) { if ( 0 == perThread . freePostingsCount ) perThread . morePostings ( ) ; p = perThread . freePostings [ -- perThread . freePostingsCount ] ; assert p != null ; p . textStart = textStart ; assert postingsHash [ hashPos ] == null ; postingsHash [ hashPos ] = p ; numPostings ++ ; if ( numPostings == postingsHashHalfSize ) rehashPostings ( 2 * postingsHashSize ) ; if ( numPostingInt + intPool . intUpto > DocumentsWriter . INT_BLOCK_SIZE ) intPool . nextBuffer ( ) ; if ( DocumentsWriter . BYTE_BLOCK_SIZE - bytePool . byteUpto < numPostingInt * ByteBlockPool . FIRST_LEVEL_SIZE ) bytePool . nextBuffer ( ) ; intUptos = intPool . buffer ; intUptoStart = intPool . intUpto ; intPool . intUpto += streamCount ; p . intStart = intUptoStart + intPool . intOffset ; for ( int i = 0 ; i < streamCount ; i ++ ) { final int upto = bytePool . newSlice ( ByteBlockPool . FIRST_LEVEL_SIZE ) ; intUptos [ intUptoStart + i ] = upto + bytePool . byteOffset ; } p . byteStart = intUptos [ intUptoStart ] ; consumer . newTerm ( token , p ) ; } else { intUptos = intPool . buffers [ p . intStart > > DocumentsWriter . INT_BLOCK_SHIFT ] ; intUptoStart = p . intStart & DocumentsWriter . INT_BLOCK_MASK ; consumer . addTerm ( token , p ) ; } } void add ( Token token ) throws IOException { assert ! postingsCompacted ; final char [ ] tokenText = token . termBuffer ( ) ; final int tokenTextLen = token . termLength ( ) ; int downto = tokenTextLen ; int code = 0 ; while ( downto > 0 ) { char ch = tokenText [ -- downto ] ; if ( ch >= UnicodeUtil . UNI_SUR_LOW_START && ch <= UnicodeUtil . UNI_SUR_LOW_END ) { if ( 0 == downto ) { ch = tokenText [ downto ] = UnicodeUtil . UNI_REPLACEMENT_CHAR ; } else { final char ch2 = tokenText [ downto - 1 ] ; if ( ch2 >= UnicodeUtil . UNI_SUR_HIGH_START && ch2 <= UnicodeUtil . UNI_SUR_HIGH_END ) { code = ( ( code * 31 ) + ch ) * 31 + ch2 ; downto -- ; continue ; } else { ch = tokenText [ downto ] = UnicodeUtil . UNI_REPLACEMENT_CHAR ; } } } else if ( ch >= UnicodeUtil . UNI_SUR_HIGH_START && ch <= UnicodeUtil . UNI_SUR_HIGH_END ) ch = tokenText [ downto ] = UnicodeUtil . UNI_REPLACEMENT_CHAR ; code = ( code * 31 ) + ch ; } int hashPos = code & postingsHashMask ; p = postingsHash [ hashPos ] ; if ( p != null && ! postingEquals ( tokenText , tokenTextLen ) ) { final int inc = ( ( code > > 8 ) + code ) | 1 ; do { code += inc ; hashPos = code & postingsHashMask ; p = postingsHash [ hashPos ] ; } while ( p != null && ! postingEquals ( tokenText , tokenTextLen ) ) ; } if ( p == null ) { final int textLen1 = 1 + tokenTextLen ; if ( textLen1 + charPool . charUpto > DocumentsWriter . CHAR_BLOCK_SIZE ) { if ( textLen1 > DocumentsWriter . CHAR_BLOCK_SIZE ) { if ( docState . maxTermPrefix == null ) docState . maxTermPrefix = new String ( tokenText , 0 , 30 ) ; consumer . skippingLongTerm ( token ) ; return ; } charPool . nextBuffer ( ) ; } if ( 0 == perThread . freePostingsCount ) perThread . morePostings ( ) ; p = perThread . freePostings [ -- perThread . freePostingsCount ] ; assert p != null ; final char [ ] text = charPool . buffer ; final int textUpto = charPool . charUpto ; p . textStart = textUpto + charPool . charOffset ; charPool . charUpto += textLen1 ; System . arraycopy ( tokenText , 0 , text , textUpto , tokenTextLen ) ; text [ textUpto + tokenTextLen ] = 0xffff ; assert postingsHash [ hashPos ] == null ; postingsHash [ hashPos ] = p ; numPostings ++ ; if ( numPostings == postingsHashHalfSize ) rehashPostings ( 2 * postingsHashSize ) ; if ( numPostingInt + intPool . intUpto > DocumentsWriter . INT_BLOCK_SIZE ) intPool . nextBuffer ( ) ; if ( DocumentsWriter . BYTE_BLOCK_SIZE - bytePool . byteUpto < numPostingInt * ByteBlockPool . FIRST_LEVEL_SIZE ) bytePool . nextBuffer ( ) ; intUptos = intPool . buffer ; intUptoStart = intPool . intUpto ; intPool . intUpto += streamCount ; p . intStart = intUptoStart + intPool . intOffset ; for ( int i = 0 ; i < streamCount ; i ++ ) { final int upto = bytePool . newSlice ( ByteBlockPool . FIRST_LEVEL_SIZE ) ; intUptos [ intUptoStart + i ] = upto + bytePool . byteOffset ; } p . byteStart = intUptos [ intUptoStart ] ; consumer . newTerm ( token , p ) ; } else { intUptos = intPool . buffers [ p . intStart > > DocumentsWriter . INT_BLOCK_SHIFT ] ; intUptoStart = p . intStart & DocumentsWriter . INT_BLOCK_MASK ; consumer . addTerm ( token , p ) ; } if ( doNextCall ) nextPerField . add ( token , p . textStart ) ; } int [ ] intUptos ; int intUptoStart ; void writeByte ( int stream , byte b ) { int upto = intUptos [ intUptoStart + stream ] ; byte [ ] bytes = bytePool . buffers [ upto > > DocumentsWriter . BYTE_BLOCK_SHIFT ] ; assert bytes != null ; int offset = upto & DocumentsWriter . BYTE_BLOCK_MASK ; if ( bytes [ offset ] != 0 ) { offset = bytePool . allocSlice ( bytes , offset ) ; bytes = bytePool . buffer ; intUptos [ intUptoStart + stream ] = offset + bytePool . byteOffset ; } bytes [ offset ] = b ; ( intUptos [ intUptoStart + stream ] ) ++ ; } public void writeBytes ( int stream , byte [ ] b , int offset , int len ) { final int end = offset + len ; for ( int i = offset ; i < end ; i ++ ) writeByte ( stream , b [ i ] ) ; } void writeVInt ( int stream , int i ) { assert stream < streamCount ; while ( ( i & ~ 0x7F ) != 0 ) { writeByte ( stream , ( byte ) ( ( i & 0x7f ) | 0x80 ) ) ; i >>>= 7 ; } writeByte ( stream , ( byte ) i ) ; } void finish ( ) throws IOException { consumer . finish ( ) ; if ( nextPerField != null ) nextPerField . finish ( ) ; } void rehashPostings ( final int newSize ) { final int newMask = newSize - 1 ; RawPostingList [ ] newHash = new RawPostingList [ newSize ] ; for ( int i = 0 ; i < postingsHashSize ; i ++ ) { RawPostingList p0 = postingsHash [ i ] ; if ( p0 != null ) { int code ; if ( perThread . primary ) { final int start = p0 . textStart & DocumentsWriter . CHAR_BLOCK_MASK ; final char [ ] text = charPool . buffers [ p0 . textStart > > DocumentsWriter . CHAR_BLOCK_SHIFT ] ; int pos = start ; while ( text [ pos ] != 0xffff ) pos ++ ; code = 0 ; while ( pos > start ) code = ( code * 31 ) + text [ -- pos ] ; } else code = p0 . textStart ; int hashPos = code & newMask ; assert hashPos >= 0 ; if ( newHash [ hashPos ] != null ) { final int inc = ( ( code > > 8 ) + code ) | 1 ; do { code += inc ; hashPos = code & newMask ; } while ( newHash [ hashPos ] != null ) ; } newHash [ hashPos ] = p0 ; } } postingsHashMask = newMask ; postingsHash = newHash ; postingsHashSize = newSize ; postingsHashHalfSize = newSize > > 1 ; } } 	0	['20', '2', '0', '23', '47', '0', '10', '16', '7', '0.785087719', '1605', '0.291666667', '11', '0.181818182', '0.157894737', '0', '0', '78.05', '12', '3.75', '0']
package org . apache . lucene . index ; import java . io . IOException ; public class StaleReaderException extends IOException { public StaleReaderException ( String message ) { super ( message ) ; } } 	0	['1', '4', '0', '3', '2', '0', '3', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Collection ; import java . util . Iterator ; import java . util . HashMap ; import java . util . Map ; import java . util . List ; import java . util . ArrayList ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . search . Similarity ; final class NormsWriter extends InvertedDocEndConsumer { private static final byte defaultNorm = Similarity . encodeNorm ( 1.0f ) ; private FieldInfos fieldInfos ; public InvertedDocEndConsumerPerThread addThread ( DocInverterPerThread docInverterPerThread ) { return new NormsWriterPerThread ( docInverterPerThread , this ) ; } public void abort ( ) { } void files ( Collection files ) { } void setFieldInfos ( FieldInfos fieldInfos ) { this . fieldInfos = fieldInfos ; } public void flush ( Map threadsAndFields , DocumentsWriter . FlushState state ) throws IOException { final Map byField = new HashMap ( ) ; final Iterator it = threadsAndFields . entrySet ( ) . iterator ( ) ; while ( it . hasNext ( ) ) { Map . Entry entry = ( Map . Entry ) it . next ( ) ; Collection fields = ( Collection ) entry . getValue ( ) ; Iterator fieldsIt = fields . iterator ( ) ; while ( fieldsIt . hasNext ( ) ) { NormsWriterPerField perField = ( NormsWriterPerField ) fieldsIt . next ( ) ; if ( perField . upto > 0 ) { List l = ( List ) byField . get ( perField . fieldInfo ) ; if ( l == null ) { l = new ArrayList ( ) ; byField . put ( perField . fieldInfo , l ) ; } l . add ( perField ) ; } else fieldsIt . remove ( ) ; } } final String normsFileName = state . segmentName + "." + IndexFileNames . NORMS_EXTENSION ; state . flushedFiles . add ( normsFileName ) ; IndexOutput normsOut = state . directory . createOutput ( normsFileName ) ; try { normsOut . writeBytes ( SegmentMerger . NORMS_HEADER , 0 , SegmentMerger . NORMS_HEADER . length ) ; final int numField = fieldInfos . size ( ) ; int normCount = 0 ; for ( int fieldNumber = 0 ; fieldNumber < numField ; fieldNumber ++ ) { final FieldInfo fieldInfo = fieldInfos . fieldInfo ( fieldNumber ) ; List toMerge = ( List ) byField . get ( fieldInfo ) ; int upto = 0 ; if ( toMerge != null ) { final int numFields = toMerge . size ( ) ; normCount ++ ; final NormsWriterPerField [ ] fields = new NormsWriterPerField [ numFields ] ; int [ ] uptos = new int [ numFields ] ; for ( int j = 0 ; j < numFields ; j ++ ) fields [ j ] = ( NormsWriterPerField ) toMerge . get ( j ) ; int numLeft = numFields ; while ( numLeft > 0 ) { assert uptos [ 0 ] < fields [ 0 ] . docIDs . length : " uptos[0]=" + uptos [ 0 ] + " len=" + ( fields [ 0 ] . docIDs . length ) ; int minLoc = 0 ; int minDocID = fields [ 0 ] . docIDs [ uptos [ 0 ] ] ; for ( int j = 1 ; j < numLeft ; j ++ ) { final int docID = fields [ j ] . docIDs [ uptos [ j ] ] ; if ( docID < minDocID ) { minDocID = docID ; minLoc = j ; } } assert minDocID < state . numDocsInRAM ; for ( ; upto < minDocID ; upto ++ ) normsOut . writeByte ( defaultNorm ) ; normsOut . writeByte ( fields [ minLoc ] . norms [ uptos [ minLoc ] ] ) ; ( uptos [ minLoc ] ) ++ ; upto ++ ; if ( uptos [ minLoc ] == fields [ minLoc ] . upto ) { fields [ minLoc ] . reset ( ) ; if ( minLoc != numLeft - 1 ) { fields [ minLoc ] = fields [ numLeft - 1 ] ; uptos [ minLoc ] = uptos [ numLeft - 1 ] ; } numLeft -- ; } } for ( ; upto < state . numDocsInRAM ; upto ++ ) normsOut . writeByte ( defaultNorm ) ; } else if ( fieldInfo . isIndexed && ! fieldInfo . omitNorms ) { normCount ++ ; for ( ; upto < state . numDocsInRAM ; upto ++ ) normsOut . writeByte ( defaultNorm ) ; } assert 4 + normCount * state . numDocsInRAM == normsOut . getFilePointer ( ) : ".nrm file size mismatch: expected=" + ( 4 + normCount * state . numDocsInRAM ) + " actual=" + normsOut . getFilePointer ( ) ; } } finally { normsOut . close ( ) ; } } void closeDocStore ( DocumentsWriter . FlushState state ) { } } 	0	['9', '2', '0', '13', '46', '32', '2', '12', '3', '0.90625', '409', '0.5', '1', '0.416666667', '0.25', '0', '0', '44', '1', '0.7778', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import org . apache . lucene . store . IndexOutput ; class DefaultSkipListWriter extends MultiLevelSkipListWriter { private int [ ] lastSkipDoc ; private int [ ] lastSkipPayloadLength ; private long [ ] lastSkipFreqPointer ; private long [ ] lastSkipProxPointer ; private IndexOutput freqOutput ; private IndexOutput proxOutput ; private int curDoc ; private boolean curStorePayloads ; private int curPayloadLength ; private long curFreqPointer ; private long curProxPointer ; DefaultSkipListWriter ( int skipInterval , int numberOfSkipLevels , int docCount , IndexOutput freqOutput , IndexOutput proxOutput ) { super ( skipInterval , numberOfSkipLevels , docCount ) ; this . freqOutput = freqOutput ; this . proxOutput = proxOutput ; lastSkipDoc = new int [ numberOfSkipLevels ] ; lastSkipPayloadLength = new int [ numberOfSkipLevels ] ; lastSkipFreqPointer = new long [ numberOfSkipLevels ] ; lastSkipProxPointer = new long [ numberOfSkipLevels ] ; } void setSkipData ( int doc , boolean storePayloads , int payloadLength ) { this . curDoc = doc ; this . curStorePayloads = storePayloads ; this . curPayloadLength = payloadLength ; this . curFreqPointer = freqOutput . getFilePointer ( ) ; if ( proxOutput != null ) this . curProxPointer = proxOutput . getFilePointer ( ) ; } protected void resetSkip ( ) { super . resetSkip ( ) ; Arrays . fill ( lastSkipDoc , 0 ) ; Arrays . fill ( lastSkipPayloadLength , - 1 ) ; Arrays . fill ( lastSkipFreqPointer , freqOutput . getFilePointer ( ) ) ; if ( proxOutput != null ) Arrays . fill ( lastSkipProxPointer , proxOutput . getFilePointer ( ) ) ; } protected void writeSkipData ( int level , IndexOutput skipBuffer ) throws IOException { if ( curStorePayloads ) { int delta = curDoc - lastSkipDoc [ level ] ; if ( curPayloadLength == lastSkipPayloadLength [ level ] ) { skipBuffer . writeVInt ( delta * 2 ) ; } else { skipBuffer . writeVInt ( delta * 2 + 1 ) ; skipBuffer . writeVInt ( curPayloadLength ) ; lastSkipPayloadLength [ level ] = curPayloadLength ; } } else { skipBuffer . writeVInt ( curDoc - lastSkipDoc [ level ] ) ; } skipBuffer . writeVInt ( ( int ) ( curFreqPointer - lastSkipFreqPointer [ level ] ) ) ; skipBuffer . writeVInt ( ( int ) ( curProxPointer - lastSkipProxPointer [ level ] ) ) ; lastSkipDoc [ level ] = curDoc ; lastSkipFreqPointer [ level ] = curFreqPointer ; lastSkipProxPointer [ level ] = curProxPointer ; } } 	0	['4', '2', '0', '4', '10', '0', '2', '2', '0', '0.484848485', '182', '1', '2', '0.625', '0.625', '1', '1', '41.75', '2', '1.25', '0']
package org . apache . lucene . util . cache ; import java . util . HashMap ; import java . util . Map ; import java . util . Set ; public class SimpleMapCache extends Cache { Map map ; public SimpleMapCache ( ) { this ( new HashMap ( ) ) ; } public SimpleMapCache ( Map map ) { this . map = map ; } public Object get ( Object key ) { return map . get ( key ) ; } public void put ( Object key , Object value ) { map . put ( key , value ) ; } public void close ( ) { } public boolean containsKey ( Object key ) { return map . containsKey ( key ) ; } public Set keySet ( ) { return map . keySet ( ) ; } Cache getSynchronizedCache ( ) { return new SynchronizedSimpleMapCache ( this ) ; } private static class SynchronizedSimpleMapCache extends SimpleMapCache { Object mutex ; SimpleMapCache cache ; SynchronizedSimpleMapCache ( SimpleMapCache cache ) { this . cache = cache ; this . mutex = this ; } public void put ( Object key , Object value ) { synchronized ( mutex ) { cache . put ( key , value ) ; } } public Object get ( Object key ) { synchronized ( mutex ) { return cache . get ( key ) ; } } public boolean containsKey ( Object key ) { synchronized ( mutex ) { return cache . containsKey ( key ) ; } } public void close ( ) { synchronized ( mutex ) { cache . close ( ) ; } } public Set keySet ( ) { synchronized ( mutex ) { return cache . keySet ( ) ; } } Cache getSynchronizedCache ( ) { return this ; } } } 	0	['8', '2', '2', '3', '15', '8', '2', '2', '7', '0.285714286', '48', '0', '0', '0.5', '0.5', '1', '1', '4.875', '1', '0.75', '0']
package org . apache . lucene . search ; import org . apache . lucene . search . Filter ; import org . apache . lucene . util . OpenBitSet ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . TermEnum ; import org . apache . lucene . index . TermDocs ; import java . util . BitSet ; import java . io . IOException ; public class PrefixFilter extends Filter { protected final Term prefix ; public PrefixFilter ( Term prefix ) { this . prefix = prefix ; } public Term getPrefix ( ) { return prefix ; } public BitSet bits ( IndexReader reader ) throws IOException { final BitSet bitSet = new BitSet ( reader . maxDoc ( ) ) ; new PrefixGenerator ( prefix ) { public void handleDoc ( int doc ) { bitSet . set ( doc ) ; } } . generate ( reader ) ; return bitSet ; } public DocIdSet getDocIdSet ( IndexReader reader ) throws IOException { final OpenBitSet bitSet = new OpenBitSet ( reader . maxDoc ( ) ) ; new PrefixGenerator ( prefix ) { public void handleDoc ( int doc ) { bitSet . set ( doc ) ; } } . generate ( reader ) ; return bitSet ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "PrefixFilter(" ) ; buffer . append ( prefix . toString ( ) ) ; buffer . append ( ")" ) ; return buffer . toString ( ) ; } } interface IdGenerator { public void generate ( IndexReader reader ) throws IOException ; public void handleDoc ( int doc ) ; } abstract class PrefixGenerator implements IdGenerator { protected final Term prefix ; PrefixGenerator ( Term prefix ) { this . prefix = prefix ; } public void generate ( IndexReader reader ) throws IOException { TermEnum enumerator = reader . terms ( prefix ) ; TermDocs termDocs = reader . termDocs ( ) ; try { String prefixText = prefix . text ( ) ; String prefixField = prefix . field ( ) ; do { Term term = enumerator . term ( ) ; if ( term != null && term . text ( ) . startsWith ( prefixText ) && term . field ( ) == prefixField ) { termDocs . seek ( term ) ; while ( termDocs . next ( ) ) { handleDoc ( termDocs . doc ( ) ) ; } } else { break ; } } while ( enumerator . next ( ) ) ; } finally { termDocs . close ( ) ; enumerator . close ( ) ; } } } 	0	['5', '2', '0', '7', '17', '0', '2', '7', '5', '0', '71', '1', '1', '0.333333333', '0.533333333', '1', '1', '13', '1', '0.8', '0']
package org . apache . lucene . index ; import java . util . * ; public class SortedTermVectorMapper extends TermVectorMapper { private SortedSet currentSet ; private Map termToTVE = new HashMap ( ) ; private boolean storeOffsets ; private boolean storePositions ; public static final String ALL = "_ALL_" ; public SortedTermVectorMapper ( Comparator comparator ) { this ( false , false , comparator ) ; } public SortedTermVectorMapper ( boolean ignoringPositions , boolean ignoringOffsets , Comparator comparator ) { super ( ignoringPositions , ignoringOffsets ) ; currentSet = new TreeSet ( comparator ) ; } public void map ( String term , int frequency , TermVectorOffsetInfo [ ] offsets , int [ ] positions ) { TermVectorEntry entry = ( TermVectorEntry ) termToTVE . get ( term ) ; if ( entry == null ) { entry = new TermVectorEntry ( ALL , term , frequency , storeOffsets == true ? offsets : null , storePositions == true ? positions : null ) ; termToTVE . put ( term , entry ) ; currentSet . add ( entry ) ; } else { entry . setFrequency ( entry . getFrequency ( ) + frequency ) ; if ( storeOffsets ) { TermVectorOffsetInfo [ ] existingOffsets = entry . getOffsets ( ) ; if ( existingOffsets != null && offsets != null && offsets . length > 0 ) { TermVectorOffsetInfo [ ] newOffsets = new TermVectorOffsetInfo [ existingOffsets . length + offsets . length ] ; System . arraycopy ( existingOffsets , 0 , newOffsets , 0 , existingOffsets . length ) ; System . arraycopy ( offsets , 0 , newOffsets , existingOffsets . length , offsets . length ) ; entry . setOffsets ( newOffsets ) ; } else if ( existingOffsets == null && offsets != null && offsets . length > 0 ) { entry . setOffsets ( offsets ) ; } } if ( storePositions ) { int [ ] existingPositions = entry . getPositions ( ) ; if ( existingPositions != null && positions != null && positions . length > 0 ) { int [ ] newPositions = new int [ existingPositions . length + positions . length ] ; System . arraycopy ( existingPositions , 0 , newPositions , 0 , existingPositions . length ) ; System . arraycopy ( positions , 0 , newPositions , existingPositions . length , positions . length ) ; entry . setPositions ( newPositions ) ; } else if ( existingPositions == null && positions != null && positions . length > 0 ) { entry . setPositions ( positions ) ; } } } } public void setExpectations ( String field , int numTerms , boolean storeOffsets , boolean storePositions ) { this . storeOffsets = storeOffsets ; this . storePositions = storePositions ; } public SortedSet getTermVectorEntrySet ( ) { return currentSet ; } } 	0	['5', '2', '0', '3', '19', '2', '0', '3', '5', '0.7', '188', '0.8', '0', '0.625', '0.428571429', '0', '0', '35.6', '18', '4', '0']
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . FieldCache ; import java . io . IOException ; public class OrdFieldSource extends ValueSource { protected String field ; public OrdFieldSource ( String field ) { this . field = field ; } public String description ( ) { return "ord(" + field + ')' ; } public DocValues getValues ( IndexReader reader ) throws IOException { final int [ ] arr = FieldCache . DEFAULT . getStringIndex ( reader , field ) . order ; return new DocValues ( ) { public float floatVal ( int doc ) { return ( float ) arr [ doc ] ; } public String strVal ( int doc ) { return Integer . toString ( arr [ doc ] ) ; } public String toString ( int doc ) { return description ( ) + '=' + intVal ( doc ) ; } Object getInnerArray ( ) { return arr ; } } ; } public boolean equals ( Object o ) { if ( o . getClass ( ) != OrdFieldSource . class ) return false ; OrdFieldSource other = ( OrdFieldSource ) o ; return this . field . equals ( other . field ) ; } private static final int hcode = OrdFieldSource . class . hashCode ( ) ; public int hashCode ( ) { return hcode + field . hashCode ( ) ; } } 	0	['7', '2', '0', '6', '21', '0', '1', '6', '5', '0.666666667', '90', '0.666666667', '0', '0.5', '0.375', '2', '2', '11.42857143', '3', '1', '0']
package org . apache . lucene . document ; import java . util . HashMap ; import java . util . List ; import java . util . Map ; public class MapFieldSelector implements FieldSelector { Map fieldSelections ; public MapFieldSelector ( Map fieldSelections ) { this . fieldSelections = fieldSelections ; } public MapFieldSelector ( List fields ) { fieldSelections = new HashMap ( fields . size ( ) * 5 / 3 ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) fieldSelections . put ( fields . get ( i ) , FieldSelectorResult . LOAD ) ; } public MapFieldSelector ( String [ ] fields ) { fieldSelections = new HashMap ( fields . length * 5 / 3 ) ; for ( int i = 0 ; i < fields . length ; i ++ ) fieldSelections . put ( fields [ i ] , FieldSelectorResult . LOAD ) ; } public FieldSelectorResult accept ( String field ) { FieldSelectorResult selection = ( FieldSelectorResult ) fieldSelections . get ( field ) ; return selection != null ? selection : FieldSelectorResult . NO_LOAD ; } } 	0	['4', '1', '0', '2', '10', '0', '0', '2', '4', '0', '83', '0', '0', '0', '0.4', '0', '0', '19.5', '2', '0.5', '0']
package org . apache . lucene . analysis ; import java . io . * ; class PorterStemmer { private char [ ] b ; private int i , j , k , k0 ; private boolean dirty = false ; private static final int INC = 50 ; private static final int EXTRA = 1 ; public PorterStemmer ( ) { b = new char [ INC ] ; i = 0 ; } public void reset ( ) { i = 0 ; dirty = false ; } public void add ( char ch ) { if ( b . length <= i + EXTRA ) { char [ ] new_b = new char [ b . length + INC ] ; System . arraycopy ( b , 0 , new_b , 0 , b . length ) ; b = new_b ; } b [ i ++ ] = ch ; } public String toString ( ) { return new String ( b , 0 , i ) ; } public int getResultLength ( ) { return i ; } public char [ ] getResultBuffer ( ) { return b ; } private final boolean cons ( int i ) { switch ( b [ i ] ) { case 'a' : case 'e' : case 'i' : case 'o' : case 'u' : return false ; case 'y' : return ( i == k0 ) ? true : ! cons ( i - 1 ) ; default : return true ; } } private final int m ( ) { int n = 0 ; int i = k0 ; while ( true ) { if ( i > j ) return n ; if ( ! cons ( i ) ) break ; i ++ ; } i ++ ; while ( true ) { while ( true ) { if ( i > j ) return n ; if ( cons ( i ) ) break ; i ++ ; } i ++ ; n ++ ; while ( true ) { if ( i > j ) return n ; if ( ! cons ( i ) ) break ; i ++ ; } i ++ ; } } private final boolean vowelinstem ( ) { int i ; for ( i = k0 ; i <= j ; i ++ ) if ( ! cons ( i ) ) return true ; return false ; } private final boolean doublec ( int j ) { if ( j < k0 + 1 ) return false ; if ( b [ j ] != b [ j - 1 ] ) return false ; return cons ( j ) ; } private final boolean cvc ( int i ) { if ( i < k0 + 2 || ! cons ( i ) || cons ( i - 1 ) || ! cons ( i - 2 ) ) return false ; else { int ch = b [ i ] ; if ( ch == 'w' || ch == 'x' || ch == 'y' ) return false ; } return true ; } private final boolean ends ( String s ) { int l = s . length ( ) ; int o = k - l + 1 ; if ( o < k0 ) return false ; for ( int i = 0 ; i < l ; i ++ ) if ( b [ o + i ] != s . charAt ( i ) ) return false ; j = k - l ; return true ; } void setto ( String s ) { int l = s . length ( ) ; int o = j + 1 ; for ( int i = 0 ; i < l ; i ++ ) b [ o + i ] = s . charAt ( i ) ; k = j + l ; dirty = true ; } void r ( String s ) { if ( m ( ) > 0 ) setto ( s ) ; } private final void step1 ( ) { if ( b [ k ] == 's' ) { if ( ends ( "sses" ) ) k -= 2 ; else if ( ends ( "ies" ) ) setto ( "i" ) ; else if ( b [ k - 1 ] != 's' ) k -- ; } if ( ends ( "eed" ) ) { if ( m ( ) > 0 ) k -- ; } else if ( ( ends ( "ed" ) || ends ( "ing" ) ) && vowelinstem ( ) ) { k = j ; if ( ends ( "at" ) ) setto ( "ate" ) ; else if ( ends ( "bl" ) ) setto ( "ble" ) ; else if ( ends ( "iz" ) ) setto ( "ize" ) ; else if ( doublec ( k ) ) { int ch = b [ k -- ] ; if ( ch == 'l' || ch == 's' || ch == 'z' ) k ++ ; } else if ( m ( ) == 1 && cvc ( k ) ) setto ( "e" ) ; } } private final void step2 ( ) { if ( ends ( "y" ) && vowelinstem ( ) ) { b [ k ] = 'i' ; dirty = true ; } } private final void step3 ( ) { if ( k == k0 ) return ; switch ( b [ k - 1 ] ) { case 'a' : if ( ends ( "ational" ) ) { r ( "ate" ) ; break ; } if ( ends ( "tional" ) ) { r ( "tion" ) ; break ; } break ; case 'c' : if ( ends ( "enci" ) ) { r ( "ence" ) ; break ; } if ( ends ( "anci" ) ) { r ( "ance" ) ; break ; } break ; case 'e' : if ( ends ( "izer" ) ) { r ( "ize" ) ; break ; } break ; case 'l' : if ( ends ( "bli" ) ) { r ( "ble" ) ; break ; } if ( ends ( "alli" ) ) { r ( "al" ) ; break ; } if ( ends ( "entli" ) ) { r ( "ent" ) ; break ; } if ( ends ( "eli" ) ) { r ( "e" ) ; break ; } if ( ends ( "ousli" ) ) { r ( "ous" ) ; break ; } break ; case 'o' : if ( ends ( "ization" ) ) { r ( "ize" ) ; break ; } if ( ends ( "ation" ) ) { r ( "ate" ) ; break ; } if ( ends ( "ator" ) ) { r ( "ate" ) ; break ; } break ; case 's' : if ( ends ( "alism" ) ) { r ( "al" ) ; break ; } if ( ends ( "iveness" ) ) { r ( "ive" ) ; break ; } if ( ends ( "fulness" ) ) { r ( "ful" ) ; break ; } if ( ends ( "ousness" ) ) { r ( "ous" ) ; break ; } break ; case 't' : if ( ends ( "aliti" ) ) { r ( "al" ) ; break ; } if ( ends ( "iviti" ) ) { r ( "ive" ) ; break ; } if ( ends ( "biliti" ) ) { r ( "ble" ) ; break ; } break ; case 'g' : if ( ends ( "logi" ) ) { r ( "log" ) ; break ; } } } private final void step4 ( ) { switch ( b [ k ] ) { case 'e' : if ( ends ( "icate" ) ) { r ( "ic" ) ; break ; } if ( ends ( "ative" ) ) { r ( "" ) ; break ; } if ( ends ( "alize" ) ) { r ( "al" ) ; break ; } break ; case 'i' : if ( ends ( "iciti" ) ) { r ( "ic" ) ; break ; } break ; case 'l' : if ( ends ( "ical" ) ) { r ( "ic" ) ; break ; } if ( ends ( "ful" ) ) { r ( "" ) ; break ; } break ; case 's' : if ( ends ( "ness" ) ) { r ( "" ) ; break ; } break ; } } private final void step5 ( ) { if ( k == k0 ) return ; switch ( b [ k - 1 ] ) { case 'a' : if ( ends ( "al" ) ) break ; return ; case 'c' : if ( ends ( "ance" ) ) break ; if ( ends ( "ence" ) ) break ; return ; case 'e' : if ( ends ( "er" ) ) break ; return ; case 'i' : if ( ends ( "ic" ) ) break ; return ; case 'l' : if ( ends ( "able" ) ) break ; if ( ends ( "ible" ) ) break ; return ; case 'n' : if ( ends ( "ant" ) ) break ; if ( ends ( "ement" ) ) break ; if ( ends ( "ment" ) ) break ; if ( ends ( "ent" ) ) break ; return ; case 'o' : if ( ends ( "ion" ) && j >= 0 && ( b [ j ] == 's' || b [ j ] == 't' ) ) break ; if ( ends ( "ou" ) ) break ; return ; case 's' : if ( ends ( "ism" ) ) break ; return ; case 't' : if ( ends ( "ate" ) ) break ; if ( ends ( "iti" ) ) break ; return ; case 'u' : if ( ends ( "ous" ) ) break ; return ; case 'v' : if ( ends ( "ive" ) ) break ; return ; case 'z' : if ( ends ( "ize" ) ) break ; return ; default : return ; } if ( m ( ) > 1 ) k = j ; } private final void step6 ( ) { j = k ; if ( b [ k ] == 'e' ) { int a = m ( ) ; if ( a > 1 || a == 1 && ! cvc ( k - 1 ) ) k -- ; } if ( b [ k ] == 'l' && doublec ( k ) && m ( ) > 1 ) k -- ; } public String stem ( String s ) { if ( stem ( s . toCharArray ( ) , s . length ( ) ) ) return toString ( ) ; else return s ; } public boolean stem ( char [ ] word ) { return stem ( word , word . length ) ; } public boolean stem ( char [ ] wordBuffer , int offset , int wordLen ) { reset ( ) ; if ( b . length < wordLen ) { char [ ] new_b = new char [ wordLen + EXTRA ] ; b = new_b ; } System . arraycopy ( wordBuffer , offset , b , 0 , wordLen ) ; i = wordLen ; return stem ( 0 ) ; } public boolean stem ( char [ ] word , int wordLen ) { return stem ( word , 0 , wordLen ) ; } public boolean stem ( ) { return stem ( 0 ) ; } public boolean stem ( int i0 ) { k = i - 1 ; k0 = i0 ; if ( k > k0 + 1 ) { step1 ( ) ; step2 ( ) ; step3 ( ) ; step4 ( ) ; step5 ( ) ; step6 ( ) ; } if ( i != k + 1 ) dirty = true ; i = k + 1 ; return dirty ; } public static void main ( String [ ] args ) { PorterStemmer s = new PorterStemmer ( ) ; for ( int i = 0 ; i < args . length ; i ++ ) { try { InputStream in = new FileInputStream ( args [ i ] ) ; byte [ ] buffer = new byte [ 1024 ] ; int bufferLen , offset , ch ; bufferLen = in . read ( buffer ) ; offset = 0 ; s . reset ( ) ; while ( true ) { if ( offset < bufferLen ) ch = buffer [ offset ++ ] ; else { bufferLen = in . read ( buffer ) ; offset = 0 ; if ( bufferLen < 0 ) ch = - 1 ; else ch = buffer [ offset ++ ] ; } if ( Character . isLetter ( ( char ) ch ) ) { s . add ( Character . toLowerCase ( ( char ) ch ) ) ; } else { s . stem ( ) ; System . out . print ( s . toString ( ) ) ; s . reset ( ) ; if ( ch < 0 ) break ; else { System . out . print ( ( char ) ch ) ; } } } in . close ( ) ; } catch ( IOException e ) { System . out . println ( "error reading " + args [ i ] ) ; } } } } 	0	['27', '1', '0', '1', '44', '13', '1', '0', '13', '0.600961538', '1158', '1', '0', '0', '0.25308642', '0', '0', '41.59259259', '26', '5.6667', '0']
package org . apache . lucene . index ; import java . io . IOException ; public interface TermPositions extends TermDocs { int nextPosition ( ) throws IOException ; int getPayloadLength ( ) ; byte [ ] getPayload ( byte [ ] data , int offset ) throws IOException ; public boolean isPayloadAvailable ( ) ; } 	0	['4', '1', '0', '25', '4', '6', '24', '1', '4', '2', '4', '0', '0', '0', '0.5', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . FieldCache ; import java . io . IOException ; public class ReverseOrdFieldSource extends ValueSource { public String field ; public ReverseOrdFieldSource ( String field ) { this . field = field ; } public String description ( ) { return "rord(" + field + ')' ; } public DocValues getValues ( IndexReader reader ) throws IOException { final FieldCache . StringIndex sindex = FieldCache . DEFAULT . getStringIndex ( reader , field ) ; final int arr [ ] = sindex . order ; final int end = sindex . lookup . length ; return new DocValues ( ) { public float floatVal ( int doc ) { return ( float ) ( end - arr [ doc ] ) ; } public int intVal ( int doc ) { return end - arr [ doc ] ; } public String strVal ( int doc ) { return Integer . toString ( intVal ( doc ) ) ; } public String toString ( int doc ) { return description ( ) + '=' + strVal ( doc ) ; } Object getInnerArray ( ) { return arr ; } } ; } public boolean equals ( Object o ) { if ( o . getClass ( ) != ReverseOrdFieldSource . class ) return false ; ReverseOrdFieldSource other = ( ReverseOrdFieldSource ) o ; return this . field . equals ( other . field ) ; } private static final int hcode = ReverseOrdFieldSource . class . hashCode ( ) ; public int hashCode ( ) { return hcode + field . hashCode ( ) ; } } 	0	['7', '2', '0', '6', '21', '0', '1', '6', '5', '0.666666667', '97', '0.333333333', '0', '0.5', '0.375', '2', '2', '12.42857143', '3', '1', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; public final class PorterStemFilter extends TokenFilter { private PorterStemmer stemmer ; public PorterStemFilter ( TokenStream in ) { super ( in ) ; stemmer = new PorterStemmer ( ) ; } public final Token next ( final Token reusableToken ) throws IOException { assert reusableToken != null ; Token nextToken = input . next ( reusableToken ) ; if ( nextToken == null ) return null ; if ( stemmer . stem ( nextToken . termBuffer ( ) , 0 , nextToken . termLength ( ) ) ) nextToken . setTermBuffer ( stemmer . getResultBuffer ( ) , 0 , stemmer . getResultLength ( ) ) ; return nextToken ; } } 	0	['4', '3', '0', '4', '18', '2', '0', '4', '2', '0.777777778', '78', '0.333333333', '1', '0.777777778', '0.416666667', '1', '2', '17.75', '1', '0.5', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import org . apache . lucene . store . IndexInput ; class DefaultSkipListReader extends MultiLevelSkipListReader { private boolean currentFieldStoresPayloads ; private long freqPointer [ ] ; private long proxPointer [ ] ; private int payloadLength [ ] ; private long lastFreqPointer ; private long lastProxPointer ; private int lastPayloadLength ; DefaultSkipListReader ( IndexInput skipStream , int maxSkipLevels , int skipInterval ) { super ( skipStream , maxSkipLevels , skipInterval ) ; freqPointer = new long [ maxSkipLevels ] ; proxPointer = new long [ maxSkipLevels ] ; payloadLength = new int [ maxSkipLevels ] ; } void init ( long skipPointer , long freqBasePointer , long proxBasePointer , int df , boolean storesPayloads ) { super . init ( skipPointer , df ) ; this . currentFieldStoresPayloads = storesPayloads ; lastFreqPointer = freqBasePointer ; lastProxPointer = proxBasePointer ; Arrays . fill ( freqPointer , freqBasePointer ) ; Arrays . fill ( proxPointer , proxBasePointer ) ; Arrays . fill ( payloadLength , 0 ) ; } long getFreqPointer ( ) { return lastFreqPointer ; } long getProxPointer ( ) { return lastProxPointer ; } int getPayloadLength ( ) { return lastPayloadLength ; } protected void seekChild ( int level ) throws IOException { super . seekChild ( level ) ; freqPointer [ level ] = lastFreqPointer ; proxPointer [ level ] = lastProxPointer ; payloadLength [ level ] = lastPayloadLength ; } protected void setLastSkipData ( int level ) { super . setLastSkipData ( level ) ; lastFreqPointer = freqPointer [ level ] ; lastProxPointer = proxPointer [ level ] ; lastPayloadLength = payloadLength [ level ] ; } protected int readSkipData ( int level , IndexInput skipStream ) throws IOException { int delta ; if ( currentFieldStoresPayloads ) { delta = skipStream . readVInt ( ) ; if ( ( delta & 1 ) != 0 ) { payloadLength [ level ] = skipStream . readVInt ( ) ; } delta >>>= 1 ; } else { delta = skipStream . readVInt ( ) ; } freqPointer [ level ] += skipStream . readVInt ( ) ; proxPointer [ level ] += skipStream . readVInt ( ) ; return delta ; } } 	0	['8', '2', '0', '3', '15', '0', '1', '2', '0', '0.571428571', '158', '1', '0', '0.5625', '0.425', '1', '3', '17.875', '1', '0.875', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexInput ; final class SegmentTermEnum extends TermEnum implements Cloneable { private IndexInput input ; FieldInfos fieldInfos ; long size ; long position = - 1 ; private TermBuffer termBuffer = new TermBuffer ( ) ; private TermBuffer prevBuffer = new TermBuffer ( ) ; private TermBuffer scanBuffer = new TermBuffer ( ) ; private TermInfo termInfo = new TermInfo ( ) ; private int format ; private boolean isIndex = false ; long indexPointer = 0 ; int indexInterval ; int skipInterval ; int maxSkipLevels ; private int formatM1SkipInterval ; SegmentTermEnum ( IndexInput i , FieldInfos fis , boolean isi ) throws CorruptIndexException , IOException { input = i ; fieldInfos = fis ; isIndex = isi ; maxSkipLevels = 1 ; int firstInt = input . readInt ( ) ; if ( firstInt >= 0 ) { format = 0 ; size = firstInt ; indexInterval = 128 ; skipInterval = Integer . MAX_VALUE ; } else { format = firstInt ; if ( format < TermInfosWriter . FORMAT_CURRENT ) throw new CorruptIndexException ( "Unknown format version:" + format + " expected " + TermInfosWriter . FORMAT_CURRENT + " or higher" ) ; size = input . readLong ( ) ; if ( format == - 1 ) { if ( ! isIndex ) { indexInterval = input . readInt ( ) ; formatM1SkipInterval = input . readInt ( ) ; } skipInterval = Integer . MAX_VALUE ; } else { indexInterval = input . readInt ( ) ; skipInterval = input . readInt ( ) ; if ( format <= TermInfosWriter . FORMAT ) { maxSkipLevels = input . readInt ( ) ; } } } if ( format > TermInfosWriter . FORMAT_VERSION_UTF8_LENGTH_IN_BYTES ) { termBuffer . setPreUTF8Strings ( ) ; scanBuffer . setPreUTF8Strings ( ) ; prevBuffer . setPreUTF8Strings ( ) ; } } protected Object clone ( ) { SegmentTermEnum clone = null ; try { clone = ( SegmentTermEnum ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . input = ( IndexInput ) input . clone ( ) ; clone . termInfo = new TermInfo ( termInfo ) ; clone . termBuffer = ( TermBuffer ) termBuffer . clone ( ) ; clone . prevBuffer = ( TermBuffer ) prevBuffer . clone ( ) ; clone . scanBuffer = new TermBuffer ( ) ; return clone ; } final void seek ( long pointer , int p , Term t , TermInfo ti ) throws IOException { input . seek ( pointer ) ; position = p ; termBuffer . set ( t ) ; prevBuffer . reset ( ) ; termInfo . set ( ti ) ; } public final boolean next ( ) throws IOException { if ( position ++ >= size - 1 ) { prevBuffer . set ( termBuffer ) ; termBuffer . reset ( ) ; return false ; } prevBuffer . set ( termBuffer ) ; termBuffer . read ( input , fieldInfos ) ; termInfo . docFreq = input . readVInt ( ) ; termInfo . freqPointer += input . readVLong ( ) ; termInfo . proxPointer += input . readVLong ( ) ; if ( format == - 1 ) { if ( ! isIndex ) { if ( termInfo . docFreq > formatM1SkipInterval ) { termInfo . skipOffset = input . readVInt ( ) ; } } } else { if ( termInfo . docFreq >= skipInterval ) termInfo . skipOffset = input . readVInt ( ) ; } if ( isIndex ) indexPointer += input . readVLong ( ) ; return true ; } final int scanTo ( Term term ) throws IOException { scanBuffer . set ( term ) ; int count = 0 ; while ( scanBuffer . compareTo ( termBuffer ) > 0 && next ( ) ) { count ++ ; } return count ; } public final Term term ( ) { return termBuffer . toTerm ( ) ; } final Term prev ( ) { return prevBuffer . toTerm ( ) ; } final TermInfo termInfo ( ) { return new TermInfo ( termInfo ) ; } final void termInfo ( TermInfo ti ) { ti . set ( termInfo ) ; } public final int docFreq ( ) { return termInfo . docFreq ; } final long freqPointer ( ) { return termInfo . freqPointer ; } final long proxPointer ( ) { return termInfo . proxPointer ; } public final void close ( ) throws IOException { input . close ( ) ; } } 	0	['13', '2', '0', '11', '39', '0', '4', '7', '4', '0.761111111', '394', '0.533333333', '6', '0.294117647', '0.211538462', '1', '2', '28.15384615', '1', '0.9231', '0']
package org . apache . lucene . index ; import java . util . Map ; import java . io . IOException ; abstract class InvertedDocEndConsumer { abstract InvertedDocEndConsumerPerThread addThread ( DocInverterPerThread docInverterPerThread ) ; abstract void flush ( Map threadsAndFields , DocumentsWriter . FlushState state ) throws IOException ; abstract void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException ; abstract void abort ( ) ; abstract void setFieldInfos ( FieldInfos fieldInfos ) ; } 	0	['6', '1', '1', '7', '7', '15', '4', '4', '0', '2', '9', '0', '0', '0', '0.366666667', '0', '0', '0.5', '1', '0.8333', '0']
package org . apache . lucene . queryParser ; public interface QueryParserConstants { int EOF = 0 ; int _NUM_CHAR = 1 ; int _ESCAPED_CHAR = 2 ; int _TERM_START_CHAR = 3 ; int _TERM_CHAR = 4 ; int _WHITESPACE = 5 ; int _QUOTED_CHAR = 6 ; int AND = 8 ; int OR = 9 ; int NOT = 10 ; int PLUS = 11 ; int MINUS = 12 ; int LPAREN = 13 ; int RPAREN = 14 ; int COLON = 15 ; int STAR = 16 ; int CARAT = 17 ; int QUOTED = 18 ; int TERM = 19 ; int FUZZY_SLOP = 20 ; int PREFIXTERM = 21 ; int WILDTERM = 22 ; int RANGEIN_START = 23 ; int RANGEEX_START = 24 ; int NUMBER = 25 ; int RANGEIN_TO = 26 ; int RANGEIN_END = 27 ; int RANGEIN_QUOTED = 28 ; int RANGEIN_GOOP = 29 ; int RANGEEX_TO = 30 ; int RANGEEX_END = 31 ; int RANGEEX_QUOTED = 32 ; int RANGEEX_GOOP = 33 ; int Boost = 0 ; int RangeEx = 1 ; int RangeIn = 2 ; int DEFAULT = 3 ; String [ ] tokenImage = { "<EOF>" , "<_NUM_CHAR>" , "<_ESCAPED_CHAR>" , "<_TERM_START_CHAR>" , "<_TERM_CHAR>" , "<_WHITESPACE>" , "<_QUOTED_CHAR>" , "<token of kind 7>" , "<AND>" , "<OR>" , "<NOT>" , "\"+\"" , "\"-\"" , "\"(\"" , "\")\"" , "\":\"" , "\"*\"" , "\"^\"" , "<QUOTED>" , "<TERM>" , "<FUZZY_SLOP>" , "<PREFIXTERM>" , "<WILDTERM>" , "\"[\"" , "\"{\"" , "<NUMBER>" , "\"TO\"" , "\"]\"" , "<RANGEIN_QUOTED>" , "<RANGEIN_GOOP>" , "\"TO\"" , "\"}\"" , "<RANGEEX_QUOTED>" , "<RANGEEX_GOOP>" , } ; } 	0	['1', '1', '0', '2', '1', '0', '2', '0', '0', '2', '179', '0', '0', '0', '0', '0', '0', '140', '0', '0', '0']
package org . apache . lucene . index ; import java . util . Map ; import java . io . IOException ; abstract class InvertedDocConsumer { abstract InvertedDocConsumerPerThread addThread ( DocInverterPerThread docInverterPerThread ) ; abstract void abort ( ) ; abstract void flush ( Map threadsAndFields , DocumentsWriter . FlushState state ) throws IOException ; abstract void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException ; abstract boolean freeRAM ( ) ; FieldInfos fieldInfos ; void setFieldInfos ( FieldInfos fieldInfos ) { this . fieldInfos = fieldInfos ; } } 	0	['7', '1', '1', '7', '8', '21', '4', '4', '0', '1', '15', '0', '1', '0', '0.342857143', '0', '0', '1', '1', '0.8571', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; public abstract class SpanFilter extends Filter { public abstract SpanFilterResult bitSpans ( IndexReader reader ) throws IOException ; } 	0	['2', '2', '2', '5', '3', '1', '2', '3', '2', '2', '5', '0', '0', '0.666666667', '0.75', '0', '0', '1.5', '1', '0.5', '0']
package org . apache . lucene . index ; final class CharBlockPool { public char [ ] [ ] buffers = new char [ 10 ] [ ] ; int numBuffer ; int bufferUpto = - 1 ; public int charUpto = DocumentsWriter . CHAR_BLOCK_SIZE ; public char [ ] buffer ; public int charOffset = - DocumentsWriter . CHAR_BLOCK_SIZE ; final private DocumentsWriter docWriter ; public CharBlockPool ( DocumentsWriter docWriter ) { this . docWriter = docWriter ; } public void reset ( ) { docWriter . recycleCharBlocks ( buffers , 1 + bufferUpto ) ; bufferUpto = - 1 ; charUpto = DocumentsWriter . CHAR_BLOCK_SIZE ; charOffset = - DocumentsWriter . CHAR_BLOCK_SIZE ; } public void nextBuffer ( ) { if ( 1 + bufferUpto == buffers . length ) { char [ ] [ ] newBuffers = new char [ ( int ) ( buffers . length * 1.5 ) ] [ ] ; System . arraycopy ( buffers , 0 , newBuffers , 0 , buffers . length ) ; buffers = newBuffers ; } buffer = buffers [ 1 + bufferUpto ] = docWriter . getCharBlock ( ) ; bufferUpto ++ ; charUpto = 0 ; charOffset += DocumentsWriter . CHAR_BLOCK_SIZE ; } } 	0	['3', '1', '0', '5', '7', '0', '4', '1', '3', '0.357142857', '106', '0.142857143', '1', '0', '0.666666667', '0', '0', '32', '2', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . util . Parameter ; public class BooleanClause implements java . io . Serializable { public static final class Occur extends Parameter implements java . io . Serializable { private Occur ( String name ) { super ( name ) ; } public String toString ( ) { if ( this == MUST ) return "+" ; if ( this == MUST_NOT ) return "-" ; return "" ; } public static final Occur MUST = new Occur ( "MUST" ) ; public static final Occur SHOULD = new Occur ( "SHOULD" ) ; public static final Occur MUST_NOT = new Occur ( "MUST_NOT" ) ; } private Query query ; private Occur occur ; public BooleanClause ( Query query , Occur occur ) { this . query = query ; this . occur = occur ; } public Occur getOccur ( ) { return occur ; } public void setOccur ( Occur occur ) { this . occur = occur ; } public Query getQuery ( ) { return query ; } public void setQuery ( Query query ) { this . query = query ; } public boolean isProhibited ( ) { return Occur . MUST_NOT . equals ( occur ) ; } public boolean isRequired ( ) { return Occur . MUST . equals ( occur ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof BooleanClause ) ) return false ; BooleanClause other = ( BooleanClause ) o ; return this . query . equals ( other . query ) && this . occur . equals ( other . occur ) ; } public int hashCode ( ) { return query . hashCode ( ) ^ ( Occur . MUST . equals ( occur ) ? 1 : 0 ) ^ ( Occur . MUST_NOT . equals ( occur ) ? 2 : 0 ) ; } public String toString ( ) { return occur . toString ( ) + query . toString ( ) ; } } 	0	['10', '1', '0', '7', '18', '0', '6', '2', '10', '0.333333333', '104', '1', '2', '0', '0.375', '1', '1', '9.2', '4', '1.4', '0']
package org . apache . lucene . index ; import java . util . Collection ; import java . io . IOException ; public interface IndexCommitPoint { public String getSegmentsFileName ( ) ; public Collection getFileNames ( ) throws IOException ; public void delete ( ) ; } 	0	['3', '1', '0', '2', '3', '3', '2', '0', '3', '2', '3', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . store ; import java . io . IOException ; public class NoLockFactory extends LockFactory { private static NoLock singletonLock = new NoLock ( ) ; private static NoLockFactory singleton = new NoLockFactory ( ) ; public static NoLockFactory getNoLockFactory ( ) { return singleton ; } public Lock makeLock ( String lockName ) { return singletonLock ; } public void clearLock ( String lockName ) { } ; } ; class NoLock extends Lock { public boolean obtain ( ) throws IOException { return true ; } public void release ( ) { } public boolean isLocked ( ) { return false ; } public String toString ( ) { return "NoLock" ; } } 	0	['5', '2', '0', '4', '7', '6', '1', '3', '4', '0.75', '24', '1', '2', '0.571428571', '0.625', '0', '0', '3.4', '1', '0.6', '0']
package org . apache . lucene . index ; import java . util . * ; class SegmentTermVector implements TermFreqVector { private String field ; private String terms [ ] ; private int termFreqs [ ] ; SegmentTermVector ( String field , String terms [ ] , int termFreqs [ ] ) { this . field = field ; this . terms = terms ; this . termFreqs = termFreqs ; } public String getField ( ) { return field ; } public String toString ( ) { StringBuffer sb = new StringBuffer ( ) ; sb . append ( '{' ) ; sb . append ( field ) . append ( ": " ) ; if ( terms != null ) { for ( int i = 0 ; i < terms . length ; i ++ ) { if ( i > 0 ) sb . append ( ", " ) ; sb . append ( terms [ i ] ) . append ( '/' ) . append ( termFreqs [ i ] ) ; } } sb . append ( '}' ) ; return sb . toString ( ) ; } public int size ( ) { return terms == null ? 0 : terms . length ; } public String [ ] getTerms ( ) { return terms ; } public int [ ] getTermFrequencies ( ) { return termFreqs ; } public int indexOf ( String termText ) { if ( terms == null ) return - 1 ; int res = Arrays . binarySearch ( terms , termText ) ; return res >= 0 ? res : - 1 ; } public int [ ] indexesOf ( String [ ] termNumbers , int start , int len ) { int res [ ] = new int [ len ] ; for ( int i = 0 ; i < len ; i ++ ) { res [ i ] = indexOf ( termNumbers [ start + i ] ) ; } return res ; } } 	0	['8', '1', '1', '4', '15', '0', '3', '1', '7', '0.571428571', '133', '1', '0', '0', '0.35', '0', '0', '15.25', '4', '1.75', '0']
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . function . DocValues ; import java . io . IOException ; import java . io . Serializable ; public abstract class ValueSource implements Serializable { public abstract DocValues getValues ( IndexReader reader ) throws IOException ; public abstract String description ( ) ; public String toString ( ) { return description ( ) ; } public abstract boolean equals ( Object o ) ; public abstract int hashCode ( ) ; } 	0	['6', '1', '3', '8', '7', '15', '6', '2', '6', '2', '12', '0', '0', '0', '0.444444444', '1', '1', '1', '1', '0.8333', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; public final class LengthFilter extends TokenFilter { final int min ; final int max ; public LengthFilter ( TokenStream in , int min , int max ) { super ( in ) ; this . min = min ; this . max = max ; } public final Token next ( final Token reusableToken ) throws IOException { assert reusableToken != null ; for ( Token nextToken = input . next ( reusableToken ) ; nextToken != null ; nextToken = input . next ( reusableToken ) ) { int len = nextToken . termLength ( ) ; if ( len >= min && len <= max ) { return nextToken ; } } return null ; } } 	0	['4', '3', '0', '3', '12', '2', '0', '3', '2', '0.75', '79', '0', '0', '0.777777778', '0.4', '1', '2', '17.75', '1', '0.5', '0']
package org . apache . lucene . index ; abstract class InvertedDocEndConsumerPerField { abstract void finish ( ) ; abstract void abort ( ) ; } 	0	['3', '1', '1', '5', '4', '3', '5', '0', '0', '2', '6', '0', '0', '0', '1', '0', '0', '1', '1', '0.6667', '0']
package org . apache . lucene . index ; import java . io . IOException ; final class FreqProxFieldMergeState { final FreqProxTermsWriterPerField field ; final int numPostings ; final CharBlockPool charPool ; final RawPostingList [ ] postings ; private FreqProxTermsWriter . PostingList p ; char [ ] text ; int textOffset ; private int postingUpto = - 1 ; final ByteSliceReader freq = new ByteSliceReader ( ) ; final ByteSliceReader prox = new ByteSliceReader ( ) ; int docID ; int termFreq ; public FreqProxFieldMergeState ( FreqProxTermsWriterPerField field ) { this . field = field ; this . charPool = field . perThread . termsHashPerThread . charPool ; this . numPostings = field . termsHashPerField . numPostings ; this . postings = field . termsHashPerField . sortPostings ( ) ; } boolean nextTerm ( ) throws IOException { postingUpto ++ ; if ( postingUpto == numPostings ) return false ; p = ( FreqProxTermsWriter . PostingList ) postings [ postingUpto ] ; docID = 0 ; text = charPool . buffers [ p . textStart > > DocumentsWriter . CHAR_BLOCK_SHIFT ] ; textOffset = p . textStart & DocumentsWriter . CHAR_BLOCK_MASK ; field . termsHashPerField . initReader ( freq , p , 0 ) ; if ( ! field . fieldInfo . omitTf ) field . termsHashPerField . initReader ( prox , p , 1 ) ; boolean result = nextDoc ( ) ; assert result ; return true ; } public boolean nextDoc ( ) throws IOException { if ( freq . eof ( ) ) { if ( p . lastDocCode != - 1 ) { docID = p . lastDocID ; if ( ! field . omitTf ) termFreq = p . docFreq ; p . lastDocCode = - 1 ; return true ; } else return false ; } final int code = freq . readVInt ( ) ; if ( field . omitTf ) docID += code ; else { docID += code > > > 1 ; if ( ( code & 1 ) != 0 ) termFreq = 1 ; else termFreq = freq . readVInt ( ) ; } assert docID != p . lastDocID ; return true ; } } 	0	['5', '1', '0', '10', '16', '0', '1', '9', '2', '0.75', '238', '0.142857143', '6', '0', '0.416666667', '0', '0', '43.8', '1', '0.6', '0']
package org . apache . lucene . index ; import java . io . File ; import java . io . FilenameFilter ; import java . util . HashSet ; public class IndexFileNameFilter implements FilenameFilter { static IndexFileNameFilter singleton = new IndexFileNameFilter ( ) ; private HashSet extensions ; private HashSet extensionsInCFS ; public IndexFileNameFilter ( ) { extensions = new HashSet ( ) ; for ( int i = 0 ; i < IndexFileNames . INDEX_EXTENSIONS . length ; i ++ ) { extensions . add ( IndexFileNames . INDEX_EXTENSIONS [ i ] ) ; } extensionsInCFS = new HashSet ( ) ; for ( int i = 0 ; i < IndexFileNames . INDEX_EXTENSIONS_IN_COMPOUND_FILE . length ; i ++ ) { extensionsInCFS . add ( IndexFileNames . INDEX_EXTENSIONS_IN_COMPOUND_FILE [ i ] ) ; } } public boolean accept ( File dir , String name ) { int i = name . lastIndexOf ( '.' ) ; if ( i != - 1 ) { String extension = name . substring ( 1 + i ) ; if ( extensions . contains ( extension ) ) { return true ; } else if ( extension . startsWith ( "f" ) && extension . matches ( "f\\d+" ) ) { return true ; } else if ( extension . startsWith ( "s" ) && extension . matches ( "s\\d+" ) ) { return true ; } } else { if ( name . equals ( IndexFileNames . DELETABLE ) ) return true ; else if ( name . startsWith ( IndexFileNames . SEGMENTS ) ) return true ; } return false ; } public boolean isCFSFile ( String name ) { int i = name . lastIndexOf ( '.' ) ; if ( i != - 1 ) { String extension = name . substring ( 1 + i ) ; if ( extensionsInCFS . contains ( extension ) ) { return true ; } if ( extension . startsWith ( "f" ) && extension . matches ( "f\\d+" ) ) { return true ; } } return false ; } public static IndexFileNameFilter getFilter ( ) { return singleton ; } } 	0	['5', '1', '0', '3', '14', '4', '2', '1', '4', '0.583333333', '145', '0.666666667', '1', '0', '0.5', '0', '0', '27.4', '7', '2.6', '0']
package org . apache . lucene . index ; public abstract class TermVectorMapper { private boolean ignoringPositions ; private boolean ignoringOffsets ; protected TermVectorMapper ( ) { } protected TermVectorMapper ( boolean ignoringPositions , boolean ignoringOffsets ) { this . ignoringPositions = ignoringPositions ; this . ignoringOffsets = ignoringOffsets ; } public abstract void setExpectations ( String field , int numTerms , boolean storeOffsets , boolean storePositions ) ; public abstract void map ( String term , int frequency , TermVectorOffsetInfo [ ] offsets , int [ ] positions ) ; public boolean isIgnoringPositions ( ) { return ignoringPositions ; } public boolean isIgnoringOffsets ( ) { return ignoringOffsets ; } public void setDocumentNumber ( int documentNumber ) { } } 	0	['7', '1', '4', '12', '8', '17', '11', '1', '5', '0.833333333', '28', '1', '0', '0', '0.380952381', '0', '0', '2.714285714', '1', '0.7143', '0']
package org . apache . lucene . util ; public abstract class StringHelper { public static final int bytesDifference ( byte [ ] bytes1 , int len1 , byte [ ] bytes2 , int len2 ) { int len = len1 < len2 ? len1 : len2 ; for ( int i = 0 ; i < len ; i ++ ) if ( bytes1 [ i ] != bytes2 [ i ] ) return i ; return len ; } public static final int stringDifference ( String s1 , String s2 ) { int len1 = s1 . length ( ) ; int len2 = s2 . length ( ) ; int len = len1 < len2 ? len1 : len2 ; for ( int i = 0 ; i < len ; i ++ ) { if ( s1 . charAt ( i ) != s2 . charAt ( i ) ) { return i ; } } return len ; } private StringHelper ( ) { } } 	0	['3', '1', '0', '1', '6', '3', '1', '0', '2', '2', '62', '0', '0', '0', '0.333333333', '0', '0', '19.66666667', '4', '2.6667', '0']
package org . apache . lucene . search ; import java . util . ArrayList ; import java . util . BitSet ; import java . util . List ; public class SpanFilterResult { private BitSet bits ; private DocIdSet docIdSet ; private List positions ; public SpanFilterResult ( BitSet bits , List positions ) { this . bits = bits ; this . positions = positions ; } public SpanFilterResult ( DocIdSet docIdSet , List positions ) { this . docIdSet = docIdSet ; this . positions = positions ; } public List getPositions ( ) { return positions ; } public BitSet getBits ( ) { return bits ; } public DocIdSet getDocIdSet ( ) { return docIdSet ; } public static class PositionInfo { private int doc ; private List positions ; public PositionInfo ( int doc ) { this . doc = doc ; positions = new ArrayList ( ) ; } public void addPosition ( int start , int end ) { positions . add ( new StartEnd ( start , end ) ) ; } public int getDoc ( ) { return doc ; } public List getPositions ( ) { return positions ; } } public static class StartEnd { private int start ; private int end ; public StartEnd ( int start , int end ) { this . start = start ; this . end = end ; } public int getEnd ( ) { return end ; } public int getStart ( ) { return start ; } } } 	0	['5', '1', '0', '4', '6', '0', '3', '1', '5', '0.666666667', '35', '1', '1', '0', '0.45', '0', '0', '5.4', '1', '0.6', '0']
package org . apache . lucene . store ; import java . io . IOException ; class RAMInputStream extends IndexInput implements Cloneable { static final int BUFFER_SIZE = RAMOutputStream . BUFFER_SIZE ; private RAMFile file ; private long length ; private byte [ ] currentBuffer ; private int currentBufferIndex ; private int bufferPosition ; private long bufferStart ; private int bufferLength ; RAMInputStream ( RAMFile f ) throws IOException { file = f ; length = file . length ; if ( length / BUFFER_SIZE >= Integer . MAX_VALUE ) { throw new IOException ( "Too large RAMFile! " + length ) ; } currentBufferIndex = - 1 ; currentBuffer = null ; } public void close ( ) { } public long length ( ) { return length ; } public byte readByte ( ) throws IOException { if ( bufferPosition >= bufferLength ) { currentBufferIndex ++ ; switchCurrentBuffer ( true ) ; } return currentBuffer [ bufferPosition ++ ] ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { while ( len > 0 ) { if ( bufferPosition >= bufferLength ) { currentBufferIndex ++ ; switchCurrentBuffer ( true ) ; } int remainInBuffer = bufferLength - bufferPosition ; int bytesToCopy = len < remainInBuffer ? len : remainInBuffer ; System . arraycopy ( currentBuffer , bufferPosition , b , offset , bytesToCopy ) ; offset += bytesToCopy ; len -= bytesToCopy ; bufferPosition += bytesToCopy ; } } private final void switchCurrentBuffer ( boolean enforceEOF ) throws IOException { if ( currentBufferIndex >= file . numBuffers ( ) ) { if ( enforceEOF ) throw new IOException ( "Read past EOF" ) ; else { currentBufferIndex -- ; bufferPosition = BUFFER_SIZE ; } } else { currentBuffer = ( byte [ ] ) file . getBuffer ( currentBufferIndex ) ; bufferPosition = 0 ; bufferStart = ( long ) BUFFER_SIZE * ( long ) currentBufferIndex ; long buflen = length - bufferStart ; bufferLength = buflen > BUFFER_SIZE ? BUFFER_SIZE : ( int ) buflen ; } } public long getFilePointer ( ) { return currentBufferIndex < 0 ? 0 : bufferStart + bufferPosition ; } public void seek ( long pos ) throws IOException { if ( currentBuffer == null || pos < bufferStart || pos >= bufferStart + BUFFER_SIZE ) { currentBufferIndex = ( int ) ( pos / BUFFER_SIZE ) ; switchCurrentBuffer ( false ) ; } bufferPosition = ( int ) ( pos % BUFFER_SIZE ) ; } } 	0	['8', '2', '0', '3', '17', '0', '1', '2', '6', '0.5', '236', '0.875', '1', '0.708333333', '0.270833333', '1', '4', '27.5', '2', '1', '0']
package org . apache . lucene . store ; import java . io . IOException ; public class LockReleaseFailedException extends IOException { public LockReleaseFailedException ( String message ) { super ( message ) ; } } 	0	['1', '4', '0', '2', '2', '0', '2', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . index ; import java . io . IOException ; final class DocFieldConsumersPerThread extends DocFieldConsumerPerThread { final DocFieldConsumerPerThread one ; final DocFieldConsumerPerThread two ; final DocFieldConsumers parent ; final DocumentsWriter . DocState docState ; public DocFieldConsumersPerThread ( DocFieldProcessorPerThread docFieldProcessorPerThread , DocFieldConsumers parent , DocFieldConsumerPerThread one , DocFieldConsumerPerThread two ) { this . parent = parent ; this . one = one ; this . two = two ; docState = docFieldProcessorPerThread . docState ; } public void startDocument ( ) throws IOException { one . startDocument ( ) ; two . startDocument ( ) ; } public void abort ( ) { try { one . abort ( ) ; } finally { two . abort ( ) ; } } public DocumentsWriter . DocWriter finishDocument ( ) throws IOException { final DocumentsWriter . DocWriter oneDoc = one . finishDocument ( ) ; final DocumentsWriter . DocWriter twoDoc = two . finishDocument ( ) ; if ( oneDoc == null ) return twoDoc ; else if ( twoDoc == null ) return oneDoc ; else { DocFieldConsumers . PerDoc both = parent . getPerDoc ( ) ; both . docID = docState . docID ; assert oneDoc . docID == docState . docID ; assert twoDoc . docID == docState . docID ; both . one = oneDoc ; both . two = twoDoc ; return both ; } } public DocFieldConsumerPerField addField ( FieldInfo fi ) { return new DocFieldConsumersPerField ( this , one . addField ( fi ) , two . addField ( fi ) ) ; } } 	0	['7', '2', '0', '9', '19', '0', '2', '9', '5', '0.694444444', '146', '0', '4', '0.444444444', '0.277777778', '0', '0', '19', '3', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; final class PhraseQueue extends PriorityQueue { PhraseQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object o1 , Object o2 ) { PhrasePositions pp1 = ( PhrasePositions ) o1 ; PhrasePositions pp2 = ( PhrasePositions ) o2 ; if ( pp1 . doc == pp2 . doc ) if ( pp1 . position == pp2 . position ) return pp1 . offset < pp2 . offset ; else return pp1 . position < pp2 . position ; else return pp1 . doc < pp2 . doc ; } } 	0	['2', '2', '0', '5', '4', '1', '3', '2', '0', '2', '51', '0', '0', '0.923076923', '0.666666667', '1', '3', '24.5', '6', '3', '0']
package org . apache . lucene . store ; import java . io . IOException ; public class RAMOutputStream extends IndexOutput { static final int BUFFER_SIZE = 1024 ; private RAMFile file ; private byte [ ] currentBuffer ; private int currentBufferIndex ; private int bufferPosition ; private long bufferStart ; private int bufferLength ; public RAMOutputStream ( ) { this ( new RAMFile ( ) ) ; } RAMOutputStream ( RAMFile f ) { file = f ; currentBufferIndex = - 1 ; currentBuffer = null ; } public void writeTo ( IndexOutput out ) throws IOException { flush ( ) ; final long end = file . length ; long pos = 0 ; int buffer = 0 ; while ( pos < end ) { int length = BUFFER_SIZE ; long nextPos = pos + length ; if ( nextPos > end ) { length = ( int ) ( end - pos ) ; } out . writeBytes ( ( byte [ ] ) file . getBuffer ( buffer ++ ) , length ) ; pos = nextPos ; } } public void reset ( ) { try { seek ( 0 ) ; } catch ( IOException e ) { throw new RuntimeException ( e . toString ( ) ) ; } file . setLength ( 0 ) ; } public void close ( ) throws IOException { flush ( ) ; } public void seek ( long pos ) throws IOException { setFileLength ( ) ; if ( pos < bufferStart || pos >= bufferStart + bufferLength ) { currentBufferIndex = ( int ) ( pos / BUFFER_SIZE ) ; switchCurrentBuffer ( ) ; } bufferPosition = ( int ) ( pos % BUFFER_SIZE ) ; } public long length ( ) { return file . length ; } public void writeByte ( byte b ) throws IOException { if ( bufferPosition == bufferLength ) { currentBufferIndex ++ ; switchCurrentBuffer ( ) ; } currentBuffer [ bufferPosition ++ ] = b ; } public void writeBytes ( byte [ ] b , int offset , int len ) throws IOException { assert b != null ; while ( len > 0 ) { if ( bufferPosition == bufferLength ) { currentBufferIndex ++ ; switchCurrentBuffer ( ) ; } int remainInBuffer = currentBuffer . length - bufferPosition ; int bytesToCopy = len < remainInBuffer ? len : remainInBuffer ; System . arraycopy ( b , offset , currentBuffer , bufferPosition , bytesToCopy ) ; offset += bytesToCopy ; len -= bytesToCopy ; bufferPosition += bytesToCopy ; } } private final void switchCurrentBuffer ( ) throws IOException { if ( currentBufferIndex == file . numBuffers ( ) ) { currentBuffer = file . addBuffer ( BUFFER_SIZE ) ; } else { currentBuffer = ( byte [ ] ) file . getBuffer ( currentBufferIndex ) ; } bufferPosition = 0 ; bufferStart = ( long ) BUFFER_SIZE * ( long ) currentBufferIndex ; bufferLength = currentBuffer . length ; } private void setFileLength ( ) { long pointer = bufferStart + bufferPosition ; if ( pointer > file . length ) { file . setLength ( pointer ) ; } } public void flush ( ) throws IOException { file . setLastModified ( System . currentTimeMillis ( ) ) ; setFileLength ( ) ; } public long getFilePointer ( ) { return currentBufferIndex < 0 ? 0 : bufferStart + bufferPosition ; } public long sizeInBytes ( ) { return file . numBuffers ( ) * BUFFER_SIZE ; } } 	0	['16', '2', '0', '11', '33', '26', '9', '2', '11', '0.688888889', '332', '0.666666667', '1', '0.566666667', '0.175', '1', '5', '19.1875', '2', '0.9375', '0']
package org . apache . lucene . store ; public class AlreadyClosedException extends IllegalStateException { public AlreadyClosedException ( String message ) { super ( message ) ; } } 	0	['1', '5', '0', '5', '2', '0', '5', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . index ; public class FieldReaderException extends RuntimeException { public FieldReaderException ( ) { } public FieldReaderException ( Throwable cause ) { super ( cause ) ; } public FieldReaderException ( String message ) { super ( message ) ; } public FieldReaderException ( String message , Throwable cause ) { super ( message , cause ) ; } } 	0	['4', '4', '0', '1', '8', '6', '1', '0', '4', '2', '20', '0', '0', '1', '0.666666667', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . search ; import java . util . Iterator ; import java . util . NoSuchElementException ; public class HitIterator implements Iterator { private Hits hits ; private int hitNumber = 0 ; HitIterator ( Hits hits ) { this . hits = hits ; } public boolean hasNext ( ) { return hitNumber < hits . length ( ) ; } public Object next ( ) { if ( hitNumber == hits . length ( ) ) throw new NoSuchElementException ( ) ; Object next = new Hit ( hits , hitNumber ) ; hitNumber ++ ; return next ; } public void remove ( ) { throw new UnsupportedOperationException ( ) ; } public int length ( ) { return hits . length ( ) ; } } 	0	['5', '1', '0', '2', '10', '0', '1', '2', '4', '0.375', '60', '1', '1', '0', '0.6', '0', '0', '10.6', '2', '1.2', '0']
package org . apache . lucene . index ; import java . io . IOException ; abstract class InvertedDocConsumerPerThread { abstract void startDocument ( ) throws IOException ; abstract InvertedDocConsumerPerField addField ( DocInverterPerField docInverterPerField , FieldInfo fieldInfo ) ; abstract DocumentsWriter . DocWriter finishDocument ( ) throws IOException ; abstract void abort ( ) ; } 	0	['5', '1', '1', '9', '6', '10', '6', '4', '0', '2', '8', '0', '0', '0', '0.466666667', '0', '0', '0.6', '1', '0.8', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; public class TeeTokenFilter extends TokenFilter { SinkTokenizer sink ; public TeeTokenFilter ( TokenStream input , SinkTokenizer sink ) { super ( input ) ; this . sink = sink ; } public Token next ( final Token reusableToken ) throws IOException { assert reusableToken != null ; Token nextToken = input . next ( reusableToken ) ; sink . add ( nextToken ) ; return nextToken ; } } 	0	['4', '3', '0', '4', '12', '2', '0', '4', '2', '0.777777778', '58', '0', '1', '0.777777778', '0.4', '1', '2', '12.75', '1', '0.5', '0']
package org . apache . lucene . index ; public interface TermPositionVector extends TermFreqVector { public int [ ] getTermPositions ( int index ) ; public TermVectorOffsetInfo [ ] getOffsets ( int index ) ; } 	0	['2', '1', '0', '4', '2', '1', '2', '2', '2', '2', '2', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . queryParser ; import java . io . * ; public final class FastCharStream implements CharStream { char [ ] buffer = null ; int bufferLength = 0 ; int bufferPosition = 0 ; int tokenStart = 0 ; int bufferStart = 0 ; Reader input ; public FastCharStream ( Reader r ) { input = r ; } public final char readChar ( ) throws IOException { if ( bufferPosition >= bufferLength ) refill ( ) ; return buffer [ bufferPosition ++ ] ; } private final void refill ( ) throws IOException { int newPosition = bufferLength - tokenStart ; if ( tokenStart == 0 ) { if ( buffer == null ) { buffer = new char [ 2048 ] ; } else if ( bufferLength == buffer . length ) { char [ ] newBuffer = new char [ buffer . length * 2 ] ; System . arraycopy ( buffer , 0 , newBuffer , 0 , bufferLength ) ; buffer = newBuffer ; } } else { System . arraycopy ( buffer , tokenStart , buffer , 0 , newPosition ) ; } bufferLength = newPosition ; bufferPosition = newPosition ; bufferStart += tokenStart ; tokenStart = 0 ; int charsRead = input . read ( buffer , newPosition , buffer . length - newPosition ) ; if ( charsRead == - 1 ) throw new IOException ( "read past eof" ) ; else bufferLength += charsRead ; } public final char BeginToken ( ) throws IOException { tokenStart = bufferPosition ; return readChar ( ) ; } public final void backup ( int amount ) { bufferPosition -= amount ; } public final String GetImage ( ) { return new String ( buffer , tokenStart , bufferPosition - tokenStart ) ; } public final char [ ] GetSuffix ( int len ) { char [ ] value = new char [ len ] ; System . arraycopy ( buffer , bufferPosition - len , value , 0 , len ) ; return value ; } public final void Done ( ) { try { input . close ( ) ; } catch ( IOException e ) { System . err . println ( "Caught: " + e + "; ignoring." ) ; } } public final int getColumn ( ) { return bufferStart + bufferPosition ; } public final int getLine ( ) { return 1 ; } public final int getEndColumn ( ) { return bufferStart + bufferPosition ; } public final int getEndLine ( ) { return 1 ; } public final int getBeginColumn ( ) { return bufferStart + tokenStart ; } public final int getBeginLine ( ) { return 1 ; } } 	0	['14', '1', '0', '2', '25', '3', '1', '1', '13', '0.602564103', '237', '0', '0', '0', '0.404761905', '0', '0', '15.5', '1', '0.9286', '0']
package org . apache . lucene . search ; public class DefaultSimilarity extends Similarity { public float lengthNorm ( String fieldName , int numTerms ) { return ( float ) ( 1.0 / Math . sqrt ( numTerms ) ) ; } public float queryNorm ( float sumOfSquaredWeights ) { return ( float ) ( 1.0 / Math . sqrt ( sumOfSquaredWeights ) ) ; } public float tf ( float freq ) { return ( float ) Math . sqrt ( freq ) ; } public float sloppyFreq ( int distance ) { return 1.0f / ( distance + 1 ) ; } public float idf ( int docFreq , int numDocs ) { return ( float ) ( Math . log ( numDocs / ( double ) ( docFreq + 1 ) ) + 1.0 ) ; } public float coord ( int overlap , int maxOverlap ) { return overlap / ( float ) maxOverlap ; } } 	0	['7', '2', '0', '3', '10', '21', '3', '1', '7', '2', '54', '0', '0', '0.714285714', '0.5', '1', '2', '6.714285714', '1', '0.8571', '0']
package org . apache . lucene . index ; import java . io . IOException ; final class TermsHashPerThread extends InvertedDocConsumerPerThread { final TermsHash termsHash ; final TermsHashConsumerPerThread consumer ; final TermsHashPerThread nextPerThread ; final CharBlockPool charPool ; final IntBlockPool intPool ; final ByteBlockPool bytePool ; final boolean primary ; final DocumentsWriter . DocState docState ; final RawPostingList freePostings [ ] = new RawPostingList [ 256 ] ; int freePostingsCount ; public TermsHashPerThread ( DocInverterPerThread docInverterPerThread , final TermsHash termsHash , final TermsHash nextTermsHash , final TermsHashPerThread primaryPerThread ) { docState = docInverterPerThread . docState ; this . termsHash = termsHash ; this . consumer = termsHash . consumer . addThread ( this ) ; if ( nextTermsHash != null ) { charPool = new CharBlockPool ( termsHash . docWriter ) ; primary = true ; } else { charPool = primaryPerThread . charPool ; primary = false ; } intPool = new IntBlockPool ( termsHash . docWriter , termsHash . trackAllocations ) ; bytePool = new ByteBlockPool ( termsHash . docWriter . byteBlockAllocator , termsHash . trackAllocations ) ; if ( nextTermsHash != null ) nextPerThread = nextTermsHash . addThread ( docInverterPerThread , this ) ; else nextPerThread = null ; } InvertedDocConsumerPerField addField ( DocInverterPerField docInverterPerField , final FieldInfo fieldInfo ) { return new TermsHashPerField ( docInverterPerField , this , nextPerThread , fieldInfo ) ; } synchronized public void abort ( ) { reset ( true ) ; consumer . abort ( ) ; if ( nextPerThread != null ) nextPerThread . abort ( ) ; } void morePostings ( ) throws IOException { assert freePostingsCount == 0 ; termsHash . getPostings ( freePostings ) ; freePostingsCount = freePostings . length ; assert noNullPostings ( freePostings , freePostingsCount , "consumer=" + consumer ) ; } private static boolean noNullPostings ( RawPostingList [ ] postings , int count , String details ) { for ( int i = 0 ; i < count ; i ++ ) assert postings [ i ] != null : "postings[" + i + "] of " + count + " is null: " + details ; return true ; } public void startDocument ( ) throws IOException { consumer . startDocument ( ) ; if ( nextPerThread != null ) nextPerThread . consumer . startDocument ( ) ; } public DocumentsWriter . DocWriter finishDocument ( ) throws IOException { final DocumentsWriter . DocWriter doc = consumer . finishDocument ( ) ; final DocumentsWriter . DocWriter doc2 ; if ( nextPerThread != null ) doc2 = nextPerThread . consumer . finishDocument ( ) ; else doc2 = null ; if ( doc == null ) return doc2 ; else { doc . setNext ( doc2 ) ; return doc ; } } void reset ( boolean recyclePostings ) { intPool . reset ( ) ; bytePool . reset ( ) ; if ( primary ) charPool . reset ( ) ; if ( recyclePostings ) { termsHash . recyclePostings ( freePostings , freePostingsCount ) ; freePostingsCount = 0 ; } } } 	0	['10', '2', '0', '24', '37', '7', '9', '18', '4', '0.759259259', '276', '0', '8', '0.333333333', '0.188888889', '0', '0', '25.4', '4', '1.4', '0']
