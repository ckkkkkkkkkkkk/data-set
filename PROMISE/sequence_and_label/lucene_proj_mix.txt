package org . apache . lucene . analysis ; import java . io . Reader ; public final class WhitespaceAnalyzer extends Analyzer { public TokenStream tokenStream ( String fieldName , Reader reader ) { return new WhitespaceTokenizer ( reader ) ; } } 	0	['2', '2', '0', '3', '4', '1', '0', '3', '2', '2', '10', '0', '0', '0.666666667', '0.666666667', '0', '0', '4', '1', '0.5', '0']
package org . apache . lucene . index ; import java . io . IOException ; public interface TermPositions extends TermDocs { int nextPosition ( ) throws IOException ; } 	1	['1', '1', '0', '21', '1', '0', '20', '1', '1', '2', '1', '0', '0', '0', '1', '0', '0', '0', '1', '1', '3']
package org . apache . lucene . search ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . analysis . TokenStream ; import org . apache . lucene . index . TermFreqVector ; import java . io . IOException ; import java . io . StringReader ; import java . util . * ; public class QueryTermVector implements TermFreqVector { private String [ ] terms = new String [ 0 ] ; private int [ ] termFreqs = new int [ 0 ] ; public String getField ( ) { return null ; } public QueryTermVector ( String [ ] queryTerms ) { processTerms ( queryTerms ) ; } public QueryTermVector ( String queryString , Analyzer analyzer ) { if ( analyzer != null ) { TokenStream stream = analyzer . tokenStream ( "" , new StringReader ( queryString ) ) ; if ( stream != null ) { Token next = null ; List terms = new ArrayList ( ) ; try { while ( ( next = stream . next ( ) ) != null ) { terms . add ( next . termText ( ) ) ; } processTerms ( ( String [ ] ) terms . toArray ( new String [ terms . size ( ) ] ) ) ; } catch ( IOException e ) { } } } } private void processTerms ( String [ ] queryTerms ) { if ( queryTerms != null ) { Arrays . sort ( queryTerms ) ; Map tmpSet = new HashMap ( queryTerms . length ) ; List tmpList = new ArrayList ( queryTerms . length ) ; List tmpFreqs = new ArrayList ( queryTerms . length ) ; int j = 0 ; for ( int i = 0 ; i < queryTerms . length ; i ++ ) { String term = queryTerms [ i ] ; Integer position = ( Integer ) tmpSet . get ( term ) ; if ( position == null ) { tmpSet . put ( term , new Integer ( j ++ ) ) ; tmpList . add ( term ) ; tmpFreqs . add ( new Integer ( 1 ) ) ; } else { Integer integer = ( Integer ) tmpFreqs . get ( position . intValue ( ) ) ; tmpFreqs . set ( position . intValue ( ) , new Integer ( integer . intValue ( ) + 1 ) ) ; } } terms = ( String [ ] ) tmpList . toArray ( terms ) ; termFreqs = new int [ tmpFreqs . size ( ) ] ; int i = 0 ; for ( Iterator iter = tmpFreqs . iterator ( ) ; iter . hasNext ( ) ; ) { Integer integer = ( Integer ) iter . next ( ) ; termFreqs [ i ++ ] = integer . intValue ( ) ; } } } public final String toString ( ) { StringBuffer sb = new StringBuffer ( ) ; sb . append ( '{' ) ; for ( int i = 0 ; i < terms . length ; i ++ ) { if ( i > 0 ) sb . append ( ", " ) ; sb . append ( terms [ i ] ) . append ( '/' ) . append ( termFreqs [ i ] ) ; } sb . append ( '}' ) ; return sb . toString ( ) ; } public int size ( ) { return terms . length ; } public String [ ] getTerms ( ) { return terms ; } public int [ ] getTermFrequencies ( ) { return termFreqs ; } public int indexOf ( String term ) { int res = Arrays . binarySearch ( terms , term ) ; return res >= 0 ? res : - 1 ; } public int [ ] indexesOf ( String [ ] terms , int start , int len ) { int res [ ] = new int [ len ] ; for ( int i = 0 ; i < len ; i ++ ) { res [ i ] = indexOf ( terms [ i ] ) ; } return res ; } } 	0	['10', '1', '0', '4', '37', '0', '0', '4', '9', '0.388888889', '278', '1', '0', '0', '0.34', '0', '0', '26.6', '5', '1.6', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; public abstract class TokenStream { public abstract Token next ( ) throws IOException ; public void close ( ) throws IOException { } } 	1	['3', '1', '2', '19', '4', '3', '18', '1', '3', '2', '7', '0', '0', '0', '1', '0', '0', '1.333333333', '1', '0.6667', '1']
package org . apache . lucene . analysis ; import java . io . * ; class PorterStemmer { private char [ ] b ; private int i , j , k , k0 ; private boolean dirty = false ; private static final int INC = 50 ; private static final int EXTRA = 1 ; public PorterStemmer ( ) { b = new char [ INC ] ; i = 0 ; } public void reset ( ) { i = 0 ; dirty = false ; } public void add ( char ch ) { if ( b . length <= i + EXTRA ) { char [ ] new_b = new char [ b . length + INC ] ; for ( int c = 0 ; c < b . length ; c ++ ) new_b [ c ] = b [ c ] ; b = new_b ; } b [ i ++ ] = ch ; } public String toString ( ) { return new String ( b , 0 , i ) ; } public int getResultLength ( ) { return i ; } public char [ ] getResultBuffer ( ) { return b ; } private final boolean cons ( int i ) { switch ( b [ i ] ) { case 'a' : case 'e' : case 'i' : case 'o' : case 'u' : return false ; case 'y' : return ( i == k0 ) ? true : ! cons ( i - 1 ) ; default : return true ; } } private final int m ( ) { int n = 0 ; int i = k0 ; while ( true ) { if ( i > j ) return n ; if ( ! cons ( i ) ) break ; i ++ ; } i ++ ; while ( true ) { while ( true ) { if ( i > j ) return n ; if ( cons ( i ) ) break ; i ++ ; } i ++ ; n ++ ; while ( true ) { if ( i > j ) return n ; if ( ! cons ( i ) ) break ; i ++ ; } i ++ ; } } private final boolean vowelinstem ( ) { int i ; for ( i = k0 ; i <= j ; i ++ ) if ( ! cons ( i ) ) return true ; return false ; } private final boolean doublec ( int j ) { if ( j < k0 + 1 ) return false ; if ( b [ j ] != b [ j - 1 ] ) return false ; return cons ( j ) ; } private final boolean cvc ( int i ) { if ( i < k0 + 2 || ! cons ( i ) || cons ( i - 1 ) || ! cons ( i - 2 ) ) return false ; else { int ch = b [ i ] ; if ( ch == 'w' || ch == 'x' || ch == 'y' ) return false ; } return true ; } private final boolean ends ( String s ) { int l = s . length ( ) ; int o = k - l + 1 ; if ( o < k0 ) return false ; for ( int i = 0 ; i < l ; i ++ ) if ( b [ o + i ] != s . charAt ( i ) ) return false ; j = k - l ; return true ; } void setto ( String s ) { int l = s . length ( ) ; int o = j + 1 ; for ( int i = 0 ; i < l ; i ++ ) b [ o + i ] = s . charAt ( i ) ; k = j + l ; dirty = true ; } void r ( String s ) { if ( m ( ) > 0 ) setto ( s ) ; } private final void step1 ( ) { if ( b [ k ] == 's' ) { if ( ends ( "sses" ) ) k -= 2 ; else if ( ends ( "ies" ) ) setto ( "i" ) ; else if ( b [ k - 1 ] != 's' ) k -- ; } if ( ends ( "eed" ) ) { if ( m ( ) > 0 ) k -- ; } else if ( ( ends ( "ed" ) || ends ( "ing" ) ) && vowelinstem ( ) ) { k = j ; if ( ends ( "at" ) ) setto ( "ate" ) ; else if ( ends ( "bl" ) ) setto ( "ble" ) ; else if ( ends ( "iz" ) ) setto ( "ize" ) ; else if ( doublec ( k ) ) { int ch = b [ k -- ] ; if ( ch == 'l' || ch == 's' || ch == 'z' ) k ++ ; } else if ( m ( ) == 1 && cvc ( k ) ) setto ( "e" ) ; } } private final void step2 ( ) { if ( ends ( "y" ) && vowelinstem ( ) ) { b [ k ] = 'i' ; dirty = true ; } } private final void step3 ( ) { if ( k == k0 ) return ; switch ( b [ k - 1 ] ) { case 'a' : if ( ends ( "ational" ) ) { r ( "ate" ) ; break ; } if ( ends ( "tional" ) ) { r ( "tion" ) ; break ; } break ; case 'c' : if ( ends ( "enci" ) ) { r ( "ence" ) ; break ; } if ( ends ( "anci" ) ) { r ( "ance" ) ; break ; } break ; case 'e' : if ( ends ( "izer" ) ) { r ( "ize" ) ; break ; } break ; case 'l' : if ( ends ( "bli" ) ) { r ( "ble" ) ; break ; } if ( ends ( "alli" ) ) { r ( "al" ) ; break ; } if ( ends ( "entli" ) ) { r ( "ent" ) ; break ; } if ( ends ( "eli" ) ) { r ( "e" ) ; break ; } if ( ends ( "ousli" ) ) { r ( "ous" ) ; break ; } break ; case 'o' : if ( ends ( "ization" ) ) { r ( "ize" ) ; break ; } if ( ends ( "ation" ) ) { r ( "ate" ) ; break ; } if ( ends ( "ator" ) ) { r ( "ate" ) ; break ; } break ; case 's' : if ( ends ( "alism" ) ) { r ( "al" ) ; break ; } if ( ends ( "iveness" ) ) { r ( "ive" ) ; break ; } if ( ends ( "fulness" ) ) { r ( "ful" ) ; break ; } if ( ends ( "ousness" ) ) { r ( "ous" ) ; break ; } break ; case 't' : if ( ends ( "aliti" ) ) { r ( "al" ) ; break ; } if ( ends ( "iviti" ) ) { r ( "ive" ) ; break ; } if ( ends ( "biliti" ) ) { r ( "ble" ) ; break ; } break ; case 'g' : if ( ends ( "logi" ) ) { r ( "log" ) ; break ; } } } private final void step4 ( ) { switch ( b [ k ] ) { case 'e' : if ( ends ( "icate" ) ) { r ( "ic" ) ; break ; } if ( ends ( "ative" ) ) { r ( "" ) ; break ; } if ( ends ( "alize" ) ) { r ( "al" ) ; break ; } break ; case 'i' : if ( ends ( "iciti" ) ) { r ( "ic" ) ; break ; } break ; case 'l' : if ( ends ( "ical" ) ) { r ( "ic" ) ; break ; } if ( ends ( "ful" ) ) { r ( "" ) ; break ; } break ; case 's' : if ( ends ( "ness" ) ) { r ( "" ) ; break ; } break ; } } private final void step5 ( ) { if ( k == k0 ) return ; switch ( b [ k - 1 ] ) { case 'a' : if ( ends ( "al" ) ) break ; return ; case 'c' : if ( ends ( "ance" ) ) break ; if ( ends ( "ence" ) ) break ; return ; case 'e' : if ( ends ( "er" ) ) break ; return ; case 'i' : if ( ends ( "ic" ) ) break ; return ; case 'l' : if ( ends ( "able" ) ) break ; if ( ends ( "ible" ) ) break ; return ; case 'n' : if ( ends ( "ant" ) ) break ; if ( ends ( "ement" ) ) break ; if ( ends ( "ment" ) ) break ; if ( ends ( "ent" ) ) break ; return ; case 'o' : if ( ends ( "ion" ) && j >= 0 && ( b [ j ] == 's' || b [ j ] == 't' ) ) break ; if ( ends ( "ou" ) ) break ; return ; case 's' : if ( ends ( "ism" ) ) break ; return ; case 't' : if ( ends ( "ate" ) ) break ; if ( ends ( "iti" ) ) break ; return ; case 'u' : if ( ends ( "ous" ) ) break ; return ; case 'v' : if ( ends ( "ive" ) ) break ; return ; case 'z' : if ( ends ( "ize" ) ) break ; return ; default : return ; } if ( m ( ) > 1 ) k = j ; } private final void step6 ( ) { j = k ; if ( b [ k ] == 'e' ) { int a = m ( ) ; if ( a > 1 || a == 1 && ! cvc ( k - 1 ) ) k -- ; } if ( b [ k ] == 'l' && doublec ( k ) && m ( ) > 1 ) k -- ; } public String stem ( String s ) { if ( stem ( s . toCharArray ( ) , s . length ( ) ) ) return toString ( ) ; else return s ; } public boolean stem ( char [ ] word ) { return stem ( word , word . length ) ; } public boolean stem ( char [ ] wordBuffer , int offset , int wordLen ) { reset ( ) ; if ( b . length < wordLen ) { char [ ] new_b = new char [ wordLen + EXTRA ] ; b = new_b ; } for ( int j = 0 ; j < wordLen ; j ++ ) b [ j ] = wordBuffer [ offset + j ] ; i = wordLen ; return stem ( 0 ) ; } public boolean stem ( char [ ] word , int wordLen ) { return stem ( word , 0 , wordLen ) ; } public boolean stem ( ) { return stem ( 0 ) ; } public boolean stem ( int i0 ) { k = i - 1 ; k0 = i0 ; if ( k > k0 + 1 ) { step1 ( ) ; step2 ( ) ; step3 ( ) ; step4 ( ) ; step5 ( ) ; step6 ( ) ; } if ( i != k + 1 ) dirty = true ; i = k + 1 ; return dirty ; } public static void main ( String [ ] args ) { PorterStemmer s = new PorterStemmer ( ) ; for ( int i = 0 ; i < args . length ; i ++ ) { try { InputStream in = new FileInputStream ( args [ i ] ) ; byte [ ] buffer = new byte [ 1024 ] ; int bufferLen , offset , ch ; bufferLen = in . read ( buffer ) ; offset = 0 ; s . reset ( ) ; while ( true ) { if ( offset < bufferLen ) ch = buffer [ offset ++ ] ; else { bufferLen = in . read ( buffer ) ; offset = 0 ; if ( bufferLen < 0 ) ch = - 1 ; else ch = buffer [ offset ++ ] ; } if ( Character . isLetter ( ( char ) ch ) ) { s . add ( Character . toLowerCase ( ( char ) ch ) ) ; } else { s . stem ( ) ; System . out . print ( s . toString ( ) ) ; s . reset ( ) ; if ( ch < 0 ) break ; else { System . out . print ( ( char ) ch ) ; } } } in . close ( ) ; } catch ( IOException e ) { System . out . println ( "error reading " + args [ i ] ) ; } } } } 	0	['27', '1', '0', '1', '43', '13', '1', '0', '13', '0.600961538', '1174', '1', '0', '0', '0.25308642', '0', '0', '42.18518519', '26', '5.7407', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . * ; final class ExactPhraseScorer extends PhraseScorer { ExactPhraseScorer ( Weight weight , TermPositions [ ] tps , int [ ] positions , Similarity similarity , byte [ ] norms ) { super ( weight , tps , positions , similarity , norms ) ; } protected final float phraseFreq ( ) throws IOException { for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) { pp . firstPosition ( ) ; pq . put ( pp ) ; } pqToList ( ) ; int freq = 0 ; do { while ( first . position < last . position ) { do { if ( ! first . nextPosition ( ) ) return ( float ) freq ; } while ( first . position < last . position ) ; firstToLast ( ) ; } freq ++ ; } while ( last . nextPosition ( ) ) ; return ( float ) freq ; } } 	1	['2', '3', '0', '8', '8', '1', '2', '6', '0', '2', '61', '0', '0', '0.952380952', '0.583333333', '1', '1', '29.5', '1', '0.5', '1']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermDocs ; import org . apache . lucene . index . TermEnum ; import java . io . IOException ; import java . util . BitSet ; public class RangeFilter extends Filter { private String fieldName ; private String lowerTerm ; private String upperTerm ; private boolean includeLower ; private boolean includeUpper ; public RangeFilter ( String fieldName , String lowerTerm , String upperTerm , boolean includeLower , boolean includeUpper ) { this . fieldName = fieldName ; this . lowerTerm = lowerTerm ; this . upperTerm = upperTerm ; this . includeLower = includeLower ; this . includeUpper = includeUpper ; if ( null == lowerTerm && null == upperTerm ) { throw new IllegalArgumentException ( "At least one value must be non-null" ) ; } if ( includeLower && null == lowerTerm ) { throw new IllegalArgumentException ( "The lower bound must be non-null to be inclusive" ) ; } if ( includeUpper && null == upperTerm ) { throw new IllegalArgumentException ( "The upper bound must be non-null to be inclusive" ) ; } } public static RangeFilter Less ( String fieldName , String upperTerm ) { return new RangeFilter ( fieldName , null , upperTerm , false , true ) ; } public static RangeFilter More ( String fieldName , String lowerTerm ) { return new RangeFilter ( fieldName , lowerTerm , null , true , false ) ; } public BitSet bits ( IndexReader reader ) throws IOException { BitSet bits = new BitSet ( reader . maxDoc ( ) ) ; TermEnum enumerator = ( null != lowerTerm ? reader . terms ( new Term ( fieldName , lowerTerm ) ) : reader . terms ( new Term ( fieldName , "" ) ) ) ; try { if ( enumerator . term ( ) == null ) { return bits ; } boolean checkLower = false ; if ( ! includeLower ) checkLower = true ; TermDocs termDocs = reader . termDocs ( ) ; try { do { Term term = enumerator . term ( ) ; if ( term != null && term . field ( ) . equals ( fieldName ) ) { if ( ! checkLower || null == lowerTerm || term . text ( ) . compareTo ( lowerTerm ) > 0 ) { checkLower = false ; if ( upperTerm != null ) { int compare = upperTerm . compareTo ( term . text ( ) ) ; if ( ( compare < 0 ) || ( ! includeUpper && compare == 0 ) ) { break ; } } termDocs . seek ( enumerator . term ( ) ) ; while ( termDocs . next ( ) ) { bits . set ( termDocs . doc ( ) ) ; } } } else { break ; } } while ( enumerator . next ( ) ) ; } finally { termDocs . close ( ) ; } } finally { enumerator . close ( ) ; } return bits ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( fieldName ) ; buffer . append ( ":" ) ; buffer . append ( includeLower ? "[" : "{" ) ; if ( null != lowerTerm ) { buffer . append ( lowerTerm ) ; } buffer . append ( "-" ) ; if ( null != upperTerm ) { buffer . append ( upperTerm ) ; } buffer . append ( includeUpper ? "]" : "}" ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof RangeFilter ) ) return false ; RangeFilter other = ( RangeFilter ) o ; if ( ! this . fieldName . equals ( other . fieldName ) || this . includeLower != other . includeLower || this . includeUpper != other . includeUpper ) { return false ; } if ( this . lowerTerm != null ? ! this . lowerTerm . equals ( other . lowerTerm ) : other . lowerTerm != null ) return false ; if ( this . upperTerm != null ? ! this . upperTerm . equals ( other . upperTerm ) : other . upperTerm != null ) return false ; return true ; } public int hashCode ( ) { int h = fieldName . hashCode ( ) ; h ^= lowerTerm != null ? lowerTerm . hashCode ( ) : 0xB6ECE882 ; h = ( h << 1 ) | ( h > > > 31 ) ; h ^= ( upperTerm != null ? ( upperTerm . hashCode ( ) ) : 0x91BEC2C2 ) ; h ^= ( includeLower ? 0xD484B933 : 0 ) ^ ( includeUpper ? 0x6AE423AC : 0 ) ; return h ; } } 	0	['7', '2', '0', '6', '30', '1', '1', '5', '7', '0', '373', '1', '0', '0.142857143', '0.314285714', '1', '1', '51.57142857', '12', '3.5714', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; final class SegmentInfo { public String name ; public int docCount ; public Directory dir ; public SegmentInfo ( String name , int docCount , Directory dir ) { this . name = name ; this . docCount = docCount ; this . dir = dir ; } } 	1	['1', '1', '0', '5', '2', '0', '4', '1', '1', '2', '16', '0', '1', '0', '1', '0', '0', '12', '0', '0', '6']
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; final class HitQueue extends PriorityQueue { HitQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object a , Object b ) { ScoreDoc hitA = ( ScoreDoc ) a ; ScoreDoc hitB = ( ScoreDoc ) b ; if ( hitA . score == hitB . score ) return hitA . doc > hitB . doc ; else return hitA . score < hitB . score ; } } 	0	['2', '2', '0', '6', '4', '1', '4', '2', '0', '2', '39', '0', '0', '0.916666667', '0.666666667', '1', '3', '18.5', '4', '2', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class BufferedIndexInput extends IndexInput { static final int BUFFER_SIZE = BufferedIndexOutput . BUFFER_SIZE ; private byte [ ] buffer ; private long bufferStart = 0 ; private int bufferLength = 0 ; private int bufferPosition = 0 ; public byte readByte ( ) throws IOException { if ( bufferPosition >= bufferLength ) refill ( ) ; return buffer [ bufferPosition ++ ] ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { if ( len < BUFFER_SIZE ) { for ( int i = 0 ; i < len ; i ++ ) b [ i + offset ] = ( byte ) readByte ( ) ; } else { long start = getFilePointer ( ) ; seekInternal ( start ) ; readInternal ( b , offset , len ) ; bufferStart = start + len ; bufferPosition = 0 ; bufferLength = 0 ; } } private void refill ( ) throws IOException { long start = bufferStart + bufferPosition ; long end = start + BUFFER_SIZE ; if ( end > length ( ) ) end = length ( ) ; bufferLength = ( int ) ( end - start ) ; if ( bufferLength <= 0 ) throw new IOException ( "read past EOF" ) ; if ( buffer == null ) buffer = new byte [ BUFFER_SIZE ] ; readInternal ( buffer , 0 , bufferLength ) ; bufferStart = start ; bufferPosition = 0 ; } protected abstract void readInternal ( byte [ ] b , int offset , int length ) throws IOException ; public long getFilePointer ( ) { return bufferStart + bufferPosition ; } public void seek ( long pos ) throws IOException { if ( pos >= bufferStart && pos < ( bufferStart + bufferLength ) ) bufferPosition = ( int ) ( pos - bufferStart ) ; else { bufferStart = pos ; bufferPosition = 0 ; bufferLength = 0 ; seekInternal ( pos ) ; } } protected abstract void seekInternal ( long pos ) throws IOException ; public Object clone ( ) { BufferedIndexInput clone = ( BufferedIndexInput ) super . clone ( ) ; if ( buffer != null ) { clone . buffer = new byte [ BUFFER_SIZE ] ; System . arraycopy ( buffer , 0 , clone . buffer , 0 , bufferLength ) ; } return clone ; } } 	1	['9', '2', '3', '4', '14', '0', '3', '1', '6', '0.575', '204', '0.8', '0', '0.619047619', '0.416666667', '1', '4', '21.11111111', '2', '1', '3']
package org . apache . lucene . search ; import java . io . IOException ; class NonMatchingScorer extends Scorer { public NonMatchingScorer ( ) { super ( null ) ; } public int doc ( ) { throw new UnsupportedOperationException ( ) ; } public boolean next ( ) throws IOException { return false ; } public float score ( ) { throw new UnsupportedOperationException ( ) ; } public boolean skipTo ( int target ) { return false ; } public Explanation explain ( int doc ) { Explanation e = new Explanation ( ) ; e . setDescription ( "No document matches." ) ; return e ; } } 	0	['6', '2', '0', '4', '10', '15', '1', '3', '6', '2', '31', '0', '0', '0.615384615', '0.666666667', '1', '3', '4.166666667', '1', '0.8333', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . Directory ; import org . apache . lucene . util . StringHelper ; final class TermInfosWriter { public static final int FORMAT = - 2 ; private FieldInfos fieldInfos ; private IndexOutput output ; private Term lastTerm = new Term ( "" , "" ) ; private TermInfo lastTi = new TermInfo ( ) ; private long size = 0 ; int indexInterval = 128 ; int skipInterval = 16 ; private long lastIndexPointer = 0 ; private boolean isIndex = false ; private TermInfosWriter other = null ; TermInfosWriter ( Directory directory , String segment , FieldInfos fis , int interval ) throws IOException { initialize ( directory , segment , fis , interval , false ) ; other = new TermInfosWriter ( directory , segment , fis , interval , true ) ; other . other = this ; } private TermInfosWriter ( Directory directory , String segment , FieldInfos fis , int interval , boolean isIndex ) throws IOException { initialize ( directory , segment , fis , interval , isIndex ) ; } private void initialize ( Directory directory , String segment , FieldInfos fis , int interval , boolean isi ) throws IOException { indexInterval = interval ; fieldInfos = fis ; isIndex = isi ; output = directory . createOutput ( segment + ( isIndex ? ".tii" : ".tis" ) ) ; output . writeInt ( FORMAT ) ; output . writeLong ( 0 ) ; output . writeInt ( indexInterval ) ; output . writeInt ( skipInterval ) ; } final void add ( Term term , TermInfo ti ) throws IOException { if ( ! isIndex && term . compareTo ( lastTerm ) <= 0 ) throw new IOException ( "term out of order (\"" + term + "\".compareTo(\"" + lastTerm + "\") <= 0)" ) ; if ( ti . freqPointer < lastTi . freqPointer ) throw new IOException ( "freqPointer out of order (" + ti . freqPointer + " < " + lastTi . freqPointer + ")" ) ; if ( ti . proxPointer < lastTi . proxPointer ) throw new IOException ( "proxPointer out of order (" + ti . proxPointer + " < " + lastTi . proxPointer + ")" ) ; if ( ! isIndex && size % indexInterval == 0 ) other . add ( lastTerm , lastTi ) ; writeTerm ( term ) ; output . writeVInt ( ti . docFreq ) ; output . writeVLong ( ti . freqPointer - lastTi . freqPointer ) ; output . writeVLong ( ti . proxPointer - lastTi . proxPointer ) ; if ( ti . docFreq >= skipInterval ) { output . writeVInt ( ti . skipOffset ) ; } if ( isIndex ) { output . writeVLong ( other . output . getFilePointer ( ) - lastIndexPointer ) ; lastIndexPointer = other . output . getFilePointer ( ) ; } lastTi . set ( ti ) ; size ++ ; } private final void writeTerm ( Term term ) throws IOException { int start = StringHelper . stringDifference ( lastTerm . text , term . text ) ; int length = term . text . length ( ) - start ; output . writeVInt ( start ) ; output . writeVInt ( length ) ; output . writeChars ( term . text , start , length ) ; output . writeVInt ( fieldInfos . fieldNumber ( term . field ) ) ; lastTerm = term ; } final void close ( ) throws IOException { output . seek ( 4 ) ; output . writeLong ( size ) ; output . close ( ) ; if ( ! isIndex ) other . close ( ) ; } } 	1	['6', '1', '0', '8', '29', '0', '2', '6', '0', '0.436363636', '383', '0.727272727', '5', '0', '0.479166667', '0', '0', '61', '1', '0.6667', '2']
package org . apache . lucene . search ; public class TopFieldDocs extends TopDocs { public SortField [ ] fields ; TopFieldDocs ( int totalHits , ScoreDoc [ ] scoreDocs , SortField [ ] fields , float maxScore ) { super ( totalHits , scoreDocs , maxScore ) ; this . fields = fields ; } } 	0	['1', '2', '0', '14', '2', '0', '11', '3', '0', '2', '11', '0', '1', '1', '1', '0', '0', '9', '0', '0', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . HashMap ; import java . util . HashSet ; import java . util . Map ; import java . util . Set ; import org . apache . lucene . document . Document ; import org . apache . lucene . index . Term ; public class MultiSearcher extends Searcher { private static class CachedDfSource extends Searcher { private Map dfMap ; private int maxDoc ; public CachedDfSource ( Map dfMap , int maxDoc ) { this . dfMap = dfMap ; this . maxDoc = maxDoc ; } public int docFreq ( Term term ) { int df ; try { df = ( ( Integer ) dfMap . get ( term ) ) . intValue ( ) ; } catch ( NullPointerException e ) { throw new IllegalArgumentException ( "df for term " + term . text ( ) + " not available" ) ; } return df ; } public int [ ] docFreqs ( Term [ ] terms ) { int [ ] result = new int [ terms . length ] ; for ( int i = 0 ; i < terms . length ; i ++ ) { result [ i ] = docFreq ( terms [ i ] ) ; } return result ; } public int maxDoc ( ) { return maxDoc ; } public Query rewrite ( Query query ) { return query ; } public void close ( ) { throw new UnsupportedOperationException ( ) ; } public Document doc ( int i ) { throw new UnsupportedOperationException ( ) ; } public Explanation explain ( Weight weight , int doc ) { throw new UnsupportedOperationException ( ) ; } public void search ( Weight weight , Filter filter , HitCollector results ) { throw new UnsupportedOperationException ( ) ; } public TopDocs search ( Weight weight , Filter filter , int n ) { throw new UnsupportedOperationException ( ) ; } public TopFieldDocs search ( Weight weight , Filter filter , int n , Sort sort ) { throw new UnsupportedOperationException ( ) ; } } ; private Searchable [ ] searchables ; private int [ ] starts ; private int maxDoc = 0 ; public MultiSearcher ( Searchable [ ] searchables ) throws IOException { this . searchables = searchables ; starts = new int [ searchables . length + 1 ] ; for ( int i = 0 ; i < searchables . length ; i ++ ) { starts [ i ] = maxDoc ; maxDoc += searchables [ i ] . maxDoc ( ) ; } starts [ searchables . length ] = maxDoc ; } public Searchable [ ] getSearchables ( ) { return searchables ; } protected int [ ] getStarts ( ) { return starts ; } public void close ( ) throws IOException { for ( int i = 0 ; i < searchables . length ; i ++ ) searchables [ i ] . close ( ) ; } public int docFreq ( Term term ) throws IOException { int docFreq = 0 ; for ( int i = 0 ; i < searchables . length ; i ++ ) docFreq += searchables [ i ] . docFreq ( term ) ; return docFreq ; } public Document doc ( int n ) throws IOException { int i = subSearcher ( n ) ; return searchables [ i ] . doc ( n - starts [ i ] ) ; } public int subSearcher ( int n ) { int lo = 0 ; int hi = searchables . length - 1 ; while ( hi >= lo ) { int mid = ( lo + hi ) > > 1 ; int midValue = starts [ mid ] ; if ( n < midValue ) hi = mid - 1 ; else if ( n > midValue ) lo = mid + 1 ; else { while ( mid + 1 < searchables . length && starts [ mid + 1 ] == midValue ) { mid ++ ; } return mid ; } } return hi ; } public int subDoc ( int n ) { return n - starts [ subSearcher ( n ) ] ; } public int maxDoc ( ) throws IOException { return maxDoc ; } public TopDocs search ( Weight weight , Filter filter , int nDocs ) throws IOException { HitQueue hq = new HitQueue ( nDocs ) ; int totalHits = 0 ; for ( int i = 0 ; i < searchables . length ; i ++ ) { TopDocs docs = searchables [ i ] . search ( weight , filter , nDocs ) ; totalHits += docs . totalHits ; ScoreDoc [ ] scoreDocs = docs . scoreDocs ; for ( int j = 0 ; j < scoreDocs . length ; j ++ ) { ScoreDoc scoreDoc = scoreDocs [ j ] ; scoreDoc . doc += starts [ i ] ; if ( ! hq . insert ( scoreDoc ) ) break ; } } ScoreDoc [ ] scoreDocs = new ScoreDoc [ hq . size ( ) ] ; for ( int i = hq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = ( ScoreDoc ) hq . pop ( ) ; float maxScore = ( totalHits == 0 ) ? Float . NEGATIVE_INFINITY : scoreDocs [ 0 ] . score ; return new TopDocs ( totalHits , scoreDocs , maxScore ) ; } public TopFieldDocs search ( Weight weight , Filter filter , int n , Sort sort ) throws IOException { FieldDocSortedHitQueue hq = null ; int totalHits = 0 ; float maxScore = Float . NEGATIVE_INFINITY ; for ( int i = 0 ; i < searchables . length ; i ++ ) { TopFieldDocs docs = searchables [ i ] . search ( weight , filter , n , sort ) ; if ( hq == null ) hq = new FieldDocSortedHitQueue ( docs . fields , n ) ; totalHits += docs . totalHits ; maxScore = Math . max ( maxScore , docs . getMaxScore ( ) ) ; ScoreDoc [ ] scoreDocs = docs . scoreDocs ; for ( int j = 0 ; j < scoreDocs . length ; j ++ ) { ScoreDoc scoreDoc = scoreDocs [ j ] ; scoreDoc . doc += starts [ i ] ; if ( ! hq . insert ( scoreDoc ) ) break ; } } ScoreDoc [ ] scoreDocs = new ScoreDoc [ hq . size ( ) ] ; for ( int i = hq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = ( ScoreDoc ) hq . pop ( ) ; return new TopFieldDocs ( totalHits , scoreDocs , hq . getFields ( ) , maxScore ) ; } public void search ( Weight weight , Filter filter , final HitCollector results ) throws IOException { for ( int i = 0 ; i < searchables . length ; i ++ ) { final int start = starts [ i ] ; searchables [ i ] . search ( weight , filter , new HitCollector ( ) { public void collect ( int doc , float score ) { results . collect ( doc + start , score ) ; } } ) ; } } public Query rewrite ( Query original ) throws IOException { Query [ ] queries = new Query [ searchables . length ] ; for ( int i = 0 ; i < searchables . length ; i ++ ) { queries [ i ] = searchables [ i ] . rewrite ( original ) ; } return queries [ 0 ] . combine ( queries ) ; } public Explanation explain ( Weight weight , int doc ) throws IOException { int i = subSearcher ( doc ) ; return searchables [ i ] . explain ( weight , doc - starts [ i ] ) ; } protected Weight createWeight ( Query original ) throws IOException { Query rewrittenQuery = rewrite ( original ) ; Set terms = new HashSet ( ) ; rewrittenQuery . extractTerms ( terms ) ; Term [ ] allTermsArray = new Term [ terms . size ( ) ] ; terms . toArray ( allTermsArray ) ; int [ ] aggregatedDfs = new int [ terms . size ( ) ] ; for ( int i = 0 ; i < searchables . length ; i ++ ) { int [ ] dfs = searchables [ i ] . docFreqs ( allTermsArray ) ; for ( int j = 0 ; j < aggregatedDfs . length ; j ++ ) { aggregatedDfs [ j ] += dfs [ j ] ; } } HashMap dfMap = new HashMap ( ) ; for ( int i = 0 ; i < allTermsArray . length ; i ++ ) { dfMap . put ( allTermsArray [ i ] , new Integer ( aggregatedDfs [ i ] ) ) ; } int numDocs = maxDoc ( ) ; CachedDfSource cacheSim = new CachedDfSource ( dfMap , numDocs ) ; return rewrittenQuery . weight ( cacheSim ) ; } } 	1	['15', '2', '1', '19', '50', '0', '2', '18', '13', '0.476190476', '557', '1', '1', '0.611111111', '0.251851852', '1', '5', '35.93333333', '6', '1.2667', '3']
package org . apache . lucene . analysis . standard ; public interface StandardTokenizerConstants { int EOF = 0 ; int ALPHANUM = 1 ; int APOSTROPHE = 2 ; int ACRONYM = 3 ; int COMPANY = 4 ; int EMAIL = 5 ; int HOST = 6 ; int NUM = 7 ; int P = 8 ; int HAS_DIGIT = 9 ; int ALPHA = 10 ; int LETTER = 11 ; int CJ = 12 ; int KOREAN = 13 ; int DIGIT = 14 ; int NOISE = 15 ; int DEFAULT = 0 ; String [ ] tokenImage = { "<EOF>" , "<ALPHANUM>" , "<APOSTROPHE>" , "<ACRONYM>" , "<COMPANY>" , "<EMAIL>" , "<HOST>" , "<NUM>" , "<P>" , "<HAS_DIGIT>" , "<ALPHA>" , "<LETTER>" , "<CJ>" , "<KOREAN>" , "<DIGIT>" , "<NOISE>" , } ; } 	0	['1', '1', '0', '3', '1', '0', '3', '0', '0', '2', '87', '0', '0', '0', '0', '0', '0', '68', '0', '0', '0']
package org . apache . lucene . analysis . standard ; public class ParseException extends java . io . IOException { public ParseException ( Token currentTokenVal , int [ ] [ ] expectedTokenSequencesVal , String [ ] tokenImageVal ) { super ( "" ) ; specialConstructor = true ; currentToken = currentTokenVal ; expectedTokenSequences = expectedTokenSequencesVal ; tokenImage = tokenImageVal ; } public ParseException ( ) { super ( ) ; specialConstructor = false ; } public ParseException ( String message ) { super ( message ) ; specialConstructor = false ; } protected boolean specialConstructor ; public Token currentToken ; public int [ ] [ ] expectedTokenSequences ; public String [ ] tokenImage ; public String getMessage ( ) { if ( ! specialConstructor ) { return super . getMessage ( ) ; } String expected = "" ; int maxSize = 0 ; for ( int i = 0 ; i < expectedTokenSequences . length ; i ++ ) { if ( maxSize < expectedTokenSequences [ i ] . length ) { maxSize = expectedTokenSequences [ i ] . length ; } for ( int j = 0 ; j < expectedTokenSequences [ i ] . length ; j ++ ) { expected += tokenImage [ expectedTokenSequences [ i ] [ j ] ] + " " ; } if ( expectedTokenSequences [ i ] [ expectedTokenSequences [ i ] . length - 1 ] != 0 ) { expected += "..." ; } expected += eol + "    " ; } String retval = "Encountered \"" ; Token tok = currentToken . next ; for ( int i = 0 ; i < maxSize ; i ++ ) { if ( i != 0 ) retval += " " ; if ( tok . kind == 0 ) { retval += tokenImage [ 0 ] ; break ; } retval += add_escapes ( tok . image ) ; tok = tok . next ; } retval += "\" at line " + currentToken . next . beginLine + ", column " + currentToken . next . beginColumn + "." + eol ; if ( expectedTokenSequences . length == 1 ) { retval += "Was expecting:" + eol + "    " ; } else { retval += "Was expecting one of:" + eol + "    " ; } retval += expected ; return retval ; } protected String eol = System . getProperty ( "line.separator" , "\n" ) ; protected String add_escapes ( String str ) { StringBuffer retval = new StringBuffer ( ) ; char ch ; for ( int i = 0 ; i < str . length ( ) ; i ++ ) { switch ( str . charAt ( i ) ) { case 0 : continue ; case '\b' : retval . append ( "\\b" ) ; continue ; case '\t' : retval . append ( "\\t" ) ; continue ; case '\n' : retval . append ( "\\n" ) ; continue ; case '\f' : retval . append ( "\\f" ) ; continue ; case '\r' : retval . append ( "\\r" ) ; continue ; case '\"' : retval . append ( "\\\"" ) ; continue ; case '\'' : retval . append ( "\\\'" ) ; continue ; case '\\' : retval . append ( "\\\\" ) ; continue ; default : if ( ( ch = str . charAt ( i ) ) < 0x20 || ch > 0x7e ) { String s = "0000" + Integer . toString ( ch , 16 ) ; retval . append ( "\\u" + s . substring ( s . length ( ) - 4 , s . length ( ) ) ) ; } else { retval . append ( ch ) ; } continue ; } } return retval . toString ( ) ; } } 	1	['5', '4', '0', '2', '18', '0', '1', '1', '4', '0.55', '380', '0.4', '1', '0.866666667', '0.4', '1', '1', '74', '14', '4.8', '1']
package org . apache . lucene . index ; import java . io . IOException ; public abstract class TermEnum { public abstract boolean next ( ) throws IOException ; public abstract Term term ( ) ; public abstract int docFreq ( ) ; public abstract void close ( ) throws IOException ; public boolean skipTo ( Term target ) throws IOException { do { if ( ! next ( ) ) return false ; } while ( target . compareTo ( term ( ) ) > 0 ) ; return true ; } } 	0	['6', '1', '5', '25', '8', '15', '24', '1', '6', '2', '21', '0', '0', '0', '0.583333333', '0', '0', '2.5', '1', '0.8333', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . * ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . MultipleTermPositions ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermPositions ; import org . apache . lucene . search . Query ; import org . apache . lucene . util . ToStringUtils ; public class MultiPhraseQuery extends Query { private String field ; private ArrayList termArrays = new ArrayList ( ) ; private Vector positions = new Vector ( ) ; private int slop = 0 ; public void setSlop ( int s ) { slop = s ; } public int getSlop ( ) { return slop ; } public void add ( Term term ) { add ( new Term [ ] { term } ) ; } public void add ( Term [ ] terms ) { int position = 0 ; if ( positions . size ( ) > 0 ) position = ( ( Integer ) positions . lastElement ( ) ) . intValue ( ) + 1 ; add ( terms , position ) ; } public void add ( Term [ ] terms , int position ) { if ( termArrays . size ( ) == 0 ) field = terms [ 0 ] . field ( ) ; for ( int i = 0 ; i < terms . length ; i ++ ) { if ( terms [ i ] . field ( ) != field ) { throw new IllegalArgumentException ( "All phrase terms must be in the same field (" + field + "): " + terms [ i ] ) ; } } termArrays . add ( terms ) ; positions . addElement ( new Integer ( position ) ) ; } public List getTermArrays ( ) { return Collections . unmodifiableList ( termArrays ) ; } public int [ ] getPositions ( ) { int [ ] result = new int [ positions . size ( ) ] ; for ( int i = 0 ; i < positions . size ( ) ; i ++ ) result [ i ] = ( ( Integer ) positions . elementAt ( i ) ) . intValue ( ) ; return result ; } public void extractTerms ( Set terms ) { for ( Iterator iter = termArrays . iterator ( ) ; iter . hasNext ( ) ; ) { Term [ ] arr = ( Term [ ] ) iter . next ( ) ; for ( int i = 0 ; i < arr . length ; i ++ ) { terms . add ( arr [ i ] ) ; } } } private class MultiPhraseWeight implements Weight { private Similarity similarity ; private float value ; private float idf ; private float queryNorm ; private float queryWeight ; public MultiPhraseWeight ( Searcher searcher ) throws IOException { this . similarity = getSimilarity ( searcher ) ; Iterator i = termArrays . iterator ( ) ; while ( i . hasNext ( ) ) { Term [ ] terms = ( Term [ ] ) i . next ( ) ; for ( int j = 0 ; j < terms . length ; j ++ ) { idf += getSimilarity ( searcher ) . idf ( terms [ j ] , searcher ) ; } } } public Query getQuery ( ) { return MultiPhraseQuery . this ; } public float getValue ( ) { return value ; } public float sumOfSquaredWeights ( ) { queryWeight = idf * getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float queryNorm ) { this . queryNorm = queryNorm ; queryWeight *= queryNorm ; value = queryWeight * idf ; } public Scorer scorer ( IndexReader reader ) throws IOException { if ( termArrays . size ( ) == 0 ) return null ; TermPositions [ ] tps = new TermPositions [ termArrays . size ( ) ] ; for ( int i = 0 ; i < tps . length ; i ++ ) { Term [ ] terms = ( Term [ ] ) termArrays . get ( i ) ; TermPositions p ; if ( terms . length > 1 ) p = new MultipleTermPositions ( reader , terms ) ; else p = reader . termPositions ( terms [ 0 ] ) ; if ( p == null ) return null ; tps [ i ] = p ; } if ( slop == 0 ) return new ExactPhraseScorer ( this , tps , getPositions ( ) , similarity , reader . norms ( field ) ) ; else return new SloppyPhraseScorer ( this , tps , getPositions ( ) , similarity , slop , reader . norms ( field ) ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { Explanation result = new Explanation ( ) ; result . setDescription ( "weight(" + getQuery ( ) + " in " + doc + "), product of:" ) ; Explanation idfExpl = new Explanation ( idf , "idf(" + getQuery ( ) + ")" ) ; Explanation queryExpl = new Explanation ( ) ; queryExpl . setDescription ( "queryWeight(" + getQuery ( ) + "), product of:" ) ; Explanation boostExpl = new Explanation ( getBoost ( ) , "boost" ) ; if ( getBoost ( ) != 1.0f ) queryExpl . addDetail ( boostExpl ) ; queryExpl . addDetail ( idfExpl ) ; Explanation queryNormExpl = new Explanation ( queryNorm , "queryNorm" ) ; queryExpl . addDetail ( queryNormExpl ) ; queryExpl . setValue ( boostExpl . getValue ( ) * idfExpl . getValue ( ) * queryNormExpl . getValue ( ) ) ; result . addDetail ( queryExpl ) ; Explanation fieldExpl = new Explanation ( ) ; fieldExpl . setDescription ( "fieldWeight(" + getQuery ( ) + " in " + doc + "), product of:" ) ; Explanation tfExpl = scorer ( reader ) . explain ( doc ) ; fieldExpl . addDetail ( tfExpl ) ; fieldExpl . addDetail ( idfExpl ) ; Explanation fieldNormExpl = new Explanation ( ) ; byte [ ] fieldNorms = reader . norms ( field ) ; float fieldNorm = fieldNorms != null ? Similarity . decodeNorm ( fieldNorms [ doc ] ) : 0.0f ; fieldNormExpl . setValue ( fieldNorm ) ; fieldNormExpl . setDescription ( "fieldNorm(field=" + field + ", doc=" + doc + ")" ) ; fieldExpl . addDetail ( fieldNormExpl ) ; fieldExpl . setValue ( tfExpl . getValue ( ) * idfExpl . getValue ( ) * fieldNormExpl . getValue ( ) ) ; result . addDetail ( fieldExpl ) ; result . setValue ( queryExpl . getValue ( ) * fieldExpl . getValue ( ) ) ; if ( queryExpl . getValue ( ) == 1.0f ) return fieldExpl ; return result ; } } public Query rewrite ( IndexReader reader ) { if ( termArrays . size ( ) == 1 ) { Term [ ] terms = ( Term [ ] ) termArrays . get ( 0 ) ; BooleanQuery boq = new BooleanQuery ( true ) ; for ( int i = 0 ; i < terms . length ; i ++ ) { boq . add ( new TermQuery ( terms [ i ] ) , BooleanClause . Occur . SHOULD ) ; } boq . setBoost ( getBoost ( ) ) ; return boq ; } else { return this ; } } protected Weight createWeight ( Searcher searcher ) throws IOException { return new MultiPhraseWeight ( searcher ) ; } public final String toString ( String f ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! field . equals ( f ) ) { buffer . append ( field ) ; buffer . append ( ":" ) ; } buffer . append ( "\"" ) ; Iterator i = termArrays . iterator ( ) ; while ( i . hasNext ( ) ) { Term [ ] terms = ( Term [ ] ) i . next ( ) ; if ( terms . length > 1 ) { buffer . append ( "(" ) ; for ( int j = 0 ; j < terms . length ; j ++ ) { buffer . append ( terms [ j ] . text ( ) ) ; if ( j < terms . length - 1 ) buffer . append ( " " ) ; } buffer . append ( ")" ) ; } else { buffer . append ( terms [ 0 ] . text ( ) ) ; } if ( i . hasNext ( ) ) buffer . append ( " " ) ; } buffer . append ( "\"" ) ; if ( slop != 0 ) { buffer . append ( "~" ) ; buffer . append ( slop ) ; } buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof MultiPhraseQuery ) ) return false ; MultiPhraseQuery other = ( MultiPhraseQuery ) o ; return this . getBoost ( ) == other . getBoost ( ) && this . slop == other . slop && this . termArrays . equals ( other . termArrays ) && this . positions . equals ( other . positions ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ slop ^ termArrays . hashCode ( ) ^ positions . hashCode ( ) ^ 0x4AC65113 ; } } 	1	['17', '2', '0', '12', '55', '12', '3', '10', '13', '0.625', '404', '1', '0', '0.428571429', '0.158823529', '2', '4', '22.52941176', '7', '2.0588', '1']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . util . StringHelper ; import java . io . IOException ; import java . util . Vector ; final class TermVectorsWriter { static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x1 ; static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x2 ; static final int FORMAT_VERSION = 2 ; static final int FORMAT_SIZE = 4 ; static final String TVX_EXTENSION = ".tvx" ; static final String TVD_EXTENSION = ".tvd" ; static final String TVF_EXTENSION = ".tvf" ; private IndexOutput tvx = null , tvd = null , tvf = null ; private Vector fields = null ; private Vector terms = null ; private FieldInfos fieldInfos ; private TVField currentField = null ; private long currentDocPointer = - 1 ; public TermVectorsWriter ( Directory directory , String segment , FieldInfos fieldInfos ) throws IOException { tvx = directory . createOutput ( segment + TVX_EXTENSION ) ; tvx . writeInt ( FORMAT_VERSION ) ; tvd = directory . createOutput ( segment + TVD_EXTENSION ) ; tvd . writeInt ( FORMAT_VERSION ) ; tvf = directory . createOutput ( segment + TVF_EXTENSION ) ; tvf . writeInt ( FORMAT_VERSION ) ; this . fieldInfos = fieldInfos ; fields = new Vector ( fieldInfos . size ( ) ) ; terms = new Vector ( ) ; } public final void openDocument ( ) throws IOException { closeDocument ( ) ; currentDocPointer = tvd . getFilePointer ( ) ; } public final void closeDocument ( ) throws IOException { if ( isDocumentOpen ( ) ) { closeField ( ) ; writeDoc ( ) ; fields . clear ( ) ; currentDocPointer = - 1 ; } } public final boolean isDocumentOpen ( ) { return currentDocPointer != - 1 ; } public final void openField ( String field ) throws IOException { FieldInfo fieldInfo = fieldInfos . fieldInfo ( field ) ; openField ( fieldInfo . number , fieldInfo . storePositionWithTermVector , fieldInfo . storeOffsetWithTermVector ) ; } private void openField ( int fieldNumber , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector ) throws IOException { if ( ! isDocumentOpen ( ) ) throw new IllegalStateException ( "Cannot open field when no document is open." ) ; closeField ( ) ; currentField = new TVField ( fieldNumber , storePositionWithTermVector , storeOffsetWithTermVector ) ; } public final void closeField ( ) throws IOException { if ( isFieldOpen ( ) ) { writeField ( ) ; fields . add ( currentField ) ; terms . clear ( ) ; currentField = null ; } } public final boolean isFieldOpen ( ) { return currentField != null ; } public final void addTerm ( String termText , int freq ) { addTerm ( termText , freq , null , null ) ; } public final void addTerm ( String termText , int freq , int [ ] positions , TermVectorOffsetInfo [ ] offsets ) { if ( ! isDocumentOpen ( ) ) throw new IllegalStateException ( "Cannot add terms when document is not open" ) ; if ( ! isFieldOpen ( ) ) throw new IllegalStateException ( "Cannot add terms when field is not open" ) ; addTermInternal ( termText , freq , positions , offsets ) ; } private final void addTermInternal ( String termText , int freq , int [ ] positions , TermVectorOffsetInfo [ ] offsets ) { TVTerm term = new TVTerm ( ) ; term . termText = termText ; term . freq = freq ; term . positions = positions ; term . offsets = offsets ; terms . add ( term ) ; } public final void addAllDocVectors ( TermFreqVector [ ] vectors ) throws IOException { openDocument ( ) ; if ( vectors != null ) { for ( int i = 0 ; i < vectors . length ; i ++ ) { boolean storePositionWithTermVector = false ; boolean storeOffsetWithTermVector = false ; try { TermPositionVector tpVector = ( TermPositionVector ) vectors [ i ] ; if ( tpVector . size ( ) > 0 && tpVector . getTermPositions ( 0 ) != null ) storePositionWithTermVector = true ; if ( tpVector . size ( ) > 0 && tpVector . getOffsets ( 0 ) != null ) storeOffsetWithTermVector = true ; FieldInfo fieldInfo = fieldInfos . fieldInfo ( tpVector . getField ( ) ) ; openField ( fieldInfo . number , storePositionWithTermVector , storeOffsetWithTermVector ) ; for ( int j = 0 ; j < tpVector . size ( ) ; j ++ ) addTermInternal ( tpVector . getTerms ( ) [ j ] , tpVector . getTermFrequencies ( ) [ j ] , tpVector . getTermPositions ( j ) , tpVector . getOffsets ( j ) ) ; closeField ( ) ; } catch ( ClassCastException ignore ) { TermFreqVector tfVector = vectors [ i ] ; FieldInfo fieldInfo = fieldInfos . fieldInfo ( tfVector . getField ( ) ) ; openField ( fieldInfo . number , storePositionWithTermVector , storeOffsetWithTermVector ) ; for ( int j = 0 ; j < tfVector . size ( ) ; j ++ ) addTermInternal ( tfVector . getTerms ( ) [ j ] , tfVector . getTermFrequencies ( ) [ j ] , null , null ) ; closeField ( ) ; } } } closeDocument ( ) ; } final void close ( ) throws IOException { try { closeDocument ( ) ; } finally { IOException keep = null ; if ( tvx != null ) try { tvx . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tvd != null ) try { tvd . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tvf != null ) try { tvf . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( keep != null ) throw ( IOException ) keep . fillInStackTrace ( ) ; } } private void writeField ( ) throws IOException { currentField . tvfPointer = tvf . getFilePointer ( ) ; final int size = terms . size ( ) ; tvf . writeVInt ( size ) ; boolean storePositions = currentField . storePositions ; boolean storeOffsets = currentField . storeOffsets ; byte bits = 0x0 ; if ( storePositions ) bits |= STORE_POSITIONS_WITH_TERMVECTOR ; if ( storeOffsets ) bits |= STORE_OFFSET_WITH_TERMVECTOR ; tvf . writeByte ( bits ) ; String lastTermText = "" ; for ( int i = 0 ; i < size ; i ++ ) { TVTerm term = ( TVTerm ) terms . elementAt ( i ) ; int start = StringHelper . stringDifference ( lastTermText , term . termText ) ; int length = term . termText . length ( ) - start ; tvf . writeVInt ( start ) ; tvf . writeVInt ( length ) ; tvf . writeChars ( term . termText , start , length ) ; tvf . writeVInt ( term . freq ) ; lastTermText = term . termText ; if ( storePositions ) { if ( term . positions == null ) throw new IllegalStateException ( "Trying to write positions that are null!" ) ; int position = 0 ; for ( int j = 0 ; j < term . freq ; j ++ ) { tvf . writeVInt ( term . positions [ j ] - position ) ; position = term . positions [ j ] ; } } if ( storeOffsets ) { if ( term . offsets == null ) throw new IllegalStateException ( "Trying to write offsets that are null!" ) ; int position = 0 ; for ( int j = 0 ; j < term . freq ; j ++ ) { tvf . writeVInt ( term . offsets [ j ] . getStartOffset ( ) - position ) ; tvf . writeVInt ( term . offsets [ j ] . getEndOffset ( ) - term . offsets [ j ] . getStartOffset ( ) ) ; position = term . offsets [ j ] . getEndOffset ( ) ; } } } } private void writeDoc ( ) throws IOException { if ( isFieldOpen ( ) ) throw new IllegalStateException ( "Field is still open while writing document" ) ; tvx . writeLong ( currentDocPointer ) ; final int size = fields . size ( ) ; tvd . writeVInt ( size ) ; for ( int i = 0 ; i < size ; i ++ ) { TVField field = ( TVField ) fields . elementAt ( i ) ; tvd . writeVInt ( field . number ) ; } long lastFieldPointer = 0 ; for ( int i = 0 ; i < size ; i ++ ) { TVField field = ( TVField ) fields . elementAt ( i ) ; tvd . writeVLong ( field . tvfPointer - lastFieldPointer ) ; lastFieldPointer = field . tvfPointer ; } } private static class TVField { int number ; long tvfPointer = 0 ; boolean storePositions = false ; boolean storeOffsets = false ; TVField ( int number , boolean storePos , boolean storeOff ) { this . number = number ; storePositions = storePos ; storeOffsets = storeOff ; } } private static class TVTerm { String termText ; int freq = 0 ; int positions [ ] = null ; TermVectorOffsetInfo [ ] offsets = null ; } } 	0	['15', '1', '0', '13', '54', '41', '2', '11', '10', '0.804761905', '675', '0.533333333', '5', '0', '0.237037037', '0', '0', '43', '3', '1.2', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class IndexOutput { public abstract void writeByte ( byte b ) throws IOException ; public abstract void writeBytes ( byte [ ] b , int length ) throws IOException ; public void writeInt ( int i ) throws IOException { writeByte ( ( byte ) ( i > > 24 ) ) ; writeByte ( ( byte ) ( i > > 16 ) ) ; writeByte ( ( byte ) ( i > > 8 ) ) ; writeByte ( ( byte ) i ) ; } public void writeVInt ( int i ) throws IOException { while ( ( i & ~ 0x7F ) != 0 ) { writeByte ( ( byte ) ( ( i & 0x7f ) | 0x80 ) ) ; i >>>= 7 ; } writeByte ( ( byte ) i ) ; } public void writeLong ( long i ) throws IOException { writeInt ( ( int ) ( i > > 32 ) ) ; writeInt ( ( int ) i ) ; } public void writeVLong ( long i ) throws IOException { while ( ( i & ~ 0x7F ) != 0 ) { writeByte ( ( byte ) ( ( i & 0x7f ) | 0x80 ) ) ; i >>>= 7 ; } writeByte ( ( byte ) i ) ; } public void writeString ( String s ) throws IOException { int length = s . length ( ) ; writeVInt ( length ) ; writeChars ( s , 0 , length ) ; } public void writeChars ( String s , int start , int length ) throws IOException { final int end = start + length ; for ( int i = start ; i < end ; i ++ ) { final int code = ( int ) s . charAt ( i ) ; if ( code >= 0x01 && code <= 0x7F ) writeByte ( ( byte ) code ) ; else if ( ( ( code >= 0x80 ) && ( code <= 0x7FF ) ) || code == 0 ) { writeByte ( ( byte ) ( 0xC0 | ( code > > 6 ) ) ) ; writeByte ( ( byte ) ( 0x80 | ( code & 0x3F ) ) ) ; } else { writeByte ( ( byte ) ( 0xE0 | ( code > > > 12 ) ) ) ; writeByte ( ( byte ) ( 0x80 | ( ( code > > 6 ) & 0x3F ) ) ) ; writeByte ( ( byte ) ( 0x80 | ( code & 0x3F ) ) ) ; } } } public abstract void flush ( ) throws IOException ; public abstract void close ( ) throws IOException ; public abstract long getFilePointer ( ) ; public abstract void seek ( long pos ) throws IOException ; public abstract long length ( ) throws IOException ; } 	1	['14', '1', '1', '18', '17', '91', '18', '0', '14', '2', '189', '0', '0', '0', '0.297619048', '0', '0', '12.5', '1', '0.9286', '1']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . HashSet ; import java . util . Iterator ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; public abstract class Query implements java . io . Serializable , Cloneable { private float boost = 1.0f ; public void setBoost ( float b ) { boost = b ; } public float getBoost ( ) { return boost ; } public abstract String toString ( String field ) ; public String toString ( ) { return toString ( "" ) ; } protected Weight createWeight ( Searcher searcher ) throws IOException { throw new UnsupportedOperationException ( ) ; } public Weight weight ( Searcher searcher ) throws IOException { Query query = searcher . rewrite ( this ) ; Weight weight = query . createWeight ( searcher ) ; float sum = weight . sumOfSquaredWeights ( ) ; float norm = getSimilarity ( searcher ) . queryNorm ( sum ) ; weight . normalize ( norm ) ; return weight ; } public Query rewrite ( IndexReader reader ) throws IOException { return this ; } public Query combine ( Query [ ] queries ) { HashSet uniques = new HashSet ( ) ; for ( int i = 0 ; i < queries . length ; i ++ ) { Query query = queries [ i ] ; BooleanClause [ ] clauses = null ; boolean splittable = ( query instanceof BooleanQuery ) ; if ( splittable ) { BooleanQuery bq = ( BooleanQuery ) query ; splittable = bq . isCoordDisabled ( ) ; clauses = bq . getClauses ( ) ; for ( int j = 0 ; splittable && j < clauses . length ; j ++ ) { splittable = ( clauses [ j ] . getOccur ( ) == BooleanClause . Occur . SHOULD ) ; } } if ( splittable ) { for ( int j = 0 ; j < clauses . length ; j ++ ) { uniques . add ( clauses [ j ] . getQuery ( ) ) ; } } else { uniques . add ( query ) ; } } if ( uniques . size ( ) == 1 ) { return ( Query ) uniques . iterator ( ) . next ( ) ; } Iterator it = uniques . iterator ( ) ; BooleanQuery result = new BooleanQuery ( true ) ; while ( it . hasNext ( ) ) result . add ( ( Query ) it . next ( ) , BooleanClause . Occur . SHOULD ) ; return result ; } public void extractTerms ( Set terms ) { throw new UnsupportedOperationException ( ) ; } public static Query mergeBooleanQueries ( Query [ ] queries ) { HashSet allClauses = new HashSet ( ) ; for ( int i = 0 ; i < queries . length ; i ++ ) { BooleanClause [ ] clauses = ( ( BooleanQuery ) queries [ i ] ) . getClauses ( ) ; for ( int j = 0 ; j < clauses . length ; j ++ ) { allClauses . add ( clauses [ j ] ) ; } } boolean coordDisabled = queries . length == 0 ? false : ( ( BooleanQuery ) queries [ 0 ] ) . isCoordDisabled ( ) ; BooleanQuery result = new BooleanQuery ( coordDisabled ) ; Iterator i = allClauses . iterator ( ) ; while ( i . hasNext ( ) ) { result . add ( ( BooleanClause ) i . next ( ) ) ; } return result ; } public Similarity getSimilarity ( Searcher searcher ) { return searcher . getSimilarity ( ) ; } public Object clone ( ) { try { return ( Query ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { throw new RuntimeException ( "Clone not supported: " + e . getMessage ( ) ) ; } } } 	0	['13', '1', '13', '45', '39', '72', '42', '7', '12', '0.833333333', '249', '1', '0', '0', '0.230769231', '0', '0', '18.07692308', '9', '1.8462', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . List ; import java . util . ArrayList ; import java . util . Iterator ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Query ; import org . apache . lucene . util . ToStringUtils ; public class SpanNearQuery extends SpanQuery { private List clauses ; private int slop ; private boolean inOrder ; private String field ; public SpanNearQuery ( SpanQuery [ ] clauses , int slop , boolean inOrder ) { this . clauses = new ArrayList ( clauses . length ) ; for ( int i = 0 ; i < clauses . length ; i ++ ) { SpanQuery clause = clauses [ i ] ; if ( i == 0 ) { field = clause . getField ( ) ; } else if ( ! clause . getField ( ) . equals ( field ) ) { throw new IllegalArgumentException ( "Clauses must have same field." ) ; } this . clauses . add ( clause ) ; } this . slop = slop ; this . inOrder = inOrder ; } public SpanQuery [ ] getClauses ( ) { return ( SpanQuery [ ] ) clauses . toArray ( new SpanQuery [ clauses . size ( ) ] ) ; } public int getSlop ( ) { return slop ; } public boolean isInOrder ( ) { return inOrder ; } public String getField ( ) { return field ; } public Collection getTerms ( ) { Collection terms = new ArrayList ( ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; terms . addAll ( clause . getTerms ( ) ) ; } return terms ; } public void extractTerms ( Set terms ) { Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; clause . extractTerms ( terms ) ; } } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "spanNear([" ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; buffer . append ( clause . toString ( field ) ) ; if ( i . hasNext ( ) ) { buffer . append ( ", " ) ; } } buffer . append ( "], " ) ; buffer . append ( slop ) ; buffer . append ( ", " ) ; buffer . append ( inOrder ) ; buffer . append ( ")" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public Spans getSpans ( final IndexReader reader ) throws IOException { if ( clauses . size ( ) == 0 ) return new SpanOrQuery ( getClauses ( ) ) . getSpans ( reader ) ; if ( clauses . size ( ) == 1 ) return ( ( SpanQuery ) clauses . get ( 0 ) ) . getSpans ( reader ) ; return new NearSpans ( this , reader ) ; } public Query rewrite ( IndexReader reader ) throws IOException { SpanNearQuery clone = null ; for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { SpanQuery c = ( SpanQuery ) clauses . get ( i ) ; SpanQuery query = ( SpanQuery ) c . rewrite ( reader ) ; if ( query != c ) { if ( clone == null ) clone = ( SpanNearQuery ) this . clone ( ) ; clone . clauses . set ( i , query ) ; } } if ( clone != null ) { return clone ; } else { return this ; } } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof SpanNearQuery ) ) return false ; final SpanNearQuery spanNearQuery = ( SpanNearQuery ) o ; if ( inOrder != spanNearQuery . inOrder ) return false ; if ( slop != spanNearQuery . slop ) return false ; if ( ! clauses . equals ( spanNearQuery . clauses ) ) return false ; return getBoost ( ) == spanNearQuery . getBoost ( ) ; } public int hashCode ( ) { int result ; result = clauses . hashCode ( ) ; result ^= ( result << 14 ) | ( result > > > 19 ) ; result += Float . floatToRawIntBits ( getBoost ( ) ) ; result += slop ; result ^= ( inOrder ? 0x99AFD3BD : 0 ) ; return result ; } } 	1	['12', '3', '0', '7', '46', '0', '1', '7', '12', '0.613636364', '343', '1', '0', '0.592592593', '0.208333333', '2', '2', '27.25', '7', '1.75', '1']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermEnum ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; public class RangeQuery extends Query { private Term lowerTerm ; private Term upperTerm ; private boolean inclusive ; public RangeQuery ( Term lowerTerm , Term upperTerm , boolean inclusive ) { if ( lowerTerm == null && upperTerm == null ) { throw new IllegalArgumentException ( "At least one term must be non-null" ) ; } if ( lowerTerm != null && upperTerm != null && lowerTerm . field ( ) != upperTerm . field ( ) ) { throw new IllegalArgumentException ( "Both terms must be for the same field" ) ; } if ( lowerTerm != null ) { this . lowerTerm = lowerTerm ; } else { this . lowerTerm = new Term ( upperTerm . field ( ) , "" ) ; } this . upperTerm = upperTerm ; this . inclusive = inclusive ; } public Query rewrite ( IndexReader reader ) throws IOException { BooleanQuery query = new BooleanQuery ( true ) ; TermEnum enumerator = reader . terms ( lowerTerm ) ; try { boolean checkLower = false ; if ( ! inclusive ) checkLower = true ; String testField = getField ( ) ; do { Term term = enumerator . term ( ) ; if ( term != null && term . field ( ) == testField ) { if ( ! checkLower || term . text ( ) . compareTo ( lowerTerm . text ( ) ) > 0 ) { checkLower = false ; if ( upperTerm != null ) { int compare = upperTerm . text ( ) . compareTo ( term . text ( ) ) ; if ( ( compare < 0 ) || ( ! inclusive && compare == 0 ) ) break ; } TermQuery tq = new TermQuery ( term ) ; tq . setBoost ( getBoost ( ) ) ; query . add ( tq , BooleanClause . Occur . SHOULD ) ; } } else { break ; } } while ( enumerator . next ( ) ) ; } finally { enumerator . close ( ) ; } return query ; } public String getField ( ) { return ( lowerTerm != null ? lowerTerm . field ( ) : upperTerm . field ( ) ) ; } public Term getLowerTerm ( ) { return lowerTerm ; } public Term getUpperTerm ( ) { return upperTerm ; } public boolean isInclusive ( ) { return inclusive ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! getField ( ) . equals ( field ) ) { buffer . append ( getField ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( inclusive ? "[" : "{" ) ; buffer . append ( lowerTerm != null ? lowerTerm . text ( ) : "null" ) ; buffer . append ( " TO " ) ; buffer . append ( upperTerm != null ? upperTerm . text ( ) : "null" ) ; buffer . append ( inclusive ? "]" : "}" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof RangeQuery ) ) return false ; final RangeQuery other = ( RangeQuery ) o ; if ( this . getBoost ( ) != other . getBoost ( ) ) return false ; if ( this . inclusive != other . inclusive ) return false ; if ( this . lowerTerm != null ? ! this . lowerTerm . equals ( other . lowerTerm ) : other . lowerTerm != null ) return false ; if ( this . upperTerm != null ? ! this . upperTerm . equals ( other . upperTerm ) : other . upperTerm != null ) return false ; return true ; } public int hashCode ( ) { int h = Float . floatToIntBits ( getBoost ( ) ) ; h ^= lowerTerm != null ? lowerTerm . hashCode ( ) : 0 ; h ^= ( h << 25 ) | ( h > > > 8 ) ; h ^= upperTerm != null ? upperTerm . hashCode ( ) : 0 ; h ^= this . inclusive ? 0x2742E74A : 0 ; return h ; } } 	0	['9', '2', '0', '9', '32', '0', '1', '8', '9', '0.291666667', '340', '1', '2', '0.6', '0.259259259', '2', '3', '36.44444444', '11', '3', '0']
package org . apache . lucene . document ; import java . util . Enumeration ; import java . util . Iterator ; import java . util . List ; import java . util . ArrayList ; import java . util . Vector ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Searcher ; import org . apache . lucene . search . Hits ; public final class Document implements java . io . Serializable { List fields = new Vector ( ) ; private float boost = 1.0f ; public Document ( ) { } public void setBoost ( float boost ) { this . boost = boost ; } public float getBoost ( ) { return boost ; } public final void add ( Field field ) { fields . add ( field ) ; } public final void removeField ( String name ) { Iterator it = fields . iterator ( ) ; while ( it . hasNext ( ) ) { Field field = ( Field ) it . next ( ) ; if ( field . name ( ) . equals ( name ) ) { it . remove ( ) ; return ; } } } public final void removeFields ( String name ) { Iterator it = fields . iterator ( ) ; while ( it . hasNext ( ) ) { Field field = ( Field ) it . next ( ) ; if ( field . name ( ) . equals ( name ) ) { it . remove ( ) ; } } } public final Field getField ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Field field = ( Field ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) return field ; } return null ; } public final String get ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Field field = ( Field ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( ! field . isBinary ( ) ) ) return field . stringValue ( ) ; } return null ; } public final Enumeration fields ( ) { return ( ( Vector ) fields ) . elements ( ) ; } public final Field [ ] getFields ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Field field = ( Field ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) { result . add ( field ) ; } } if ( result . size ( ) == 0 ) return null ; return ( Field [ ] ) result . toArray ( new Field [ result . size ( ) ] ) ; } public final String [ ] getValues ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Field field = ( Field ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( ! field . isBinary ( ) ) ) result . add ( field . stringValue ( ) ) ; } if ( result . size ( ) == 0 ) return null ; return ( String [ ] ) result . toArray ( new String [ result . size ( ) ] ) ; } public final byte [ ] [ ] getBinaryValues ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Field field = ( Field ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( field . isBinary ( ) ) ) result . add ( field . binaryValue ( ) ) ; } if ( result . size ( ) == 0 ) return null ; return ( byte [ ] [ ] ) result . toArray ( new byte [ result . size ( ) ] [ ] ) ; } public final byte [ ] getBinaryValue ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Field field = ( Field ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( field . isBinary ( ) ) ) return field . binaryValue ( ) ; } return null ; } public final String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "Document<" ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Field field = ( Field ) fields . get ( i ) ; buffer . append ( field . toString ( ) ) ; if ( i != fields . size ( ) - 1 ) buffer . append ( " " ) ; } buffer . append ( ">" ) ; return buffer . toString ( ) ; } } 	1	['14', '1', '0', '23', '35', '0', '22', '1', '14', '0.5', '341', '0.5', '0', '0', '0.428571429', '0', '0', '23.21428571', '5', '2.5714', '1']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermEnum ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; public class PrefixQuery extends Query { private Term prefix ; public PrefixQuery ( Term prefix ) { this . prefix = prefix ; } public Term getPrefix ( ) { return prefix ; } public Query rewrite ( IndexReader reader ) throws IOException { BooleanQuery query = new BooleanQuery ( true ) ; TermEnum enumerator = reader . terms ( prefix ) ; try { String prefixText = prefix . text ( ) ; String prefixField = prefix . field ( ) ; do { Term term = enumerator . term ( ) ; if ( term != null && term . text ( ) . startsWith ( prefixText ) && term . field ( ) == prefixField ) { TermQuery tq = new TermQuery ( term ) ; tq . setBoost ( getBoost ( ) ) ; query . add ( tq , BooleanClause . Occur . SHOULD ) ; } else { break ; } } while ( enumerator . next ( ) ) ; } finally { enumerator . close ( ) ; } return query ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! prefix . field ( ) . equals ( field ) ) { buffer . append ( prefix . field ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( prefix . text ( ) ) ; buffer . append ( '*' ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof PrefixQuery ) ) return false ; PrefixQuery other = ( PrefixQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && this . prefix . equals ( other . prefix ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ prefix . hashCode ( ) ^ 0x6634D93C ; } } 	0	['6', '2', '0', '9', '28', '0', '1', '8', '6', '0', '147', '1', '1', '0.705882353', '0.333333333', '2', '3', '23.33333333', '4', '1.5', '0']
package org . apache . lucene . queryParser ; import java . util . Vector ; import java . io . * ; import java . text . * ; import java . util . * ; import org . apache . lucene . index . Term ; import org . apache . lucene . analysis . * ; import org . apache . lucene . document . * ; import org . apache . lucene . search . * ; import org . apache . lucene . util . Parameter ; public class QueryParserTokenManager implements QueryParserConstants { public java . io . PrintStream debugStream = System . out ; public void setDebugStream ( java . io . PrintStream ds ) { debugStream = ds ; } private final int jjStopStringLiteralDfa_3 ( int pos , long active0 ) { switch ( pos ) { default : return - 1 ; } } private final int jjStartNfa_3 ( int pos , long active0 ) { return jjMoveNfa_3 ( jjStopStringLiteralDfa_3 ( pos , active0 ) , pos + 1 ) ; } private final int jjStopAtPos ( int pos , int kind ) { jjmatchedKind = kind ; jjmatchedPos = pos ; return pos + 1 ; } private final int jjStartNfaWithStates_3 ( int pos , int kind , int state ) { jjmatchedKind = kind ; jjmatchedPos = pos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return pos + 1 ; } return jjMoveNfa_3 ( state , pos + 1 ) ; } private final int jjMoveStringLiteralDfa0_3 ( ) { switch ( curChar ) { case 40 : return jjStopAtPos ( 0 , 12 ) ; case 41 : return jjStopAtPos ( 0 , 13 ) ; case 43 : return jjStopAtPos ( 0 , 10 ) ; case 45 : return jjStopAtPos ( 0 , 11 ) ; case 58 : return jjStopAtPos ( 0 , 14 ) ; case 91 : return jjStopAtPos ( 0 , 21 ) ; case 94 : return jjStopAtPos ( 0 , 15 ) ; case 123 : return jjStopAtPos ( 0 , 22 ) ; default : return jjMoveNfa_3 ( 0 , 0 ) ; } } private final void jjCheckNAdd ( int state ) { if ( jjrounds [ state ] != jjround ) { jjstateSet [ jjnewStateCnt ++ ] = state ; jjrounds [ state ] = jjround ; } } private final void jjAddStates ( int start , int end ) { do { jjstateSet [ jjnewStateCnt ++ ] = jjnextStates [ start ] ; } while ( start ++ != end ) ; } private final void jjCheckNAddTwoStates ( int state1 , int state2 ) { jjCheckNAdd ( state1 ) ; jjCheckNAdd ( state2 ) ; } private final void jjCheckNAddStates ( int start , int end ) { do { jjCheckNAdd ( jjnextStates [ start ] ) ; } while ( start ++ != end ) ; } private final void jjCheckNAddStates ( int start ) { jjCheckNAdd ( jjnextStates [ start ] ) ; jjCheckNAdd ( jjnextStates [ start + 1 ] ) ; } static final long [ ] jjbitVec0 = { 0xfffffffffffffffeL , 0xffffffffffffffffL , 0xffffffffffffffffL , 0xffffffffffffffffL } ; static final long [ ] jjbitVec2 = { 0x0L , 0x0L , 0xffffffffffffffffL , 0xffffffffffffffffL } ; private final int jjMoveNfa_3 ( int startState , int curPos ) { int [ ] nextStates ; int startsAt = 0 ; jjnewStateCnt = 33 ; int i = 1 ; jjstateSet [ 0 ] = startState ; int j , kind = 0x7fffffff ; for ( ; ; ) { if ( ++ jjround == 0x7fffffff ) ReInitRounds ( ) ; if ( curChar < 64 ) { long l = 1L << curChar ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( ( 0x7bffd0f8ffffd9ffL & l ) != 0L ) { if ( kind > 17 ) kind = 17 ; jjCheckNAddStates ( 0 , 6 ) ; } else if ( ( 0x100002600L & l ) != 0L ) { if ( kind > 6 ) kind = 6 ; } else if ( curChar == 34 ) jjCheckNAdd ( 15 ) ; else if ( curChar == 33 ) { if ( kind > 9 ) kind = 9 ; } if ( curChar == 38 ) jjstateSet [ jjnewStateCnt ++ ] = 4 ; break ; case 4 : if ( curChar == 38 && kind > 7 ) kind = 7 ; break ; case 5 : if ( curChar == 38 ) jjstateSet [ jjnewStateCnt ++ ] = 4 ; break ; case 13 : if ( curChar == 33 && kind > 9 ) kind = 9 ; break ; case 14 : if ( curChar == 34 ) jjCheckNAdd ( 15 ) ; break ; case 15 : if ( ( 0xfffffffbffffffffL & l ) != 0L ) jjCheckNAddTwoStates ( 15 , 16 ) ; break ; case 16 : if ( curChar == 34 && kind > 16 ) kind = 16 ; break ; case 18 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 18 ) kind = 18 ; jjAddStates ( 7 , 8 ) ; break ; case 19 : if ( curChar == 46 ) jjCheckNAdd ( 20 ) ; break ; case 20 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 18 ) kind = 18 ; jjCheckNAdd ( 20 ) ; break ; case 21 : if ( ( 0x7bffd0f8ffffd9ffL & l ) == 0L ) break ; if ( kind > 17 ) kind = 17 ; jjCheckNAddStates ( 0 , 6 ) ; break ; case 22 : if ( ( 0x7bfff8f8ffffd9ffL & l ) == 0L ) break ; if ( kind > 17 ) kind = 17 ; jjCheckNAddTwoStates ( 22 , 23 ) ; break ; case 24 : if ( ( 0x84002f0600000000L & l ) == 0L ) break ; if ( kind > 17 ) kind = 17 ; jjCheckNAddTwoStates ( 22 , 23 ) ; break ; case 25 : if ( ( 0x7bfff8f8ffffd9ffL & l ) != 0L ) jjCheckNAddStates ( 9 , 11 ) ; break ; case 26 : if ( curChar == 42 && kind > 19 ) kind = 19 ; break ; case 28 : if ( ( 0x84002f0600000000L & l ) != 0L ) jjCheckNAddStates ( 9 , 11 ) ; break ; case 29 : if ( ( 0xfbfffcf8ffffd9ffL & l ) == 0L ) break ; if ( kind > 20 ) kind = 20 ; jjCheckNAddTwoStates ( 29 , 30 ) ; break ; case 31 : if ( ( 0x84002f0600000000L & l ) == 0L ) break ; if ( kind > 20 ) kind = 20 ; jjCheckNAddTwoStates ( 29 , 30 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else if ( curChar < 128 ) { long l = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( ( 0x97ffffff97ffffffL & l ) != 0L ) { if ( kind > 17 ) kind = 17 ; jjCheckNAddStates ( 0 , 6 ) ; } else if ( curChar == 126 ) { if ( kind > 18 ) kind = 18 ; jjstateSet [ jjnewStateCnt ++ ] = 18 ; } if ( curChar == 92 ) jjCheckNAddStates ( 12 , 14 ) ; else if ( curChar == 78 ) jjstateSet [ jjnewStateCnt ++ ] = 11 ; else if ( curChar == 124 ) jjstateSet [ jjnewStateCnt ++ ] = 8 ; else if ( curChar == 79 ) jjstateSet [ jjnewStateCnt ++ ] = 6 ; else if ( curChar == 65 ) jjstateSet [ jjnewStateCnt ++ ] = 2 ; break ; case 1 : if ( curChar == 68 && kind > 7 ) kind = 7 ; break ; case 2 : if ( curChar == 78 ) jjstateSet [ jjnewStateCnt ++ ] = 1 ; break ; case 3 : if ( curChar == 65 ) jjstateSet [ jjnewStateCnt ++ ] = 2 ; break ; case 6 : if ( curChar == 82 && kind > 8 ) kind = 8 ; break ; case 7 : if ( curChar == 79 ) jjstateSet [ jjnewStateCnt ++ ] = 6 ; break ; case 8 : if ( curChar == 124 && kind > 8 ) kind = 8 ; break ; case 9 : if ( curChar == 124 ) jjstateSet [ jjnewStateCnt ++ ] = 8 ; break ; case 10 : if ( curChar == 84 && kind > 9 ) kind = 9 ; break ; case 11 : if ( curChar == 79 ) jjstateSet [ jjnewStateCnt ++ ] = 10 ; break ; case 12 : if ( curChar == 78 ) jjstateSet [ jjnewStateCnt ++ ] = 11 ; break ; case 15 : jjAddStates ( 15 , 16 ) ; break ; case 17 : if ( curChar != 126 ) break ; if ( kind > 18 ) kind = 18 ; jjstateSet [ jjnewStateCnt ++ ] = 18 ; break ; case 21 : if ( ( 0x97ffffff97ffffffL & l ) == 0L ) break ; if ( kind > 17 ) kind = 17 ; jjCheckNAddStates ( 0 , 6 ) ; break ; case 22 : if ( ( 0x97ffffff97ffffffL & l ) == 0L ) break ; if ( kind > 17 ) kind = 17 ; jjCheckNAddTwoStates ( 22 , 23 ) ; break ; case 23 : if ( curChar == 92 ) jjCheckNAddTwoStates ( 24 , 24 ) ; break ; case 24 : if ( ( 0x6800000078000000L & l ) == 0L ) break ; if ( kind > 17 ) kind = 17 ; jjCheckNAddTwoStates ( 22 , 23 ) ; break ; case 25 : if ( ( 0x97ffffff97ffffffL & l ) != 0L ) jjCheckNAddStates ( 9 , 11 ) ; break ; case 27 : if ( curChar == 92 ) jjCheckNAddTwoStates ( 28 , 28 ) ; break ; case 28 : if ( ( 0x6800000078000000L & l ) != 0L ) jjCheckNAddStates ( 9 , 11 ) ; break ; case 29 : if ( ( 0x97ffffff97ffffffL & l ) == 0L ) break ; if ( kind > 20 ) kind = 20 ; jjCheckNAddTwoStates ( 29 , 30 ) ; break ; case 30 : if ( curChar == 92 ) jjCheckNAddTwoStates ( 31 , 31 ) ; break ; case 31 : if ( ( 0x6800000078000000L & l ) == 0L ) break ; if ( kind > 20 ) kind = 20 ; jjCheckNAddTwoStates ( 29 , 30 ) ; break ; case 32 : if ( curChar == 92 ) jjCheckNAddStates ( 12 , 14 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else { int hiByte = ( int ) ( curChar > > 8 ) ; int i1 = hiByte > > 6 ; long l1 = 1L << ( hiByte & 077 ) ; int i2 = ( curChar & 0xff ) > > 6 ; long l2 = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( ! jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 17 ) kind = 17 ; jjCheckNAddStates ( 0 , 6 ) ; break ; case 15 : if ( jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 15 , 16 ) ; break ; case 22 : if ( ! jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 17 ) kind = 17 ; jjCheckNAddTwoStates ( 22 , 23 ) ; break ; case 25 : if ( jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 9 , 11 ) ; break ; case 29 : if ( ! jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 20 ) kind = 20 ; jjCheckNAddTwoStates ( 29 , 30 ) ; break ; default : break ; } } while ( i != startsAt ) ; } if ( kind != 0x7fffffff ) { jjmatchedKind = kind ; jjmatchedPos = curPos ; kind = 0x7fffffff ; } ++ curPos ; if ( ( i = jjnewStateCnt ) == ( startsAt = 33 - ( jjnewStateCnt = startsAt ) ) ) return curPos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return curPos ; } } } private final int jjStopStringLiteralDfa_1 ( int pos , long active0 ) { switch ( pos ) { case 0 : if ( ( active0 & 0x10000000L ) != 0L ) { jjmatchedKind = 31 ; return 4 ; } return - 1 ; default : return - 1 ; } } private final int jjStartNfa_1 ( int pos , long active0 ) { return jjMoveNfa_1 ( jjStopStringLiteralDfa_1 ( pos , active0 ) , pos + 1 ) ; } private final int jjStartNfaWithStates_1 ( int pos , int kind , int state ) { jjmatchedKind = kind ; jjmatchedPos = pos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return pos + 1 ; } return jjMoveNfa_1 ( state , pos + 1 ) ; } private final int jjMoveStringLiteralDfa0_1 ( ) { switch ( curChar ) { case 84 : return jjMoveStringLiteralDfa1_1 ( 0x10000000L ) ; case 125 : return jjStopAtPos ( 0 , 29 ) ; default : return jjMoveNfa_1 ( 0 , 0 ) ; } } private final int jjMoveStringLiteralDfa1_1 ( long active0 ) { try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { jjStopStringLiteralDfa_1 ( 0 , active0 ) ; return 1 ; } switch ( curChar ) { case 79 : if ( ( active0 & 0x10000000L ) != 0L ) return jjStartNfaWithStates_1 ( 1 , 28 , 4 ) ; break ; default : break ; } return jjStartNfa_1 ( 0 , active0 ) ; } private final int jjMoveNfa_1 ( int startState , int curPos ) { int [ ] nextStates ; int startsAt = 0 ; jjnewStateCnt = 5 ; int i = 1 ; jjstateSet [ 0 ] = startState ; int j , kind = 0x7fffffff ; for ( ; ; ) { if ( ++ jjround == 0x7fffffff ) ReInitRounds ( ) ; if ( curChar < 64 ) { long l = 1L << curChar ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( ( 0xfffffffeffffffffL & l ) != 0L ) { if ( kind > 31 ) kind = 31 ; jjCheckNAdd ( 4 ) ; } if ( ( 0x100002600L & l ) != 0L ) { if ( kind > 6 ) kind = 6 ; } else if ( curChar == 34 ) jjCheckNAdd ( 2 ) ; break ; case 1 : if ( curChar == 34 ) jjCheckNAdd ( 2 ) ; break ; case 2 : if ( ( 0xfffffffbffffffffL & l ) != 0L ) jjCheckNAddTwoStates ( 2 , 3 ) ; break ; case 3 : if ( curChar == 34 && kind > 30 ) kind = 30 ; break ; case 4 : if ( ( 0xfffffffeffffffffL & l ) == 0L ) break ; if ( kind > 31 ) kind = 31 ; jjCheckNAdd ( 4 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else if ( curChar < 128 ) { long l = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : case 4 : if ( ( 0xdfffffffffffffffL & l ) == 0L ) break ; if ( kind > 31 ) kind = 31 ; jjCheckNAdd ( 4 ) ; break ; case 2 : jjAddStates ( 17 , 18 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else { int hiByte = ( int ) ( curChar > > 8 ) ; int i1 = hiByte > > 6 ; long l1 = 1L << ( hiByte & 077 ) ; int i2 = ( curChar & 0xff ) > > 6 ; long l2 = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : case 4 : if ( ! jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 31 ) kind = 31 ; jjCheckNAdd ( 4 ) ; break ; case 2 : if ( jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 17 , 18 ) ; break ; default : break ; } } while ( i != startsAt ) ; } if ( kind != 0x7fffffff ) { jjmatchedKind = kind ; jjmatchedPos = curPos ; kind = 0x7fffffff ; } ++ curPos ; if ( ( i = jjnewStateCnt ) == ( startsAt = 5 - ( jjnewStateCnt = startsAt ) ) ) return curPos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return curPos ; } } } private final int jjMoveStringLiteralDfa0_0 ( ) { return jjMoveNfa_0 ( 0 , 0 ) ; } private final int jjMoveNfa_0 ( int startState , int curPos ) { int [ ] nextStates ; int startsAt = 0 ; jjnewStateCnt = 3 ; int i = 1 ; jjstateSet [ 0 ] = startState ; int j , kind = 0x7fffffff ; for ( ; ; ) { if ( ++ jjround == 0x7fffffff ) ReInitRounds ( ) ; if ( curChar < 64 ) { long l = 1L << curChar ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 23 ) kind = 23 ; jjAddStates ( 19 , 20 ) ; break ; case 1 : if ( curChar == 46 ) jjCheckNAdd ( 2 ) ; break ; case 2 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 23 ) kind = 23 ; jjCheckNAdd ( 2 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else if ( curChar < 128 ) { long l = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { default : break ; } } while ( i != startsAt ) ; } else { int hiByte = ( int ) ( curChar > > 8 ) ; int i1 = hiByte > > 6 ; long l1 = 1L << ( hiByte & 077 ) ; int i2 = ( curChar & 0xff ) > > 6 ; long l2 = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { default : break ; } } while ( i != startsAt ) ; } if ( kind != 0x7fffffff ) { jjmatchedKind = kind ; jjmatchedPos = curPos ; kind = 0x7fffffff ; } ++ curPos ; if ( ( i = jjnewStateCnt ) == ( startsAt = 3 - ( jjnewStateCnt = startsAt ) ) ) return curPos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return curPos ; } } } private final int jjStopStringLiteralDfa_2 ( int pos , long active0 ) { switch ( pos ) { case 0 : if ( ( active0 & 0x1000000L ) != 0L ) { jjmatchedKind = 27 ; return 4 ; } return - 1 ; default : return - 1 ; } } private final int jjStartNfa_2 ( int pos , long active0 ) { return jjMoveNfa_2 ( jjStopStringLiteralDfa_2 ( pos , active0 ) , pos + 1 ) ; } private final int jjStartNfaWithStates_2 ( int pos , int kind , int state ) { jjmatchedKind = kind ; jjmatchedPos = pos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return pos + 1 ; } return jjMoveNfa_2 ( state , pos + 1 ) ; } private final int jjMoveStringLiteralDfa0_2 ( ) { switch ( curChar ) { case 84 : return jjMoveStringLiteralDfa1_2 ( 0x1000000L ) ; case 93 : return jjStopAtPos ( 0 , 25 ) ; default : return jjMoveNfa_2 ( 0 , 0 ) ; } } private final int jjMoveStringLiteralDfa1_2 ( long active0 ) { try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { jjStopStringLiteralDfa_2 ( 0 , active0 ) ; return 1 ; } switch ( curChar ) { case 79 : if ( ( active0 & 0x1000000L ) != 0L ) return jjStartNfaWithStates_2 ( 1 , 24 , 4 ) ; break ; default : break ; } return jjStartNfa_2 ( 0 , active0 ) ; } private final int jjMoveNfa_2 ( int startState , int curPos ) { int [ ] nextStates ; int startsAt = 0 ; jjnewStateCnt = 5 ; int i = 1 ; jjstateSet [ 0 ] = startState ; int j , kind = 0x7fffffff ; for ( ; ; ) { if ( ++ jjround == 0x7fffffff ) ReInitRounds ( ) ; if ( curChar < 64 ) { long l = 1L << curChar ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( ( 0xfffffffeffffffffL & l ) != 0L ) { if ( kind > 27 ) kind = 27 ; jjCheckNAdd ( 4 ) ; } if ( ( 0x100002600L & l ) != 0L ) { if ( kind > 6 ) kind = 6 ; } else if ( curChar == 34 ) jjCheckNAdd ( 2 ) ; break ; case 1 : if ( curChar == 34 ) jjCheckNAdd ( 2 ) ; break ; case 2 : if ( ( 0xfffffffbffffffffL & l ) != 0L ) jjCheckNAddTwoStates ( 2 , 3 ) ; break ; case 3 : if ( curChar == 34 && kind > 26 ) kind = 26 ; break ; case 4 : if ( ( 0xfffffffeffffffffL & l ) == 0L ) break ; if ( kind > 27 ) kind = 27 ; jjCheckNAdd ( 4 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else if ( curChar < 128 ) { long l = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : case 4 : if ( ( 0xffffffffdfffffffL & l ) == 0L ) break ; if ( kind > 27 ) kind = 27 ; jjCheckNAdd ( 4 ) ; break ; case 2 : jjAddStates ( 17 , 18 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else { int hiByte = ( int ) ( curChar > > 8 ) ; int i1 = hiByte > > 6 ; long l1 = 1L << ( hiByte & 077 ) ; int i2 = ( curChar & 0xff ) > > 6 ; long l2 = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : case 4 : if ( ! jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 27 ) kind = 27 ; jjCheckNAdd ( 4 ) ; break ; case 2 : if ( jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 17 , 18 ) ; break ; default : break ; } } while ( i != startsAt ) ; } if ( kind != 0x7fffffff ) { jjmatchedKind = kind ; jjmatchedPos = curPos ; kind = 0x7fffffff ; } ++ curPos ; if ( ( i = jjnewStateCnt ) == ( startsAt = 5 - ( jjnewStateCnt = startsAt ) ) ) return curPos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return curPos ; } } } static final int [ ] jjnextStates = { 22 , 25 , 26 , 29 , 30 , 27 , 23 , 18 , 19 , 25 , 26 , 27 , 24 , 28 , 31 , 15 , 16 , 2 , 3 , 0 , 1 , } ; private static final boolean jjCanMove_0 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 0 : return ( ( jjbitVec2 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec0 [ i1 ] & l1 ) != 0L ) return true ; return false ; } } public static final String [ ] jjstrLiteralImages = { "" , null , null , null , null , null , null , null , null , null , "\53" , "\55" , "\50" , "\51" , "\72" , "\136" , null , null , null , null , null , "\133" , "\173" , null , "\124\117" , "\135" , null , null , "\124\117" , "\175" , null , null , } ; public static final String [ ] lexStateNames = { "Boost" , "RangeEx" , "RangeIn" , "DEFAULT" , } ; public static final int [ ] jjnewLexState = { - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , 0 , - 1 , - 1 , - 1 , - 1 , - 1 , 2 , 1 , 3 , - 1 , 3 , - 1 , - 1 , - 1 , 3 , - 1 , - 1 , } ; static final long [ ] jjtoToken = { 0xffffff81L , } ; static final long [ ] jjtoSkip = { 0x40L , } ; protected CharStream input_stream ; private final int [ ] jjrounds = new int [ 33 ] ; private final int [ ] jjstateSet = new int [ 66 ] ; protected char curChar ; public QueryParserTokenManager ( CharStream stream ) { input_stream = stream ; } public QueryParserTokenManager ( CharStream stream , int lexState ) { this ( stream ) ; SwitchTo ( lexState ) ; } public void ReInit ( CharStream stream ) { jjmatchedPos = jjnewStateCnt = 0 ; curLexState = defaultLexState ; input_stream = stream ; ReInitRounds ( ) ; } private final void ReInitRounds ( ) { int i ; jjround = 0x80000001 ; for ( i = 33 ; i -- > 0 ; ) jjrounds [ i ] = 0x80000000 ; } public void ReInit ( CharStream stream , int lexState ) { ReInit ( stream ) ; SwitchTo ( lexState ) ; } public void SwitchTo ( int lexState ) { if ( lexState >= 4 || lexState < 0 ) throw new TokenMgrError ( "Error: Ignoring invalid lexical state : " + lexState + ". State unchanged." , TokenMgrError . INVALID_LEXICAL_STATE ) ; else curLexState = lexState ; } protected Token jjFillToken ( ) { Token t = Token . newToken ( jjmatchedKind ) ; t . kind = jjmatchedKind ; String im = jjstrLiteralImages [ jjmatchedKind ] ; t . image = ( im == null ) ? input_stream . GetImage ( ) : im ; t . beginLine = input_stream . getBeginLine ( ) ; t . beginColumn = input_stream . getBeginColumn ( ) ; t . endLine = input_stream . getEndLine ( ) ; t . endColumn = input_stream . getEndColumn ( ) ; return t ; } int curLexState = 3 ; int defaultLexState = 3 ; int jjnewStateCnt ; int jjround ; int jjmatchedPos ; int jjmatchedKind ; public Token getNextToken ( ) { int kind ; Token specialToken = null ; Token matchedToken ; int curPos = 0 ; EOFLoop : for ( ; ; ) { try { curChar = input_stream . BeginToken ( ) ; } catch ( java . io . IOException e ) { jjmatchedKind = 0 ; matchedToken = jjFillToken ( ) ; return matchedToken ; } switch ( curLexState ) { case 0 : jjmatchedKind = 0x7fffffff ; jjmatchedPos = 0 ; curPos = jjMoveStringLiteralDfa0_0 ( ) ; break ; case 1 : jjmatchedKind = 0x7fffffff ; jjmatchedPos = 0 ; curPos = jjMoveStringLiteralDfa0_1 ( ) ; break ; case 2 : jjmatchedKind = 0x7fffffff ; jjmatchedPos = 0 ; curPos = jjMoveStringLiteralDfa0_2 ( ) ; break ; case 3 : jjmatchedKind = 0x7fffffff ; jjmatchedPos = 0 ; curPos = jjMoveStringLiteralDfa0_3 ( ) ; break ; } if ( jjmatchedKind != 0x7fffffff ) { if ( jjmatchedPos + 1 < curPos ) input_stream . backup ( curPos - jjmatchedPos - 1 ) ; if ( ( jjtoToken [ jjmatchedKind > > 6 ] & ( 1L << ( jjmatchedKind & 077 ) ) ) != 0L ) { matchedToken = jjFillToken ( ) ; if ( jjnewLexState [ jjmatchedKind ] != - 1 ) curLexState = jjnewLexState [ jjmatchedKind ] ; return matchedToken ; } else { if ( jjnewLexState [ jjmatchedKind ] != - 1 ) curLexState = jjnewLexState [ jjmatchedKind ] ; continue EOFLoop ; } } int error_line = input_stream . getEndLine ( ) ; int error_column = input_stream . getEndColumn ( ) ; String error_after = null ; boolean EOFSeen = false ; try { input_stream . readChar ( ) ; input_stream . backup ( 1 ) ; } catch ( java . io . IOException e1 ) { EOFSeen = true ; error_after = curPos <= 1 ? "" : input_stream . GetImage ( ) ; if ( curChar == '\n' || curChar == '\r' ) { error_line ++ ; error_column = 0 ; } else error_column ++ ; } if ( ! EOFSeen ) { input_stream . backup ( 1 ) ; error_after = curPos <= 1 ? "" : input_stream . GetImage ( ) ; } throw new TokenMgrError ( EOFSeen , curLexState , error_line , error_column , error_after , curChar , TokenMgrError . LEXICAL_ERROR ) ; } } } 	1	['36', '1', '0', '5', '52', '282', '1', '4', '7', '0.720300752', '2855', '0.210526316', '1', '0', '0.405714286', '0', '0', '77.77777778', '102', '7.0556', '4']
package org . apache . lucene . analysis . standard ; import java . io . * ; public class StandardTokenizer extends org . apache . lucene . analysis . Tokenizer implements StandardTokenizerConstants { public StandardTokenizer ( Reader reader ) { this ( new FastCharStream ( reader ) ) ; this . input = reader ; } final public org . apache . lucene . analysis . Token next ( ) throws ParseException , IOException { Token token = null ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case ALPHANUM : token = jj_consume_token ( ALPHANUM ) ; break ; case APOSTROPHE : token = jj_consume_token ( APOSTROPHE ) ; break ; case ACRONYM : token = jj_consume_token ( ACRONYM ) ; break ; case COMPANY : token = jj_consume_token ( COMPANY ) ; break ; case EMAIL : token = jj_consume_token ( EMAIL ) ; break ; case HOST : token = jj_consume_token ( HOST ) ; break ; case NUM : token = jj_consume_token ( NUM ) ; break ; case CJ : token = jj_consume_token ( CJ ) ; break ; case 0 : token = jj_consume_token ( 0 ) ; break ; default : jj_la1 [ 0 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } if ( token . kind == EOF ) { { if ( true ) return null ; } } else { { if ( true ) return new org . apache . lucene . analysis . Token ( token . image , token . beginColumn , token . endColumn , tokenImage [ token . kind ] ) ; } } throw new Error ( "Missing return statement in function" ) ; } public StandardTokenizerTokenManager token_source ; public Token token , jj_nt ; private int jj_ntk ; private int jj_gen ; final private int [ ] jj_la1 = new int [ 1 ] ; static private int [ ] jj_la1_0 ; static { jj_la1_0 ( ) ; } private static void jj_la1_0 ( ) { jj_la1_0 = new int [ ] { 0x10ff , } ; } public StandardTokenizer ( CharStream stream ) { token_source = new StandardTokenizerTokenManager ( stream ) ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 1 ; i ++ ) jj_la1 [ i ] = - 1 ; } public void ReInit ( CharStream stream ) { token_source . ReInit ( stream ) ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 1 ; i ++ ) jj_la1 [ i ] = - 1 ; } public StandardTokenizer ( StandardTokenizerTokenManager tm ) { token_source = tm ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 1 ; i ++ ) jj_la1 [ i ] = - 1 ; } public void ReInit ( StandardTokenizerTokenManager tm ) { token_source = tm ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 1 ; i ++ ) jj_la1 [ i ] = - 1 ; } final private Token jj_consume_token ( int kind ) throws ParseException { Token oldToken ; if ( ( oldToken = token ) . next != null ) token = token . next ; else token = token . next = token_source . getNextToken ( ) ; jj_ntk = - 1 ; if ( token . kind == kind ) { jj_gen ++ ; return token ; } token = oldToken ; jj_kind = kind ; throw generateParseException ( ) ; } final public Token getNextToken ( ) { if ( token . next != null ) token = token . next ; else token = token . next = token_source . getNextToken ( ) ; jj_ntk = - 1 ; jj_gen ++ ; return token ; } final public Token getToken ( int index ) { Token t = token ; for ( int i = 0 ; i < index ; i ++ ) { if ( t . next != null ) t = t . next ; else t = t . next = token_source . getNextToken ( ) ; } return t ; } final private int jj_ntk ( ) { if ( ( jj_nt = token . next ) == null ) return ( jj_ntk = ( token . next = token_source . getNextToken ( ) ) . kind ) ; else return ( jj_ntk = jj_nt . kind ) ; } private java . util . Vector jj_expentries = new java . util . Vector ( ) ; private int [ ] jj_expentry ; private int jj_kind = - 1 ; public ParseException generateParseException ( ) { jj_expentries . removeAllElements ( ) ; boolean [ ] la1tokens = new boolean [ 16 ] ; for ( int i = 0 ; i < 16 ; i ++ ) { la1tokens [ i ] = false ; } if ( jj_kind >= 0 ) { la1tokens [ jj_kind ] = true ; jj_kind = - 1 ; } for ( int i = 0 ; i < 1 ; i ++ ) { if ( jj_la1 [ i ] == jj_gen ) { for ( int j = 0 ; j < 32 ; j ++ ) { if ( ( jj_la1_0 [ i ] & ( 1 << j ) ) != 0 ) { la1tokens [ j ] = true ; } } } } for ( int i = 0 ; i < 16 ; i ++ ) { if ( la1tokens [ i ] ) { jj_expentry = new int [ 1 ] ; jj_expentry [ 0 ] = i ; jj_expentries . addElement ( jj_expentry ) ; } } int [ ] [ ] exptokseq = new int [ jj_expentries . size ( ) ] [ ] ; for ( int i = 0 ; i < jj_expentries . size ( ) ; i ++ ) { exptokseq [ i ] = ( int [ ] ) jj_expentries . elementAt ( i ) ; } return new ParseException ( token , exptokseq , tokenImage ) ; } final public void enable_tracing ( ) { } final public void disable_tracing ( ) { } } 	0	['15', '3', '0', '9', '29', '15', '1', '8', '11', '0.6', '523', '0.7', '3', '0.214285714', '0.285714286', '1', '1', '33.2', '10', '1.7333', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . TermPositions ; import java . io . IOException ; final class SloppyPhraseScorer extends PhraseScorer { private int slop ; SloppyPhraseScorer ( Weight weight , TermPositions [ ] tps , int [ ] positions , Similarity similarity , int slop , byte [ ] norms ) { super ( weight , tps , positions , similarity , norms ) ; this . slop = slop ; } protected final float phraseFreq ( ) throws IOException { pq . clear ( ) ; int end = 0 ; for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) { pp . firstPosition ( ) ; if ( pp . position > end ) end = pp . position ; pq . put ( pp ) ; } float freq = 0.0f ; boolean done = false ; do { PhrasePositions pp = ( PhrasePositions ) pq . pop ( ) ; int start = pp . position ; int next = ( ( PhrasePositions ) pq . top ( ) ) . position ; for ( int pos = start ; pos <= next ; pos = pp . position ) { start = pos ; if ( ! pp . nextPosition ( ) ) { done = true ; break ; } } int matchLength = end - start ; if ( matchLength <= slop ) freq += getSimilarity ( ) . sloppyFreq ( matchLength ) ; if ( pp . position > end ) end = pp . position ; pq . put ( pp ) ; } while ( ! done ) ; return freq ; } } 	1	['2', '3', '0', '8', '11', '0', '2', '6', '0', '0', '106', '1', '0', '0.952380952', '0.571428571', '1', '1', '51.5', '1', '0.5', '1']
package org . apache . lucene . search ; public class TopDocs implements java . io . Serializable { public int totalHits ; public ScoreDoc [ ] scoreDocs ; private float maxScore ; public float getMaxScore ( ) { return maxScore ; } public void setMaxScore ( float maxScore ) { this . maxScore = maxScore ; } TopDocs ( int totalHits , ScoreDoc [ ] scoreDocs , float maxScore ) { this . totalHits = totalHits ; this . scoreDocs = scoreDocs ; this . maxScore = maxScore ; } } 	0	['3', '1', '1', '14', '4', '0', '13', '1', '2', '0.666666667', '25', '0.333333333', '1', '0', '0.583333333', '0', '0', '6.333333333', '1', '0.6667', '0']
package org . apache . lucene . index ; import java . io . ByteArrayOutputStream ; import java . io . IOException ; import java . util . zip . DataFormatException ; import java . util . zip . Inflater ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . Field ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; final class FieldsReader { private FieldInfos fieldInfos ; private IndexInput fieldsStream ; private IndexInput indexStream ; private int size ; FieldsReader ( Directory d , String segment , FieldInfos fn ) throws IOException { fieldInfos = fn ; fieldsStream = d . openInput ( segment + ".fdt" ) ; indexStream = d . openInput ( segment + ".fdx" ) ; size = ( int ) ( indexStream . length ( ) / 8 ) ; } final void close ( ) throws IOException { fieldsStream . close ( ) ; indexStream . close ( ) ; } final int size ( ) { return size ; } final Document doc ( int n ) throws IOException { indexStream . seek ( n * 8L ) ; long position = indexStream . readLong ( ) ; fieldsStream . seek ( position ) ; Document doc = new Document ( ) ; int numFields = fieldsStream . readVInt ( ) ; for ( int i = 0 ; i < numFields ; i ++ ) { int fieldNumber = fieldsStream . readVInt ( ) ; FieldInfo fi = fieldInfos . fieldInfo ( fieldNumber ) ; byte bits = fieldsStream . readByte ( ) ; boolean compressed = ( bits & FieldsWriter . FIELD_IS_COMPRESSED ) != 0 ; boolean tokenize = ( bits & FieldsWriter . FIELD_IS_TOKENIZED ) != 0 ; if ( ( bits & FieldsWriter . FIELD_IS_BINARY ) != 0 ) { final byte [ ] b = new byte [ fieldsStream . readVInt ( ) ] ; fieldsStream . readBytes ( b , 0 , b . length ) ; if ( compressed ) doc . add ( new Field ( fi . name , uncompress ( b ) , Field . Store . COMPRESS ) ) ; else doc . add ( new Field ( fi . name , b , Field . Store . YES ) ) ; } else { Field . Index index ; Field . Store store = Field . Store . YES ; if ( fi . isIndexed && tokenize ) index = Field . Index . TOKENIZED ; else if ( fi . isIndexed && ! tokenize ) index = Field . Index . UN_TOKENIZED ; else index = Field . Index . NO ; Field . TermVector termVector = null ; if ( fi . storeTermVector ) { if ( fi . storeOffsetWithTermVector ) { if ( fi . storePositionWithTermVector ) { termVector = Field . TermVector . WITH_POSITIONS_OFFSETS ; } else { termVector = Field . TermVector . WITH_OFFSETS ; } } else if ( fi . storePositionWithTermVector ) { termVector = Field . TermVector . WITH_POSITIONS ; } else { termVector = Field . TermVector . YES ; } } else { termVector = Field . TermVector . NO ; } if ( compressed ) { store = Field . Store . COMPRESS ; final byte [ ] b = new byte [ fieldsStream . readVInt ( ) ] ; fieldsStream . readBytes ( b , 0 , b . length ) ; Field f = new Field ( fi . name , new String ( uncompress ( b ) , "UTF-8" ) , store , index , termVector ) ; f . setOmitNorms ( fi . omitNorms ) ; doc . add ( f ) ; } else { Field f = new Field ( fi . name , fieldsStream . readString ( ) , store , index , termVector ) ; f . setOmitNorms ( fi . omitNorms ) ; doc . add ( f ) ; } } } return doc ; } private final byte [ ] uncompress ( final byte [ ] input ) throws IOException { Inflater decompressor = new Inflater ( ) ; decompressor . setInput ( input ) ; ByteArrayOutputStream bos = new ByteArrayOutputStream ( input . length ) ; byte [ ] buf = new byte [ 1024 ] ; while ( ! decompressor . finished ( ) ) { try { int count = decompressor . inflate ( buf ) ; bos . write ( buf , 0 , count ) ; } catch ( DataFormatException e ) { IOException newException = new IOException ( "field data are in wrong format: " + e . toString ( ) ) ; newException . initCause ( e ) ; throw newException ; } } decompressor . end ( ) ; return bos . toByteArray ( ) ; } } 	1	['5', '1', '0', '10', '36', '2', '1', '9', '0', '0.625', '319', '1', '3', '0', '0.333333333', '0', '0', '62', '1', '0.8', '9']
package org . apache . lucene . analysis ; import java . io . Reader ; import java . io . IOException ; public abstract class Tokenizer extends TokenStream { protected Reader input ; protected Tokenizer ( ) { } protected Tokenizer ( Reader input ) { this . input = input ; } public void close ( ) throws IOException { input . close ( ) ; } } 	0	['3', '2', '3', '4', '5', '1', '3', '1', '1', '0.5', '17', '1', '0', '0.666666667', '0.666666667', '0', '0', '4.333333333', '1', '0.3333', '0']
package org . apache . lucene . util ; import java . io . IOException ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . IndexOutput ; public final class BitVector { private byte [ ] bits ; private int size ; private int count = - 1 ; public BitVector ( int n ) { size = n ; bits = new byte [ ( size > > 3 ) + 1 ] ; } public final void set ( int bit ) { bits [ bit > > 3 ] |= 1 << ( bit & 7 ) ; count = - 1 ; } public final void clear ( int bit ) { bits [ bit > > 3 ] &= ~ ( 1 << ( bit & 7 ) ) ; count = - 1 ; } public final boolean get ( int bit ) { return ( bits [ bit > > 3 ] & ( 1 << ( bit & 7 ) ) ) != 0 ; } public final int size ( ) { return size ; } public final int count ( ) { if ( count == - 1 ) { int c = 0 ; int end = bits . length ; for ( int i = 0 ; i < end ; i ++ ) c += BYTE_COUNTS [ bits [ i ] & 0xFF ] ; count = c ; } return count ; } private static final byte [ ] BYTE_COUNTS = { 0 , 1 , 1 , 2 , 1 , 2 , 2 , 3 , 1 , 2 , 2 , 3 , 2 , 3 , 3 , 4 , 1 , 2 , 2 , 3 , 2 , 3 , 3 , 4 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 1 , 2 , 2 , 3 , 2 , 3 , 3 , 4 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 1 , 2 , 2 , 3 , 2 , 3 , 3 , 4 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 4 , 5 , 5 , 6 , 5 , 6 , 6 , 7 , 1 , 2 , 2 , 3 , 2 , 3 , 3 , 4 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 4 , 5 , 5 , 6 , 5 , 6 , 6 , 7 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 4 , 5 , 5 , 6 , 5 , 6 , 6 , 7 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 4 , 5 , 5 , 6 , 5 , 6 , 6 , 7 , 4 , 5 , 5 , 6 , 5 , 6 , 6 , 7 , 5 , 6 , 6 , 7 , 6 , 7 , 7 , 8 } ; public final void write ( Directory d , String name ) throws IOException { IndexOutput output = d . createOutput ( name ) ; try { output . writeInt ( size ( ) ) ; output . writeInt ( count ( ) ) ; output . writeBytes ( bits , bits . length ) ; } finally { output . close ( ) ; } } public BitVector ( Directory d , String name ) throws IOException { IndexInput input = d . openInput ( name ) ; try { size = input . readInt ( ) ; count = input . readInt ( ) ; bits = new byte [ ( size > > 3 ) + 1 ] ; input . readBytes ( bits , 0 , bits . length ) ; } finally { input . close ( ) ; } } } 	1	['9', '1', '0', '5', '18', '0', '2', '3', '8', '0.5', '1224', '1', '0', '0', '0.5', '0', '0', '134.5555556', '3', '1', '2']
package org . apache . lucene ; public final class LucenePackage { private LucenePackage ( ) { } public static Package get ( ) { return LucenePackage . class . getPackage ( ) ; } } 	0	['3', '1', '0', '0', '8', '3', '0', '0', '1', '1', '27', '0', '0', '0', '0.333333333', '0', '0', '7.666666667', '2', '1', '0']
package org . apache . lucene . store ; import java . io . IOException ; import java . io . File ; import java . util . Hashtable ; import java . util . Enumeration ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . IndexOutput ; public final class RAMDirectory extends Directory { Hashtable files = new Hashtable ( ) ; public RAMDirectory ( ) { } public RAMDirectory ( Directory dir ) throws IOException { this ( dir , false ) ; } private RAMDirectory ( Directory dir , boolean closeDir ) throws IOException { final String [ ] files = dir . list ( ) ; byte [ ] buf = new byte [ BufferedIndexOutput . BUFFER_SIZE ] ; for ( int i = 0 ; i < files . length ; i ++ ) { IndexOutput os = createOutput ( files [ i ] ) ; IndexInput is = dir . openInput ( files [ i ] ) ; long len = is . length ( ) ; long readCount = 0 ; while ( readCount < len ) { int toRead = readCount + BufferedIndexOutput . BUFFER_SIZE > len ? ( int ) ( len - readCount ) : BufferedIndexOutput . BUFFER_SIZE ; is . readBytes ( buf , 0 , toRead ) ; os . writeBytes ( buf , toRead ) ; readCount += toRead ; } is . close ( ) ; os . close ( ) ; } if ( closeDir ) dir . close ( ) ; } public RAMDirectory ( File dir ) throws IOException { this ( FSDirectory . getDirectory ( dir , false ) , true ) ; } public RAMDirectory ( String dir ) throws IOException { this ( FSDirectory . getDirectory ( dir , false ) , true ) ; } public final String [ ] list ( ) { String [ ] result = new String [ files . size ( ) ] ; int i = 0 ; Enumeration names = files . keys ( ) ; while ( names . hasMoreElements ( ) ) result [ i ++ ] = ( String ) names . nextElement ( ) ; return result ; } public final boolean fileExists ( String name ) { RAMFile file = ( RAMFile ) files . get ( name ) ; return file != null ; } public final long fileModified ( String name ) { RAMFile file = ( RAMFile ) files . get ( name ) ; return file . lastModified ; } public void touchFile ( String name ) { RAMFile file = ( RAMFile ) files . get ( name ) ; long ts2 , ts1 = System . currentTimeMillis ( ) ; do { try { Thread . sleep ( 0 , 1 ) ; } catch ( InterruptedException e ) { } ts2 = System . currentTimeMillis ( ) ; } while ( ts1 == ts2 ) ; file . lastModified = ts2 ; } public final long fileLength ( String name ) { RAMFile file = ( RAMFile ) files . get ( name ) ; return file . length ; } public final void deleteFile ( String name ) { files . remove ( name ) ; } public final void renameFile ( String from , String to ) { RAMFile file = ( RAMFile ) files . get ( from ) ; files . remove ( from ) ; files . put ( to , file ) ; } public final IndexOutput createOutput ( String name ) { RAMFile file = new RAMFile ( ) ; files . put ( name , file ) ; return new RAMOutputStream ( file ) ; } public final IndexInput openInput ( String name ) { RAMFile file = ( RAMFile ) files . get ( name ) ; return new RAMInputStream ( file ) ; } public final Lock makeLock ( final String name ) { return new Lock ( ) { public boolean obtain ( ) throws IOException { synchronized ( files ) { if ( ! fileExists ( name ) ) { createOutput ( name ) . close ( ) ; return true ; } return false ; } } public void release ( ) { deleteFile ( name ) ; } public boolean isLocked ( ) { return fileExists ( name ) ; } } ; } public final void close ( ) { } } 	1	['16', '2', '0', '10', '41', '10', '2', '9', '15', '0.133333333', '257', '0', '0', '0.5', '0.375', '0', '0', '15', '2', '0.875', '12']
package org . apache . lucene . util ; import java . io . ObjectStreamException ; import java . io . Serializable ; import java . io . StreamCorruptedException ; import java . util . HashMap ; import java . util . Map ; public abstract class Parameter implements Serializable { static Map allParameters = new HashMap ( ) ; private String name ; private Parameter ( ) { } protected Parameter ( String name ) { this . name = name ; String key = makeKey ( name ) ; if ( allParameters . containsKey ( key ) ) throw new IllegalArgumentException ( "Parameter name " + key + " already used!" ) ; allParameters . put ( key , this ) ; } private String makeKey ( String name ) { return getClass ( ) + " " + name ; } public String toString ( ) { return name ; } protected Object readResolve ( ) throws ObjectStreamException { Object par = allParameters . get ( makeKey ( name ) ) ; if ( par == null ) throw new StreamCorruptedException ( "Unknown parameter value: " + name ) ; return par ; } } 	0	['6', '1', '5', '5', '18', '5', '5', '0', '1', '0.6', '88', '0.5', '0', '0', '0.7', '0', '0', '13.33333333', '1', '0.5', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import java . util . Iterator ; import java . util . LinkedList ; import java . util . List ; import org . apache . lucene . util . PriorityQueue ; public class MultipleTermPositions implements TermPositions { private static final class TermPositionsQueue extends PriorityQueue { TermPositionsQueue ( List termPositions ) throws IOException { initialize ( termPositions . size ( ) ) ; Iterator i = termPositions . iterator ( ) ; while ( i . hasNext ( ) ) { TermPositions tp = ( TermPositions ) i . next ( ) ; if ( tp . next ( ) ) put ( tp ) ; } } final TermPositions peek ( ) { return ( TermPositions ) top ( ) ; } public final boolean lessThan ( Object a , Object b ) { return ( ( TermPositions ) a ) . doc ( ) < ( ( TermPositions ) b ) . doc ( ) ; } } private static final class IntQueue { private int _arraySize = 16 ; private int _index = 0 ; private int _lastIndex = 0 ; private int [ ] _array = new int [ _arraySize ] ; final void add ( int i ) { if ( _lastIndex == _arraySize ) growArray ( ) ; _array [ _lastIndex ++ ] = i ; } final int next ( ) { return _array [ _index ++ ] ; } final void sort ( ) { Arrays . sort ( _array , _index , _lastIndex ) ; } final void clear ( ) { _index = 0 ; _lastIndex = 0 ; } final int size ( ) { return ( _lastIndex - _index ) ; } private void growArray ( ) { int [ ] newArray = new int [ _arraySize * 2 ] ; System . arraycopy ( _array , 0 , newArray , 0 , _arraySize ) ; _array = newArray ; _arraySize *= 2 ; } } private int _doc ; private int _freq ; private TermPositionsQueue _termPositionsQueue ; private IntQueue _posList ; public MultipleTermPositions ( IndexReader indexReader , Term [ ] terms ) throws IOException { List termPositions = new LinkedList ( ) ; for ( int i = 0 ; i < terms . length ; i ++ ) termPositions . add ( indexReader . termPositions ( terms [ i ] ) ) ; _termPositionsQueue = new TermPositionsQueue ( termPositions ) ; _posList = new IntQueue ( ) ; } public final boolean next ( ) throws IOException { if ( _termPositionsQueue . size ( ) == 0 ) return false ; _posList . clear ( ) ; _doc = _termPositionsQueue . peek ( ) . doc ( ) ; TermPositions tp ; do { tp = _termPositionsQueue . peek ( ) ; for ( int i = 0 ; i < tp . freq ( ) ; i ++ ) _posList . add ( tp . nextPosition ( ) ) ; if ( tp . next ( ) ) _termPositionsQueue . adjustTop ( ) ; else { _termPositionsQueue . pop ( ) ; tp . close ( ) ; } } while ( _termPositionsQueue . size ( ) > 0 && _termPositionsQueue . peek ( ) . doc ( ) == _doc ) ; _posList . sort ( ) ; _freq = _posList . size ( ) ; return true ; } public final int nextPosition ( ) { return _posList . next ( ) ; } public final boolean skipTo ( int target ) throws IOException { while ( _termPositionsQueue . peek ( ) != null && target > _termPositionsQueue . peek ( ) . doc ( ) ) { TermPositions tp = ( TermPositions ) _termPositionsQueue . pop ( ) ; if ( tp . skipTo ( target ) ) _termPositionsQueue . put ( tp ) ; else tp . close ( ) ; } return next ( ) ; } public final int doc ( ) { return _doc ; } public final int freq ( ) { return _freq ; } public final void close ( ) throws IOException { while ( _termPositionsQueue . size ( ) > 0 ) ( ( TermPositions ) _termPositionsQueue . pop ( ) ) . close ( ) ; } public void seek ( Term arg0 ) throws IOException { throw new UnsupportedOperationException ( ) ; } public void seek ( TermEnum termEnum ) throws IOException { throw new UnsupportedOperationException ( ) ; } public int read ( int [ ] arg0 , int [ ] arg1 ) throws IOException { throw new UnsupportedOperationException ( ) ; } } 	1	['10', '1', '0', '8', '33', '25', '1', '7', '10', '0.722222222', '178', '1', '2', '0', '0.228571429', '0', '0', '16.4', '1', '0.9', '2']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Query ; import org . apache . lucene . search . Weight ; import org . apache . lucene . search . Searcher ; public abstract class SpanQuery extends Query { public abstract Spans getSpans ( IndexReader reader ) throws IOException ; public abstract String getField ( ) ; public abstract Collection getTerms ( ) ; protected Weight createWeight ( Searcher searcher ) throws IOException { return new SpanWeight ( this , searcher ) ; } } 	0	['5', '2', '5', '15', '7', '10', '10', '6', '4', '2', '14', '0', '0', '0.75', '0.466666667', '1', '1', '1.8', '1', '0.8', '0']
package org . apache . lucene . queryParser ; import java . util . Vector ; import java . io . * ; import java . text . * ; import java . util . * ; import org . apache . lucene . index . Term ; import org . apache . lucene . analysis . * ; import org . apache . lucene . document . * ; import org . apache . lucene . search . * ; import org . apache . lucene . util . Parameter ; public class QueryParser implements QueryParserConstants { private static final int CONJ_NONE = 0 ; private static final int CONJ_AND = 1 ; private static final int CONJ_OR = 2 ; private static final int MOD_NONE = 0 ; private static final int MOD_NOT = 10 ; private static final int MOD_REQ = 11 ; public static final Operator AND_OPERATOR = Operator . AND ; public static final Operator OR_OPERATOR = Operator . OR ; private Operator operator = OR_OPERATOR ; boolean lowercaseExpandedTerms = true ; Analyzer analyzer ; String field ; int phraseSlop = 0 ; float fuzzyMinSim = FuzzyQuery . defaultMinSimilarity ; int fuzzyPrefixLength = FuzzyQuery . defaultPrefixLength ; Locale locale = Locale . getDefault ( ) ; static public final class Operator extends Parameter { private Operator ( String name ) { super ( name ) ; } static public final Operator OR = new Operator ( "OR" ) ; static public final Operator AND = new Operator ( "AND" ) ; } public QueryParser ( String f , Analyzer a ) { this ( new FastCharStream ( new StringReader ( "" ) ) ) ; analyzer = a ; field = f ; } public Query parse ( String query ) throws ParseException { ReInit ( new FastCharStream ( new StringReader ( query ) ) ) ; try { return Query ( field ) ; } catch ( TokenMgrError tme ) { throw new ParseException ( tme . getMessage ( ) ) ; } catch ( BooleanQuery . TooManyClauses tmc ) { throw new ParseException ( "Too many boolean clauses" ) ; } } public Analyzer getAnalyzer ( ) { return analyzer ; } public String getField ( ) { return field ; } public float getFuzzyMinSim ( ) { return fuzzyMinSim ; } public void setFuzzyMinSim ( float fuzzyMinSim ) { this . fuzzyMinSim = fuzzyMinSim ; } public int getFuzzyPrefixLength ( ) { return fuzzyPrefixLength ; } public void setFuzzyPrefixLength ( int fuzzyPrefixLength ) { this . fuzzyPrefixLength = fuzzyPrefixLength ; } public void setPhraseSlop ( int phraseSlop ) { this . phraseSlop = phraseSlop ; } public int getPhraseSlop ( ) { return phraseSlop ; } public void setDefaultOperator ( Operator op ) { this . operator = op ; } public Operator getDefaultOperator ( ) { return operator ; } public void setLowercaseExpandedTerms ( boolean lowercaseExpandedTerms ) { this . lowercaseExpandedTerms = lowercaseExpandedTerms ; } public boolean getLowercaseExpandedTerms ( ) { return lowercaseExpandedTerms ; } public void setLocale ( Locale locale ) { this . locale = locale ; } public Locale getLocale ( ) { return locale ; } protected void addClause ( Vector clauses , int conj , int mods , Query q ) { boolean required , prohibited ; if ( clauses . size ( ) > 0 && conj == CONJ_AND ) { BooleanClause c = ( BooleanClause ) clauses . elementAt ( clauses . size ( ) - 1 ) ; if ( ! c . isProhibited ( ) ) c . setOccur ( BooleanClause . Occur . MUST ) ; } if ( clauses . size ( ) > 0 && operator == AND_OPERATOR && conj == CONJ_OR ) { BooleanClause c = ( BooleanClause ) clauses . elementAt ( clauses . size ( ) - 1 ) ; if ( ! c . isProhibited ( ) ) c . setOccur ( BooleanClause . Occur . SHOULD ) ; } if ( q == null ) return ; if ( operator == OR_OPERATOR ) { prohibited = ( mods == MOD_NOT ) ; required = ( mods == MOD_REQ ) ; if ( conj == CONJ_AND && ! prohibited ) { required = true ; } } else { prohibited = ( mods == MOD_NOT ) ; required = ( ! prohibited && conj != CONJ_OR ) ; } if ( required && ! prohibited ) clauses . addElement ( new BooleanClause ( q , BooleanClause . Occur . MUST ) ) ; else if ( ! required && ! prohibited ) clauses . addElement ( new BooleanClause ( q , BooleanClause . Occur . SHOULD ) ) ; else if ( ! required && prohibited ) clauses . addElement ( new BooleanClause ( q , BooleanClause . Occur . MUST_NOT ) ) ; else throw new RuntimeException ( "Clause cannot be both required and prohibited" ) ; } protected Query getFieldQuery ( String field , String queryText ) throws ParseException { TokenStream source = analyzer . tokenStream ( field , new StringReader ( queryText ) ) ; Vector v = new Vector ( ) ; org . apache . lucene . analysis . Token t ; int positionCount = 0 ; boolean severalTokensAtSamePosition = false ; while ( true ) { try { t = source . next ( ) ; } catch ( IOException e ) { t = null ; } if ( t == null ) break ; v . addElement ( t ) ; if ( t . getPositionIncrement ( ) != 0 ) positionCount += t . getPositionIncrement ( ) ; else severalTokensAtSamePosition = true ; } try { source . close ( ) ; } catch ( IOException e ) { } if ( v . size ( ) == 0 ) return null ; else if ( v . size ( ) == 1 ) { t = ( org . apache . lucene . analysis . Token ) v . elementAt ( 0 ) ; return new TermQuery ( new Term ( field , t . termText ( ) ) ) ; } else { if ( severalTokensAtSamePosition ) { if ( positionCount == 1 ) { BooleanQuery q = new BooleanQuery ( true ) ; for ( int i = 0 ; i < v . size ( ) ; i ++ ) { t = ( org . apache . lucene . analysis . Token ) v . elementAt ( i ) ; TermQuery currentQuery = new TermQuery ( new Term ( field , t . termText ( ) ) ) ; q . add ( currentQuery , BooleanClause . Occur . SHOULD ) ; } return q ; } else { MultiPhraseQuery mpq = new MultiPhraseQuery ( ) ; mpq . setSlop ( phraseSlop ) ; List multiTerms = new ArrayList ( ) ; for ( int i = 0 ; i < v . size ( ) ; i ++ ) { t = ( org . apache . lucene . analysis . Token ) v . elementAt ( i ) ; if ( t . getPositionIncrement ( ) == 1 && multiTerms . size ( ) > 0 ) { mpq . add ( ( Term [ ] ) multiTerms . toArray ( new Term [ 0 ] ) ) ; multiTerms . clear ( ) ; } multiTerms . add ( new Term ( field , t . termText ( ) ) ) ; } mpq . add ( ( Term [ ] ) multiTerms . toArray ( new Term [ 0 ] ) ) ; return mpq ; } } else { PhraseQuery q = new PhraseQuery ( ) ; q . setSlop ( phraseSlop ) ; for ( int i = 0 ; i < v . size ( ) ; i ++ ) { q . add ( new Term ( field , ( ( org . apache . lucene . analysis . Token ) v . elementAt ( i ) ) . termText ( ) ) ) ; } return q ; } } } protected Query getFieldQuery ( String field , String queryText , int slop ) throws ParseException { Query query = getFieldQuery ( field , queryText ) ; if ( query instanceof PhraseQuery ) { ( ( PhraseQuery ) query ) . setSlop ( slop ) ; } if ( query instanceof MultiPhraseQuery ) { ( ( MultiPhraseQuery ) query ) . setSlop ( slop ) ; } return query ; } protected Query getRangeQuery ( String field , String part1 , String part2 , boolean inclusive ) throws ParseException { if ( lowercaseExpandedTerms ) { part1 = part1 . toLowerCase ( ) ; part2 = part2 . toLowerCase ( ) ; } try { DateFormat df = DateFormat . getDateInstance ( DateFormat . SHORT , locale ) ; df . setLenient ( true ) ; Date d1 = df . parse ( part1 ) ; Date d2 = df . parse ( part2 ) ; if ( inclusive ) { Calendar cal = Calendar . getInstance ( locale ) ; cal . setTime ( d2 ) ; cal . set ( Calendar . HOUR_OF_DAY , 23 ) ; cal . set ( Calendar . MINUTE , 59 ) ; cal . set ( Calendar . SECOND , 59 ) ; cal . set ( Calendar . MILLISECOND , 999 ) ; d2 = cal . getTime ( ) ; } part1 = DateField . dateToString ( d1 ) ; part2 = DateField . dateToString ( d2 ) ; } catch ( Exception e ) { } return new RangeQuery ( new Term ( field , part1 ) , new Term ( field , part2 ) , inclusive ) ; } protected Query getBooleanQuery ( Vector clauses ) throws ParseException { return getBooleanQuery ( clauses , false ) ; } protected Query getBooleanQuery ( Vector clauses , boolean disableCoord ) throws ParseException { BooleanQuery query = new BooleanQuery ( disableCoord ) ; for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { query . add ( ( BooleanClause ) clauses . elementAt ( i ) ) ; } return query ; } protected Query getWildcardQuery ( String field , String termStr ) throws ParseException { if ( lowercaseExpandedTerms ) { termStr = termStr . toLowerCase ( ) ; } Term t = new Term ( field , termStr ) ; return new WildcardQuery ( t ) ; } protected Query getPrefixQuery ( String field , String termStr ) throws ParseException { if ( lowercaseExpandedTerms ) { termStr = termStr . toLowerCase ( ) ; } Term t = new Term ( field , termStr ) ; return new PrefixQuery ( t ) ; } protected Query getFuzzyQuery ( String field , String termStr , float minSimilarity ) throws ParseException { if ( lowercaseExpandedTerms ) { termStr = termStr . toLowerCase ( ) ; } Term t = new Term ( field , termStr ) ; return new FuzzyQuery ( t , minSimilarity , fuzzyPrefixLength ) ; } private String discardEscapeChar ( String input ) { char [ ] caSource = input . toCharArray ( ) ; char [ ] caDest = new char [ caSource . length ] ; int j = 0 ; for ( int i = 0 ; i < caSource . length ; i ++ ) { if ( ( caSource [ i ] != '\\' ) || ( i > 0 && caSource [ i - 1 ] == '\\' ) ) { caDest [ j ++ ] = caSource [ i ] ; } } return new String ( caDest , 0 , j ) ; } public static String escape ( String s ) { StringBuffer sb = new StringBuffer ( ) ; for ( int i = 0 ; i < s . length ( ) ; i ++ ) { char c = s . charAt ( i ) ; if ( c == '\\' || c == '+' || c == '-' || c == '!' || c == '(' || c == ')' || c == ':' || c == '^' || c == '[' || c == ']' || c == '\"' || c == '{' || c == '}' || c == '~' || c == '*' || c == '?' ) { sb . append ( '\\' ) ; } sb . append ( c ) ; } return sb . toString ( ) ; } public static void main ( String [ ] args ) throws Exception { if ( args . length == 0 ) { System . out . println ( "Usage: java org.apache.lucene.queryParser.QueryParser <input>" ) ; System . exit ( 0 ) ; } QueryParser qp = new QueryParser ( "field" , new org . apache . lucene . analysis . SimpleAnalyzer ( ) ) ; Query q = qp . parse ( args [ 0 ] ) ; System . out . println ( q . toString ( "field" ) ) ; } final public int Conjunction ( ) throws ParseException { int ret = CONJ_NONE ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case AND : case OR : switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case AND : jj_consume_token ( AND ) ; ret = CONJ_AND ; break ; case OR : jj_consume_token ( OR ) ; ret = CONJ_OR ; break ; default : jj_la1 [ 0 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } break ; default : jj_la1 [ 1 ] = jj_gen ; ; } { if ( true ) return ret ; } throw new Error ( "Missing return statement in function" ) ; } final public int Modifiers ( ) throws ParseException { int ret = MOD_NONE ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case NOT : case PLUS : case MINUS : switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case PLUS : jj_consume_token ( PLUS ) ; ret = MOD_REQ ; break ; case MINUS : jj_consume_token ( MINUS ) ; ret = MOD_NOT ; break ; case NOT : jj_consume_token ( NOT ) ; ret = MOD_NOT ; break ; default : jj_la1 [ 2 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } break ; default : jj_la1 [ 3 ] = jj_gen ; ; } { if ( true ) return ret ; } throw new Error ( "Missing return statement in function" ) ; } final public Query Query ( String field ) throws ParseException { Vector clauses = new Vector ( ) ; Query q , firstQuery = null ; int conj , mods ; mods = Modifiers ( ) ; q = Clause ( field ) ; addClause ( clauses , CONJ_NONE , mods , q ) ; if ( mods == MOD_NONE ) firstQuery = q ; label_1 : while ( true ) { switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case AND : case OR : case NOT : case PLUS : case MINUS : case LPAREN : case QUOTED : case TERM : case PREFIXTERM : case WILDTERM : case RANGEIN_START : case RANGEEX_START : case NUMBER : ; break ; default : jj_la1 [ 4 ] = jj_gen ; break label_1 ; } conj = Conjunction ( ) ; mods = Modifiers ( ) ; q = Clause ( field ) ; addClause ( clauses , conj , mods , q ) ; } if ( clauses . size ( ) == 1 && firstQuery != null ) { if ( true ) return firstQuery ; } else { { if ( true ) return getBooleanQuery ( clauses ) ; } } throw new Error ( "Missing return statement in function" ) ; } final public Query Clause ( String field ) throws ParseException { Query q ; Token fieldToken = null , boost = null ; if ( jj_2_1 ( 2 ) ) { fieldToken = jj_consume_token ( TERM ) ; jj_consume_token ( COLON ) ; field = discardEscapeChar ( fieldToken . image ) ; } else { ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case QUOTED : case TERM : case PREFIXTERM : case WILDTERM : case RANGEIN_START : case RANGEEX_START : case NUMBER : q = Term ( field ) ; break ; case LPAREN : jj_consume_token ( LPAREN ) ; q = Query ( field ) ; jj_consume_token ( RPAREN ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case CARAT : jj_consume_token ( CARAT ) ; boost = jj_consume_token ( NUMBER ) ; break ; default : jj_la1 [ 5 ] = jj_gen ; ; } break ; default : jj_la1 [ 6 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } if ( boost != null ) { float f = ( float ) 1.0 ; try { f = Float . valueOf ( boost . image ) . floatValue ( ) ; q . setBoost ( f ) ; } catch ( Exception ignored ) { } } { if ( true ) return q ; } throw new Error ( "Missing return statement in function" ) ; } final public Query Term ( String field ) throws ParseException { Token term , boost = null , fuzzySlop = null , goop1 , goop2 ; boolean prefix = false ; boolean wildcard = false ; boolean fuzzy = false ; boolean rangein = false ; Query q ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case TERM : case PREFIXTERM : case WILDTERM : case NUMBER : switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case TERM : term = jj_consume_token ( TERM ) ; break ; case PREFIXTERM : term = jj_consume_token ( PREFIXTERM ) ; prefix = true ; break ; case WILDTERM : term = jj_consume_token ( WILDTERM ) ; wildcard = true ; break ; case NUMBER : term = jj_consume_token ( NUMBER ) ; break ; default : jj_la1 [ 7 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case FUZZY_SLOP : fuzzySlop = jj_consume_token ( FUZZY_SLOP ) ; fuzzy = true ; break ; default : jj_la1 [ 8 ] = jj_gen ; ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case CARAT : jj_consume_token ( CARAT ) ; boost = jj_consume_token ( NUMBER ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case FUZZY_SLOP : fuzzySlop = jj_consume_token ( FUZZY_SLOP ) ; fuzzy = true ; break ; default : jj_la1 [ 9 ] = jj_gen ; ; } break ; default : jj_la1 [ 10 ] = jj_gen ; ; } String termImage = discardEscapeChar ( term . image ) ; if ( wildcard ) { q = getWildcardQuery ( field , termImage ) ; } else if ( prefix ) { q = getPrefixQuery ( field , discardEscapeChar ( term . image . substring ( 0 , term . image . length ( ) - 1 ) ) ) ; } else if ( fuzzy ) { float fms = fuzzyMinSim ; try { fms = Float . valueOf ( fuzzySlop . image . substring ( 1 ) ) . floatValue ( ) ; } catch ( Exception ignored ) { } if ( fms < 0.0f || fms > 1.0f ) { { if ( true ) throw new ParseException ( "Minimum similarity for a FuzzyQuery has to be between 0.0f and 1.0f !" ) ; } } q = getFuzzyQuery ( field , termImage , fms ) ; } else { q = getFieldQuery ( field , termImage ) ; } break ; case RANGEIN_START : jj_consume_token ( RANGEIN_START ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case RANGEIN_GOOP : goop1 = jj_consume_token ( RANGEIN_GOOP ) ; break ; case RANGEIN_QUOTED : goop1 = jj_consume_token ( RANGEIN_QUOTED ) ; break ; default : jj_la1 [ 11 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case RANGEIN_TO : jj_consume_token ( RANGEIN_TO ) ; break ; default : jj_la1 [ 12 ] = jj_gen ; ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case RANGEIN_GOOP : goop2 = jj_consume_token ( RANGEIN_GOOP ) ; break ; case RANGEIN_QUOTED : goop2 = jj_consume_token ( RANGEIN_QUOTED ) ; break ; default : jj_la1 [ 13 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } jj_consume_token ( RANGEIN_END ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case CARAT : jj_consume_token ( CARAT ) ; boost = jj_consume_token ( NUMBER ) ; break ; default : jj_la1 [ 14 ] = jj_gen ; ; } if ( goop1 . kind == RANGEIN_QUOTED ) { goop1 . image = goop1 . image . substring ( 1 , goop1 . image . length ( ) - 1 ) ; } else { goop1 . image = discardEscapeChar ( goop1 . image ) ; } if ( goop2 . kind == RANGEIN_QUOTED ) { goop2 . image = goop2 . image . substring ( 1 , goop2 . image . length ( ) - 1 ) ; } else { goop2 . image = discardEscapeChar ( goop2 . image ) ; } q = getRangeQuery ( field , goop1 . image , goop2 . image , true ) ; break ; case RANGEEX_START : jj_consume_token ( RANGEEX_START ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case RANGEEX_GOOP : goop1 = jj_consume_token ( RANGEEX_GOOP ) ; break ; case RANGEEX_QUOTED : goop1 = jj_consume_token ( RANGEEX_QUOTED ) ; break ; default : jj_la1 [ 15 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case RANGEEX_TO : jj_consume_token ( RANGEEX_TO ) ; break ; default : jj_la1 [ 16 ] = jj_gen ; ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case RANGEEX_GOOP : goop2 = jj_consume_token ( RANGEEX_GOOP ) ; break ; case RANGEEX_QUOTED : goop2 = jj_consume_token ( RANGEEX_QUOTED ) ; break ; default : jj_la1 [ 17 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } jj_consume_token ( RANGEEX_END ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case CARAT : jj_consume_token ( CARAT ) ; boost = jj_consume_token ( NUMBER ) ; break ; default : jj_la1 [ 18 ] = jj_gen ; ; } if ( goop1 . kind == RANGEEX_QUOTED ) { goop1 . image = goop1 . image . substring ( 1 , goop1 . image . length ( ) - 1 ) ; } else { goop1 . image = discardEscapeChar ( goop1 . image ) ; } if ( goop2 . kind == RANGEEX_QUOTED ) { goop2 . image = goop2 . image . substring ( 1 , goop2 . image . length ( ) - 1 ) ; } else { goop2 . image = discardEscapeChar ( goop2 . image ) ; } q = getRangeQuery ( field , goop1 . image , goop2 . image , false ) ; break ; case QUOTED : term = jj_consume_token ( QUOTED ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case FUZZY_SLOP : fuzzySlop = jj_consume_token ( FUZZY_SLOP ) ; break ; default : jj_la1 [ 19 ] = jj_gen ; ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case CARAT : jj_consume_token ( CARAT ) ; boost = jj_consume_token ( NUMBER ) ; break ; default : jj_la1 [ 20 ] = jj_gen ; ; } int s = phraseSlop ; if ( fuzzySlop != null ) { try { s = Float . valueOf ( fuzzySlop . image . substring ( 1 ) ) . intValue ( ) ; } catch ( Exception ignored ) { } } q = getFieldQuery ( field , term . image . substring ( 1 , term . image . length ( ) - 1 ) , s ) ; break ; default : jj_la1 [ 21 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } if ( boost != null ) { float f = ( float ) 1.0 ; try { f = Float . valueOf ( boost . image ) . floatValue ( ) ; } catch ( Exception ignored ) { } if ( q != null ) { q . setBoost ( f ) ; } } { if ( true ) return q ; } throw new Error ( "Missing return statement in function" ) ; } final private boolean jj_2_1 ( int xla ) { jj_la = xla ; jj_lastpos = jj_scanpos = token ; try { return ! jj_3_1 ( ) ; } catch ( LookaheadSuccess ls ) { return true ; } finally { jj_save ( 0 , xla ) ; } } final private boolean jj_3_1 ( ) { if ( jj_scan_token ( TERM ) ) return true ; if ( jj_scan_token ( COLON ) ) return true ; return false ; } public QueryParserTokenManager token_source ; public Token token , jj_nt ; private int jj_ntk ; private Token jj_scanpos , jj_lastpos ; private int jj_la ; public boolean lookingAhead = false ; private boolean jj_semLA ; private int jj_gen ; final private int [ ] jj_la1 = new int [ 22 ] ; static private int [ ] jj_la1_0 ; static { jj_la1_0 ( ) ; } private static void jj_la1_0 ( ) { jj_la1_0 = new int [ ] { 0x180 , 0x180 , 0xe00 , 0xe00 , 0xfb1f80 , 0x8000 , 0xfb1000 , 0x9a0000 , 0x40000 , 0x40000 , 0x8000 , 0xc000000 , 0x1000000 , 0xc000000 , 0x8000 , 0xc0000000 , 0x10000000 , 0xc0000000 , 0x8000 , 0x40000 , 0x8000 , 0xfb0000 , } ; } final private JJCalls [ ] jj_2_rtns = new JJCalls [ 1 ] ; private boolean jj_rescan = false ; private int jj_gc = 0 ; public QueryParser ( CharStream stream ) { token_source = new QueryParserTokenManager ( stream ) ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 22 ; i ++ ) jj_la1 [ i ] = - 1 ; for ( int i = 0 ; i < jj_2_rtns . length ; i ++ ) jj_2_rtns [ i ] = new JJCalls ( ) ; } public void ReInit ( CharStream stream ) { token_source . ReInit ( stream ) ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 22 ; i ++ ) jj_la1 [ i ] = - 1 ; for ( int i = 0 ; i < jj_2_rtns . length ; i ++ ) jj_2_rtns [ i ] = new JJCalls ( ) ; } public QueryParser ( QueryParserTokenManager tm ) { token_source = tm ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 22 ; i ++ ) jj_la1 [ i ] = - 1 ; for ( int i = 0 ; i < jj_2_rtns . length ; i ++ ) jj_2_rtns [ i ] = new JJCalls ( ) ; } public void ReInit ( QueryParserTokenManager tm ) { token_source = tm ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 22 ; i ++ ) jj_la1 [ i ] = - 1 ; for ( int i = 0 ; i < jj_2_rtns . length ; i ++ ) jj_2_rtns [ i ] = new JJCalls ( ) ; } final private Token jj_consume_token ( int kind ) throws ParseException { Token oldToken ; if ( ( oldToken = token ) . next != null ) token = token . next ; else token = token . next = token_source . getNextToken ( ) ; jj_ntk = - 1 ; if ( token . kind == kind ) { jj_gen ++ ; if ( ++ jj_gc > 100 ) { jj_gc = 0 ; for ( int i = 0 ; i < jj_2_rtns . length ; i ++ ) { JJCalls c = jj_2_rtns [ i ] ; while ( c != null ) { if ( c . gen < jj_gen ) c . first = null ; c = c . next ; } } } return token ; } token = oldToken ; jj_kind = kind ; throw generateParseException ( ) ; } static private final class LookaheadSuccess extends java . lang . Error { } final private LookaheadSuccess jj_ls = new LookaheadSuccess ( ) ; final private boolean jj_scan_token ( int kind ) { if ( jj_scanpos == jj_lastpos ) { jj_la -- ; if ( jj_scanpos . next == null ) { jj_lastpos = jj_scanpos = jj_scanpos . next = token_source . getNextToken ( ) ; } else { jj_lastpos = jj_scanpos = jj_scanpos . next ; } } else { jj_scanpos = jj_scanpos . next ; } if ( jj_rescan ) { int i = 0 ; Token tok = token ; while ( tok != null && tok != jj_scanpos ) { i ++ ; tok = tok . next ; } if ( tok != null ) jj_add_error_token ( kind , i ) ; } if ( jj_scanpos . kind != kind ) return true ; if ( jj_la == 0 && jj_scanpos == jj_lastpos ) throw jj_ls ; return false ; } final public Token getNextToken ( ) { if ( token . next != null ) token = token . next ; else token = token . next = token_source . getNextToken ( ) ; jj_ntk = - 1 ; jj_gen ++ ; return token ; } final public Token getToken ( int index ) { Token t = lookingAhead ? jj_scanpos : token ; for ( int i = 0 ; i < index ; i ++ ) { if ( t . next != null ) t = t . next ; else t = t . next = token_source . getNextToken ( ) ; } return t ; } final private int jj_ntk ( ) { if ( ( jj_nt = token . next ) == null ) return ( jj_ntk = ( token . next = token_source . getNextToken ( ) ) . kind ) ; else return ( jj_ntk = jj_nt . kind ) ; } private java . util . Vector jj_expentries = new java . util . Vector ( ) ; private int [ ] jj_expentry ; private int jj_kind = - 1 ; private int [ ] jj_lasttokens = new int [ 100 ] ; private int jj_endpos ; private void jj_add_error_token ( int kind , int pos ) { if ( pos >= 100 ) return ; if ( pos == jj_endpos + 1 ) { jj_lasttokens [ jj_endpos ++ ] = kind ; } else if ( jj_endpos != 0 ) { jj_expentry = new int [ jj_endpos ] ; for ( int i = 0 ; i < jj_endpos ; i ++ ) { jj_expentry [ i ] = jj_lasttokens [ i ] ; } boolean exists = false ; for ( java . util . Enumeration e = jj_expentries . elements ( ) ; e . hasMoreElements ( ) ; ) { int [ ] oldentry = ( int [ ] ) ( e . nextElement ( ) ) ; if ( oldentry . length == jj_expentry . length ) { exists = true ; for ( int i = 0 ; i < jj_expentry . length ; i ++ ) { if ( oldentry [ i ] != jj_expentry [ i ] ) { exists = false ; break ; } } if ( exists ) break ; } } if ( ! exists ) jj_expentries . addElement ( jj_expentry ) ; if ( pos != 0 ) jj_lasttokens [ ( jj_endpos = pos ) - 1 ] = kind ; } } public ParseException generateParseException ( ) { jj_expentries . removeAllElements ( ) ; boolean [ ] la1tokens = new boolean [ 32 ] ; for ( int i = 0 ; i < 32 ; i ++ ) { la1tokens [ i ] = false ; } if ( jj_kind >= 0 ) { la1tokens [ jj_kind ] = true ; jj_kind = - 1 ; } for ( int i = 0 ; i < 22 ; i ++ ) { if ( jj_la1 [ i ] == jj_gen ) { for ( int j = 0 ; j < 32 ; j ++ ) { if ( ( jj_la1_0 [ i ] & ( 1 << j ) ) != 0 ) { la1tokens [ j ] = true ; } } } } for ( int i = 0 ; i < 32 ; i ++ ) { if ( la1tokens [ i ] ) { jj_expentry = new int [ 1 ] ; jj_expentry [ 0 ] = i ; jj_expentries . addElement ( jj_expentry ) ; } } jj_endpos = 0 ; jj_rescan_token ( ) ; jj_add_error_token ( 0 , 0 ) ; int [ ] [ ] exptokseq = new int [ jj_expentries . size ( ) ] [ ] ; for ( int i = 0 ; i < jj_expentries . size ( ) ; i ++ ) { exptokseq [ i ] = ( int [ ] ) jj_expentries . elementAt ( i ) ; } return new ParseException ( token , exptokseq , tokenImage ) ; } final public void enable_tracing ( ) { } final public void disable_tracing ( ) { } final private void jj_rescan_token ( ) { jj_rescan = true ; for ( int i = 0 ; i < 1 ; i ++ ) { JJCalls p = jj_2_rtns [ i ] ; do { if ( p . gen > jj_gen ) { jj_la = p . arg ; jj_lastpos = jj_scanpos = p . first ; switch ( i ) { case 0 : jj_3_1 ( ) ; break ; } } p = p . next ; } while ( p != null ) ; } jj_rescan = false ; } final private void jj_save ( int index , int xla ) { JJCalls p = jj_2_rtns [ index ] ; while ( p . gen > jj_gen ) { if ( p . next == null ) { p = p . next = new JJCalls ( ) ; break ; } p = p . next ; } p . gen = jj_gen + xla - jj_la ; p . first = token ; p . arg = xla ; } static final class JJCalls { int gen ; Token first ; int arg ; JJCalls next ; } } 	1	['52', '1', '1', '30', '129', '888', '1', '29', '32', '0.873873874', '2830', '0.648648649', '11', '0', '0.132730015', '0', '0', '52.71153846', '23', '2.6731', '7']
package org . apache . lucene . analysis ; import java . io . Reader ; public final class SimpleAnalyzer extends Analyzer { public TokenStream tokenStream ( String fieldName , Reader reader ) { return new LowerCaseTokenizer ( reader ) ; } } 	0	['2', '2', '0', '4', '4', '1', '1', '3', '2', '2', '10', '0', '0', '0.666666667', '0.666666667', '0', '0', '4', '1', '0.5', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . Set ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermDocs ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; public class TermQuery extends Query { private Term term ; private class TermWeight implements Weight { private Similarity similarity ; private float value ; private float idf ; private float queryNorm ; private float queryWeight ; public TermWeight ( Searcher searcher ) throws IOException { this . similarity = getSimilarity ( searcher ) ; idf = similarity . idf ( term , searcher ) ; } public String toString ( ) { return "weight(" + TermQuery . this + ")" ; } public Query getQuery ( ) { return TermQuery . this ; } public float getValue ( ) { return value ; } public float sumOfSquaredWeights ( ) { queryWeight = idf * getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float queryNorm ) { this . queryNorm = queryNorm ; queryWeight *= queryNorm ; value = queryWeight * idf ; } public Scorer scorer ( IndexReader reader ) throws IOException { TermDocs termDocs = reader . termDocs ( term ) ; if ( termDocs == null ) return null ; return new TermScorer ( this , termDocs , similarity , reader . norms ( term . field ( ) ) ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { Explanation result = new Explanation ( ) ; result . setDescription ( "weight(" + getQuery ( ) + " in " + doc + "), product of:" ) ; Explanation idfExpl = new Explanation ( idf , "idf(docFreq=" + reader . docFreq ( term ) + ")" ) ; Explanation queryExpl = new Explanation ( ) ; queryExpl . setDescription ( "queryWeight(" + getQuery ( ) + "), product of:" ) ; Explanation boostExpl = new Explanation ( getBoost ( ) , "boost" ) ; if ( getBoost ( ) != 1.0f ) queryExpl . addDetail ( boostExpl ) ; queryExpl . addDetail ( idfExpl ) ; Explanation queryNormExpl = new Explanation ( queryNorm , "queryNorm" ) ; queryExpl . addDetail ( queryNormExpl ) ; queryExpl . setValue ( boostExpl . getValue ( ) * idfExpl . getValue ( ) * queryNormExpl . getValue ( ) ) ; result . addDetail ( queryExpl ) ; String field = term . field ( ) ; Explanation fieldExpl = new Explanation ( ) ; fieldExpl . setDescription ( "fieldWeight(" + term + " in " + doc + "), product of:" ) ; Explanation tfExpl = scorer ( reader ) . explain ( doc ) ; fieldExpl . addDetail ( tfExpl ) ; fieldExpl . addDetail ( idfExpl ) ; Explanation fieldNormExpl = new Explanation ( ) ; byte [ ] fieldNorms = reader . norms ( field ) ; float fieldNorm = fieldNorms != null ? Similarity . decodeNorm ( fieldNorms [ doc ] ) : 0.0f ; fieldNormExpl . setValue ( fieldNorm ) ; fieldNormExpl . setDescription ( "fieldNorm(field=" + field + ", doc=" + doc + ")" ) ; fieldExpl . addDetail ( fieldNormExpl ) ; fieldExpl . setValue ( tfExpl . getValue ( ) * idfExpl . getValue ( ) * fieldNormExpl . getValue ( ) ) ; result . addDetail ( fieldExpl ) ; result . setValue ( queryExpl . getValue ( ) * fieldExpl . getValue ( ) ) ; if ( queryExpl . getValue ( ) == 1.0f ) return fieldExpl ; return result ; } } public TermQuery ( Term t ) { term = t ; } public Term getTerm ( ) { return term ; } protected Weight createWeight ( Searcher searcher ) throws IOException { return new TermWeight ( searcher ) ; } public void extractTerms ( Set terms ) { terms . add ( getTerm ( ) ) ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! term . field ( ) . equals ( field ) ) { buffer . append ( term . field ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( term . text ( ) ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof TermQuery ) ) return false ; TermQuery other = ( TermQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && this . term . equals ( other . term ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ term . hashCode ( ) ; } } 	1	['8', '2', '0', '14', '22', '0', '9', '6', '6', '0.142857143', '100', '1', '1', '0.631578947', '0.232142857', '2', '2', '11.375', '4', '1.375', '1']
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; import java . text . Collator ; import java . util . Locale ; class FieldDocSortedHitQueue extends PriorityQueue { volatile SortField [ ] fields ; volatile Collator [ ] collators ; FieldDocSortedHitQueue ( SortField [ ] fields , int size ) { this . fields = fields ; this . collators = hasCollators ( fields ) ; initialize ( size ) ; } synchronized void setFields ( SortField [ ] fields ) { if ( this . fields == null ) { this . fields = fields ; this . collators = hasCollators ( fields ) ; } } SortField [ ] getFields ( ) { return fields ; } private Collator [ ] hasCollators ( final SortField [ ] fields ) { if ( fields == null ) return null ; Collator [ ] ret = new Collator [ fields . length ] ; for ( int i = 0 ; i < fields . length ; ++ i ) { Locale locale = fields [ i ] . getLocale ( ) ; if ( locale != null ) ret [ i ] = Collator . getInstance ( locale ) ; } return ret ; } protected final boolean lessThan ( final Object a , final Object b ) { final FieldDoc docA = ( FieldDoc ) a ; final FieldDoc docB = ( FieldDoc ) b ; final int n = fields . length ; int c = 0 ; for ( int i = 0 ; i < n && c == 0 ; ++ i ) { final int type = fields [ i ] . getType ( ) ; switch ( type ) { case SortField . SCORE : float r1 = ( ( Float ) docA . fields [ i ] ) . floatValue ( ) ; float r2 = ( ( Float ) docB . fields [ i ] ) . floatValue ( ) ; if ( r1 > r2 ) c = - 1 ; if ( r1 < r2 ) c = 1 ; break ; case SortField . DOC : case SortField . INT : int i1 = ( ( Integer ) docA . fields [ i ] ) . intValue ( ) ; int i2 = ( ( Integer ) docB . fields [ i ] ) . intValue ( ) ; if ( i1 < i2 ) c = - 1 ; if ( i1 > i2 ) c = 1 ; break ; case SortField . STRING : String s1 = ( String ) docA . fields [ i ] ; String s2 = ( String ) docB . fields [ i ] ; if ( s1 == null ) c = ( s2 == null ) ? 0 : - 1 ; else if ( s2 == null ) c = 1 ; else if ( fields [ i ] . getLocale ( ) == null ) { c = s1 . compareTo ( s2 ) ; } else { c = collators [ i ] . compare ( s1 , s2 ) ; } break ; case SortField . FLOAT : float f1 = ( ( Float ) docA . fields [ i ] ) . floatValue ( ) ; float f2 = ( ( Float ) docB . fields [ i ] ) . floatValue ( ) ; if ( f1 < f2 ) c = - 1 ; if ( f1 > f2 ) c = 1 ; break ; case SortField . CUSTOM : c = docA . fields [ i ] . compareTo ( docB . fields [ i ] ) ; break ; case SortField . AUTO : throw new RuntimeException ( "FieldDocSortedHitQueue cannot use an AUTO SortField" ) ; default : throw new RuntimeException ( "invalid SortField type: " + type ) ; } if ( fields [ i ] . getReverse ( ) ) { c = - c ; } } if ( c == 0 ) return docA . doc > docB . doc ; return c > 0 ; } } 	0	['5', '2', '0', '6', '21', '0', '3', '3', '0', '0.375', '274', '0', '1', '0.733333333', '0.5', '1', '3', '53.4', '18', '5', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class Directory { public abstract String [ ] list ( ) throws IOException ; public abstract boolean fileExists ( String name ) throws IOException ; public abstract long fileModified ( String name ) throws IOException ; public abstract void touchFile ( String name ) throws IOException ; public abstract void deleteFile ( String name ) throws IOException ; public abstract void renameFile ( String from , String to ) throws IOException ; public abstract long fileLength ( String name ) throws IOException ; public abstract IndexOutput createOutput ( String name ) throws IOException ; public abstract IndexInput openInput ( String name ) throws IOException ; public abstract Lock makeLock ( String name ) ; public abstract void close ( ) throws IOException ; } 	1	['12', '1', '3', '35', '13', '66', '32', '3', '12', '2', '15', '0', '0', '0', '0.875', '0', '0', '0.25', '1', '0.9167', '6']
package org . apache . lucene . search ; public abstract class HitCollector { public abstract void collect ( int doc , float score ) ; } 	0	['2', '1', '6', '21', '3', '1', '21', '0', '2', '2', '5', '0', '0', '0', '0.666666667', '0', '0', '1.5', '1', '0.5', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . List ; import java . util . Collection ; import java . util . ArrayList ; import java . util . Iterator ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . PriorityQueue ; import org . apache . lucene . util . ToStringUtils ; import org . apache . lucene . search . Query ; public class SpanOrQuery extends SpanQuery { private List clauses ; private String field ; public SpanOrQuery ( SpanQuery [ ] clauses ) { this . clauses = new ArrayList ( clauses . length ) ; for ( int i = 0 ; i < clauses . length ; i ++ ) { SpanQuery clause = clauses [ i ] ; if ( i == 0 ) { field = clause . getField ( ) ; } else if ( ! clause . getField ( ) . equals ( field ) ) { throw new IllegalArgumentException ( "Clauses must have same field." ) ; } this . clauses . add ( clause ) ; } } public SpanQuery [ ] getClauses ( ) { return ( SpanQuery [ ] ) clauses . toArray ( new SpanQuery [ clauses . size ( ) ] ) ; } public String getField ( ) { return field ; } public Collection getTerms ( ) { Collection terms = new ArrayList ( ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; terms . addAll ( clause . getTerms ( ) ) ; } return terms ; } public void extractTerms ( Set terms ) { Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; clause . extractTerms ( terms ) ; } } public Query rewrite ( IndexReader reader ) throws IOException { SpanOrQuery clone = null ; for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { SpanQuery c = ( SpanQuery ) clauses . get ( i ) ; SpanQuery query = ( SpanQuery ) c . rewrite ( reader ) ; if ( query != c ) { if ( clone == null ) clone = ( SpanOrQuery ) this . clone ( ) ; clone . clauses . set ( i , query ) ; } } if ( clone != null ) { return clone ; } else { return this ; } } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "spanOr([" ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; buffer . append ( clause . toString ( field ) ) ; if ( i . hasNext ( ) ) { buffer . append ( ", " ) ; } } buffer . append ( "])" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( o == null || getClass ( ) != o . getClass ( ) ) return false ; final SpanOrQuery that = ( SpanOrQuery ) o ; if ( ! clauses . equals ( that . clauses ) ) return false ; if ( ! field . equals ( that . field ) ) return false ; return getBoost ( ) == that . getBoost ( ) ; } public int hashCode ( ) { int h = clauses . hashCode ( ) ; h ^= ( h << 10 ) | ( h > > > 23 ) ; h ^= Float . floatToRawIntBits ( getBoost ( ) ) ; return h ; } private class SpanQueue extends PriorityQueue { public SpanQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object o1 , Object o2 ) { Spans spans1 = ( Spans ) o1 ; Spans spans2 = ( Spans ) o2 ; if ( spans1 . doc ( ) == spans2 . doc ( ) ) { if ( spans1 . start ( ) == spans2 . start ( ) ) { return spans1 . end ( ) < spans2 . end ( ) ; } else { return spans1 . start ( ) < spans2 . start ( ) ; } } else { return spans1 . doc ( ) < spans2 . doc ( ) ; } } } public Spans getSpans ( final IndexReader reader ) throws IOException { if ( clauses . size ( ) == 1 ) return ( ( SpanQuery ) clauses . get ( 0 ) ) . getSpans ( reader ) ; return new Spans ( ) { private List all = new ArrayList ( clauses . size ( ) ) ; private SpanQueue queue = new SpanQueue ( clauses . size ( ) ) ; { Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { all . add ( ( ( SpanQuery ) i . next ( ) ) . getSpans ( reader ) ) ; } } private boolean firstTime = true ; public boolean next ( ) throws IOException { if ( firstTime ) { for ( int i = 0 ; i < all . size ( ) ; i ++ ) { Spans spans = ( Spans ) all . get ( i ) ; if ( spans . next ( ) ) { queue . put ( spans ) ; } else { all . remove ( i -- ) ; } } firstTime = false ; return queue . size ( ) != 0 ; } if ( queue . size ( ) == 0 ) { return false ; } if ( top ( ) . next ( ) ) { queue . adjustTop ( ) ; return true ; } all . remove ( queue . pop ( ) ) ; return queue . size ( ) != 0 ; } private Spans top ( ) { return ( Spans ) queue . top ( ) ; } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { for ( int i = 0 ; i < all . size ( ) ; i ++ ) { Spans spans = ( Spans ) all . get ( i ) ; if ( spans . skipTo ( target ) ) { queue . put ( spans ) ; } else { all . remove ( i -- ) ; } } firstTime = false ; } else { while ( queue . size ( ) != 0 && top ( ) . doc ( ) < target ) { if ( top ( ) . skipTo ( target ) ) { queue . adjustTop ( ) ; } else { all . remove ( queue . pop ( ) ) ; } } } return queue . size ( ) != 0 ; } public int doc ( ) { return top ( ) . doc ( ) ; } public int start ( ) { return top ( ) . start ( ) ; } public int end ( ) { return top ( ) . end ( ) ; } public String toString ( ) { return "spans(" + SpanOrQuery . this + ")@" + ( firstTime ? "START" : ( queue . size ( ) > 0 ? ( doc ( ) + ":" + start ( ) + "-" + end ( ) ) : "END" ) ) ; } } ; } } 	1	['11', '3', '0', '8', '42', '0', '3', '6', '10', '0.45', '285', '1', '0', '0.615384615', '0.220779221', '2', '2', '24.72727273', '7', '1.7273', '1']
package org . apache . lucene . analysis ; import java . io . Reader ; public class WhitespaceTokenizer extends CharTokenizer { public WhitespaceTokenizer ( Reader in ) { super ( in ) ; } protected boolean isTokenChar ( char c ) { return ! Character . isWhitespace ( c ) ; } } 	0	['2', '4', '0', '2', '4', '1', '1', '1', '1', '2', '13', '0', '0', '0.857142857', '0.666666667', '1', '1', '5.5', '2', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import java . io . IOException ; public class WildcardQuery extends MultiTermQuery { public WildcardQuery ( Term term ) { super ( term ) ; } protected FilteredTermEnum getEnum ( IndexReader reader ) throws IOException { return new WildcardTermEnum ( reader , getTerm ( ) ) ; } public boolean equals ( Object o ) { if ( o instanceof WildcardQuery ) return super . equals ( o ) ; return false ; } } 	1	['3', '3', '0', '6', '7', '3', '1', '5', '2', '2', '23', '0', '0', '0.9', '0.5', '1', '1', '6.666666667', '2', '1', '1']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Query ; import org . apache . lucene . util . ToStringUtils ; public class SpanNotQuery extends SpanQuery { private SpanQuery include ; private SpanQuery exclude ; public SpanNotQuery ( SpanQuery include , SpanQuery exclude ) { this . include = include ; this . exclude = exclude ; if ( ! include . getField ( ) . equals ( exclude . getField ( ) ) ) throw new IllegalArgumentException ( "Clauses must have same field." ) ; } public SpanQuery getInclude ( ) { return include ; } public SpanQuery getExclude ( ) { return exclude ; } public String getField ( ) { return include . getField ( ) ; } public Collection getTerms ( ) { return include . getTerms ( ) ; } public void extractTerms ( Set terms ) { include . extractTerms ( terms ) ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "spanNot(" ) ; buffer . append ( include . toString ( field ) ) ; buffer . append ( ", " ) ; buffer . append ( exclude . toString ( field ) ) ; buffer . append ( ")" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public Spans getSpans ( final IndexReader reader ) throws IOException { return new Spans ( ) { private Spans includeSpans = include . getSpans ( reader ) ; private boolean moreInclude = true ; private Spans excludeSpans = exclude . getSpans ( reader ) ; private boolean moreExclude = excludeSpans . next ( ) ; public boolean next ( ) throws IOException { if ( moreInclude ) moreInclude = includeSpans . next ( ) ; while ( moreInclude && moreExclude ) { if ( includeSpans . doc ( ) > excludeSpans . doc ( ) ) moreExclude = excludeSpans . skipTo ( includeSpans . doc ( ) ) ; while ( moreExclude && includeSpans . doc ( ) == excludeSpans . doc ( ) && excludeSpans . end ( ) <= includeSpans . start ( ) ) { moreExclude = excludeSpans . next ( ) ; } if ( ! moreExclude || includeSpans . doc ( ) != excludeSpans . doc ( ) || includeSpans . end ( ) <= excludeSpans . start ( ) ) break ; moreInclude = includeSpans . next ( ) ; } return moreInclude ; } public boolean skipTo ( int target ) throws IOException { if ( moreInclude ) moreInclude = includeSpans . skipTo ( target ) ; if ( ! moreInclude ) return false ; if ( moreExclude && includeSpans . doc ( ) > excludeSpans . doc ( ) ) moreExclude = excludeSpans . skipTo ( includeSpans . doc ( ) ) ; while ( moreExclude && includeSpans . doc ( ) == excludeSpans . doc ( ) && excludeSpans . end ( ) <= includeSpans . start ( ) ) { moreExclude = excludeSpans . next ( ) ; } if ( ! moreExclude || includeSpans . doc ( ) != excludeSpans . doc ( ) || includeSpans . end ( ) <= excludeSpans . start ( ) ) return true ; return next ( ) ; } public int doc ( ) { return includeSpans . doc ( ) ; } public int start ( ) { return includeSpans . start ( ) ; } public int end ( ) { return includeSpans . end ( ) ; } public String toString ( ) { return "spans(" + SpanNotQuery . this . toString ( ) + ")" ; } } ; } public Query rewrite ( IndexReader reader ) throws IOException { SpanNotQuery clone = null ; SpanQuery rewrittenInclude = ( SpanQuery ) include . rewrite ( reader ) ; if ( rewrittenInclude != include ) { clone = ( SpanNotQuery ) this . clone ( ) ; clone . include = rewrittenInclude ; } SpanQuery rewrittenExclude = ( SpanQuery ) exclude . rewrite ( reader ) ; if ( rewrittenExclude != exclude ) { if ( clone == null ) clone = ( SpanNotQuery ) this . clone ( ) ; clone . exclude = rewrittenExclude ; } if ( clone != null ) { return clone ; } else { return this ; } } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof SpanNotQuery ) ) return false ; SpanNotQuery other = ( SpanNotQuery ) o ; return this . include . equals ( other . include ) && this . exclude . equals ( other . exclude ) && this . getBoost ( ) == other . getBoost ( ) ; } public int hashCode ( ) { int h = include . hashCode ( ) ; h = ( h << 1 ) | ( h > > > 31 ) ; h ^= exclude . hashCode ( ) ; h = ( h << 1 ) | ( h > > > 31 ) ; h ^= Float . floatToRawIntBits ( getBoost ( ) ) ; return h ; } } 	0	['13', '3', '0', '6', '31', '0', '1', '6', '11', '0.375', '218', '1', '2', '0.571428571', '0.208791209', '2', '2', '15.61538462', '6', '1.3077', '0']
package org . apache . lucene . index ; import java . util . Vector ; import java . util . Iterator ; import java . util . Collection ; import java . io . IOException ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . RAMOutputStream ; final class SegmentMerger { private Directory directory ; private String segment ; private int termIndexInterval = IndexWriter . DEFAULT_TERM_INDEX_INTERVAL ; private Vector readers = new Vector ( ) ; private FieldInfos fieldInfos ; SegmentMerger ( Directory dir , String name ) { directory = dir ; segment = name ; } SegmentMerger ( IndexWriter writer , String name ) { directory = writer . getDirectory ( ) ; segment = name ; termIndexInterval = writer . getTermIndexInterval ( ) ; } final void add ( IndexReader reader ) { readers . addElement ( reader ) ; } final IndexReader segmentReader ( int i ) { return ( IndexReader ) readers . elementAt ( i ) ; } final int merge ( ) throws IOException { int value ; value = mergeFields ( ) ; mergeTerms ( ) ; mergeNorms ( ) ; if ( fieldInfos . hasVectors ( ) ) mergeVectors ( ) ; return value ; } final void closeReaders ( ) throws IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader reader = ( IndexReader ) readers . elementAt ( i ) ; reader . close ( ) ; } } final Vector createCompoundFile ( String fileName ) throws IOException { CompoundFileWriter cfsWriter = new CompoundFileWriter ( directory , fileName ) ; Vector files = new Vector ( IndexFileNames . COMPOUND_EXTENSIONS . length + fieldInfos . size ( ) ) ; for ( int i = 0 ; i < IndexFileNames . COMPOUND_EXTENSIONS . length ; i ++ ) { files . add ( segment + "." + IndexFileNames . COMPOUND_EXTENSIONS [ i ] ) ; } for ( int i = 0 ; i < fieldInfos . size ( ) ; i ++ ) { FieldInfo fi = fieldInfos . fieldInfo ( i ) ; if ( fi . isIndexed && ! fi . omitNorms ) { files . add ( segment + ".f" + i ) ; } } if ( fieldInfos . hasVectors ( ) ) { for ( int i = 0 ; i < IndexFileNames . VECTOR_EXTENSIONS . length ; i ++ ) { files . add ( segment + "." + IndexFileNames . VECTOR_EXTENSIONS [ i ] ) ; } } Iterator it = files . iterator ( ) ; while ( it . hasNext ( ) ) { cfsWriter . addFile ( ( String ) it . next ( ) ) ; } cfsWriter . close ( ) ; return files ; } private void addIndexed ( IndexReader reader , FieldInfos fieldInfos , Collection names , boolean storeTermVectors , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector ) throws IOException { Iterator i = names . iterator ( ) ; while ( i . hasNext ( ) ) { String field = ( String ) i . next ( ) ; fieldInfos . add ( field , true , storeTermVectors , storePositionWithTermVector , storeOffsetWithTermVector , ! reader . hasNorms ( field ) ) ; } } private final int mergeFields ( ) throws IOException { fieldInfos = new FieldInfos ( ) ; int docCount = 0 ; for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader reader = ( IndexReader ) readers . elementAt ( i ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . TERMVECTOR_WITH_POSITION_OFFSET ) , true , true , true ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . TERMVECTOR_WITH_POSITION ) , true , true , false ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . TERMVECTOR_WITH_OFFSET ) , true , false , true ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . TERMVECTOR ) , true , false , false ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . INDEXED ) , false , false , false ) ; fieldInfos . add ( reader . getFieldNames ( IndexReader . FieldOption . UNINDEXED ) , false ) ; } fieldInfos . write ( directory , segment + ".fnm" ) ; FieldsWriter fieldsWriter = new FieldsWriter ( directory , segment , fieldInfos ) ; try { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader reader = ( IndexReader ) readers . elementAt ( i ) ; int maxDoc = reader . maxDoc ( ) ; for ( int j = 0 ; j < maxDoc ; j ++ ) if ( ! reader . isDeleted ( j ) ) { fieldsWriter . addDocument ( reader . document ( j ) ) ; docCount ++ ; } } } finally { fieldsWriter . close ( ) ; } return docCount ; } private final void mergeVectors ( ) throws IOException { TermVectorsWriter termVectorsWriter = new TermVectorsWriter ( directory , segment , fieldInfos ) ; try { for ( int r = 0 ; r < readers . size ( ) ; r ++ ) { IndexReader reader = ( IndexReader ) readers . elementAt ( r ) ; int maxDoc = reader . maxDoc ( ) ; for ( int docNum = 0 ; docNum < maxDoc ; docNum ++ ) { if ( reader . isDeleted ( docNum ) ) continue ; termVectorsWriter . addAllDocVectors ( reader . getTermFreqVectors ( docNum ) ) ; } } } finally { termVectorsWriter . close ( ) ; } } private IndexOutput freqOutput = null ; private IndexOutput proxOutput = null ; private TermInfosWriter termInfosWriter = null ; private int skipInterval ; private SegmentMergeQueue queue = null ; private final void mergeTerms ( ) throws IOException { try { freqOutput = directory . createOutput ( segment + ".frq" ) ; proxOutput = directory . createOutput ( segment + ".prx" ) ; termInfosWriter = new TermInfosWriter ( directory , segment , fieldInfos , termIndexInterval ) ; skipInterval = termInfosWriter . skipInterval ; queue = new SegmentMergeQueue ( readers . size ( ) ) ; mergeTermInfos ( ) ; } finally { if ( freqOutput != null ) freqOutput . close ( ) ; if ( proxOutput != null ) proxOutput . close ( ) ; if ( termInfosWriter != null ) termInfosWriter . close ( ) ; if ( queue != null ) queue . close ( ) ; } } private final void mergeTermInfos ( ) throws IOException { int base = 0 ; for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader reader = ( IndexReader ) readers . elementAt ( i ) ; TermEnum termEnum = reader . terms ( ) ; SegmentMergeInfo smi = new SegmentMergeInfo ( base , termEnum , reader ) ; base += reader . numDocs ( ) ; if ( smi . next ( ) ) queue . put ( smi ) ; else smi . close ( ) ; } SegmentMergeInfo [ ] match = new SegmentMergeInfo [ readers . size ( ) ] ; while ( queue . size ( ) > 0 ) { int matchSize = 0 ; match [ matchSize ++ ] = ( SegmentMergeInfo ) queue . pop ( ) ; Term term = match [ 0 ] . term ; SegmentMergeInfo top = ( SegmentMergeInfo ) queue . top ( ) ; while ( top != null && term . compareTo ( top . term ) == 0 ) { match [ matchSize ++ ] = ( SegmentMergeInfo ) queue . pop ( ) ; top = ( SegmentMergeInfo ) queue . top ( ) ; } mergeTermInfo ( match , matchSize ) ; while ( matchSize > 0 ) { SegmentMergeInfo smi = match [ -- matchSize ] ; if ( smi . next ( ) ) queue . put ( smi ) ; else smi . close ( ) ; } } } private final TermInfo termInfo = new TermInfo ( ) ; private final void mergeTermInfo ( SegmentMergeInfo [ ] smis , int n ) throws IOException { long freqPointer = freqOutput . getFilePointer ( ) ; long proxPointer = proxOutput . getFilePointer ( ) ; int df = appendPostings ( smis , n ) ; long skipPointer = writeSkip ( ) ; if ( df > 0 ) { termInfo . set ( df , freqPointer , proxPointer , ( int ) ( skipPointer - freqPointer ) ) ; termInfosWriter . add ( smis [ 0 ] . term , termInfo ) ; } } private final int appendPostings ( SegmentMergeInfo [ ] smis , int n ) throws IOException { int lastDoc = 0 ; int df = 0 ; resetSkip ( ) ; for ( int i = 0 ; i < n ; i ++ ) { SegmentMergeInfo smi = smis [ i ] ; TermPositions postings = smi . getPositions ( ) ; int base = smi . base ; int [ ] docMap = smi . getDocMap ( ) ; postings . seek ( smi . termEnum ) ; while ( postings . next ( ) ) { int doc = postings . doc ( ) ; if ( docMap != null ) doc = docMap [ doc ] ; doc += base ; if ( doc < lastDoc ) throw new IllegalStateException ( "docs out of order (" + doc + " < " + lastDoc + " )" ) ; df ++ ; if ( ( df % skipInterval ) == 0 ) { bufferSkip ( lastDoc ) ; } int docCode = ( doc - lastDoc ) << 1 ; lastDoc = doc ; int freq = postings . freq ( ) ; if ( freq == 1 ) { freqOutput . writeVInt ( docCode | 1 ) ; } else { freqOutput . writeVInt ( docCode ) ; freqOutput . writeVInt ( freq ) ; } int lastPosition = 0 ; for ( int j = 0 ; j < freq ; j ++ ) { int position = postings . nextPosition ( ) ; proxOutput . writeVInt ( position - lastPosition ) ; lastPosition = position ; } } } return df ; } private RAMOutputStream skipBuffer = new RAMOutputStream ( ) ; private int lastSkipDoc ; private long lastSkipFreqPointer ; private long lastSkipProxPointer ; private void resetSkip ( ) { skipBuffer . reset ( ) ; lastSkipDoc = 0 ; lastSkipFreqPointer = freqOutput . getFilePointer ( ) ; lastSkipProxPointer = proxOutput . getFilePointer ( ) ; } private void bufferSkip ( int doc ) throws IOException { long freqPointer = freqOutput . getFilePointer ( ) ; long proxPointer = proxOutput . getFilePointer ( ) ; skipBuffer . writeVInt ( doc - lastSkipDoc ) ; skipBuffer . writeVInt ( ( int ) ( freqPointer - lastSkipFreqPointer ) ) ; skipBuffer . writeVInt ( ( int ) ( proxPointer - lastSkipProxPointer ) ) ; lastSkipDoc = doc ; lastSkipFreqPointer = freqPointer ; lastSkipProxPointer = proxPointer ; } private long writeSkip ( ) throws IOException { long skipPointer = freqOutput . getFilePointer ( ) ; skipBuffer . writeTo ( freqOutput ) ; return skipPointer ; } private void mergeNorms ( ) throws IOException { for ( int i = 0 ; i < fieldInfos . size ( ) ; i ++ ) { FieldInfo fi = fieldInfos . fieldInfo ( i ) ; if ( fi . isIndexed && ! fi . omitNorms ) { IndexOutput output = directory . createOutput ( segment + ".f" + i ) ; try { for ( int j = 0 ; j < readers . size ( ) ; j ++ ) { IndexReader reader = ( IndexReader ) readers . elementAt ( j ) ; int maxDoc = reader . maxDoc ( ) ; byte [ ] input = new byte [ maxDoc ] ; reader . norms ( fi . name , input , 0 ) ; for ( int k = 0 ; k < maxDoc ; k ++ ) { if ( ! reader . isDeleted ( k ) ) { output . writeByte ( input [ k ] ) ; } } } } finally { output . close ( ) ; } } } } } 	1	['18', '1', '0', '21', '93', '0', '1', '21', '0', '0.701960784', '1039', '1', '8', '0', '0.188888889', '0', '0', '55.88888889', '1', '0.8889', '8']
package org . apache . lucene . analysis . standard ; import org . apache . lucene . analysis . * ; public final class StandardFilter extends TokenFilter implements StandardTokenizerConstants { public StandardFilter ( TokenStream in ) { super ( in ) ; } private static final String APOSTROPHE_TYPE = tokenImage [ APOSTROPHE ] ; private static final String ACRONYM_TYPE = tokenImage [ ACRONYM ] ; public final org . apache . lucene . analysis . Token next ( ) throws java . io . IOException { org . apache . lucene . analysis . Token t = input . next ( ) ; if ( t == null ) return null ; String text = t . termText ( ) ; String type = t . type ( ) ; if ( type == APOSTROPHE_TYPE && ( text . endsWith ( "'s" ) || text . endsWith ( "'S" ) ) ) { return new org . apache . lucene . analysis . Token ( text . substring ( 0 , text . length ( ) - 2 ) , t . startOffset ( ) , t . endOffset ( ) , type ) ; } else if ( type == ACRONYM_TYPE ) { StringBuffer trimmed = new StringBuffer ( ) ; for ( int i = 0 ; i < text . length ( ) ; i ++ ) { char c = text . charAt ( i ) ; if ( c != '.' ) trimmed . append ( c ) ; } return new org . apache . lucene . analysis . Token ( trimmed . toString ( ) , t . startOffset ( ) , t . endOffset ( ) , type ) ; } else { return t ; } } } 	0	['3', '3', '0', '5', '17', '1', '1', '4', '2', '0.5', '98', '1', '0', '0.75', '0.75', '0', '0', '31', '1', '0.3333', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . ArrayList ; class DisjunctionMaxScorer extends Scorer { private ArrayList subScorers = new ArrayList ( ) ; private float tieBreakerMultiplier ; private boolean more = false ; private boolean firstTime = true ; public DisjunctionMaxScorer ( float tieBreakerMultiplier , Similarity similarity ) { super ( similarity ) ; this . tieBreakerMultiplier = tieBreakerMultiplier ; } public void add ( Scorer scorer ) throws IOException { if ( scorer . next ( ) ) { subScorers . add ( scorer ) ; more = true ; } } public boolean next ( ) throws IOException { if ( ! more ) return false ; if ( firstTime ) { heapify ( ) ; firstTime = false ; return true ; } int lastdoc = ( ( Scorer ) subScorers . get ( 0 ) ) . doc ( ) ; do { if ( ( ( Scorer ) subScorers . get ( 0 ) ) . next ( ) ) heapAdjust ( 0 ) ; else { heapRemoveRoot ( ) ; if ( subScorers . isEmpty ( ) ) return ( more = false ) ; } } while ( ( ( Scorer ) subScorers . get ( 0 ) ) . doc ( ) == lastdoc ) ; return true ; } public int doc ( ) { return ( ( Scorer ) subScorers . get ( 0 ) ) . doc ( ) ; } public float score ( ) throws IOException { int doc = ( ( Scorer ) subScorers . get ( 0 ) ) . doc ( ) ; float [ ] sum = { ( ( Scorer ) subScorers . get ( 0 ) ) . score ( ) } , max = { sum [ 0 ] } ; int size = subScorers . size ( ) ; scoreAll ( 1 , size , doc , sum , max ) ; scoreAll ( 2 , size , doc , sum , max ) ; return max [ 0 ] + ( sum [ 0 ] - max [ 0 ] ) * tieBreakerMultiplier ; } private void scoreAll ( int root , int size , int doc , float [ ] sum , float [ ] max ) throws IOException { if ( root < size && ( ( Scorer ) subScorers . get ( root ) ) . doc ( ) == doc ) { float sub = ( ( Scorer ) subScorers . get ( root ) ) . score ( ) ; sum [ 0 ] += sub ; max [ 0 ] = Math . max ( max [ 0 ] , sub ) ; scoreAll ( ( root << 1 ) + 1 , size , doc , sum , max ) ; scoreAll ( ( root << 1 ) + 2 , size , doc , sum , max ) ; } } public boolean skipTo ( int target ) throws IOException { while ( subScorers . size ( ) > 0 && ( ( Scorer ) subScorers . get ( 0 ) ) . doc ( ) < target ) { if ( ( ( Scorer ) subScorers . get ( 0 ) ) . skipTo ( target ) ) heapAdjust ( 0 ) ; else heapRemoveRoot ( ) ; } if ( ( subScorers . size ( ) == 0 ) ) return ( more = false ) ; return true ; } public Explanation explain ( int doc ) throws IOException { throw new UnsupportedOperationException ( ) ; } private void heapify ( ) { int size = subScorers . size ( ) ; for ( int i = ( size > > 1 ) - 1 ; i >= 0 ; i -- ) heapAdjust ( i ) ; } private void heapAdjust ( int root ) { Scorer scorer = ( Scorer ) subScorers . get ( root ) ; int doc = scorer . doc ( ) ; int i = root , size = subScorers . size ( ) ; while ( i <= ( size > > 1 ) - 1 ) { int lchild = ( i << 1 ) + 1 ; Scorer lscorer = ( Scorer ) subScorers . get ( lchild ) ; int ldoc = lscorer . doc ( ) ; int rdoc = Integer . MAX_VALUE , rchild = ( i << 1 ) + 2 ; Scorer rscorer = null ; if ( rchild < size ) { rscorer = ( Scorer ) subScorers . get ( rchild ) ; rdoc = rscorer . doc ( ) ; } if ( ldoc < doc ) { if ( rdoc < ldoc ) { subScorers . set ( i , rscorer ) ; subScorers . set ( rchild , scorer ) ; i = rchild ; } else { subScorers . set ( i , lscorer ) ; subScorers . set ( lchild , scorer ) ; i = lchild ; } } else if ( rdoc < doc ) { subScorers . set ( i , rscorer ) ; subScorers . set ( rchild , scorer ) ; i = rchild ; } else return ; } } private void heapRemoveRoot ( ) { int size = subScorers . size ( ) ; if ( size == 1 ) subScorers . remove ( 0 ) ; else { subScorers . set ( 0 , subScorers . get ( size - 1 ) ) ; subScorers . remove ( size - 1 ) ; heapAdjust ( 0 ) ; } } } 	1	['11', '2', '0', '4', '25', '0', '1', '3', '7', '0.65', '434', '1', '0', '0.444444444', '0.287878788', '1', '3', '38.09090909', '6', '1.5455', '2']
package org . apache . lucene . analysis ; import java . io . IOException ; import java . io . Reader ; public abstract class CharTokenizer extends Tokenizer { public CharTokenizer ( Reader input ) { super ( input ) ; } private int offset = 0 , bufferIndex = 0 , dataLen = 0 ; private static final int MAX_WORD_LEN = 255 ; private static final int IO_BUFFER_SIZE = 1024 ; private final char [ ] buffer = new char [ MAX_WORD_LEN ] ; private final char [ ] ioBuffer = new char [ IO_BUFFER_SIZE ] ; protected abstract boolean isTokenChar ( char c ) ; protected char normalize ( char c ) { return c ; } public final Token next ( ) throws IOException { int length = 0 ; int start = offset ; while ( true ) { final char c ; offset ++ ; if ( bufferIndex >= dataLen ) { dataLen = input . read ( ioBuffer ) ; bufferIndex = 0 ; } ; if ( dataLen == - 1 ) { if ( length > 0 ) break ; else return null ; } else c = ioBuffer [ bufferIndex ++ ] ; if ( isTokenChar ( c ) ) { if ( length == 0 ) start = offset - 1 ; buffer [ length ++ ] = normalize ( c ) ; if ( length == MAX_WORD_LEN ) break ; } else if ( length > 0 ) break ; } return new Token ( new String ( buffer , 0 , length ) , start , start + length ) ; } } 	0	['4', '3', '2', '4', '8', '4', '2', '2', '2', '0.857142857', '122', '1', '0', '0.5', '0.583333333', '0', '0', '27.75', '1', '0.75', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . util . BitSet ; import java . util . WeakHashMap ; import java . util . Map ; import java . io . IOException ; public class CachingWrapperFilter extends Filter { private Filter filter ; private transient Map cache ; public CachingWrapperFilter ( Filter filter ) { this . filter = filter ; } public BitSet bits ( IndexReader reader ) throws IOException { if ( cache == null ) { cache = new WeakHashMap ( ) ; } synchronized ( cache ) { BitSet cached = ( BitSet ) cache . get ( reader ) ; if ( cached != null ) { return cached ; } } final BitSet bits = filter . bits ( reader ) ; synchronized ( cache ) { cache . put ( reader , bits ) ; } return bits ; } public String toString ( ) { return "CachingWrapperFilter(" + filter + ")" ; } public boolean equals ( Object o ) { if ( ! ( o instanceof CachingWrapperFilter ) ) return false ; return this . filter . equals ( ( ( CachingWrapperFilter ) o ) . filter ) ; } public int hashCode ( ) { return filter . hashCode ( ) ^ 0x1117BF25 ; } } 	1	['5', '2', '0', '2', '16', '0', '0', '2', '5', '0.5', '102', '1', '1', '0.2', '0.4', '1', '1', '19', '2', '1', '3']
package org . apache . lucene . document ; import java . text . ParseException ; import java . text . SimpleDateFormat ; import java . util . Calendar ; import java . util . Date ; import java . util . TimeZone ; public class DateTools { private final static TimeZone GMT = TimeZone . getTimeZone ( "GMT" ) ; private DateTools ( ) { } public static String dateToString ( Date date , Resolution resolution ) { return timeToString ( date . getTime ( ) , resolution ) ; } public static String timeToString ( long time , Resolution resolution ) { Calendar cal = Calendar . getInstance ( GMT ) ; cal . setTime ( new Date ( round ( time , resolution ) ) ) ; SimpleDateFormat sdf = new SimpleDateFormat ( ) ; sdf . setTimeZone ( GMT ) ; String pattern = null ; if ( resolution == Resolution . YEAR ) { pattern = "yyyy" ; } else if ( resolution == Resolution . MONTH ) { pattern = "yyyyMM" ; } else if ( resolution == Resolution . DAY ) { pattern = "yyyyMMdd" ; } else if ( resolution == Resolution . HOUR ) { pattern = "yyyyMMddHH" ; } else if ( resolution == Resolution . MINUTE ) { pattern = "yyyyMMddHHmm" ; } else if ( resolution == Resolution . SECOND ) { pattern = "yyyyMMddHHmmss" ; } else if ( resolution == Resolution . MILLISECOND ) { pattern = "yyyyMMddHHmmssSSS" ; } else { throw new IllegalArgumentException ( "unknown resolution " + resolution ) ; } sdf . applyPattern ( pattern ) ; return sdf . format ( cal . getTime ( ) ) ; } public static long stringToTime ( String dateString ) throws ParseException { return stringToDate ( dateString ) . getTime ( ) ; } public static Date stringToDate ( String dateString ) throws ParseException { String pattern = null ; if ( dateString . length ( ) == 4 ) pattern = "yyyy" ; else if ( dateString . length ( ) == 6 ) pattern = "yyyyMM" ; else if ( dateString . length ( ) == 8 ) pattern = "yyyyMMdd" ; else if ( dateString . length ( ) == 10 ) pattern = "yyyyMMddHH" ; else if ( dateString . length ( ) == 12 ) pattern = "yyyyMMddHHmm" ; else if ( dateString . length ( ) == 14 ) pattern = "yyyyMMddHHmmss" ; else if ( dateString . length ( ) == 17 ) pattern = "yyyyMMddHHmmssSSS" ; else throw new ParseException ( "Input is not valid date string: " + dateString , 0 ) ; SimpleDateFormat sdf = new SimpleDateFormat ( pattern ) ; sdf . setTimeZone ( GMT ) ; Date date = sdf . parse ( dateString ) ; return date ; } public static Date round ( Date date , Resolution resolution ) { return new Date ( round ( date . getTime ( ) , resolution ) ) ; } public static long round ( long time , Resolution resolution ) { Calendar cal = Calendar . getInstance ( GMT ) ; cal . setTime ( new Date ( time ) ) ; if ( resolution == Resolution . YEAR ) { cal . set ( Calendar . MONTH , 0 ) ; cal . set ( Calendar . DAY_OF_MONTH , 1 ) ; cal . set ( Calendar . HOUR_OF_DAY , 0 ) ; cal . set ( Calendar . MINUTE , 0 ) ; cal . set ( Calendar . SECOND , 0 ) ; cal . set ( Calendar . MILLISECOND , 0 ) ; } else if ( resolution == Resolution . MONTH ) { cal . set ( Calendar . DAY_OF_MONTH , 1 ) ; cal . set ( Calendar . HOUR_OF_DAY , 0 ) ; cal . set ( Calendar . MINUTE , 0 ) ; cal . set ( Calendar . SECOND , 0 ) ; cal . set ( Calendar . MILLISECOND , 0 ) ; } else if ( resolution == Resolution . DAY ) { cal . set ( Calendar . HOUR_OF_DAY , 0 ) ; cal . set ( Calendar . MINUTE , 0 ) ; cal . set ( Calendar . SECOND , 0 ) ; cal . set ( Calendar . MILLISECOND , 0 ) ; } else if ( resolution == Resolution . HOUR ) { cal . set ( Calendar . MINUTE , 0 ) ; cal . set ( Calendar . SECOND , 0 ) ; cal . set ( Calendar . MILLISECOND , 0 ) ; } else if ( resolution == Resolution . MINUTE ) { cal . set ( Calendar . SECOND , 0 ) ; cal . set ( Calendar . MILLISECOND , 0 ) ; } else if ( resolution == Resolution . SECOND ) { cal . set ( Calendar . MILLISECOND , 0 ) ; } else if ( resolution == Resolution . MILLISECOND ) { } else { throw new IllegalArgumentException ( "unknown resolution " + resolution ) ; } return cal . getTime ( ) . getTime ( ) ; } public static class Resolution { public static final Resolution YEAR = new Resolution ( "year" ) ; public static final Resolution MONTH = new Resolution ( "month" ) ; public static final Resolution DAY = new Resolution ( "day" ) ; public static final Resolution HOUR = new Resolution ( "hour" ) ; public static final Resolution MINUTE = new Resolution ( "minute" ) ; public static final Resolution SECOND = new Resolution ( "second" ) ; public static final Resolution MILLISECOND = new Resolution ( "millisecond" ) ; private String resolution ; private Resolution ( ) { } private Resolution ( String resolution ) { this . resolution = resolution ; } public String toString ( ) { return resolution ; } } } 	0	['8', '1', '0', '1', '29', '16', '0', '1', '6', '0.142857143', '330', '1', '0', '0', '0.314285714', '0', '0', '40.125', '8', '2.5', '0']
package org . apache . lucene . store ; import java . util . Vector ; class RAMFile { Vector buffers = new Vector ( ) ; long length ; long lastModified = System . currentTimeMillis ( ) ; } 	1	['1', '1', '0', '3', '4', '0', '3', '0', '0', '2', '15', '0', '0', '0', '1', '0', '0', '11', '0', '0', '1']
package org . apache . lucene . search ; public interface ScoreDocComparator { static final ScoreDocComparator RELEVANCE = new ScoreDocComparator ( ) { public int compare ( ScoreDoc i , ScoreDoc j ) { if ( i . score > j . score ) return - 1 ; if ( i . score < j . score ) return 1 ; return 0 ; } public Comparable sortValue ( ScoreDoc i ) { return new Float ( i . score ) ; } public int sortType ( ) { return SortField . SCORE ; } } ; static final ScoreDocComparator INDEXORDER = new ScoreDocComparator ( ) { public int compare ( ScoreDoc i , ScoreDoc j ) { if ( i . doc < j . doc ) return - 1 ; if ( i . doc > j . doc ) return 1 ; return 0 ; } public Comparable sortValue ( ScoreDoc i ) { return new Integer ( i . doc ) ; } public int sortType ( ) { return SortField . DOC ; } } ; int compare ( ScoreDoc i , ScoreDoc j ) ; Comparable sortValue ( ScoreDoc i ) ; int sortType ( ) ; } 	0	['4', '1', '0', '11', '6', '6', '10', '3', '3', '1', '15', '0', '2', '0', '0.833333333', '0', '0', '2.25', '1', '0.75', '0']
package org . apache . lucene . index ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . document . Document ; import org . apache . lucene . search . Similarity ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . FSDirectory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . Lock ; import org . apache . lucene . store . RAMDirectory ; import java . io . File ; import java . io . IOException ; import java . io . PrintStream ; import java . util . Vector ; public class IndexWriter { public final static long WRITE_LOCK_TIMEOUT = 1000 ; private long writeLockTimeout = WRITE_LOCK_TIMEOUT ; public final static long COMMIT_LOCK_TIMEOUT = 10000 ; private long commitLockTimeout = COMMIT_LOCK_TIMEOUT ; public static final String WRITE_LOCK_NAME = "write.lock" ; public static final String COMMIT_LOCK_NAME = "commit.lock" ; public final static int DEFAULT_MERGE_FACTOR = 10 ; public final static int DEFAULT_MAX_BUFFERED_DOCS = 10 ; public final static int DEFAULT_MAX_MERGE_DOCS = Integer . MAX_VALUE ; public final static int DEFAULT_MAX_FIELD_LENGTH = 10000 ; public final static int DEFAULT_TERM_INDEX_INTERVAL = 128 ; private Directory directory ; private Analyzer analyzer ; private Similarity similarity = Similarity . getDefault ( ) ; private SegmentInfos segmentInfos = new SegmentInfos ( ) ; private final Directory ramDirectory = new RAMDirectory ( ) ; private Lock writeLock ; private int termIndexInterval = DEFAULT_TERM_INDEX_INTERVAL ; private boolean useCompoundFile = true ; private boolean closeDir ; public boolean getUseCompoundFile ( ) { return useCompoundFile ; } public void setUseCompoundFile ( boolean value ) { useCompoundFile = value ; } public void setSimilarity ( Similarity similarity ) { this . similarity = similarity ; } public Similarity getSimilarity ( ) { return this . similarity ; } public void setTermIndexInterval ( int interval ) { this . termIndexInterval = interval ; } public int getTermIndexInterval ( ) { return termIndexInterval ; } public IndexWriter ( String path , Analyzer a , boolean create ) throws IOException { this ( FSDirectory . getDirectory ( path , create ) , a , create , true ) ; } public IndexWriter ( File path , Analyzer a , boolean create ) throws IOException { this ( FSDirectory . getDirectory ( path , create ) , a , create , true ) ; } public IndexWriter ( Directory d , Analyzer a , boolean create ) throws IOException { this ( d , a , create , false ) ; } private IndexWriter ( Directory d , Analyzer a , final boolean create , boolean closeDir ) throws IOException { this . closeDir = closeDir ; directory = d ; analyzer = a ; Lock writeLock = directory . makeLock ( IndexWriter . WRITE_LOCK_NAME ) ; if ( ! writeLock . obtain ( writeLockTimeout ) ) throw new IOException ( "Index locked for write: " + writeLock ) ; this . writeLock = writeLock ; synchronized ( directory ) { new Lock . With ( directory . makeLock ( IndexWriter . COMMIT_LOCK_NAME ) , commitLockTimeout ) { public Object doBody ( ) throws IOException { if ( create ) segmentInfos . write ( directory ) ; else segmentInfos . read ( directory ) ; return null ; } } . run ( ) ; } } public void setMaxMergeDocs ( int maxMergeDocs ) { this . maxMergeDocs = maxMergeDocs ; } public int getMaxMergeDocs ( ) { return maxMergeDocs ; } public void setMaxFieldLength ( int maxFieldLength ) { this . maxFieldLength = maxFieldLength ; } public int getMaxFieldLength ( ) { return maxFieldLength ; } public void setMaxBufferedDocs ( int maxBufferedDocs ) { if ( maxBufferedDocs < 2 ) throw new IllegalArgumentException ( "maxBufferedDocs must at least be 2" ) ; this . minMergeDocs = maxBufferedDocs ; } public int getMaxBufferedDocs ( ) { return minMergeDocs ; } public void setMergeFactor ( int mergeFactor ) { if ( mergeFactor < 2 ) throw new IllegalArgumentException ( "mergeFactor cannot be less than 2" ) ; this . mergeFactor = mergeFactor ; } public int getMergeFactor ( ) { return mergeFactor ; } public void setInfoStream ( PrintStream infoStream ) { this . infoStream = infoStream ; } public PrintStream getInfoStream ( ) { return infoStream ; } public void setCommitLockTimeout ( long commitLockTimeout ) { this . commitLockTimeout = commitLockTimeout ; } public long getCommitLockTimeout ( ) { return commitLockTimeout ; } public void setWriteLockTimeout ( long writeLockTimeout ) { this . writeLockTimeout = writeLockTimeout ; } public long getWriteLockTimeout ( ) { return writeLockTimeout ; } public synchronized void close ( ) throws IOException { flushRamSegments ( ) ; ramDirectory . close ( ) ; if ( writeLock != null ) { writeLock . release ( ) ; writeLock = null ; } if ( closeDir ) directory . close ( ) ; } protected void finalize ( ) throws IOException { if ( writeLock != null ) { writeLock . release ( ) ; writeLock = null ; } } public Directory getDirectory ( ) { return directory ; } public Analyzer getAnalyzer ( ) { return analyzer ; } public synchronized int docCount ( ) { int count = 0 ; for ( int i = 0 ; i < segmentInfos . size ( ) ; i ++ ) { SegmentInfo si = segmentInfos . info ( i ) ; count += si . docCount ; } return count ; } private int maxFieldLength = DEFAULT_MAX_FIELD_LENGTH ; public void addDocument ( Document doc ) throws IOException { addDocument ( doc , analyzer ) ; } public void addDocument ( Document doc , Analyzer analyzer ) throws IOException { DocumentWriter dw = new DocumentWriter ( ramDirectory , analyzer , this ) ; dw . setInfoStream ( infoStream ) ; String segmentName = newSegmentName ( ) ; dw . addDocument ( segmentName , doc ) ; synchronized ( this ) { segmentInfos . addElement ( new SegmentInfo ( segmentName , 1 , ramDirectory ) ) ; maybeMergeSegments ( ) ; } } final int getSegmentsCounter ( ) { return segmentInfos . counter ; } private final synchronized String newSegmentName ( ) { return "_" + Integer . toString ( segmentInfos . counter ++ , Character . MAX_RADIX ) ; } private int mergeFactor = DEFAULT_MERGE_FACTOR ; private int minMergeDocs = DEFAULT_MAX_BUFFERED_DOCS ; private int maxMergeDocs = DEFAULT_MAX_MERGE_DOCS ; private PrintStream infoStream = null ; public synchronized void optimize ( ) throws IOException { flushRamSegments ( ) ; while ( segmentInfos . size ( ) > 1 || ( segmentInfos . size ( ) == 1 && ( SegmentReader . hasDeletions ( segmentInfos . info ( 0 ) ) || segmentInfos . info ( 0 ) . dir != directory || ( useCompoundFile && ( ! SegmentReader . usesCompoundFile ( segmentInfos . info ( 0 ) ) || SegmentReader . hasSeparateNorms ( segmentInfos . info ( 0 ) ) ) ) ) ) ) { int minSegment = segmentInfos . size ( ) - mergeFactor ; mergeSegments ( minSegment < 0 ? 0 : minSegment ) ; } } public synchronized void addIndexes ( Directory [ ] dirs ) throws IOException { optimize ( ) ; int start = segmentInfos . size ( ) ; for ( int i = 0 ; i < dirs . length ; i ++ ) { SegmentInfos sis = new SegmentInfos ( ) ; sis . read ( dirs [ i ] ) ; for ( int j = 0 ; j < sis . size ( ) ; j ++ ) { segmentInfos . addElement ( sis . info ( j ) ) ; } } while ( segmentInfos . size ( ) > start + mergeFactor ) { for ( int base = start ; base < segmentInfos . size ( ) ; base ++ ) { int end = Math . min ( segmentInfos . size ( ) , base + mergeFactor ) ; if ( end - base > 1 ) mergeSegments ( base , end ) ; } } optimize ( ) ; } public synchronized void addIndexes ( IndexReader [ ] readers ) throws IOException { optimize ( ) ; final String mergedName = newSegmentName ( ) ; SegmentMerger merger = new SegmentMerger ( this , mergedName ) ; final Vector segmentsToDelete = new Vector ( ) ; IndexReader sReader = null ; if ( segmentInfos . size ( ) == 1 ) { sReader = SegmentReader . get ( segmentInfos . info ( 0 ) ) ; merger . add ( sReader ) ; segmentsToDelete . addElement ( sReader ) ; } for ( int i = 0 ; i < readers . length ; i ++ ) merger . add ( readers [ i ] ) ; int docCount = merger . merge ( ) ; segmentInfos . setSize ( 0 ) ; segmentInfos . addElement ( new SegmentInfo ( mergedName , docCount , directory ) ) ; if ( sReader != null ) sReader . close ( ) ; synchronized ( directory ) { new Lock . With ( directory . makeLock ( COMMIT_LOCK_NAME ) , commitLockTimeout ) { public Object doBody ( ) throws IOException { segmentInfos . write ( directory ) ; return null ; } } . run ( ) ; } deleteSegments ( segmentsToDelete ) ; if ( useCompoundFile ) { final Vector filesToDelete = merger . createCompoundFile ( mergedName + ".tmp" ) ; synchronized ( directory ) { new Lock . With ( directory . makeLock ( COMMIT_LOCK_NAME ) , commitLockTimeout ) { public Object doBody ( ) throws IOException { directory . renameFile ( mergedName + ".tmp" , mergedName + ".cfs" ) ; return null ; } } . run ( ) ; } deleteFiles ( filesToDelete ) ; } } private final void flushRamSegments ( ) throws IOException { int minSegment = segmentInfos . size ( ) - 1 ; int docCount = 0 ; while ( minSegment >= 0 && ( segmentInfos . info ( minSegment ) ) . dir == ramDirectory ) { docCount += segmentInfos . info ( minSegment ) . docCount ; minSegment -- ; } if ( minSegment < 0 || ( docCount + segmentInfos . info ( minSegment ) . docCount ) > mergeFactor || ! ( segmentInfos . info ( segmentInfos . size ( ) - 1 ) . dir == ramDirectory ) ) minSegment ++ ; if ( minSegment >= segmentInfos . size ( ) ) return ; mergeSegments ( minSegment ) ; } private final void maybeMergeSegments ( ) throws IOException { long targetMergeDocs = minMergeDocs ; while ( targetMergeDocs <= maxMergeDocs ) { int minSegment = segmentInfos . size ( ) ; int mergeDocs = 0 ; while ( -- minSegment >= 0 ) { SegmentInfo si = segmentInfos . info ( minSegment ) ; if ( si . docCount >= targetMergeDocs ) break ; mergeDocs += si . docCount ; } if ( mergeDocs >= targetMergeDocs ) mergeSegments ( minSegment + 1 ) ; else break ; targetMergeDocs *= mergeFactor ; } } private final void mergeSegments ( int minSegment ) throws IOException { mergeSegments ( minSegment , segmentInfos . size ( ) ) ; } private final void mergeSegments ( int minSegment , int end ) throws IOException { final String mergedName = newSegmentName ( ) ; if ( infoStream != null ) infoStream . print ( "merging segments" ) ; SegmentMerger merger = new SegmentMerger ( this , mergedName ) ; final Vector segmentsToDelete = new Vector ( ) ; for ( int i = minSegment ; i < end ; i ++ ) { SegmentInfo si = segmentInfos . info ( i ) ; if ( infoStream != null ) infoStream . print ( " " + si . name + " (" + si . docCount + " docs)" ) ; IndexReader reader = SegmentReader . get ( si ) ; merger . add ( reader ) ; if ( ( reader . directory ( ) == this . directory ) || ( reader . directory ( ) == this . ramDirectory ) ) segmentsToDelete . addElement ( reader ) ; } int mergedDocCount = merger . merge ( ) ; if ( infoStream != null ) { infoStream . println ( " into " + mergedName + " (" + mergedDocCount + " docs)" ) ; } for ( int i = end - 1 ; i > minSegment ; i -- ) segmentInfos . remove ( i ) ; segmentInfos . set ( minSegment , new SegmentInfo ( mergedName , mergedDocCount , directory ) ) ; merger . closeReaders ( ) ; synchronized ( directory ) { new Lock . With ( directory . makeLock ( COMMIT_LOCK_NAME ) , commitLockTimeout ) { public Object doBody ( ) throws IOException { segmentInfos . write ( directory ) ; return null ; } } . run ( ) ; } deleteSegments ( segmentsToDelete ) ; if ( useCompoundFile ) { final Vector filesToDelete = merger . createCompoundFile ( mergedName + ".tmp" ) ; synchronized ( directory ) { new Lock . With ( directory . makeLock ( COMMIT_LOCK_NAME ) , commitLockTimeout ) { public Object doBody ( ) throws IOException { directory . renameFile ( mergedName + ".tmp" , mergedName + ".cfs" ) ; return null ; } } . run ( ) ; } deleteFiles ( filesToDelete ) ; } } private final void deleteSegments ( Vector segments ) throws IOException { Vector deletable = new Vector ( ) ; deleteFiles ( readDeleteableFiles ( ) , deletable ) ; for ( int i = 0 ; i < segments . size ( ) ; i ++ ) { SegmentReader reader = ( SegmentReader ) segments . elementAt ( i ) ; if ( reader . directory ( ) == this . directory ) deleteFiles ( reader . files ( ) , deletable ) ; else deleteFiles ( reader . files ( ) , reader . directory ( ) ) ; } writeDeleteableFiles ( deletable ) ; } private final void deleteFiles ( Vector files ) throws IOException { Vector deletable = new Vector ( ) ; deleteFiles ( readDeleteableFiles ( ) , deletable ) ; deleteFiles ( files , deletable ) ; writeDeleteableFiles ( deletable ) ; } private final void deleteFiles ( Vector files , Directory directory ) throws IOException { for ( int i = 0 ; i < files . size ( ) ; i ++ ) directory . deleteFile ( ( String ) files . elementAt ( i ) ) ; } private final void deleteFiles ( Vector files , Vector deletable ) throws IOException { for ( int i = 0 ; i < files . size ( ) ; i ++ ) { String file = ( String ) files . elementAt ( i ) ; try { directory . deleteFile ( file ) ; } catch ( IOException e ) { if ( directory . fileExists ( file ) ) { if ( infoStream != null ) infoStream . println ( e . toString ( ) + "; Will re-try later." ) ; deletable . addElement ( file ) ; } } } } private final Vector readDeleteableFiles ( ) throws IOException { Vector result = new Vector ( ) ; if ( ! directory . fileExists ( IndexFileNames . DELETABLE ) ) return result ; IndexInput input = directory . openInput ( IndexFileNames . DELETABLE ) ; try { for ( int i = input . readInt ( ) ; i > 0 ; i -- ) result . addElement ( input . readString ( ) ) ; } finally { input . close ( ) ; } return result ; } private final void writeDeleteableFiles ( Vector files ) throws IOException { IndexOutput output = directory . createOutput ( "deleteable.new" ) ; try { output . writeInt ( files . size ( ) ) ; for ( int i = 0 ; i < files . size ( ) ; i ++ ) output . writeString ( ( String ) files . elementAt ( i ) ) ; } finally { output . close ( ) ; } directory . renameFile ( "deleteable.new" , IndexFileNames . DELETABLE ) ; } } 	1	['48', '1', '0', '21', '119', '742', '8', '20', '32', '0.89106383', '1211', '0.64', '6', '0', '0.115277778', '0', '0', '23.70833333', '2', '0.9792', '22']
package org . apache . lucene . index ; import java . util . * ; class SegmentTermVector implements TermFreqVector { private String field ; private String terms [ ] ; private int termFreqs [ ] ; SegmentTermVector ( String field , String terms [ ] , int termFreqs [ ] ) { this . field = field ; this . terms = terms ; this . termFreqs = termFreqs ; } public String getField ( ) { return field ; } public String toString ( ) { StringBuffer sb = new StringBuffer ( ) ; sb . append ( '{' ) ; sb . append ( field ) . append ( ": " ) ; if ( terms != null ) { for ( int i = 0 ; i < terms . length ; i ++ ) { if ( i > 0 ) sb . append ( ", " ) ; sb . append ( terms [ i ] ) . append ( '/' ) . append ( termFreqs [ i ] ) ; } } sb . append ( '}' ) ; return sb . toString ( ) ; } public int size ( ) { return terms == null ? 0 : terms . length ; } public String [ ] getTerms ( ) { return terms ; } public int [ ] getTermFrequencies ( ) { return termFreqs ; } public int indexOf ( String termText ) { if ( terms == null ) return - 1 ; int res = Arrays . binarySearch ( terms , termText ) ; return res >= 0 ? res : - 1 ; } public int [ ] indexesOf ( String [ ] termNumbers , int start , int len ) { int res [ ] = new int [ len ] ; for ( int i = 0 ; i < len ; i ++ ) { res [ i ] = indexOf ( termNumbers [ start + i ] ) ; } return res ; } } 	0	['8', '1', '1', '3', '15', '0', '2', '1', '7', '0.571428571', '133', '1', '0', '0', '0.35', '0', '0', '15.25', '4', '1.75', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . Directory ; final class TermInfosReader { private Directory directory ; private String segment ; private FieldInfos fieldInfos ; private ThreadLocal enumerators = new ThreadLocal ( ) ; private SegmentTermEnum origEnum ; private long size ; private Term [ ] indexTerms = null ; private TermInfo [ ] indexInfos ; private long [ ] indexPointers ; private SegmentTermEnum indexEnum ; TermInfosReader ( Directory dir , String seg , FieldInfos fis ) throws IOException { directory = dir ; segment = seg ; fieldInfos = fis ; origEnum = new SegmentTermEnum ( directory . openInput ( segment + ".tis" ) , fieldInfos , false ) ; size = origEnum . size ; indexEnum = new SegmentTermEnum ( directory . openInput ( segment + ".tii" ) , fieldInfos , true ) ; } protected void finalize ( ) { enumerators . set ( null ) ; } public int getSkipInterval ( ) { return origEnum . skipInterval ; } final void close ( ) throws IOException { if ( origEnum != null ) origEnum . close ( ) ; if ( indexEnum != null ) indexEnum . close ( ) ; } final long size ( ) { return size ; } private SegmentTermEnum getEnum ( ) { SegmentTermEnum termEnum = ( SegmentTermEnum ) enumerators . get ( ) ; if ( termEnum == null ) { termEnum = terms ( ) ; enumerators . set ( termEnum ) ; } return termEnum ; } private synchronized void ensureIndexIsRead ( ) throws IOException { if ( indexTerms != null ) return ; try { int indexSize = ( int ) indexEnum . size ; indexTerms = new Term [ indexSize ] ; indexInfos = new TermInfo [ indexSize ] ; indexPointers = new long [ indexSize ] ; for ( int i = 0 ; indexEnum . next ( ) ; i ++ ) { indexTerms [ i ] = indexEnum . term ( ) ; indexInfos [ i ] = indexEnum . termInfo ( ) ; indexPointers [ i ] = indexEnum . indexPointer ; } } finally { indexEnum . close ( ) ; indexEnum = null ; } } private final int getIndexOffset ( Term term ) { int lo = 0 ; int hi = indexTerms . length - 1 ; while ( hi >= lo ) { int mid = ( lo + hi ) > > 1 ; int delta = term . compareTo ( indexTerms [ mid ] ) ; if ( delta < 0 ) hi = mid - 1 ; else if ( delta > 0 ) lo = mid + 1 ; else return mid ; } return hi ; } private final void seekEnum ( int indexOffset ) throws IOException { getEnum ( ) . seek ( indexPointers [ indexOffset ] , ( indexOffset * getEnum ( ) . indexInterval ) - 1 , indexTerms [ indexOffset ] , indexInfos [ indexOffset ] ) ; } TermInfo get ( Term term ) throws IOException { if ( size == 0 ) return null ; ensureIndexIsRead ( ) ; SegmentTermEnum enumerator = getEnum ( ) ; if ( enumerator . term ( ) != null && ( ( enumerator . prev ( ) != null && term . compareTo ( enumerator . prev ( ) ) > 0 ) || term . compareTo ( enumerator . term ( ) ) >= 0 ) ) { int enumOffset = ( int ) ( enumerator . position / enumerator . indexInterval ) + 1 ; if ( indexTerms . length == enumOffset || term . compareTo ( indexTerms [ enumOffset ] ) < 0 ) return scanEnum ( term ) ; } seekEnum ( getIndexOffset ( term ) ) ; return scanEnum ( term ) ; } private final TermInfo scanEnum ( Term term ) throws IOException { SegmentTermEnum enumerator = getEnum ( ) ; enumerator . scanTo ( term ) ; if ( enumerator . term ( ) != null && term . compareTo ( enumerator . term ( ) ) == 0 ) return enumerator . termInfo ( ) ; else return null ; } final Term get ( int position ) throws IOException { if ( size == 0 ) return null ; SegmentTermEnum enumerator = getEnum ( ) ; if ( enumerator != null && enumerator . term ( ) != null && position >= enumerator . position && position < ( enumerator . position + enumerator . indexInterval ) ) return scanEnum ( position ) ; seekEnum ( position / enumerator . indexInterval ) ; return scanEnum ( position ) ; } private final Term scanEnum ( int position ) throws IOException { SegmentTermEnum enumerator = getEnum ( ) ; while ( enumerator . position < position ) if ( ! enumerator . next ( ) ) return null ; return enumerator . term ( ) ; } final long getPosition ( Term term ) throws IOException { if ( size == 0 ) return - 1 ; ensureIndexIsRead ( ) ; int indexOffset = getIndexOffset ( term ) ; seekEnum ( indexOffset ) ; SegmentTermEnum enumerator = getEnum ( ) ; while ( term . compareTo ( enumerator . term ( ) ) > 0 && enumerator . next ( ) ) { } if ( term . compareTo ( enumerator . term ( ) ) == 0 ) return enumerator . position ; else return - 1 ; } public SegmentTermEnum terms ( ) { return ( SegmentTermEnum ) origEnum . clone ( ) ; } public SegmentTermEnum terms ( Term term ) throws IOException { get ( term ) ; return ( SegmentTermEnum ) getEnum ( ) . clone ( ) ; } } 	1	['16', '1', '0', '8', '34', '62', '2', '6', '3', '0.686666667', '461', '1', '6', '0', '0.28125', '0', '0', '27.1875', '4', '1.1875', '4']
package org . apache . lucene . document ; import org . apache . lucene . search . PrefixQuery ; import org . apache . lucene . search . RangeQuery ; import java . util . Date ; public class DateField { private DateField ( ) { } private static int DATE_LEN = Long . toString ( 1000L * 365 * 24 * 60 * 60 * 1000 , Character . MAX_RADIX ) . length ( ) ; public static String MIN_DATE_STRING ( ) { return timeToString ( 0 ) ; } public static String MAX_DATE_STRING ( ) { char [ ] buffer = new char [ DATE_LEN ] ; char c = Character . forDigit ( Character . MAX_RADIX - 1 , Character . MAX_RADIX ) ; for ( int i = 0 ; i < DATE_LEN ; i ++ ) buffer [ i ] = c ; return new String ( buffer ) ; } public static String dateToString ( Date date ) { return timeToString ( date . getTime ( ) ) ; } public static String timeToString ( long time ) { if ( time < 0 ) throw new RuntimeException ( "time '" + time + "' is too early, must be >= 0" ) ; String s = Long . toString ( time , Character . MAX_RADIX ) ; if ( s . length ( ) > DATE_LEN ) throw new RuntimeException ( "time '" + time + "' is too late, length of string " + "representation must be <= " + DATE_LEN ) ; if ( s . length ( ) < DATE_LEN ) { StringBuffer sb = new StringBuffer ( s ) ; while ( sb . length ( ) < DATE_LEN ) sb . insert ( 0 , 0 ) ; s = sb . toString ( ) ; } return s ; } public static long stringToTime ( String s ) { return Long . parseLong ( s , Character . MAX_RADIX ) ; } public static Date stringToDate ( String s ) { return new Date ( stringToTime ( s ) ) ; } } 	0	['8', '1', '0', '1', '25', '22', '1', '0', '6', '0.428571429', '126', '1', '0', '0', '0.178571429', '0', '0', '14.625', '5', '1.375', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . * ; abstract class PhraseScorer extends Scorer { private Weight weight ; protected byte [ ] norms ; protected float value ; private boolean firstTime = true ; private boolean more = true ; protected PhraseQueue pq ; protected PhrasePositions first , last ; private float freq ; PhraseScorer ( Weight weight , TermPositions [ ] tps , int [ ] positions , Similarity similarity , byte [ ] norms ) { super ( similarity ) ; this . norms = norms ; this . weight = weight ; this . value = weight . getValue ( ) ; for ( int i = 0 ; i < tps . length ; i ++ ) { PhrasePositions pp = new PhrasePositions ( tps [ i ] , positions [ i ] ) ; if ( last != null ) { last . next = pp ; } else first = pp ; last = pp ; } pq = new PhraseQueue ( tps . length ) ; } public int doc ( ) { return first . doc ; } public boolean next ( ) throws IOException { if ( firstTime ) { init ( ) ; firstTime = false ; } else if ( more ) { more = last . next ( ) ; } return doNext ( ) ; } private boolean doNext ( ) throws IOException { while ( more ) { while ( more && first . doc < last . doc ) { more = first . skipTo ( last . doc ) ; firstToLast ( ) ; } if ( more ) { freq = phraseFreq ( ) ; if ( freq == 0.0f ) more = last . next ( ) ; else return true ; } } return false ; } public float score ( ) throws IOException { float raw = getSimilarity ( ) . tf ( freq ) * value ; return raw * Similarity . decodeNorm ( norms [ first . doc ] ) ; } public boolean skipTo ( int target ) throws IOException { for ( PhrasePositions pp = first ; more && pp != null ; pp = pp . next ) { more = pp . skipTo ( target ) ; } if ( more ) sort ( ) ; return doNext ( ) ; } protected abstract float phraseFreq ( ) throws IOException ; private void init ( ) throws IOException { for ( PhrasePositions pp = first ; more && pp != null ; pp = pp . next ) more = pp . next ( ) ; if ( more ) sort ( ) ; } private void sort ( ) { pq . clear ( ) ; for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) pq . put ( pp ) ; pqToList ( ) ; } protected final void pqToList ( ) { last = first = null ; while ( pq . top ( ) != null ) { PhrasePositions pp = ( PhrasePositions ) pq . pop ( ) ; if ( last != null ) { last . next = pp ; } else first = pp ; last = pp ; pp . next = null ; } } protected final void firstToLast ( ) { last . next = first ; last = first ; first = first . next ; last . next = null ; } public Explanation explain ( final int doc ) throws IOException { Explanation tfExplanation = new Explanation ( ) ; while ( next ( ) && doc ( ) < doc ) { } float phraseFreq = ( doc ( ) == doc ) ? freq : 0.0f ; tfExplanation . setValue ( getSimilarity ( ) . tf ( phraseFreq ) ) ; tfExplanation . setDescription ( "tf(phraseFreq=" + phraseFreq + ")" ) ; return tfExplanation ; } public String toString ( ) { return "scorer(" + weight + ")" ; } } 	1	['13', '2', '2', '9', '34', '0', '2', '7', '6', '0.675925926', '342', '1', '4', '0.4', '0.21978022', '1', '3', '24.61538462', '3', '1.1538', '1']
package org . apache . lucene . search ; public class FieldDoc extends ScoreDoc { public Comparable [ ] fields ; public FieldDoc ( int doc , float score ) { super ( doc , score ) ; } public FieldDoc ( int doc , float score , Comparable [ ] fields ) { super ( doc , score ) ; this . fields = fields ; } } 	0	['2', '2', '0', '4', '3', '1', '3', '1', '2', '1', '16', '0', '0', '0', '0.875', '0', '0', '6.5', '0', '0', '0']
package org . apache . lucene . store ; import java . io . IOException ; public class RAMOutputStream extends BufferedIndexOutput { private RAMFile file ; private long pointer = 0 ; public RAMOutputStream ( ) { this ( new RAMFile ( ) ) ; } RAMOutputStream ( RAMFile f ) { file = f ; } public void writeTo ( IndexOutput out ) throws IOException { flush ( ) ; final long end = file . length ; long pos = 0 ; int buffer = 0 ; while ( pos < end ) { int length = BUFFER_SIZE ; long nextPos = pos + length ; if ( nextPos > end ) { length = ( int ) ( end - pos ) ; } out . writeBytes ( ( byte [ ] ) file . buffers . elementAt ( buffer ++ ) , length ) ; pos = nextPos ; } } public void reset ( ) { try { seek ( 0 ) ; } catch ( IOException e ) { throw new RuntimeException ( e . toString ( ) ) ; } file . length = 0 ; } public void flushBuffer ( byte [ ] src , int len ) { byte [ ] buffer ; int bufferPos = 0 ; while ( bufferPos != len ) { int bufferNumber = ( int ) ( pointer / BUFFER_SIZE ) ; int bufferOffset = ( int ) ( pointer % BUFFER_SIZE ) ; int bytesInBuffer = BUFFER_SIZE - bufferOffset ; int remainInSrcBuffer = len - bufferPos ; int bytesToCopy = bytesInBuffer >= remainInSrcBuffer ? remainInSrcBuffer : bytesInBuffer ; if ( bufferNumber == file . buffers . size ( ) ) { buffer = new byte [ BUFFER_SIZE ] ; file . buffers . addElement ( buffer ) ; } else { buffer = ( byte [ ] ) file . buffers . elementAt ( bufferNumber ) ; } System . arraycopy ( src , bufferPos , buffer , bufferOffset , bytesToCopy ) ; bufferPos += bytesToCopy ; pointer += bytesToCopy ; } if ( pointer > file . length ) file . length = pointer ; file . lastModified = System . currentTimeMillis ( ) ; } public void close ( ) throws IOException { super . close ( ) ; } public void seek ( long pos ) throws IOException { super . seek ( pos ) ; pointer = pos ; } public long length ( ) { return file . length ; } } 	1	['8', '3', '0', '5', '21', '4', '2', '3', '7', '0.357142857', '188', '1', '1', '0.777777778', '0.270833333', '1', '2', '22.25', '5', '1.25', '4']
package org . apache . lucene . queryParser ; public class TokenMgrError extends Error { static final int LEXICAL_ERROR = 0 ; static final int STATIC_LEXER_ERROR = 1 ; static final int INVALID_LEXICAL_STATE = 2 ; static final int LOOP_DETECTED = 3 ; int errorCode ; protected static final String addEscapes ( String str ) { StringBuffer retval = new StringBuffer ( ) ; char ch ; for ( int i = 0 ; i < str . length ( ) ; i ++ ) { switch ( str . charAt ( i ) ) { case 0 : continue ; case '\b' : retval . append ( "\\b" ) ; continue ; case '\t' : retval . append ( "\\t" ) ; continue ; case '\n' : retval . append ( "\\n" ) ; continue ; case '\f' : retval . append ( "\\f" ) ; continue ; case '\r' : retval . append ( "\\r" ) ; continue ; case '\"' : retval . append ( "\\\"" ) ; continue ; case '\'' : retval . append ( "\\\'" ) ; continue ; case '\\' : retval . append ( "\\\\" ) ; continue ; default : if ( ( ch = str . charAt ( i ) ) < 0x20 || ch > 0x7e ) { String s = "0000" + Integer . toString ( ch , 16 ) ; retval . append ( "\\u" + s . substring ( s . length ( ) - 4 , s . length ( ) ) ) ; } else { retval . append ( ch ) ; } continue ; } } return retval . toString ( ) ; } protected static String LexicalError ( boolean EOFSeen , int lexState , int errorLine , int errorColumn , String errorAfter , char curChar ) { return ( "Lexical error at line " + errorLine + ", column " + errorColumn + ".  Encountered: " + ( EOFSeen ? "<EOF> " : ( "\"" + addEscapes ( String . valueOf ( curChar ) ) + "\"" ) + " (" + ( int ) curChar + "), " ) + "after : \"" + addEscapes ( errorAfter ) + "\"" ) ; } public String getMessage ( ) { return super . getMessage ( ) ; } public TokenMgrError ( ) { } public TokenMgrError ( String message , int reason ) { super ( message ) ; errorCode = reason ; } public TokenMgrError ( boolean EOFSeen , int lexState , int errorLine , int errorColumn , String errorAfter , char curChar , int reason ) { this ( LexicalError ( EOFSeen , lexState , errorLine , errorColumn , errorAfter , curChar ) , reason ) ; } } 	0	['6', '3', '0', '2', '19', '15', '2', '0', '4', '1.12', '184', '0', '0', '0.8125', '0.5', '1', '1', '28.83333333', '14', '2.8333', '0']
package org . apache . lucene . index ; final class FieldInfo { String name ; boolean isIndexed ; int number ; boolean storeTermVector ; boolean storeOffsetWithTermVector ; boolean storePositionWithTermVector ; boolean omitNorms ; FieldInfo ( String na , boolean tk , int nu , boolean storeTermVector , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector , boolean omitNorms ) { name = na ; isIndexed = tk ; number = nu ; this . storeTermVector = storeTermVector ; this . storeOffsetWithTermVector = storeOffsetWithTermVector ; this . storePositionWithTermVector = storePositionWithTermVector ; this . omitNorms = omitNorms ; } } 	1	['1', '1', '0', '6', '2', '0', '6', '0', '0', '2', '32', '0', '0', '0', '1', '0', '0', '24', '0', '0', '1']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; import java . io . Serializable ; public interface SortComparatorSource extends Serializable { ScoreDocComparator newComparator ( IndexReader reader , String fieldname ) throws IOException ; } 	0	['1', '1', '0', '5', '1', '0', '3', '2', '1', '2', '1', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . ArrayList ; import java . util . List ; import java . util . Iterator ; class BooleanScorer2 extends Scorer { private ArrayList requiredScorers = new ArrayList ( ) ; private ArrayList optionalScorers = new ArrayList ( ) ; private ArrayList prohibitedScorers = new ArrayList ( ) ; private class Coordinator { int maxCoord = 0 ; private float [ ] coordFactors = null ; void init ( ) { coordFactors = new float [ maxCoord + 1 ] ; Similarity sim = getSimilarity ( ) ; for ( int i = 0 ; i <= maxCoord ; i ++ ) { coordFactors [ i ] = sim . coord ( i , maxCoord ) ; } } int nrMatchers ; void initDoc ( ) { nrMatchers = 0 ; } float coordFactor ( ) { return coordFactors [ nrMatchers ] ; } } private final Coordinator coordinator ; private Scorer countingSumScorer = null ; private final int minNrShouldMatch ; public BooleanScorer2 ( Similarity similarity , int minNrShouldMatch ) { super ( similarity ) ; if ( minNrShouldMatch < 0 ) { throw new IllegalArgumentException ( "Minimum number of optional scorers should not be negative" ) ; } coordinator = new Coordinator ( ) ; this . minNrShouldMatch = minNrShouldMatch ; } public BooleanScorer2 ( Similarity similarity ) { this ( similarity , 0 ) ; } public void add ( final Scorer scorer , boolean required , boolean prohibited ) { if ( ! prohibited ) { coordinator . maxCoord ++ ; } if ( required ) { if ( prohibited ) { throw new IllegalArgumentException ( "scorer cannot be required and prohibited" ) ; } requiredScorers . add ( scorer ) ; } else if ( prohibited ) { prohibitedScorers . add ( scorer ) ; } else { optionalScorers . add ( scorer ) ; } } private void initCountingSumScorer ( ) { coordinator . init ( ) ; countingSumScorer = makeCountingSumScorer ( ) ; } private class SingleMatchScorer extends Scorer { private Scorer scorer ; private int lastScoredDoc = - 1 ; SingleMatchScorer ( Scorer scorer ) { super ( scorer . getSimilarity ( ) ) ; this . scorer = scorer ; } public float score ( ) throws IOException { if ( doc ( ) > lastScoredDoc ) { lastScoredDoc = doc ( ) ; coordinator . nrMatchers ++ ; } return scorer . score ( ) ; } public int doc ( ) { return scorer . doc ( ) ; } public boolean next ( ) throws IOException { return scorer . next ( ) ; } public boolean skipTo ( int docNr ) throws IOException { return scorer . skipTo ( docNr ) ; } public Explanation explain ( int docNr ) throws IOException { return scorer . explain ( docNr ) ; } } private Scorer countingDisjunctionSumScorer ( List scorers , int minMrShouldMatch ) { return new DisjunctionSumScorer ( scorers , minMrShouldMatch ) { private int lastScoredDoc = - 1 ; public float score ( ) throws IOException { if ( doc ( ) > lastScoredDoc ) { lastScoredDoc = doc ( ) ; coordinator . nrMatchers += super . nrMatchers ; } return super . score ( ) ; } } ; } private static Similarity defaultSimilarity = new DefaultSimilarity ( ) ; private Scorer countingConjunctionSumScorer ( List requiredScorers ) { final int requiredNrMatchers = requiredScorers . size ( ) ; ConjunctionScorer cs = new ConjunctionScorer ( defaultSimilarity ) { private int lastScoredDoc = - 1 ; public float score ( ) throws IOException { if ( doc ( ) > lastScoredDoc ) { lastScoredDoc = doc ( ) ; coordinator . nrMatchers += requiredNrMatchers ; } return super . score ( ) ; } } ; Iterator rsi = requiredScorers . iterator ( ) ; while ( rsi . hasNext ( ) ) { cs . add ( ( Scorer ) rsi . next ( ) ) ; } return cs ; } private Scorer dualConjunctionSumScorer ( Scorer req1 , Scorer req2 ) { final int requiredNrMatchers = requiredScorers . size ( ) ; ConjunctionScorer cs = new ConjunctionScorer ( defaultSimilarity ) ; cs . add ( req1 ) ; cs . add ( req2 ) ; return cs ; } private Scorer makeCountingSumScorer ( ) { return ( requiredScorers . size ( ) == 0 ) ? makeCountingSumScorerNoReq ( ) : makeCountingSumScorerSomeReq ( ) ; } private Scorer makeCountingSumScorerNoReq ( ) { if ( optionalScorers . size ( ) == 0 ) { return new NonMatchingScorer ( ) ; } else { int nrOptRequired = ( minNrShouldMatch < 1 ) ? 1 : minNrShouldMatch ; if ( optionalScorers . size ( ) < nrOptRequired ) { return new NonMatchingScorer ( ) ; } else { Scorer requiredCountingSumScorer = ( optionalScorers . size ( ) > nrOptRequired ) ? countingDisjunctionSumScorer ( optionalScorers , nrOptRequired ) : ( optionalScorers . size ( ) == 1 ) ? new SingleMatchScorer ( ( Scorer ) optionalScorers . get ( 0 ) ) : countingConjunctionSumScorer ( optionalScorers ) ; return addProhibitedScorers ( requiredCountingSumScorer ) ; } } } private Scorer makeCountingSumScorerSomeReq ( ) { if ( optionalScorers . size ( ) < minNrShouldMatch ) { return new NonMatchingScorer ( ) ; } else if ( optionalScorers . size ( ) == minNrShouldMatch ) { ArrayList allReq = new ArrayList ( requiredScorers ) ; allReq . addAll ( optionalScorers ) ; return addProhibitedScorers ( countingConjunctionSumScorer ( allReq ) ) ; } else { Scorer requiredCountingSumScorer = ( requiredScorers . size ( ) == 1 ) ? new SingleMatchScorer ( ( Scorer ) requiredScorers . get ( 0 ) ) : countingConjunctionSumScorer ( requiredScorers ) ; if ( minNrShouldMatch > 0 ) { return addProhibitedScorers ( dualConjunctionSumScorer ( requiredCountingSumScorer , countingDisjunctionSumScorer ( optionalScorers , minNrShouldMatch ) ) ) ; } else { return new ReqOptSumScorer ( addProhibitedScorers ( requiredCountingSumScorer ) , ( ( optionalScorers . size ( ) == 1 ) ? new SingleMatchScorer ( ( Scorer ) optionalScorers . get ( 0 ) ) : countingDisjunctionSumScorer ( optionalScorers , 1 ) ) ) ; } } } private Scorer addProhibitedScorers ( Scorer requiredCountingSumScorer ) { return ( prohibitedScorers . size ( ) == 0 ) ? requiredCountingSumScorer : new ReqExclScorer ( requiredCountingSumScorer , ( ( prohibitedScorers . size ( ) == 1 ) ? ( Scorer ) prohibitedScorers . get ( 0 ) : new DisjunctionSumScorer ( prohibitedScorers ) ) ) ; } public void score ( HitCollector hc ) throws IOException { if ( countingSumScorer == null ) { initCountingSumScorer ( ) ; } while ( countingSumScorer . next ( ) ) { hc . collect ( countingSumScorer . doc ( ) , score ( ) ) ; } } protected boolean score ( HitCollector hc , int max ) throws IOException { int docNr = countingSumScorer . doc ( ) ; while ( docNr < max ) { hc . collect ( docNr , score ( ) ) ; if ( ! countingSumScorer . next ( ) ) { return false ; } docNr = countingSumScorer . doc ( ) ; } return true ; } public int doc ( ) { return countingSumScorer . doc ( ) ; } public boolean next ( ) throws IOException { if ( countingSumScorer == null ) { initCountingSumScorer ( ) ; } return countingSumScorer . next ( ) ; } public float score ( ) throws IOException { coordinator . initDoc ( ) ; float sum = countingSumScorer . score ( ) ; return sum * coordinator . coordFactor ( ) ; } public boolean skipTo ( int target ) throws IOException { if ( countingSumScorer == null ) { initCountingSumScorer ( ) ; } return countingSumScorer . skipTo ( target ) ; } public Explanation explain ( int doc ) { throw new UnsupportedOperationException ( ) ; } } 	1	['20', '2', '0', '15', '52', '86', '5', '14', '9', '0.669172932', '449', '1', '3', '0.32', '0.223684211', '1', '3', '21.1', '6', '1.75', '4']
package org . apache . lucene . search ; import java . io . IOException ; public class ReqOptSumScorer extends Scorer { private Scorer reqScorer ; private Scorer optScorer ; public ReqOptSumScorer ( Scorer reqScorer , Scorer optScorer ) { super ( null ) ; this . reqScorer = reqScorer ; this . optScorer = optScorer ; } private boolean firstTimeOptScorer = true ; public boolean next ( ) throws IOException { return reqScorer . next ( ) ; } public boolean skipTo ( int target ) throws IOException { return reqScorer . skipTo ( target ) ; } public int doc ( ) { return reqScorer . doc ( ) ; } public float score ( ) throws IOException { int curDoc = reqScorer . doc ( ) ; float reqScore = reqScorer . score ( ) ; if ( firstTimeOptScorer ) { firstTimeOptScorer = false ; if ( ! optScorer . skipTo ( curDoc ) ) { optScorer = null ; return reqScore ; } } else if ( optScorer == null ) { return reqScore ; } else if ( ( optScorer . doc ( ) < curDoc ) && ( ! optScorer . skipTo ( curDoc ) ) ) { optScorer = null ; return reqScore ; } return ( optScorer . doc ( ) == curDoc ) ? reqScore + optScorer . score ( ) : reqScore ; } public Explanation explain ( int doc ) throws IOException { Explanation res = new Explanation ( ) ; res . setDescription ( "required, optional" ) ; res . addDetail ( reqScorer . explain ( doc ) ) ; res . addDetail ( optScorer . explain ( doc ) ) ; return res ; } } 	0	['6', '2', '0', '4', '15', '0', '1', '3', '6', '0.466666667', '113', '1', '2', '0.615384615', '0.5', '1', '3', '17.33333333', '1', '0.8333', '0']
package org . apache . lucene . analysis ; public final class Token { String termText ; int startOffset ; int endOffset ; String type = "word" ; private int positionIncrement = 1 ; public Token ( String text , int start , int end ) { termText = text ; startOffset = start ; endOffset = end ; } public Token ( String text , int start , int end , String typ ) { termText = text ; startOffset = start ; endOffset = end ; type = typ ; } public void setPositionIncrement ( int positionIncrement ) { if ( positionIncrement < 0 ) throw new IllegalArgumentException ( "Increment must be zero or greater: " + positionIncrement ) ; this . positionIncrement = positionIncrement ; } public int getPositionIncrement ( ) { return positionIncrement ; } public final String termText ( ) { return termText ; } public final int startOffset ( ) { return startOffset ; } public final int endOffset ( ) { return endOffset ; } public final String type ( ) { return type ; } public final String toString ( ) { StringBuffer sb = new StringBuffer ( ) ; sb . append ( "(" + termText + "," + startOffset + "," + endOffset ) ; if ( ! type . equals ( "word" ) ) sb . append ( ",type=" + type ) ; if ( positionIncrement != 1 ) sb . append ( ",posIncr=" + positionIncrement ) ; sb . append ( ")" ) ; return sb . toString ( ) ; } } 	1	['9', '1', '0', '13', '16', '0', '13', '0', '9', '0.6', '152', '0.2', '0', '0', '0.518518519', '1', '1', '15.33333333', '3', '1.1111', '2']
package org . apache . lucene . document ; public class NumberTools { private static final int RADIX = 36 ; private static final char NEGATIVE_PREFIX = '-' ; private static final char POSITIVE_PREFIX = '0' ; public static final String MIN_STRING_VALUE = NEGATIVE_PREFIX + "0000000000000" ; public static final String MAX_STRING_VALUE = POSITIVE_PREFIX + "1y2p0ij32e8e7" ; public static final int STR_SIZE = MIN_STRING_VALUE . length ( ) ; public static String longToString ( long l ) { if ( l == Long . MIN_VALUE ) { return MIN_STRING_VALUE ; } StringBuffer buf = new StringBuffer ( STR_SIZE ) ; if ( l < 0 ) { buf . append ( NEGATIVE_PREFIX ) ; l = Long . MAX_VALUE + l + 1 ; } else { buf . append ( POSITIVE_PREFIX ) ; } String num = Long . toString ( l , RADIX ) ; int padLen = STR_SIZE - num . length ( ) - buf . length ( ) ; while ( padLen -- > 0 ) { buf . append ( '0' ) ; } buf . append ( num ) ; return buf . toString ( ) ; } public static long stringToLong ( String str ) { if ( str == null ) { throw new NullPointerException ( "string cannot be null" ) ; } if ( str . length ( ) != STR_SIZE ) { throw new NumberFormatException ( "string is the wrong size" ) ; } if ( str . equals ( MIN_STRING_VALUE ) ) { return Long . MIN_VALUE ; } char prefix = str . charAt ( 0 ) ; long l = Long . parseLong ( str . substring ( 1 ) , RADIX ) ; if ( prefix == POSITIVE_PREFIX ) { } else if ( prefix == NEGATIVE_PREFIX ) { l = l - Long . MAX_VALUE - 1 ; } else { throw new NumberFormatException ( "string does not begin with the correct prefix" ) ; } return l ; } } 	0	['4', '1', '0', '0', '18', '0', '0', '0', '3', '1.166666667', '130', '0.5', '0', '0', '0.333333333', '0', '0', '30', '6', '2.5', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . io . PrintStream ; import java . io . Reader ; import java . io . StringReader ; import java . util . Hashtable ; import java . util . Enumeration ; import java . util . Arrays ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . Field ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . analysis . TokenStream ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . search . Similarity ; final class DocumentWriter { private Analyzer analyzer ; private Directory directory ; private Similarity similarity ; private FieldInfos fieldInfos ; private int maxFieldLength ; private int termIndexInterval = IndexWriter . DEFAULT_TERM_INDEX_INTERVAL ; private PrintStream infoStream ; DocumentWriter ( Directory directory , Analyzer analyzer , Similarity similarity , int maxFieldLength ) { this . directory = directory ; this . analyzer = analyzer ; this . similarity = similarity ; this . maxFieldLength = maxFieldLength ; } DocumentWriter ( Directory directory , Analyzer analyzer , IndexWriter writer ) { this . directory = directory ; this . analyzer = analyzer ; this . similarity = writer . getSimilarity ( ) ; this . maxFieldLength = writer . getMaxFieldLength ( ) ; this . termIndexInterval = writer . getTermIndexInterval ( ) ; } final void addDocument ( String segment , Document doc ) throws IOException { fieldInfos = new FieldInfos ( ) ; fieldInfos . add ( doc ) ; fieldInfos . write ( directory , segment + ".fnm" ) ; FieldsWriter fieldsWriter = new FieldsWriter ( directory , segment , fieldInfos ) ; try { fieldsWriter . addDocument ( doc ) ; } finally { fieldsWriter . close ( ) ; } postingTable . clear ( ) ; fieldLengths = new int [ fieldInfos . size ( ) ] ; fieldPositions = new int [ fieldInfos . size ( ) ] ; fieldOffsets = new int [ fieldInfos . size ( ) ] ; fieldBoosts = new float [ fieldInfos . size ( ) ] ; Arrays . fill ( fieldBoosts , doc . getBoost ( ) ) ; invertDocument ( doc ) ; Posting [ ] postings = sortPostingTable ( ) ; writePostings ( postings , segment ) ; writeNorms ( segment ) ; } private final Hashtable postingTable = new Hashtable ( ) ; private int [ ] fieldLengths ; private int [ ] fieldPositions ; private int [ ] fieldOffsets ; private float [ ] fieldBoosts ; private final void invertDocument ( Document doc ) throws IOException { Enumeration fields = doc . fields ( ) ; while ( fields . hasMoreElements ( ) ) { Field field = ( Field ) fields . nextElement ( ) ; String fieldName = field . name ( ) ; int fieldNumber = fieldInfos . fieldNumber ( fieldName ) ; int length = fieldLengths [ fieldNumber ] ; int position = fieldPositions [ fieldNumber ] ; if ( length > 0 ) position += analyzer . getPositionIncrementGap ( fieldName ) ; int offset = fieldOffsets [ fieldNumber ] ; if ( field . isIndexed ( ) ) { if ( ! field . isTokenized ( ) ) { String stringValue = field . stringValue ( ) ; if ( field . isStoreOffsetWithTermVector ( ) ) addPosition ( fieldName , stringValue , position ++ , new TermVectorOffsetInfo ( offset , offset + stringValue . length ( ) ) ) ; else addPosition ( fieldName , stringValue , position ++ , null ) ; offset += stringValue . length ( ) ; length ++ ; } else { Reader reader ; if ( field . readerValue ( ) != null ) reader = field . readerValue ( ) ; else if ( field . stringValue ( ) != null ) reader = new StringReader ( field . stringValue ( ) ) ; else throw new IllegalArgumentException ( "field must have either String or Reader value" ) ; TokenStream stream = analyzer . tokenStream ( fieldName , reader ) ; try { Token lastToken = null ; for ( Token t = stream . next ( ) ; t != null ; t = stream . next ( ) ) { position += ( t . getPositionIncrement ( ) - 1 ) ; if ( field . isStoreOffsetWithTermVector ( ) ) addPosition ( fieldName , t . termText ( ) , position ++ , new TermVectorOffsetInfo ( offset + t . startOffset ( ) , offset + t . endOffset ( ) ) ) ; else addPosition ( fieldName , t . termText ( ) , position ++ , null ) ; lastToken = t ; if ( ++ length > maxFieldLength ) { if ( infoStream != null ) infoStream . println ( "maxFieldLength " + maxFieldLength + " reached, ignoring following tokens" ) ; break ; } } if ( lastToken != null ) offset += lastToken . endOffset ( ) + 1 ; } finally { stream . close ( ) ; } } fieldLengths [ fieldNumber ] = length ; fieldPositions [ fieldNumber ] = position ; fieldBoosts [ fieldNumber ] *= field . getBoost ( ) ; fieldOffsets [ fieldNumber ] = offset ; } } } private final Term termBuffer = new Term ( "" , "" ) ; private final void addPosition ( String field , String text , int position , TermVectorOffsetInfo offset ) { termBuffer . set ( field , text ) ; Posting ti = ( Posting ) postingTable . get ( termBuffer ) ; if ( ti != null ) { int freq = ti . freq ; if ( ti . positions . length == freq ) { int [ ] newPositions = new int [ freq * 2 ] ; int [ ] positions = ti . positions ; for ( int i = 0 ; i < freq ; i ++ ) newPositions [ i ] = positions [ i ] ; ti . positions = newPositions ; } ti . positions [ freq ] = position ; if ( offset != null ) { if ( ti . offsets . length == freq ) { TermVectorOffsetInfo [ ] newOffsets = new TermVectorOffsetInfo [ freq * 2 ] ; TermVectorOffsetInfo [ ] offsets = ti . offsets ; for ( int i = 0 ; i < freq ; i ++ ) { newOffsets [ i ] = offsets [ i ] ; } ti . offsets = newOffsets ; } ti . offsets [ freq ] = offset ; } ti . freq = freq + 1 ; } else { Term term = new Term ( field , text , false ) ; postingTable . put ( term , new Posting ( term , position , offset ) ) ; } } private final Posting [ ] sortPostingTable ( ) { Posting [ ] array = new Posting [ postingTable . size ( ) ] ; Enumeration postings = postingTable . elements ( ) ; for ( int i = 0 ; postings . hasMoreElements ( ) ; i ++ ) array [ i ] = ( Posting ) postings . nextElement ( ) ; quickSort ( array , 0 , array . length - 1 ) ; return array ; } private static final void quickSort ( Posting [ ] postings , int lo , int hi ) { if ( lo >= hi ) return ; int mid = ( lo + hi ) / 2 ; if ( postings [ lo ] . term . compareTo ( postings [ mid ] . term ) > 0 ) { Posting tmp = postings [ lo ] ; postings [ lo ] = postings [ mid ] ; postings [ mid ] = tmp ; } if ( postings [ mid ] . term . compareTo ( postings [ hi ] . term ) > 0 ) { Posting tmp = postings [ mid ] ; postings [ mid ] = postings [ hi ] ; postings [ hi ] = tmp ; if ( postings [ lo ] . term . compareTo ( postings [ mid ] . term ) > 0 ) { Posting tmp2 = postings [ lo ] ; postings [ lo ] = postings [ mid ] ; postings [ mid ] = tmp2 ; } } int left = lo + 1 ; int right = hi - 1 ; if ( left >= right ) return ; Term partition = postings [ mid ] . term ; for ( ; ; ) { while ( postings [ right ] . term . compareTo ( partition ) > 0 ) -- right ; while ( left < right && postings [ left ] . term . compareTo ( partition ) <= 0 ) ++ left ; if ( left < right ) { Posting tmp = postings [ left ] ; postings [ left ] = postings [ right ] ; postings [ right ] = tmp ; -- right ; } else { break ; } } quickSort ( postings , lo , left ) ; quickSort ( postings , left + 1 , hi ) ; } private final void writePostings ( Posting [ ] postings , String segment ) throws IOException { IndexOutput freq = null , prox = null ; TermInfosWriter tis = null ; TermVectorsWriter termVectorWriter = null ; try { freq = directory . createOutput ( segment + ".frq" ) ; prox = directory . createOutput ( segment + ".prx" ) ; tis = new TermInfosWriter ( directory , segment , fieldInfos , termIndexInterval ) ; TermInfo ti = new TermInfo ( ) ; String currentField = null ; for ( int i = 0 ; i < postings . length ; i ++ ) { Posting posting = postings [ i ] ; ti . set ( 1 , freq . getFilePointer ( ) , prox . getFilePointer ( ) , - 1 ) ; tis . add ( posting . term , ti ) ; int postingFreq = posting . freq ; if ( postingFreq == 1 ) freq . writeVInt ( 1 ) ; else { freq . writeVInt ( 0 ) ; freq . writeVInt ( postingFreq ) ; } int lastPosition = 0 ; int [ ] positions = posting . positions ; for ( int j = 0 ; j < postingFreq ; j ++ ) { int position = positions [ j ] ; prox . writeVInt ( position - lastPosition ) ; lastPosition = position ; } String termField = posting . term . field ( ) ; if ( currentField != termField ) { currentField = termField ; FieldInfo fi = fieldInfos . fieldInfo ( currentField ) ; if ( fi . storeTermVector ) { if ( termVectorWriter == null ) { termVectorWriter = new TermVectorsWriter ( directory , segment , fieldInfos ) ; termVectorWriter . openDocument ( ) ; } termVectorWriter . openField ( currentField ) ; } else if ( termVectorWriter != null ) { termVectorWriter . closeField ( ) ; } } if ( termVectorWriter != null && termVectorWriter . isFieldOpen ( ) ) { termVectorWriter . addTerm ( posting . term . text ( ) , postingFreq , posting . positions , posting . offsets ) ; } } if ( termVectorWriter != null ) termVectorWriter . closeDocument ( ) ; } finally { IOException keep = null ; if ( freq != null ) try { freq . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( prox != null ) try { prox . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tis != null ) try { tis . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( termVectorWriter != null ) try { termVectorWriter . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( keep != null ) throw ( IOException ) keep . fillInStackTrace ( ) ; } } private final void writeNorms ( String segment ) throws IOException { for ( int n = 0 ; n < fieldInfos . size ( ) ; n ++ ) { FieldInfo fi = fieldInfos . fieldInfo ( n ) ; if ( fi . isIndexed && ! fi . omitNorms ) { float norm = fieldBoosts [ n ] * similarity . lengthNorm ( fi . name , fieldLengths [ n ] ) ; IndexOutput norms = directory . createOutput ( segment + ".f" + n ) ; try { norms . writeByte ( Similarity . encodeNorm ( norm ) ) ; } finally { norms . close ( ) ; } } } } void setInfoStream ( PrintStream infoStream ) { this . infoStream = infoStream ; } } final class Posting { Term term ; int freq ; int [ ] positions ; TermVectorOffsetInfo [ ] offsets ; Posting ( Term t , int position , TermVectorOffsetInfo offset ) { term = t ; freq = 1 ; positions = new int [ 1 ] ; positions [ 0 ] = position ; if ( offset != null ) { offsets = new TermVectorOffsetInfo [ 1 ] ; offsets [ 0 ] = offset ; } else offsets = null ; } } 	1	['10', '1', '0', '18', '87', '0', '1', '18', '0', '0.700854701', '990', '1', '5', '0', '0.254545455', '0', '0', '96.7', '10', '2.4', '6']
package org . apache . lucene . search ; public class SimilarityDelegator extends Similarity { private Similarity delegee ; public SimilarityDelegator ( Similarity delegee ) { this . delegee = delegee ; } public float lengthNorm ( String fieldName , int numTerms ) { return delegee . lengthNorm ( fieldName , numTerms ) ; } public float queryNorm ( float sumOfSquaredWeights ) { return delegee . queryNorm ( sumOfSquaredWeights ) ; } public float tf ( float freq ) { return delegee . tf ( freq ) ; } public float sloppyFreq ( int distance ) { return delegee . sloppyFreq ( distance ) ; } public float idf ( int docFreq , int numDocs ) { return delegee . idf ( docFreq , numDocs ) ; } public float coord ( int overlap , int maxOverlap ) { return delegee . coord ( overlap , maxOverlap ) ; } } 	0	['7', '2', '1', '2', '14', '0', '1', '1', '7', '0', '47', '1', '1', '0.7', '0.428571429', '1', '2', '5.571428571', '1', '0.8571', '0']
package org . apache . lucene . search ; import java . io . IOException ; final class BooleanScorer extends Scorer { private SubScorer scorers = null ; private BucketTable bucketTable = new BucketTable ( this ) ; private int maxCoord = 1 ; private float [ ] coordFactors = null ; private int requiredMask = 0 ; private int prohibitedMask = 0 ; private int nextMask = 1 ; BooleanScorer ( Similarity similarity ) { super ( similarity ) ; } static final class SubScorer { public Scorer scorer ; public boolean done ; public boolean required = false ; public boolean prohibited = false ; public HitCollector collector ; public SubScorer next ; public SubScorer ( Scorer scorer , boolean required , boolean prohibited , HitCollector collector , SubScorer next ) throws IOException { this . scorer = scorer ; this . done = ! scorer . next ( ) ; this . required = required ; this . prohibited = prohibited ; this . collector = collector ; this . next = next ; } } final void add ( Scorer scorer , boolean required , boolean prohibited ) throws IOException { int mask = 0 ; if ( required || prohibited ) { if ( nextMask == 0 ) throw new IndexOutOfBoundsException ( "More than 32 required/prohibited clauses in query." ) ; mask = nextMask ; nextMask = nextMask << 1 ; } else mask = 0 ; if ( ! prohibited ) maxCoord ++ ; if ( prohibited ) prohibitedMask |= mask ; else if ( required ) requiredMask |= mask ; scorers = new SubScorer ( scorer , required , prohibited , bucketTable . newCollector ( mask ) , scorers ) ; } private final void computeCoordFactors ( ) { coordFactors = new float [ maxCoord ] ; for ( int i = 0 ; i < maxCoord ; i ++ ) coordFactors [ i ] = getSimilarity ( ) . coord ( i , maxCoord - 1 ) ; } private int end ; private Bucket current ; public void score ( HitCollector hc ) throws IOException { next ( ) ; score ( hc , Integer . MAX_VALUE ) ; } protected boolean score ( HitCollector hc , int max ) throws IOException { if ( coordFactors == null ) computeCoordFactors ( ) ; boolean more ; Bucket tmp ; do { bucketTable . first = null ; while ( current != null ) { if ( ( current . bits & prohibitedMask ) == 0 && ( current . bits & requiredMask ) == requiredMask ) { if ( current . doc >= max ) { tmp = current ; current = current . next ; tmp . next = bucketTable . first ; bucketTable . first = tmp ; continue ; } hc . collect ( current . doc , current . score * coordFactors [ current . coord ] ) ; } current = current . next ; } if ( bucketTable . first != null ) { current = bucketTable . first ; bucketTable . first = current . next ; return true ; } more = false ; end += BucketTable . SIZE ; for ( SubScorer sub = scorers ; sub != null ; sub = sub . next ) { if ( ! sub . done ) { sub . done = ! sub . scorer . score ( sub . collector , end ) ; if ( ! sub . done ) more = true ; } } current = bucketTable . first ; } while ( current != null || more ) ; return false ; } public int doc ( ) { return current . doc ; } public boolean next ( ) throws IOException { boolean more ; do { while ( bucketTable . first != null ) { current = bucketTable . first ; bucketTable . first = current . next ; if ( ( current . bits & prohibitedMask ) == 0 && ( current . bits & requiredMask ) == requiredMask ) { return true ; } } more = false ; end += BucketTable . SIZE ; for ( SubScorer sub = scorers ; sub != null ; sub = sub . next ) { Scorer scorer = sub . scorer ; while ( ! sub . done && scorer . doc ( ) < end ) { sub . collector . collect ( scorer . doc ( ) , scorer . score ( ) ) ; sub . done = ! scorer . next ( ) ; } if ( ! sub . done ) { more = true ; } } } while ( bucketTable . first != null || more ) ; return false ; } public float score ( ) { if ( coordFactors == null ) computeCoordFactors ( ) ; return current . score * coordFactors [ current . coord ] ; } static final class Bucket { int doc = - 1 ; float score ; int bits ; int coord ; Bucket next ; } static final class BucketTable { public static final int SIZE = 1 << 11 ; public static final int MASK = SIZE - 1 ; final Bucket [ ] buckets = new Bucket [ SIZE ] ; Bucket first = null ; private BooleanScorer scorer ; public BucketTable ( BooleanScorer scorer ) { this . scorer = scorer ; } public final int size ( ) { return SIZE ; } public HitCollector newCollector ( int mask ) { return new Collector ( mask , this ) ; } } static final class Collector extends HitCollector { private BucketTable bucketTable ; private int mask ; public Collector ( int mask , BucketTable bucketTable ) { this . mask = mask ; this . bucketTable = bucketTable ; } public final void collect ( final int doc , final float score ) { final BucketTable table = bucketTable ; final int i = doc & BucketTable . MASK ; Bucket bucket = table . buckets [ i ] ; if ( bucket == null ) table . buckets [ i ] = bucket = new Bucket ( ) ; if ( bucket . doc != doc ) { bucket . doc = doc ; bucket . score = score ; bucket . bits = mask ; bucket . coord = 1 ; bucket . next = table . first ; table . first = bucket ; } else { bucket . score += score ; bucket . bits |= mask ; bucket . coord ++ ; } } } public boolean skipTo ( int target ) { throw new UnsupportedOperationException ( ) ; } public Explanation explain ( int doc ) { throw new UnsupportedOperationException ( ) ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "boolean(" ) ; for ( SubScorer sub = scorers ; sub != null ; sub = sub . next ) { buffer . append ( sub . scorer . toString ( ) ) ; buffer . append ( " " ) ; } buffer . append ( ")" ) ; return buffer . toString ( ) ; } } 	1	['11', '2', '0', '8', '28', '15', '2', '7', '7', '0.644444444', '440', '1', '3', '0.444444444', '0.287878788', '1', '3', '38.18181818', '2', '1.1818', '1']
package org . apache . lucene . analysis . standard ; public class Token { public int kind ; public int beginLine , beginColumn , endLine , endColumn ; public String image ; public Token next ; public Token specialToken ; public String toString ( ) { return image ; } public static final Token newToken ( int ofKind ) { switch ( ofKind ) { default : return new Token ( ) ; } } } 	0	['3', '1', '0', '3', '4', '3', '3', '0', '3', '1.4375', '23', '0', '2', '0', '0.5', '0', '0', '4', '2', '1', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . WeakHashMap ; import java . util . BitSet ; import org . apache . lucene . index . IndexReader ; public class QueryFilter extends Filter { private Query query ; private transient WeakHashMap cache = null ; public QueryFilter ( Query query ) { this . query = query ; } public BitSet bits ( IndexReader reader ) throws IOException { if ( cache == null ) { cache = new WeakHashMap ( ) ; } synchronized ( cache ) { BitSet cached = ( BitSet ) cache . get ( reader ) ; if ( cached != null ) { return cached ; } } final BitSet bits = new BitSet ( reader . maxDoc ( ) ) ; new IndexSearcher ( reader ) . search ( query , new HitCollector ( ) { public final void collect ( int doc , float score ) { bits . set ( doc ) ; } } ) ; synchronized ( cache ) { cache . put ( reader , bits ) ; } return bits ; } public String toString ( ) { return "QueryFilter(" + query + ")" ; } public boolean equals ( Object o ) { if ( ! ( o instanceof QueryFilter ) ) return false ; return this . query . equals ( ( ( QueryFilter ) o ) . query ) ; } public int hashCode ( ) { return query . hashCode ( ) ^ 0x923F64B9 ; } } 	1	['5', '2', '0', '6', '20', '0', '1', '6', '5', '0.375', '118', '1', '1', '0.2', '0.4', '1', '1', '22.2', '2', '1', '2']
package org . apache . lucene . index ; public interface TermPositionVector extends TermFreqVector { public int [ ] getTermPositions ( int index ) ; public TermVectorOffsetInfo [ ] getOffsets ( int index ) ; } 	0	['2', '1', '0', '4', '2', '1', '2', '2', '2', '2', '2', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . Arrays ; import java . util . Comparator ; import java . util . Iterator ; import java . util . LinkedList ; class ConjunctionScorer extends Scorer { private LinkedList scorers = new LinkedList ( ) ; private boolean firstTime = true ; private boolean more = true ; private float coord ; public ConjunctionScorer ( Similarity similarity ) { super ( similarity ) ; } final void add ( Scorer scorer ) { scorers . addLast ( scorer ) ; } private Scorer first ( ) { return ( Scorer ) scorers . getFirst ( ) ; } private Scorer last ( ) { return ( Scorer ) scorers . getLast ( ) ; } public int doc ( ) { return first ( ) . doc ( ) ; } public boolean next ( ) throws IOException { if ( firstTime ) { init ( true ) ; } else if ( more ) { more = last ( ) . next ( ) ; } return doNext ( ) ; } private boolean doNext ( ) throws IOException { while ( more && first ( ) . doc ( ) < last ( ) . doc ( ) ) { more = first ( ) . skipTo ( last ( ) . doc ( ) ) ; scorers . addLast ( scorers . removeFirst ( ) ) ; } return more ; } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { init ( false ) ; } Iterator i = scorers . iterator ( ) ; while ( more && i . hasNext ( ) ) { more = ( ( Scorer ) i . next ( ) ) . skipTo ( target ) ; } if ( more ) sortScorers ( ) ; return doNext ( ) ; } public float score ( ) throws IOException { float score = 0.0f ; Iterator i = scorers . iterator ( ) ; while ( i . hasNext ( ) ) score += ( ( Scorer ) i . next ( ) ) . score ( ) ; score *= coord ; return score ; } private void init ( boolean initScorers ) throws IOException { coord = getSimilarity ( ) . coord ( scorers . size ( ) , scorers . size ( ) ) ; more = scorers . size ( ) > 0 ; if ( initScorers ) { Iterator i = scorers . iterator ( ) ; while ( more && i . hasNext ( ) ) { more = ( ( Scorer ) i . next ( ) ) . next ( ) ; } if ( more ) sortScorers ( ) ; } firstTime = false ; } private void sortScorers ( ) { Scorer [ ] array = ( Scorer [ ] ) scorers . toArray ( new Scorer [ scorers . size ( ) ] ) ; scorers . clear ( ) ; Arrays . sort ( array , new Comparator ( ) { public int compare ( Object o1 , Object o2 ) { return ( ( Scorer ) o1 ) . doc ( ) - ( ( Scorer ) o2 ) . doc ( ) ; } } ) ; for ( int i = 0 ; i < array . length ; i ++ ) { scorers . addLast ( array [ i ] ) ; } } public Explanation explain ( int doc ) { throw new UnsupportedOperationException ( ) ; } } 	1	['12', '2', '1', '7', '33', '0', '4', '4', '6', '0.545454545', '237', '1', '0', '0.421052632', '0.283333333', '1', '3', '18.41666667', '2', '1', '1']
package org . apache . lucene . analysis ; import java . io . IOException ; import java . util . HashSet ; import java . util . Set ; public final class StopFilter extends TokenFilter { private final Set stopWords ; private final boolean ignoreCase ; public StopFilter ( TokenStream input , String [ ] stopWords ) { this ( input , stopWords , false ) ; } public StopFilter ( TokenStream in , String [ ] stopWords , boolean ignoreCase ) { super ( in ) ; this . ignoreCase = ignoreCase ; this . stopWords = makeStopSet ( stopWords , ignoreCase ) ; } public StopFilter ( TokenStream input , Set stopWords , boolean ignoreCase ) { super ( input ) ; this . ignoreCase = ignoreCase ; this . stopWords = stopWords ; } public StopFilter ( TokenStream in , Set stopWords ) { this ( in , stopWords , false ) ; } public static final Set makeStopSet ( String [ ] stopWords ) { return makeStopSet ( stopWords , false ) ; } public static final Set makeStopSet ( String [ ] stopWords , boolean ignoreCase ) { HashSet stopTable = new HashSet ( stopWords . length ) ; for ( int i = 0 ; i < stopWords . length ; i ++ ) stopTable . add ( ignoreCase ? stopWords [ i ] . toLowerCase ( ) : stopWords [ i ] ) ; return stopTable ; } public final Token next ( ) throws IOException { for ( Token token = input . next ( ) ; token != null ; token = input . next ( ) ) { String termText = ignoreCase ? token . termText . toLowerCase ( ) : token . termText ; if ( ! stopWords . contains ( termText ) ) return token ; } return null ; } } 	0	['7', '3', '0', '5', '13', '15', '2', '3', '7', '0.333333333', '106', '1', '0', '0.5', '0.514285714', '0', '0', '13.85714286', '3', '0.7143', '0']
package org . apache . lucene . queryParser ; public interface QueryParserConstants { int EOF = 0 ; int _NUM_CHAR = 1 ; int _ESCAPED_CHAR = 2 ; int _TERM_START_CHAR = 3 ; int _TERM_CHAR = 4 ; int _WHITESPACE = 5 ; int AND = 7 ; int OR = 8 ; int NOT = 9 ; int PLUS = 10 ; int MINUS = 11 ; int LPAREN = 12 ; int RPAREN = 13 ; int COLON = 14 ; int CARAT = 15 ; int QUOTED = 16 ; int TERM = 17 ; int FUZZY_SLOP = 18 ; int PREFIXTERM = 19 ; int WILDTERM = 20 ; int RANGEIN_START = 21 ; int RANGEEX_START = 22 ; int NUMBER = 23 ; int RANGEIN_TO = 24 ; int RANGEIN_END = 25 ; int RANGEIN_QUOTED = 26 ; int RANGEIN_GOOP = 27 ; int RANGEEX_TO = 28 ; int RANGEEX_END = 29 ; int RANGEEX_QUOTED = 30 ; int RANGEEX_GOOP = 31 ; int Boost = 0 ; int RangeEx = 1 ; int RangeIn = 2 ; int DEFAULT = 3 ; String [ ] tokenImage = { "<EOF>" , "<_NUM_CHAR>" , "<_ESCAPED_CHAR>" , "<_TERM_START_CHAR>" , "<_TERM_CHAR>" , "<_WHITESPACE>" , "<token of kind 6>" , "<AND>" , "<OR>" , "<NOT>" , "\"+\"" , "\"-\"" , "\"(\"" , "\")\"" , "\":\"" , "\"^\"" , "<QUOTED>" , "<TERM>" , "<FUZZY_SLOP>" , "<PREFIXTERM>" , "<WILDTERM>" , "\"[\"" , "\"{\"" , "<NUMBER>" , "\"TO\"" , "\"]\"" , "<RANGEIN_QUOTED>" , "<RANGEIN_GOOP>" , "\"TO\"" , "\"}\"" , "<RANGEEX_QUOTED>" , "<RANGEEX_GOOP>" , } ; } 	1	['1', '1', '0', '2', '1', '0', '2', '0', '0', '2', '169', '0', '0', '0', '0', '0', '0', '132', '0', '0', '1']
package org . apache . lucene . analysis . standard ; import org . apache . lucene . analysis . * ; import java . io . File ; import java . io . IOException ; import java . io . Reader ; import java . util . Set ; public class StandardAnalyzer extends Analyzer { private Set stopSet ; public static final String [ ] STOP_WORDS = StopAnalyzer . ENGLISH_STOP_WORDS ; public StandardAnalyzer ( ) { this ( STOP_WORDS ) ; } public StandardAnalyzer ( Set stopWords ) { stopSet = stopWords ; } public StandardAnalyzer ( String [ ] stopWords ) { stopSet = StopFilter . makeStopSet ( stopWords ) ; } public StandardAnalyzer ( File stopwords ) throws IOException { stopSet = WordlistLoader . getWordSet ( stopwords ) ; } public StandardAnalyzer ( Reader stopwords ) throws IOException { stopSet = WordlistLoader . getWordSet ( stopwords ) ; } public TokenStream tokenStream ( String fieldName , Reader reader ) { TokenStream result = new StandardTokenizer ( reader ) ; result = new StandardFilter ( result ) ; result = new LowerCaseFilter ( result ) ; result = new StopFilter ( result , stopSet ) ; return result ; } } 	0	['7', '2', '0', '8', '15', '0', '0', '8', '6', '0.5', '67', '0.5', '0', '0.666666667', '0.333333333', '0', '0', '8.285714286', '1', '0.1429', '0']
package org . apache . lucene . index ; final class IndexFileNames { static final String SEGMENTS = "segments" ; static final String DELETABLE = "deletable" ; static final String INDEX_EXTENSIONS [ ] = new String [ ] { "cfs" , "fnm" , "fdx" , "fdt" , "tii" , "tis" , "frq" , "prx" , "del" , "tvx" , "tvd" , "tvf" , "tvp" } ; static final String COMPOUND_EXTENSIONS [ ] = new String [ ] { "fnm" , "frq" , "prx" , "fdx" , "fdt" , "tii" , "tis" } ; static final String VECTOR_EXTENSIONS [ ] = new String [ ] { "tvx" , "tvd" , "tvf" } ; } 	1	['2', '1', '0', '3', '3', '1', '3', '0', '0', '1.4', '112', '0', '0', '0', '1', '0', '0', '52.5', '0', '0', '2']
package org . apache . lucene . search . spans ; import java . io . IOException ; public interface Spans { boolean next ( ) throws IOException ; boolean skipTo ( int target ) throws IOException ; int doc ( ) ; int start ( ) ; int end ( ) ; } 	0	['5', '1', '0', '15', '5', '10', '15', '0', '5', '2', '5', '0', '0', '0', '0.6', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . index ; import java . util . Vector ; import java . io . IOException ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . util . Constants ; final class SegmentInfos extends Vector { public static final int FORMAT = - 1 ; public int counter = 0 ; private long version = System . currentTimeMillis ( ) ; public final SegmentInfo info ( int i ) { return ( SegmentInfo ) elementAt ( i ) ; } public final void read ( Directory directory ) throws IOException { IndexInput input = directory . openInput ( IndexFileNames . SEGMENTS ) ; try { int format = input . readInt ( ) ; if ( format < 0 ) { if ( format < FORMAT ) throw new IOException ( "Unknown format version: " + format ) ; version = input . readLong ( ) ; counter = input . readInt ( ) ; } else { counter = format ; } for ( int i = input . readInt ( ) ; i > 0 ; i -- ) { SegmentInfo si = new SegmentInfo ( input . readString ( ) , input . readInt ( ) , directory ) ; addElement ( si ) ; } if ( format >= 0 ) { if ( input . getFilePointer ( ) >= input . length ( ) ) version = System . currentTimeMillis ( ) ; else version = input . readLong ( ) ; } } finally { input . close ( ) ; } } public final void write ( Directory directory ) throws IOException { IndexOutput output = directory . createOutput ( "segments.new" ) ; try { output . writeInt ( FORMAT ) ; output . writeLong ( ++ version ) ; output . writeInt ( counter ) ; output . writeInt ( size ( ) ) ; for ( int i = 0 ; i < size ( ) ; i ++ ) { SegmentInfo si = info ( i ) ; output . writeString ( si . name ) ; output . writeInt ( si . docCount ) ; } } finally { output . close ( ) ; } directory . renameFile ( "segments.new" , IndexFileNames . SEGMENTS ) ; } public long getVersion ( ) { return version ; } public static long readCurrentVersion ( Directory directory ) throws IOException { IndexInput input = directory . openInput ( IndexFileNames . SEGMENTS ) ; int format = 0 ; long version = 0 ; try { format = input . readInt ( ) ; if ( format < 0 ) { if ( format < FORMAT ) throw new IOException ( "Unknown format version: " + format ) ; version = input . readLong ( ) ; } } finally { input . close ( ) ; } if ( format < 0 ) return version ; SegmentInfos sis = new SegmentInfos ( ) ; sis . read ( directory ) ; return sis . getVersion ( ) ; } } 	1	['6', '4', '0', '13', '30', '3', '9', '4', '5', '0.6', '216', '0.333333333', '0', '0.94047619', '0.5', '0', '0', '34.5', '1', '0.8333', '7']
package org . apache . lucene . search ; import java . util . Iterator ; import java . util . NoSuchElementException ; public class HitIterator implements Iterator { private Hits hits ; private int hitNumber = 0 ; HitIterator ( Hits hits ) { this . hits = hits ; } public boolean hasNext ( ) { return hitNumber < hits . length ( ) ; } public Object next ( ) { if ( hitNumber == hits . length ( ) ) throw new NoSuchElementException ( ) ; Object next = new Hit ( hits , hitNumber ) ; hitNumber ++ ; return next ; } public void remove ( ) { throw new UnsupportedOperationException ( ) ; } public int length ( ) { return hits . length ( ) ; } } 	0	['5', '1', '0', '2', '10', '0', '1', '2', '4', '0.375', '60', '1', '1', '0', '0.6', '0', '0', '10.6', '2', '1.2', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Explanation ; import org . apache . lucene . search . Query ; import org . apache . lucene . search . Scorer ; import org . apache . lucene . search . Searcher ; import org . apache . lucene . search . Similarity ; import org . apache . lucene . search . Weight ; import org . apache . lucene . util . ToStringUtils ; import java . util . Set ; public class MatchAllDocsQuery extends Query { public MatchAllDocsQuery ( ) { } private class MatchAllScorer extends Scorer { final IndexReader reader ; int id ; final int maxId ; final float score ; MatchAllScorer ( IndexReader reader , Similarity similarity , Weight w ) { super ( similarity ) ; this . reader = reader ; id = - 1 ; maxId = reader . maxDoc ( ) - 1 ; score = w . getValue ( ) ; } public Explanation explain ( int doc ) { return null ; } public int doc ( ) { return id ; } public boolean next ( ) { while ( id < maxId ) { id ++ ; if ( ! reader . isDeleted ( id ) ) { return true ; } } return false ; } public float score ( ) { return score ; } public boolean skipTo ( int target ) { id = target - 1 ; return next ( ) ; } } private class MatchAllDocsWeight implements Weight { private Searcher searcher ; private float queryWeight ; private float queryNorm ; public MatchAllDocsWeight ( Searcher searcher ) { this . searcher = searcher ; } public String toString ( ) { return "weight(" + MatchAllDocsQuery . this + ")" ; } public Query getQuery ( ) { return MatchAllDocsQuery . this ; } public float getValue ( ) { return queryWeight ; } public float sumOfSquaredWeights ( ) { queryWeight = getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float queryNorm ) { this . queryNorm = queryNorm ; queryWeight *= this . queryNorm ; } public Scorer scorer ( IndexReader reader ) { return new MatchAllScorer ( reader , getSimilarity ( searcher ) , this ) ; } public Explanation explain ( IndexReader reader , int doc ) { Explanation queryExpl = new Explanation ( ) ; queryExpl . setDescription ( "MatchAllDocsQuery, product of:" ) ; queryExpl . setValue ( getValue ( ) ) ; if ( getBoost ( ) != 1.0f ) { queryExpl . addDetail ( new Explanation ( getBoost ( ) , "boost" ) ) ; } queryExpl . addDetail ( new Explanation ( queryNorm , "queryNorm" ) ) ; return queryExpl ; } } protected Weight createWeight ( Searcher searcher ) { return new MatchAllDocsWeight ( searcher ) ; } public void extractTerms ( Set terms ) { } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "MatchAllDocsQuery" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof MatchAllDocsQuery ) ) return false ; MatchAllDocsQuery other = ( MatchAllDocsQuery ) o ; return this . getBoost ( ) == other . getBoost ( ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ 0x1AA71190 ; } } 	1	['6', '2', '0', '6', '14', '15', '2', '5', '5', '2', '57', '0', '0', '0.705882353', '0.333333333', '2', '3', '8.5', '3', '1.1667', '1']
package org . apache . lucene . util ; public final class Constants { private Constants ( ) { } public static final String JAVA_VERSION = System . getProperty ( "java.version" ) ; public static final boolean JAVA_1_1 = JAVA_VERSION . startsWith ( "1.1." ) ; public static final boolean JAVA_1_2 = JAVA_VERSION . startsWith ( "1.2." ) ; public static final boolean JAVA_1_3 = JAVA_VERSION . startsWith ( "1.3." ) ; public static final String OS_NAME = System . getProperty ( "os.name" ) ; public static final boolean LINUX = OS_NAME . startsWith ( "Linux" ) ; public static final boolean WINDOWS = OS_NAME . startsWith ( "Windows" ) ; public static final boolean SUN_OS = OS_NAME . startsWith ( "SunOS" ) ; } 	0	['2', '1', '0', '0', '5', '1', '0', '0', '0', '1', '44', '0', '0', '0', '1', '0', '0', '17', '0', '0', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . util . PriorityQueue ; import org . apache . lucene . util . ToStringUtils ; import java . io . IOException ; public final class FuzzyQuery extends MultiTermQuery { public final static float defaultMinSimilarity = 0.5f ; public final static int defaultPrefixLength = 0 ; private float minimumSimilarity ; private int prefixLength ; public FuzzyQuery ( Term term , float minimumSimilarity , int prefixLength ) throws IllegalArgumentException { super ( term ) ; if ( minimumSimilarity >= 1.0f ) throw new IllegalArgumentException ( "minimumSimilarity >= 1" ) ; else if ( minimumSimilarity < 0.0f ) throw new IllegalArgumentException ( "minimumSimilarity < 0" ) ; if ( prefixLength < 0 ) throw new IllegalArgumentException ( "prefixLength < 0" ) ; this . minimumSimilarity = minimumSimilarity ; this . prefixLength = prefixLength ; } public FuzzyQuery ( Term term , float minimumSimilarity ) throws IllegalArgumentException { this ( term , minimumSimilarity , defaultPrefixLength ) ; } public FuzzyQuery ( Term term ) { this ( term , defaultMinSimilarity , defaultPrefixLength ) ; } public float getMinSimilarity ( ) { return minimumSimilarity ; } public int getPrefixLength ( ) { return prefixLength ; } protected FilteredTermEnum getEnum ( IndexReader reader ) throws IOException { return new FuzzyTermEnum ( reader , getTerm ( ) , minimumSimilarity , prefixLength ) ; } public Query rewrite ( IndexReader reader ) throws IOException { FilteredTermEnum enumerator = getEnum ( reader ) ; int maxClauseCount = BooleanQuery . getMaxClauseCount ( ) ; ScoreTermQueue stQueue = new ScoreTermQueue ( maxClauseCount ) ; try { do { float minScore = 0.0f ; float score = 0.0f ; Term t = enumerator . term ( ) ; if ( t != null ) { score = enumerator . difference ( ) ; if ( stQueue . size ( ) < maxClauseCount || score > minScore ) { stQueue . insert ( new ScoreTerm ( t , score ) ) ; minScore = ( ( ScoreTerm ) stQueue . top ( ) ) . score ; } } } while ( enumerator . next ( ) ) ; } finally { enumerator . close ( ) ; } BooleanQuery query = new BooleanQuery ( true ) ; int size = stQueue . size ( ) ; for ( int i = 0 ; i < size ; i ++ ) { ScoreTerm st = ( ScoreTerm ) stQueue . pop ( ) ; TermQuery tq = new TermQuery ( st . term ) ; tq . setBoost ( getBoost ( ) * st . score ) ; query . add ( tq , BooleanClause . Occur . SHOULD ) ; } return query ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; Term term = getTerm ( ) ; if ( ! term . field ( ) . equals ( field ) ) { buffer . append ( term . field ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( term . text ( ) ) ; buffer . append ( '~' ) ; buffer . append ( Float . toString ( minimumSimilarity ) ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } private static class ScoreTerm { public Term term ; public float score ; public ScoreTerm ( Term term , float score ) { this . term = term ; this . score = score ; } } private static class ScoreTermQueue extends PriorityQueue { public ScoreTermQueue ( int size ) { initialize ( size ) ; } protected boolean lessThan ( Object a , Object b ) { ScoreTerm termA = ( ScoreTerm ) a ; ScoreTerm termB = ( ScoreTerm ) b ; if ( termA . score == termB . score ) return termA . term . compareTo ( termB . term ) > 0 ; else return termA . score < termB . score ; } } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof FuzzyQuery ) ) return false ; if ( ! super . equals ( o ) ) return false ; final FuzzyQuery fuzzyQuery = ( FuzzyQuery ) o ; if ( minimumSimilarity != fuzzyQuery . minimumSimilarity ) return false ; if ( prefixLength != fuzzyQuery . prefixLength ) return false ; return true ; } public int hashCode ( ) { int result = super . hashCode ( ) ; result = 29 * result + minimumSimilarity != + 0.0f ? Float . floatToIntBits ( minimumSimilarity ) : 0 ; result = 29 * result + prefixLength ; return result ; } } 	1	['10', '3', '0', '13', '42', '7', '1', '12', '9', '0.638888889', '280', '0.5', '0', '0.72', '0.285714286', '3', '4', '26.6', '6', '1.4', '1']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . util . ToStringUtils ; public abstract class MultiTermQuery extends Query { private Term term ; public MultiTermQuery ( Term term ) { this . term = term ; } public Term getTerm ( ) { return term ; } protected abstract FilteredTermEnum getEnum ( IndexReader reader ) throws IOException ; public Query rewrite ( IndexReader reader ) throws IOException { FilteredTermEnum enumerator = getEnum ( reader ) ; BooleanQuery query = new BooleanQuery ( true ) ; try { do { Term t = enumerator . term ( ) ; if ( t != null ) { TermQuery tq = new TermQuery ( t ) ; tq . setBoost ( getBoost ( ) * enumerator . difference ( ) ) ; query . add ( tq , BooleanClause . Occur . SHOULD ) ; } } while ( enumerator . next ( ) ) ; } finally { enumerator . close ( ) ; } return query ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! term . field ( ) . equals ( field ) ) { buffer . append ( term . field ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( term . text ( ) ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof MultiTermQuery ) ) return false ; final MultiTermQuery multiTermQuery = ( MultiTermQuery ) o ; if ( ! term . equals ( multiTermQuery . term ) ) return false ; return getBoost ( ) == multiTermQuery . getBoost ( ) ; } public int hashCode ( ) { return term . hashCode ( ) + Float . floatToRawIntBits ( getBoost ( ) ) ; } } 	0	['7', '2', '2', '10', '27', '1', '2', '8', '6', '0.333333333', '134', '1', '1', '0.666666667', '0.342857143', '2', '3', '18', '5', '1.5714', '0']
package org . apache . lucene . analysis . standard ; public interface CharStream { char readChar ( ) throws java . io . IOException ; int getEndColumn ( ) ; int getEndLine ( ) ; int getBeginColumn ( ) ; int getBeginLine ( ) ; void backup ( int amount ) ; char BeginToken ( ) throws java . io . IOException ; String GetImage ( ) ; char [ ] GetSuffix ( int len ) ; void Done ( ) ; } 	1	['10', '1', '0', '3', '10', '45', '3', '0', '10', '2', '10', '0', '0', '0', '0.6', '0', '0', '0', '1', '1', '1']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . util . PriorityQueue ; public class ParallelMultiSearcher extends MultiSearcher { private Searchable [ ] searchables ; private int [ ] starts ; public ParallelMultiSearcher ( Searchable [ ] searchables ) throws IOException { super ( searchables ) ; this . searchables = searchables ; this . starts = getStarts ( ) ; } public int docFreq ( Term term ) throws IOException { return super . docFreq ( term ) ; } public TopDocs search ( Weight weight , Filter filter , int nDocs ) throws IOException { HitQueue hq = new HitQueue ( nDocs ) ; int totalHits = 0 ; MultiSearcherThread [ ] msta = new MultiSearcherThread [ searchables . length ] ; for ( int i = 0 ; i < searchables . length ; i ++ ) { msta [ i ] = new MultiSearcherThread ( searchables [ i ] , weight , filter , nDocs , hq , i , starts , "MultiSearcher thread #" + ( i + 1 ) ) ; msta [ i ] . start ( ) ; } for ( int i = 0 ; i < searchables . length ; i ++ ) { try { msta [ i ] . join ( ) ; } catch ( InterruptedException ie ) { ; } IOException ioe = msta [ i ] . getIOException ( ) ; if ( ioe == null ) { totalHits += msta [ i ] . hits ( ) ; } else { throw ioe ; } } ScoreDoc [ ] scoreDocs = new ScoreDoc [ hq . size ( ) ] ; for ( int i = hq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = ( ScoreDoc ) hq . pop ( ) ; float maxScore = ( totalHits == 0 ) ? Float . NEGATIVE_INFINITY : scoreDocs [ 0 ] . score ; return new TopDocs ( totalHits , scoreDocs , maxScore ) ; } public TopFieldDocs search ( Weight weight , Filter filter , int nDocs , Sort sort ) throws IOException { FieldDocSortedHitQueue hq = new FieldDocSortedHitQueue ( null , nDocs ) ; int totalHits = 0 ; MultiSearcherThread [ ] msta = new MultiSearcherThread [ searchables . length ] ; for ( int i = 0 ; i < searchables . length ; i ++ ) { msta [ i ] = new MultiSearcherThread ( searchables [ i ] , weight , filter , nDocs , hq , sort , i , starts , "MultiSearcher thread #" + ( i + 1 ) ) ; msta [ i ] . start ( ) ; } float maxScore = Float . NEGATIVE_INFINITY ; for ( int i = 0 ; i < searchables . length ; i ++ ) { try { msta [ i ] . join ( ) ; } catch ( InterruptedException ie ) { ; } IOException ioe = msta [ i ] . getIOException ( ) ; if ( ioe == null ) { totalHits += msta [ i ] . hits ( ) ; maxScore = Math . max ( maxScore , msta [ i ] . getMaxScore ( ) ) ; } else { throw ioe ; } } ScoreDoc [ ] scoreDocs = new ScoreDoc [ hq . size ( ) ] ; for ( int i = hq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = ( ScoreDoc ) hq . pop ( ) ; return new TopFieldDocs ( totalHits , scoreDocs , hq . getFields ( ) , maxScore ) ; } public void search ( Weight weight , Filter filter , final HitCollector results ) throws IOException { for ( int i = 0 ; i < searchables . length ; i ++ ) { final int start = starts [ i ] ; searchables [ i ] . search ( weight , filter , new HitCollector ( ) { public void collect ( int doc , float score ) { results . collect ( doc + start , score ) ; } } ) ; } } public Query rewrite ( Query original ) throws IOException { return super . rewrite ( original ) ; } } class MultiSearcherThread extends Thread { private Searchable searchable ; private Weight weight ; private Filter filter ; private int nDocs ; private TopDocs docs ; private int i ; private PriorityQueue hq ; private int [ ] starts ; private IOException ioe ; private Sort sort ; public MultiSearcherThread ( Searchable searchable , Weight weight , Filter filter , int nDocs , HitQueue hq , int i , int [ ] starts , String name ) { super ( name ) ; this . searchable = searchable ; this . weight = weight ; this . filter = filter ; this . nDocs = nDocs ; this . hq = hq ; this . i = i ; this . starts = starts ; } public MultiSearcherThread ( Searchable searchable , Weight weight , Filter filter , int nDocs , FieldDocSortedHitQueue hq , Sort sort , int i , int [ ] starts , String name ) { super ( name ) ; this . searchable = searchable ; this . weight = weight ; this . filter = filter ; this . nDocs = nDocs ; this . hq = hq ; this . i = i ; this . starts = starts ; this . sort = sort ; } public void run ( ) { try { docs = ( sort == null ) ? searchable . search ( weight , filter , nDocs ) : searchable . search ( weight , filter , nDocs , sort ) ; } catch ( IOException ioe ) { this . ioe = ioe ; } if ( ioe == null ) { if ( sort != null ) { ( ( FieldDocSortedHitQueue ) hq ) . setFields ( ( ( TopFieldDocs ) docs ) . fields ) ; } ScoreDoc [ ] scoreDocs = docs . scoreDocs ; for ( int j = 0 ; j < scoreDocs . length ; j ++ ) { ScoreDoc scoreDoc = scoreDocs [ j ] ; scoreDoc . doc += starts [ i ] ; synchronized ( hq ) { if ( ! hq . insert ( scoreDoc ) ) break ; } } } } public int hits ( ) { return docs . totalHits ; } public float getMaxScore ( ) { return docs . getMaxScore ( ) ; } public IOException getIOException ( ) { return ioe ; } } 	0	['6', '3', '0', '16', '33', '3', '1', '16', '6', '0.4', '297', '1', '1', '0.87804878', '0.351851852', '2', '3', '48.16666667', '1', '0.8333', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermDocs ; import org . apache . lucene . index . TermEnum ; import org . apache . lucene . search . FieldCache . StringIndex ; import java . io . IOException ; import java . util . Locale ; import java . util . Map ; import java . util . WeakHashMap ; import java . util . HashMap ; class FieldCacheImpl implements FieldCache { static class Entry { final String field ; final int type ; final Object custom ; final Locale locale ; Entry ( String field , int type , Locale locale ) { this . field = field . intern ( ) ; this . type = type ; this . custom = null ; this . locale = locale ; } Entry ( String field , Object custom ) { this . field = field . intern ( ) ; this . type = SortField . CUSTOM ; this . custom = custom ; this . locale = null ; } public boolean equals ( Object o ) { if ( o instanceof Entry ) { Entry other = ( Entry ) o ; if ( other . field == field && other . type == type ) { if ( other . locale == null ? locale == null : other . locale . equals ( locale ) ) { if ( other . custom == null ) { if ( custom == null ) return true ; } else if ( other . custom . equals ( custom ) ) { return true ; } } } } return false ; } public int hashCode ( ) { return field . hashCode ( ) ^ type ^ ( custom == null ? 0 : custom . hashCode ( ) ) ^ ( locale == null ? 0 : locale . hashCode ( ) ) ; } } private static final IntParser INT_PARSER = new IntParser ( ) { public int parseInt ( String value ) { return Integer . parseInt ( value ) ; } } ; private static final FloatParser FLOAT_PARSER = new FloatParser ( ) { public float parseFloat ( String value ) { return Float . parseFloat ( value ) ; } } ; final Map cache = new WeakHashMap ( ) ; Object lookup ( IndexReader reader , String field , int type , Locale locale ) { Entry entry = new Entry ( field , type , locale ) ; synchronized ( this ) { HashMap readerCache = ( HashMap ) cache . get ( reader ) ; if ( readerCache == null ) return null ; return readerCache . get ( entry ) ; } } Object lookup ( IndexReader reader , String field , Object comparer ) { Entry entry = new Entry ( field , comparer ) ; synchronized ( this ) { HashMap readerCache = ( HashMap ) cache . get ( reader ) ; if ( readerCache == null ) return null ; return readerCache . get ( entry ) ; } } Object store ( IndexReader reader , String field , int type , Locale locale , Object value ) { Entry entry = new Entry ( field , type , locale ) ; synchronized ( this ) { HashMap readerCache = ( HashMap ) cache . get ( reader ) ; if ( readerCache == null ) { readerCache = new HashMap ( ) ; cache . put ( reader , readerCache ) ; } return readerCache . put ( entry , value ) ; } } Object store ( IndexReader reader , String field , Object comparer , Object value ) { Entry entry = new Entry ( field , comparer ) ; synchronized ( this ) { HashMap readerCache = ( HashMap ) cache . get ( reader ) ; if ( readerCache == null ) { readerCache = new HashMap ( ) ; cache . put ( reader , readerCache ) ; } return readerCache . put ( entry , value ) ; } } public int [ ] getInts ( IndexReader reader , String field ) throws IOException { return getInts ( reader , field , INT_PARSER ) ; } public int [ ] getInts ( IndexReader reader , String field , IntParser parser ) throws IOException { field = field . intern ( ) ; Object ret = lookup ( reader , field , parser ) ; if ( ret == null ) { final int [ ] retArray = new int [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field , "" ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; int termval = parser . parseInt ( term . text ( ) ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } store ( reader , field , parser , retArray ) ; return retArray ; } return ( int [ ] ) ret ; } public float [ ] getFloats ( IndexReader reader , String field ) throws IOException { return getFloats ( reader , field , FLOAT_PARSER ) ; } public float [ ] getFloats ( IndexReader reader , String field , FloatParser parser ) throws IOException { field = field . intern ( ) ; Object ret = lookup ( reader , field , parser ) ; if ( ret == null ) { final float [ ] retArray = new float [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field , "" ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; float termval = parser . parseFloat ( term . text ( ) ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } store ( reader , field , parser , retArray ) ; return retArray ; } return ( float [ ] ) ret ; } public String [ ] getStrings ( IndexReader reader , String field ) throws IOException { field = field . intern ( ) ; Object ret = lookup ( reader , field , SortField . STRING , null ) ; if ( ret == null ) { final String [ ] retArray = new String [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field , "" ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; String termval = term . text ( ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } store ( reader , field , SortField . STRING , null , retArray ) ; return retArray ; } return ( String [ ] ) ret ; } public StringIndex getStringIndex ( IndexReader reader , String field ) throws IOException { field = field . intern ( ) ; Object ret = lookup ( reader , field , STRING_INDEX , null ) ; if ( ret == null ) { final int [ ] retArray = new int [ reader . maxDoc ( ) ] ; String [ ] mterms = new String [ reader . maxDoc ( ) + 1 ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field , "" ) ) ; int t = 0 ; mterms [ t ++ ] = null ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; if ( t >= mterms . length ) throw new RuntimeException ( "there are more terms than " + "documents in field \"" + field + "\", but it's impossible to sort on " + "tokenized fields" ) ; mterms [ t ] = term . text ( ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = t ; } t ++ ; } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } if ( t == 0 ) { mterms = new String [ 1 ] ; } else if ( t < mterms . length ) { String [ ] terms = new String [ t ] ; System . arraycopy ( mterms , 0 , terms , 0 , t ) ; mterms = terms ; } StringIndex value = new StringIndex ( retArray , mterms ) ; store ( reader , field , STRING_INDEX , null , value ) ; return value ; } return ( StringIndex ) ret ; } public Object getAuto ( IndexReader reader , String field ) throws IOException { field = field . intern ( ) ; Object ret = lookup ( reader , field , SortField . AUTO , null ) ; if ( ret == null ) { TermEnum enumerator = reader . terms ( new Term ( field , "" ) ) ; try { Term term = enumerator . term ( ) ; if ( term == null ) { throw new RuntimeException ( "no terms in field " + field + " - cannot determine sort type" ) ; } if ( term . field ( ) == field ) { String termtext = term . text ( ) . trim ( ) ; try { Integer . parseInt ( termtext ) ; ret = getInts ( reader , field ) ; } catch ( NumberFormatException nfe1 ) { try { Float . parseFloat ( termtext ) ; ret = getFloats ( reader , field ) ; } catch ( NumberFormatException nfe2 ) { ret = getStringIndex ( reader , field ) ; } } if ( ret != null ) { store ( reader , field , SortField . AUTO , null , ret ) ; } } else { throw new RuntimeException ( "field \"" + field + "\" does not appear to be indexed" ) ; } } finally { enumerator . close ( ) ; } } return ret ; } public Comparable [ ] getCustom ( IndexReader reader , String field , SortComparator comparator ) throws IOException { field = field . intern ( ) ; Object ret = lookup ( reader , field , comparator ) ; if ( ret == null ) { final Comparable [ ] retArray = new Comparable [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field , "" ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; Comparable termval = comparator . getComparable ( term . text ( ) ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } store ( reader , field , comparator , retArray ) ; return retArray ; } return ( Comparable [ ] ) ret ; } } 	1	['14', '1', '0', '12', '51', '67', '1', '12', '8', '0.641025641', '764', '0.666666667', '2', '0', '0.401709402', '0', '0', '53.35714286', '2', '1.1429', '2']
package org . apache . lucene . util ; public class SmallFloat { public static byte floatToByte ( float f , int numMantissaBits , int zeroExp ) { int fzero = ( 63 - zeroExp ) << numMantissaBits ; int bits = Float . floatToRawIntBits ( f ) ; int smallfloat = bits > > ( 24 - numMantissaBits ) ; if ( smallfloat < fzero ) { return ( bits <= 0 ) ? ( byte ) 0 : ( byte ) 1 ; } else if ( smallfloat >= fzero + 0x100 ) { return - 1 ; } else { return ( byte ) ( smallfloat - fzero ) ; } } public static float byteToFloat ( byte b , int numMantissaBits , int zeroExp ) { if ( b == 0 ) return 0.0f ; int bits = ( b & 0xff ) << ( 24 - numMantissaBits ) ; bits += ( 63 - zeroExp ) << 24 ; return Float . intBitsToFloat ( bits ) ; } public static byte floatToByte315 ( float f ) { int bits = Float . floatToRawIntBits ( f ) ; int smallfloat = bits > > ( 24 - 3 ) ; if ( smallfloat < ( 63 - 15 ) << 3 ) { return ( bits <= 0 ) ? ( byte ) 0 : ( byte ) 1 ; } if ( smallfloat >= ( ( 63 - 15 ) << 3 ) + 0x100 ) { return - 1 ; } return ( byte ) ( smallfloat - ( ( 63 - 15 ) << 3 ) ) ; } public static float byte315ToFloat ( byte b ) { if ( b == 0 ) return 0.0f ; int bits = ( b & 0xff ) << ( 24 - 3 ) ; bits += ( 63 - 15 ) << 24 ; return Float . intBitsToFloat ( bits ) ; } public static byte floatToByte52 ( float f ) { int bits = Float . floatToRawIntBits ( f ) ; int smallfloat = bits > > ( 24 - 5 ) ; if ( smallfloat < ( 63 - 2 ) << 5 ) { return ( bits <= 0 ) ? ( byte ) 0 : ( byte ) 1 ; } if ( smallfloat >= ( ( 63 - 2 ) << 5 ) + 0x100 ) { return - 1 ; } return ( byte ) ( smallfloat - ( ( 63 - 2 ) << 5 ) ) ; } public static float byte52ToFloat ( byte b ) { if ( b == 0 ) return 0.0f ; int bits = ( b & 0xff ) << ( 24 - 5 ) ; bits += ( 63 - 2 ) << 24 ; return Float . intBitsToFloat ( bits ) ; } } 	0	['7', '1', '0', '1', '10', '21', '1', '0', '7', '2', '155', '0', '0', '0', '0.321428571', '0', '0', '21.14285714', '4', '2.5714', '0']
package org . apache . lucene . index ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . document . Document ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . FSDirectory ; import java . io . File ; import java . io . IOException ; import java . io . PrintStream ; public class IndexModifier { protected IndexWriter indexWriter = null ; protected IndexReader indexReader = null ; protected Directory directory = null ; protected Analyzer analyzer = null ; protected boolean open = false ; protected PrintStream infoStream = null ; protected boolean useCompoundFile = true ; protected int maxBufferedDocs = IndexWriter . DEFAULT_MAX_BUFFERED_DOCS ; protected int maxFieldLength = IndexWriter . DEFAULT_MAX_FIELD_LENGTH ; protected int mergeFactor = IndexWriter . DEFAULT_MERGE_FACTOR ; public IndexModifier ( Directory directory , Analyzer analyzer , boolean create ) throws IOException { init ( directory , analyzer , create ) ; } public IndexModifier ( String dirName , Analyzer analyzer , boolean create ) throws IOException { Directory dir = FSDirectory . getDirectory ( dirName , create ) ; init ( dir , analyzer , create ) ; } public IndexModifier ( File file , Analyzer analyzer , boolean create ) throws IOException { Directory dir = FSDirectory . getDirectory ( file , create ) ; init ( dir , analyzer , create ) ; } protected void init ( Directory directory , Analyzer analyzer , boolean create ) throws IOException { this . directory = directory ; synchronized ( this . directory ) { this . analyzer = analyzer ; indexWriter = new IndexWriter ( directory , analyzer , create ) ; open = true ; } } protected void assureOpen ( ) { if ( ! open ) { throw new IllegalStateException ( "Index is closed" ) ; } } protected void createIndexWriter ( ) throws IOException { if ( indexWriter == null ) { if ( indexReader != null ) { indexReader . close ( ) ; indexReader = null ; } indexWriter = new IndexWriter ( directory , analyzer , false ) ; indexWriter . setInfoStream ( infoStream ) ; indexWriter . setUseCompoundFile ( useCompoundFile ) ; indexWriter . setMaxBufferedDocs ( maxBufferedDocs ) ; indexWriter . setMaxFieldLength ( maxFieldLength ) ; indexWriter . setMergeFactor ( mergeFactor ) ; } } protected void createIndexReader ( ) throws IOException { if ( indexReader == null ) { if ( indexWriter != null ) { indexWriter . close ( ) ; indexWriter = null ; } indexReader = IndexReader . open ( directory ) ; } } public void flush ( ) throws IOException { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . close ( ) ; indexWriter = null ; createIndexWriter ( ) ; } else { indexReader . close ( ) ; indexReader = null ; createIndexReader ( ) ; } } } public void addDocument ( Document doc , Analyzer docAnalyzer ) throws IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; if ( docAnalyzer != null ) indexWriter . addDocument ( doc , docAnalyzer ) ; else indexWriter . addDocument ( doc ) ; } } public void addDocument ( Document doc ) throws IOException { addDocument ( doc , null ) ; } public int deleteDocuments ( Term term ) throws IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexReader ( ) ; return indexReader . deleteDocuments ( term ) ; } } public void deleteDocument ( int docNum ) throws IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexReader ( ) ; indexReader . deleteDocument ( docNum ) ; } } public int docCount ( ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { return indexWriter . docCount ( ) ; } else { return indexReader . numDocs ( ) ; } } } public void optimize ( ) throws IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; indexWriter . optimize ( ) ; } } public void setInfoStream ( PrintStream infoStream ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . setInfoStream ( infoStream ) ; } this . infoStream = infoStream ; } } public PrintStream getInfoStream ( ) throws IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; return indexWriter . getInfoStream ( ) ; } } public void setUseCompoundFile ( boolean useCompoundFile ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . setUseCompoundFile ( useCompoundFile ) ; } this . useCompoundFile = useCompoundFile ; } } public boolean getUseCompoundFile ( ) throws IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; return indexWriter . getUseCompoundFile ( ) ; } } public void setMaxFieldLength ( int maxFieldLength ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . setMaxFieldLength ( maxFieldLength ) ; } this . maxFieldLength = maxFieldLength ; } } public int getMaxFieldLength ( ) throws IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; return indexWriter . getMaxFieldLength ( ) ; } } public void setMaxBufferedDocs ( int maxBufferedDocs ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . setMaxBufferedDocs ( maxBufferedDocs ) ; } this . maxBufferedDocs = maxBufferedDocs ; } } public int getMaxBufferedDocs ( ) throws IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; return indexWriter . getMaxBufferedDocs ( ) ; } } public void setMergeFactor ( int mergeFactor ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . setMergeFactor ( mergeFactor ) ; } this . mergeFactor = mergeFactor ; } } public int getMergeFactor ( ) throws IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; return indexWriter . getMergeFactor ( ) ; } } public void close ( ) throws IOException { synchronized ( directory ) { if ( ! open ) throw new IllegalStateException ( "Index is closed already" ) ; if ( indexWriter != null ) { indexWriter . close ( ) ; indexWriter = null ; } else { indexReader . close ( ) ; indexReader = null ; } open = false ; } } public String toString ( ) { return "Index@" + directory ; } } 	1	['26', '1', '0', '7', '55', '0', '0', '7', '22', '0.344', '707', '1', '4', '0', '0.184615385', '0', '0', '25.80769231', '2', '1.1538', '2']
package org . apache . lucene . analysis ; import java . io . IOException ; import java . io . Reader ; public class KeywordTokenizer extends Tokenizer { private static final int DEFAULT_BUFFER_SIZE = 256 ; private boolean done ; private final char [ ] buffer ; public KeywordTokenizer ( Reader input ) { this ( input , DEFAULT_BUFFER_SIZE ) ; } public KeywordTokenizer ( Reader input , int bufferSize ) { super ( input ) ; this . buffer = new char [ bufferSize ] ; this . done = false ; } public Token next ( ) throws IOException { if ( ! done ) { done = true ; StringBuffer buffer = new StringBuffer ( ) ; int length ; while ( true ) { length = input . read ( this . buffer ) ; if ( length == - 1 ) break ; buffer . append ( this . buffer , 0 , length ) ; } String text = buffer . toString ( ) ; return new Token ( text , 0 , text . length ( ) ) ; } return null ; } } 	0	['3', '3', '0', '3', '10', '1', '1', '2', '3', '0.5', '63', '1', '0', '0.75', '0.666666667', '0', '0', '19', '1', '0.3333', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexInput ; final class SegmentTermEnum extends TermEnum implements Cloneable { private IndexInput input ; FieldInfos fieldInfos ; long size ; long position = - 1 ; private TermBuffer termBuffer = new TermBuffer ( ) ; private TermBuffer prevBuffer = new TermBuffer ( ) ; private TermBuffer scratch ; private TermInfo termInfo = new TermInfo ( ) ; private int format ; private boolean isIndex = false ; long indexPointer = 0 ; int indexInterval ; int skipInterval ; private int formatM1SkipInterval ; SegmentTermEnum ( IndexInput i , FieldInfos fis , boolean isi ) throws IOException { input = i ; fieldInfos = fis ; isIndex = isi ; int firstInt = input . readInt ( ) ; if ( firstInt >= 0 ) { format = 0 ; size = firstInt ; indexInterval = 128 ; skipInterval = Integer . MAX_VALUE ; } else { format = firstInt ; if ( format < TermInfosWriter . FORMAT ) throw new IOException ( "Unknown format version:" + format ) ; size = input . readLong ( ) ; if ( format == - 1 ) { if ( ! isIndex ) { indexInterval = input . readInt ( ) ; formatM1SkipInterval = input . readInt ( ) ; } skipInterval = Integer . MAX_VALUE ; } else { indexInterval = input . readInt ( ) ; skipInterval = input . readInt ( ) ; } } } protected Object clone ( ) { SegmentTermEnum clone = null ; try { clone = ( SegmentTermEnum ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . input = ( IndexInput ) input . clone ( ) ; clone . termInfo = new TermInfo ( termInfo ) ; clone . termBuffer = ( TermBuffer ) termBuffer . clone ( ) ; clone . prevBuffer = ( TermBuffer ) prevBuffer . clone ( ) ; clone . scratch = null ; return clone ; } final void seek ( long pointer , int p , Term t , TermInfo ti ) throws IOException { input . seek ( pointer ) ; position = p ; termBuffer . set ( t ) ; prevBuffer . reset ( ) ; termInfo . set ( ti ) ; } public final boolean next ( ) throws IOException { if ( position ++ >= size - 1 ) { termBuffer . reset ( ) ; return false ; } prevBuffer . set ( termBuffer ) ; termBuffer . read ( input , fieldInfos ) ; termInfo . docFreq = input . readVInt ( ) ; termInfo . freqPointer += input . readVLong ( ) ; termInfo . proxPointer += input . readVLong ( ) ; if ( format == - 1 ) { if ( ! isIndex ) { if ( termInfo . docFreq > formatM1SkipInterval ) { termInfo . skipOffset = input . readVInt ( ) ; } } } else { if ( termInfo . docFreq >= skipInterval ) termInfo . skipOffset = input . readVInt ( ) ; } if ( isIndex ) indexPointer += input . readVLong ( ) ; return true ; } final void scanTo ( Term term ) throws IOException { if ( scratch == null ) scratch = new TermBuffer ( ) ; scratch . set ( term ) ; while ( scratch . compareTo ( termBuffer ) > 0 && next ( ) ) { } } public final Term term ( ) { return termBuffer . toTerm ( ) ; } final Term prev ( ) { return prevBuffer . toTerm ( ) ; } final TermInfo termInfo ( ) { return new TermInfo ( termInfo ) ; } final void termInfo ( TermInfo ti ) { ti . set ( termInfo ) ; } public final int docFreq ( ) { return termInfo . docFreq ; } final long freqPointer ( ) { return termInfo . freqPointer ; } final long proxPointer ( ) { return termInfo . proxPointer ; } public final void close ( ) throws IOException { input . close ( ) ; } } 	1	['13', '2', '0', '9', '38', '0', '3', '6', '4', '0.75', '354', '0.571428571', '6', '0.294117647', '0.211538462', '1', '2', '25.15384615', '1', '0.9231', '2']
package org . apache . lucene . index ; public interface TermFreqVector { public String getField ( ) ; public int size ( ) ; public String [ ] getTerms ( ) ; public int [ ] getTermFrequencies ( ) ; public int indexOf ( String term ) ; public int [ ] indexesOf ( String [ ] terms , int start , int len ) ; } 	0	['6', '1', '0', '11', '6', '15', '11', '0', '6', '2', '6', '0', '0', '0', '0.375', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . document . Document ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import java . io . IOException ; public interface Searchable extends java . rmi . Remote { void search ( Weight weight , Filter filter , HitCollector results ) throws IOException ; void close ( ) throws IOException ; int docFreq ( Term term ) throws IOException ; int [ ] docFreqs ( Term [ ] terms ) throws IOException ; int maxDoc ( ) throws IOException ; TopDocs search ( Weight weight , Filter filter , int n ) throws IOException ; Document doc ( int i ) throws IOException ; Query rewrite ( Query query ) throws IOException ; Explanation explain ( Weight weight , int doc ) throws IOException ; TopFieldDocs search ( Weight weight , Filter filter , int n , Sort sort ) throws IOException ; } 	1	['10', '1', '0', '16', '10', '45', '6', '10', '10', '2', '10', '0', '0', '0', '0.288888889', '0', '0', '0', '1', '1', '2']
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; public class TopDocCollector extends HitCollector { private int numHits ; private float minScore = 0.0f ; int totalHits ; PriorityQueue hq ; public TopDocCollector ( int numHits ) { this ( numHits , new HitQueue ( numHits ) ) ; } TopDocCollector ( int numHits , PriorityQueue hq ) { this . numHits = numHits ; this . hq = hq ; } public void collect ( int doc , float score ) { if ( score > 0.0f ) { totalHits ++ ; if ( hq . size ( ) < numHits || score >= minScore ) { hq . insert ( new ScoreDoc ( doc , score ) ) ; minScore = ( ( ScoreDoc ) hq . top ( ) ) . score ; } } } public int getTotalHits ( ) { return totalHits ; } public TopDocs topDocs ( ) { ScoreDoc [ ] scoreDocs = new ScoreDoc [ hq . size ( ) ] ; for ( int i = hq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = ( ScoreDoc ) hq . pop ( ) ; float maxScore = ( totalHits == 0 ) ? Float . NEGATIVE_INFINITY : scoreDocs [ 0 ] . score ; return new TopDocs ( totalHits , scoreDocs , maxScore ) ; } } 	0	['5', '2', '1', '7', '13', '0', '2', '5', '4', '0.4375', '110', '0.5', '1', '0.25', '0.5', '0', '0', '20.2', '4', '1.6', '0']
package org . apache . lucene . index ; public class SegmentTermPositionVector extends SegmentTermVector implements TermPositionVector { protected int [ ] [ ] positions ; protected TermVectorOffsetInfo [ ] [ ] offsets ; public static final int [ ] EMPTY_TERM_POS = new int [ 0 ] ; public SegmentTermPositionVector ( String field , String terms [ ] , int termFreqs [ ] , int [ ] [ ] positions , TermVectorOffsetInfo [ ] [ ] offsets ) { super ( field , terms , termFreqs ) ; this . offsets = offsets ; this . positions = positions ; } public TermVectorOffsetInfo [ ] getOffsets ( int index ) { TermVectorOffsetInfo [ ] result = TermVectorOffsetInfo . EMPTY_OFFSET_INFO ; if ( offsets == null ) return null ; if ( index >= 0 && index < offsets . length ) { result = offsets [ index ] ; } return result ; } public int [ ] getTermPositions ( int index ) { int [ ] result = EMPTY_TERM_POS ; if ( positions == null ) return null ; if ( index >= 0 && index < positions . length ) { result = positions [ index ] ; } return result ; } } 	1	['4', '2', '0', '4', '5', '0', '1', '3', '3', '0.666666667', '65', '0.666666667', '1', '0.777777778', '0.476190476', '0', '0', '14.5', '4', '2', '1']
package org . apache . lucene . analysis ; import java . io . Reader ; public class LetterTokenizer extends CharTokenizer { public LetterTokenizer ( Reader in ) { super ( in ) ; } protected boolean isTokenChar ( char c ) { return Character . isLetter ( c ) ; } } 	0	['2', '4', '1', '2', '4', '1', '1', '1', '1', '2', '9', '0', '0', '0.857142857', '0.666666667', '1', '1', '3.5', '1', '0.5', '0']
package org . apache . lucene . index ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . Field ; import org . apache . lucene . store . Directory ; import java . io . IOException ; import java . util . * ; public class MultiReader extends IndexReader { private IndexReader [ ] subReaders ; private int [ ] starts ; private Hashtable normsCache = new Hashtable ( ) ; private int maxDoc = 0 ; private int numDocs = - 1 ; private boolean hasDeletions = false ; public MultiReader ( IndexReader [ ] subReaders ) throws IOException { super ( subReaders . length == 0 ? null : subReaders [ 0 ] . directory ( ) ) ; initialize ( subReaders ) ; } MultiReader ( Directory directory , SegmentInfos sis , boolean closeDirectory , IndexReader [ ] subReaders ) { super ( directory , sis , closeDirectory ) ; initialize ( subReaders ) ; } private void initialize ( IndexReader [ ] subReaders ) { this . subReaders = subReaders ; starts = new int [ subReaders . length + 1 ] ; for ( int i = 0 ; i < subReaders . length ; i ++ ) { starts [ i ] = maxDoc ; maxDoc += subReaders [ i ] . maxDoc ( ) ; if ( subReaders [ i ] . hasDeletions ( ) ) hasDeletions = true ; } starts [ subReaders . length ] = maxDoc ; } public TermFreqVector [ ] getTermFreqVectors ( int n ) throws IOException { int i = readerIndex ( n ) ; return subReaders [ i ] . getTermFreqVectors ( n - starts [ i ] ) ; } public TermFreqVector getTermFreqVector ( int n , String field ) throws IOException { int i = readerIndex ( n ) ; return subReaders [ i ] . getTermFreqVector ( n - starts [ i ] , field ) ; } public synchronized int numDocs ( ) { if ( numDocs == - 1 ) { int n = 0 ; for ( int i = 0 ; i < subReaders . length ; i ++ ) n += subReaders [ i ] . numDocs ( ) ; numDocs = n ; } return numDocs ; } public int maxDoc ( ) { return maxDoc ; } public Document document ( int n ) throws IOException { int i = readerIndex ( n ) ; return subReaders [ i ] . document ( n - starts [ i ] ) ; } public boolean isDeleted ( int n ) { int i = readerIndex ( n ) ; return subReaders [ i ] . isDeleted ( n - starts [ i ] ) ; } public boolean hasDeletions ( ) { return hasDeletions ; } protected void doDelete ( int n ) throws IOException { numDocs = - 1 ; int i = readerIndex ( n ) ; subReaders [ i ] . deleteDocument ( n - starts [ i ] ) ; hasDeletions = true ; } protected void doUndeleteAll ( ) throws IOException { for ( int i = 0 ; i < subReaders . length ; i ++ ) subReaders [ i ] . undeleteAll ( ) ; hasDeletions = false ; numDocs = - 1 ; } private int readerIndex ( int n ) { int lo = 0 ; int hi = subReaders . length - 1 ; while ( hi >= lo ) { int mid = ( lo + hi ) > > 1 ; int midValue = starts [ mid ] ; if ( n < midValue ) hi = mid - 1 ; else if ( n > midValue ) lo = mid + 1 ; else { while ( mid + 1 < subReaders . length && starts [ mid + 1 ] == midValue ) { mid ++ ; } return mid ; } } return hi ; } public boolean hasNorms ( String field ) throws IOException { for ( int i = 0 ; i < subReaders . length ; i ++ ) { if ( subReaders [ i ] . hasNorms ( field ) ) return true ; } return false ; } private byte [ ] ones ; private byte [ ] fakeNorms ( ) { if ( ones == null ) ones = SegmentReader . createFakeNorms ( maxDoc ( ) ) ; return ones ; } public synchronized byte [ ] norms ( String field ) throws IOException { byte [ ] bytes = ( byte [ ] ) normsCache . get ( field ) ; if ( bytes != null ) return bytes ; if ( ! hasNorms ( field ) ) return fakeNorms ( ) ; bytes = new byte [ maxDoc ( ) ] ; for ( int i = 0 ; i < subReaders . length ; i ++ ) subReaders [ i ] . norms ( field , bytes , starts [ i ] ) ; normsCache . put ( field , bytes ) ; return bytes ; } public synchronized void norms ( String field , byte [ ] result , int offset ) throws IOException { byte [ ] bytes = ( byte [ ] ) normsCache . get ( field ) ; if ( bytes == null && ! hasNorms ( field ) ) bytes = fakeNorms ( ) ; if ( bytes != null ) System . arraycopy ( bytes , 0 , result , offset , maxDoc ( ) ) ; for ( int i = 0 ; i < subReaders . length ; i ++ ) subReaders [ i ] . norms ( field , result , offset + starts [ i ] ) ; } protected void doSetNorm ( int n , String field , byte value ) throws IOException { normsCache . remove ( field ) ; int i = readerIndex ( n ) ; subReaders [ i ] . setNorm ( n - starts [ i ] , field , value ) ; } public TermEnum terms ( ) throws IOException { return new MultiTermEnum ( subReaders , starts , null ) ; } public TermEnum terms ( Term term ) throws IOException { return new MultiTermEnum ( subReaders , starts , term ) ; } public int docFreq ( Term t ) throws IOException { int total = 0 ; for ( int i = 0 ; i < subReaders . length ; i ++ ) total += subReaders [ i ] . docFreq ( t ) ; return total ; } public TermDocs termDocs ( ) throws IOException { return new MultiTermDocs ( subReaders , starts ) ; } public TermPositions termPositions ( ) throws IOException { return new MultiTermPositions ( subReaders , starts ) ; } protected void doCommit ( ) throws IOException { for ( int i = 0 ; i < subReaders . length ; i ++ ) subReaders [ i ] . commit ( ) ; } protected synchronized void doClose ( ) throws IOException { for ( int i = 0 ; i < subReaders . length ; i ++ ) subReaders [ i ] . close ( ) ; } public Collection getFieldNames ( IndexReader . FieldOption fieldNames ) { Set fieldSet = new HashSet ( ) ; for ( int i = 0 ; i < subReaders . length ; i ++ ) { IndexReader reader = subReaders [ i ] ; Collection names = reader . getFieldNames ( fieldNames ) ; fieldSet . addAll ( names ) ; } return fieldSet ; } } class MultiTermEnum extends TermEnum { private SegmentMergeQueue queue ; private Term term ; private int docFreq ; public MultiTermEnum ( IndexReader [ ] readers , int [ ] starts , Term t ) throws IOException { queue = new SegmentMergeQueue ( readers . length ) ; for ( int i = 0 ; i < readers . length ; i ++ ) { IndexReader reader = readers [ i ] ; TermEnum termEnum ; if ( t != null ) { termEnum = reader . terms ( t ) ; } else termEnum = reader . terms ( ) ; SegmentMergeInfo smi = new SegmentMergeInfo ( starts [ i ] , termEnum , reader ) ; if ( t == null ? smi . next ( ) : termEnum . term ( ) != null ) queue . put ( smi ) ; else smi . close ( ) ; } if ( t != null && queue . size ( ) > 0 ) { next ( ) ; } } public boolean next ( ) throws IOException { SegmentMergeInfo top = ( SegmentMergeInfo ) queue . top ( ) ; if ( top == null ) { term = null ; return false ; } term = top . term ; docFreq = 0 ; while ( top != null && term . compareTo ( top . term ) == 0 ) { queue . pop ( ) ; docFreq += top . termEnum . docFreq ( ) ; if ( top . next ( ) ) queue . put ( top ) ; else top . close ( ) ; top = ( SegmentMergeInfo ) queue . top ( ) ; } return true ; } public Term term ( ) { return term ; } public int docFreq ( ) { return docFreq ; } public void close ( ) throws IOException { queue . close ( ) ; } } class MultiTermDocs implements TermDocs { protected IndexReader [ ] readers ; protected int [ ] starts ; protected Term term ; protected int base = 0 ; protected int pointer = 0 ; private TermDocs [ ] readerTermDocs ; protected TermDocs current ; public MultiTermDocs ( IndexReader [ ] r , int [ ] s ) { readers = r ; starts = s ; readerTermDocs = new TermDocs [ r . length ] ; } public int doc ( ) { return base + current . doc ( ) ; } public int freq ( ) { return current . freq ( ) ; } public void seek ( Term term ) { this . term = term ; this . base = 0 ; this . pointer = 0 ; this . current = null ; } public void seek ( TermEnum termEnum ) throws IOException { seek ( termEnum . term ( ) ) ; } public boolean next ( ) throws IOException { if ( current != null && current . next ( ) ) { return true ; } else if ( pointer < readers . length ) { base = starts [ pointer ] ; current = termDocs ( pointer ++ ) ; return next ( ) ; } else return false ; } public int read ( final int [ ] docs , final int [ ] freqs ) throws IOException { while ( true ) { while ( current == null ) { if ( pointer < readers . length ) { base = starts [ pointer ] ; current = termDocs ( pointer ++ ) ; } else { return 0 ; } } int end = current . read ( docs , freqs ) ; if ( end == 0 ) { current = null ; } else { final int b = base ; for ( int i = 0 ; i < end ; i ++ ) docs [ i ] += b ; return end ; } } } public boolean skipTo ( int target ) throws IOException { do { if ( ! next ( ) ) return false ; } while ( target > doc ( ) ) ; return true ; } private TermDocs termDocs ( int i ) throws IOException { if ( term == null ) return null ; TermDocs result = readerTermDocs [ i ] ; if ( result == null ) result = readerTermDocs [ i ] = termDocs ( readers [ i ] ) ; result . seek ( term ) ; return result ; } protected TermDocs termDocs ( IndexReader reader ) throws IOException { return reader . termDocs ( ) ; } public void close ( ) throws IOException { for ( int i = 0 ; i < readerTermDocs . length ; i ++ ) { if ( readerTermDocs [ i ] != null ) readerTermDocs [ i ] . close ( ) ; } } } class MultiTermPositions extends MultiTermDocs implements TermPositions { public MultiTermPositions ( IndexReader [ ] r , int [ ] s ) { super ( r , s ) ; } protected TermDocs termDocs ( IndexReader reader ) throws IOException { return ( TermDocs ) reader . termPositions ( ) ; } public int nextPosition ( ) throws IOException { return ( ( TermPositions ) current ) . nextPosition ( ) ; } } 	1	['26', '2', '0', '15', '56', '0', '1', '14', '17', '0.668571429', '601', '1', '1', '0.696202532', '0.174825175', '1', '5', '21.84615385', '6', '1.3462', '9']
package org . apache . lucene . analysis . standard ; public class TokenMgrError extends Error { static final int LEXICAL_ERROR = 0 ; static final int STATIC_LEXER_ERROR = 1 ; static final int INVALID_LEXICAL_STATE = 2 ; static final int LOOP_DETECTED = 3 ; int errorCode ; protected static final String addEscapes ( String str ) { StringBuffer retval = new StringBuffer ( ) ; char ch ; for ( int i = 0 ; i < str . length ( ) ; i ++ ) { switch ( str . charAt ( i ) ) { case 0 : continue ; case '\b' : retval . append ( "\\b" ) ; continue ; case '\t' : retval . append ( "\\t" ) ; continue ; case '\n' : retval . append ( "\\n" ) ; continue ; case '\f' : retval . append ( "\\f" ) ; continue ; case '\r' : retval . append ( "\\r" ) ; continue ; case '\"' : retval . append ( "\\\"" ) ; continue ; case '\'' : retval . append ( "\\\'" ) ; continue ; case '\\' : retval . append ( "\\\\" ) ; continue ; default : if ( ( ch = str . charAt ( i ) ) < 0x20 || ch > 0x7e ) { String s = "0000" + Integer . toString ( ch , 16 ) ; retval . append ( "\\u" + s . substring ( s . length ( ) - 4 , s . length ( ) ) ) ; } else { retval . append ( ch ) ; } continue ; } } return retval . toString ( ) ; } protected static String LexicalError ( boolean EOFSeen , int lexState , int errorLine , int errorColumn , String errorAfter , char curChar ) { return ( "Lexical error at line " + errorLine + ", column " + errorColumn + ".  Encountered: " + ( EOFSeen ? "<EOF> " : ( "\"" + addEscapes ( String . valueOf ( curChar ) ) + "\"" ) + " (" + ( int ) curChar + "), " ) + "after : \"" + addEscapes ( errorAfter ) + "\"" ) ; } public String getMessage ( ) { return super . getMessage ( ) ; } public TokenMgrError ( ) { } public TokenMgrError ( String message , int reason ) { super ( message ) ; errorCode = reason ; } public TokenMgrError ( boolean EOFSeen , int lexState , int errorLine , int errorColumn , String errorAfter , char curChar , int reason ) { this ( LexicalError ( EOFSeen , lexState , errorLine , errorColumn , errorAfter , curChar ) , reason ) ; } } 	0	['6', '3', '0', '1', '19', '15', '1', '0', '4', '1.12', '184', '0', '0', '0.8125', '0.5', '1', '1', '28.83333333', '14', '2.8333', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; import org . apache . lucene . search . Weight ; import org . apache . lucene . search . Scorer ; import org . apache . lucene . search . Explanation ; import org . apache . lucene . search . Similarity ; class SpanScorer extends Scorer { private Spans spans ; private Weight weight ; private byte [ ] norms ; private float value ; private boolean firstTime = true ; private boolean more = true ; private int doc ; private float freq ; SpanScorer ( Spans spans , Weight weight , Similarity similarity , byte [ ] norms ) throws IOException { super ( similarity ) ; this . spans = spans ; this . norms = norms ; this . weight = weight ; this . value = weight . getValue ( ) ; doc = - 1 ; } public boolean next ( ) throws IOException { if ( firstTime ) { more = spans . next ( ) ; firstTime = false ; } return setFreqCurrentDoc ( ) ; } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { more = spans . skipTo ( target ) ; firstTime = false ; } if ( ! more ) { return false ; } if ( spans . doc ( ) < target ) { more = spans . skipTo ( target ) ; } return setFreqCurrentDoc ( ) ; } private boolean setFreqCurrentDoc ( ) throws IOException { if ( ! more ) { return false ; } doc = spans . doc ( ) ; freq = 0.0f ; while ( more && doc == spans . doc ( ) ) { int matchLength = spans . end ( ) - spans . start ( ) ; freq += getSimilarity ( ) . sloppyFreq ( matchLength ) ; more = spans . next ( ) ; } return more || ( freq != 0 ) ; } public int doc ( ) { return doc ; } public float score ( ) throws IOException { float raw = getSimilarity ( ) . tf ( freq ) * value ; return raw * Similarity . decodeNorm ( norms [ doc ] ) ; } public Explanation explain ( final int doc ) throws IOException { Explanation tfExplanation = new Explanation ( ) ; skipTo ( doc ) ; float phraseFreq = ( doc ( ) == doc ) ? freq : 0.0f ; tfExplanation . setValue ( getSimilarity ( ) . tf ( phraseFreq ) ) ; tfExplanation . setDescription ( "tf(phraseFreq=" + phraseFreq + ")" ) ; return tfExplanation ; } } 	1	['7', '2', '0', '6', '25', '0', '1', '5', '5', '0.520833333', '201', '1', '2', '0.571428571', '0.30952381', '1', '3', '26.57142857', '1', '0.8571', '2']
package org . apache . lucene . search ; import java . util . BitSet ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; public abstract class Filter implements java . io . Serializable { public abstract BitSet bits ( IndexReader reader ) throws IOException ; } 	0	['2', '1', '3', '20', '3', '1', '19', '1', '2', '2', '5', '0', '0', '0', '0.75', '0', '0', '1.5', '1', '0.5', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . HashSet ; import java . util . Iterator ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . search . Query ; import org . apache . lucene . search . Weight ; import org . apache . lucene . search . Searcher ; import org . apache . lucene . search . Scorer ; import org . apache . lucene . search . Explanation ; import org . apache . lucene . search . Similarity ; class SpanWeight implements Weight { private Similarity similarity ; private float value ; private float idf ; private float queryNorm ; private float queryWeight ; private Set terms ; private SpanQuery query ; public SpanWeight ( SpanQuery query , Searcher searcher ) throws IOException { this . similarity = query . getSimilarity ( searcher ) ; this . query = query ; terms = new HashSet ( ) ; query . extractTerms ( terms ) ; idf = this . query . getSimilarity ( searcher ) . idf ( terms , searcher ) ; } public Query getQuery ( ) { return query ; } public float getValue ( ) { return value ; } public float sumOfSquaredWeights ( ) throws IOException { queryWeight = idf * query . getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float queryNorm ) { this . queryNorm = queryNorm ; queryWeight *= queryNorm ; value = queryWeight * idf ; } public Scorer scorer ( IndexReader reader ) throws IOException { return new SpanScorer ( query . getSpans ( reader ) , this , similarity , reader . norms ( query . getField ( ) ) ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { Explanation result = new Explanation ( ) ; result . setDescription ( "weight(" + getQuery ( ) + " in " + doc + "), product of:" ) ; String field = ( ( SpanQuery ) getQuery ( ) ) . getField ( ) ; StringBuffer docFreqs = new StringBuffer ( ) ; Iterator i = terms . iterator ( ) ; while ( i . hasNext ( ) ) { Term term = ( Term ) i . next ( ) ; docFreqs . append ( term . text ( ) ) ; docFreqs . append ( "=" ) ; docFreqs . append ( reader . docFreq ( term ) ) ; if ( i . hasNext ( ) ) { docFreqs . append ( " " ) ; } } Explanation idfExpl = new Explanation ( idf , "idf(" + field + ": " + docFreqs + ")" ) ; Explanation queryExpl = new Explanation ( ) ; queryExpl . setDescription ( "queryWeight(" + getQuery ( ) + "), product of:" ) ; Explanation boostExpl = new Explanation ( getQuery ( ) . getBoost ( ) , "boost" ) ; if ( getQuery ( ) . getBoost ( ) != 1.0f ) queryExpl . addDetail ( boostExpl ) ; queryExpl . addDetail ( idfExpl ) ; Explanation queryNormExpl = new Explanation ( queryNorm , "queryNorm" ) ; queryExpl . addDetail ( queryNormExpl ) ; queryExpl . setValue ( boostExpl . getValue ( ) * idfExpl . getValue ( ) * queryNormExpl . getValue ( ) ) ; result . addDetail ( queryExpl ) ; Explanation fieldExpl = new Explanation ( ) ; fieldExpl . setDescription ( "fieldWeight(" + field + ":" + query . toString ( field ) + " in " + doc + "), product of:" ) ; Explanation tfExpl = scorer ( reader ) . explain ( doc ) ; fieldExpl . addDetail ( tfExpl ) ; fieldExpl . addDetail ( idfExpl ) ; Explanation fieldNormExpl = new Explanation ( ) ; byte [ ] fieldNorms = reader . norms ( field ) ; float fieldNorm = fieldNorms != null ? Similarity . decodeNorm ( fieldNorms [ doc ] ) : 0.0f ; fieldNormExpl . setValue ( fieldNorm ) ; fieldNormExpl . setDescription ( "fieldNorm(field=" + field + ", doc=" + doc + ")" ) ; fieldExpl . addDetail ( fieldNormExpl ) ; fieldExpl . setValue ( tfExpl . getValue ( ) * idfExpl . getValue ( ) * fieldNormExpl . getValue ( ) ) ; result . addDetail ( fieldExpl ) ; result . setValue ( queryExpl . getValue ( ) * fieldExpl . getValue ( ) ) ; if ( queryExpl . getValue ( ) == 1.0f ) return fieldExpl ; return result ; } } 	1	['7', '1', '0', '11', '37', '0', '1', '11', '7', '0.69047619', '348', '1', '2', '0', '0.30952381', '0', '0', '47.71428571', '1', '0.8571', '2']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; public interface FieldCache { public static final int STRING_INDEX = - 1 ; public static class StringIndex { public final String [ ] lookup ; public final int [ ] order ; public StringIndex ( int [ ] values , String [ ] lookup ) { this . order = values ; this . lookup = lookup ; } } public interface IntParser { public int parseInt ( String string ) ; } public interface FloatParser { public float parseFloat ( String string ) ; } public static FieldCache DEFAULT = new FieldCacheImpl ( ) ; public int [ ] getInts ( IndexReader reader , String field ) throws IOException ; public int [ ] getInts ( IndexReader reader , String field , IntParser parser ) throws IOException ; public float [ ] getFloats ( IndexReader reader , String field ) throws IOException ; public float [ ] getFloats ( IndexReader reader , String field , FloatParser parser ) throws IOException ; public String [ ] getStrings ( IndexReader reader , String field ) throws IOException ; public StringIndex getStringIndex ( IndexReader reader , String field ) throws IOException ; public Object getAuto ( IndexReader reader , String field ) throws IOException ; public Comparable [ ] getCustom ( IndexReader reader , String field , SortComparator comparator ) throws IOException ; } 	0	['9', '1', '0', '7', '10', '36', '3', '6', '8', '1.0625', '16', '0', '1', '0', '0.5625', '0', '0', '0.555555556', '1', '0.8889', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . * ; final class PhrasePositions { int doc ; int position ; int count ; int offset ; TermPositions tp ; PhrasePositions next ; PhrasePositions ( TermPositions t , int o ) { tp = t ; offset = o ; } final boolean next ( ) throws IOException { if ( ! tp . next ( ) ) { tp . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } doc = tp . doc ( ) ; position = 0 ; return true ; } final boolean skipTo ( int target ) throws IOException { if ( ! tp . skipTo ( target ) ) { tp . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } doc = tp . doc ( ) ; position = 0 ; return true ; } final void firstPosition ( ) throws IOException { count = tp . freq ( ) ; nextPosition ( ) ; } final boolean nextPosition ( ) throws IOException { if ( count -- > 0 ) { position = tp . nextPosition ( ) - offset ; return true ; } else return false ; } } 	1	['5', '1', '0', '5', '12', '0', '4', '1', '0', '0.583333333', '94', '0', '2', '0', '0.533333333', '0', '0', '16.6', '1', '0.8', '1']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class IndexInput implements Cloneable { private char [ ] chars ; public abstract byte readByte ( ) throws IOException ; public abstract void readBytes ( byte [ ] b , int offset , int len ) throws IOException ; public int readInt ( ) throws IOException { return ( ( readByte ( ) & 0xFF ) << 24 ) | ( ( readByte ( ) & 0xFF ) << 16 ) | ( ( readByte ( ) & 0xFF ) << 8 ) | ( readByte ( ) & 0xFF ) ; } public int readVInt ( ) throws IOException { byte b = readByte ( ) ; int i = b & 0x7F ; for ( int shift = 7 ; ( b & 0x80 ) != 0 ; shift += 7 ) { b = readByte ( ) ; i |= ( b & 0x7F ) << shift ; } return i ; } public long readLong ( ) throws IOException { return ( ( ( long ) readInt ( ) ) << 32 ) | ( readInt ( ) & 0xFFFFFFFFL ) ; } public long readVLong ( ) throws IOException { byte b = readByte ( ) ; long i = b & 0x7F ; for ( int shift = 7 ; ( b & 0x80 ) != 0 ; shift += 7 ) { b = readByte ( ) ; i |= ( b & 0x7FL ) << shift ; } return i ; } public String readString ( ) throws IOException { int length = readVInt ( ) ; if ( chars == null || length > chars . length ) chars = new char [ length ] ; readChars ( chars , 0 , length ) ; return new String ( chars , 0 , length ) ; } public void readChars ( char [ ] buffer , int start , int length ) throws IOException { final int end = start + length ; for ( int i = start ; i < end ; i ++ ) { byte b = readByte ( ) ; if ( ( b & 0x80 ) == 0 ) buffer [ i ] = ( char ) ( b & 0x7F ) ; else if ( ( b & 0xE0 ) != 0xE0 ) { buffer [ i ] = ( char ) ( ( ( b & 0x1F ) << 6 ) | ( readByte ( ) & 0x3F ) ) ; } else buffer [ i ] = ( char ) ( ( ( b & 0x0F ) << 12 ) | ( ( readByte ( ) & 0x3F ) << 6 ) | ( readByte ( ) & 0x3F ) ) ; } } public abstract void close ( ) throws IOException ; public abstract long getFilePointer ( ) ; public abstract void seek ( long pos ) throws IOException ; public abstract long length ( ) ; public Object clone ( ) { IndexInput clone = null ; try { clone = ( IndexInput ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . chars = null ; return clone ; } } 	0	['14', '1', '3', '24', '17', '89', '24', '0', '14', '0.923076923', '224', '1', '0', '0', '0.271428571', '0', '0', '14.92857143', '1', '0.9286', '0']
package org . apache . lucene . index ; import java . util . * ; import java . io . IOException ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . Field ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . IndexInput ; final class FieldInfos { static final byte IS_INDEXED = 0x1 ; static final byte STORE_TERMVECTOR = 0x2 ; static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x4 ; static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x8 ; static final byte OMIT_NORMS = 0x10 ; private ArrayList byNumber = new ArrayList ( ) ; private HashMap byName = new HashMap ( ) ; FieldInfos ( ) { } FieldInfos ( Directory d , String name ) throws IOException { IndexInput input = d . openInput ( name ) ; try { read ( input ) ; } finally { input . close ( ) ; } } public void add ( Document doc ) { Enumeration fields = doc . fields ( ) ; while ( fields . hasMoreElements ( ) ) { Field field = ( Field ) fields . nextElement ( ) ; add ( field . name ( ) , field . isIndexed ( ) , field . isTermVectorStored ( ) , field . isStorePositionWithTermVector ( ) , field . isStoreOffsetWithTermVector ( ) , field . getOmitNorms ( ) ) ; } } public void addIndexed ( Collection names , boolean storeTermVectors , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector ) { Iterator i = names . iterator ( ) ; while ( i . hasNext ( ) ) { add ( ( String ) i . next ( ) , true , storeTermVectors , storePositionWithTermVector , storeOffsetWithTermVector ) ; } } public void add ( Collection names , boolean isIndexed ) { Iterator i = names . iterator ( ) ; while ( i . hasNext ( ) ) { add ( ( String ) i . next ( ) , isIndexed ) ; } } public void add ( String name , boolean isIndexed ) { add ( name , isIndexed , false , false , false , false ) ; } public void add ( String name , boolean isIndexed , boolean storeTermVector ) { add ( name , isIndexed , storeTermVector , false , false , false ) ; } public void add ( String name , boolean isIndexed , boolean storeTermVector , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector ) { add ( name , isIndexed , storeTermVector , storePositionWithTermVector , storeOffsetWithTermVector , false ) ; } public void add ( String name , boolean isIndexed , boolean storeTermVector , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector , boolean omitNorms ) { FieldInfo fi = fieldInfo ( name ) ; if ( fi == null ) { addInternal ( name , isIndexed , storeTermVector , storePositionWithTermVector , storeOffsetWithTermVector , omitNorms ) ; } else { if ( fi . isIndexed != isIndexed ) { fi . isIndexed = true ; } if ( fi . storeTermVector != storeTermVector ) { fi . storeTermVector = true ; } if ( fi . storePositionWithTermVector != storePositionWithTermVector ) { fi . storePositionWithTermVector = true ; } if ( fi . storeOffsetWithTermVector != storeOffsetWithTermVector ) { fi . storeOffsetWithTermVector = true ; } if ( fi . omitNorms != omitNorms ) { fi . omitNorms = false ; } } } private void addInternal ( String name , boolean isIndexed , boolean storeTermVector , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector , boolean omitNorms ) { FieldInfo fi = new FieldInfo ( name , isIndexed , byNumber . size ( ) , storeTermVector , storePositionWithTermVector , storeOffsetWithTermVector , omitNorms ) ; byNumber . add ( fi ) ; byName . put ( name , fi ) ; } public int fieldNumber ( String fieldName ) { try { FieldInfo fi = fieldInfo ( fieldName ) ; if ( fi != null ) return fi . number ; } catch ( IndexOutOfBoundsException ioobe ) { return - 1 ; } return - 1 ; } public FieldInfo fieldInfo ( String fieldName ) { return ( FieldInfo ) byName . get ( fieldName ) ; } public String fieldName ( int fieldNumber ) { try { return fieldInfo ( fieldNumber ) . name ; } catch ( NullPointerException npe ) { return "" ; } } public FieldInfo fieldInfo ( int fieldNumber ) { try { return ( FieldInfo ) byNumber . get ( fieldNumber ) ; } catch ( IndexOutOfBoundsException ioobe ) { return null ; } } public int size ( ) { return byNumber . size ( ) ; } public boolean hasVectors ( ) { boolean hasVectors = false ; for ( int i = 0 ; i < size ( ) ; i ++ ) { if ( fieldInfo ( i ) . storeTermVector ) { hasVectors = true ; break ; } } return hasVectors ; } public void write ( Directory d , String name ) throws IOException { IndexOutput output = d . createOutput ( name ) ; try { write ( output ) ; } finally { output . close ( ) ; } } public void write ( IndexOutput output ) throws IOException { output . writeVInt ( size ( ) ) ; for ( int i = 0 ; i < size ( ) ; i ++ ) { FieldInfo fi = fieldInfo ( i ) ; byte bits = 0x0 ; if ( fi . isIndexed ) bits |= IS_INDEXED ; if ( fi . storeTermVector ) bits |= STORE_TERMVECTOR ; if ( fi . storePositionWithTermVector ) bits |= STORE_POSITIONS_WITH_TERMVECTOR ; if ( fi . storeOffsetWithTermVector ) bits |= STORE_OFFSET_WITH_TERMVECTOR ; if ( fi . omitNorms ) bits |= OMIT_NORMS ; output . writeString ( fi . name ) ; output . writeByte ( bits ) ; } } private void read ( IndexInput input ) throws IOException { int size = input . readVInt ( ) ; for ( int i = 0 ; i < size ; i ++ ) { String name = input . readString ( ) . intern ( ) ; byte bits = input . readByte ( ) ; boolean isIndexed = ( bits & IS_INDEXED ) != 0 ; boolean storeTermVector = ( bits & STORE_TERMVECTOR ) != 0 ; boolean storePositionsWithTermVector = ( bits & STORE_POSITIONS_WITH_TERMVECTOR ) != 0 ; boolean storeOffsetWithTermVector = ( bits & STORE_OFFSET_WITH_TERMVECTOR ) != 0 ; boolean omitNorms = ( bits & OMIT_NORMS ) != 0 ; addInternal ( name , isIndexed , storeTermVector , storePositionsWithTermVector , storeOffsetWithTermVector , omitNorms ) ; } } } 	1	['19', '1', '0', '18', '51', '145', '12', '6', '15', '0.920634921', '437', '0.285714286', '0', '0', '0.257309942', '0', '0', '21.63157895', '7', '1.4737', '1']
package org . apache . lucene . analysis . standard ; import java . io . * ; public final class FastCharStream implements CharStream { char [ ] buffer = null ; int bufferLength = 0 ; int bufferPosition = 0 ; int tokenStart = 0 ; int bufferStart = 0 ; Reader input ; public FastCharStream ( Reader r ) { input = r ; } public final char readChar ( ) throws IOException { if ( bufferPosition >= bufferLength ) refill ( ) ; return buffer [ bufferPosition ++ ] ; } private final void refill ( ) throws IOException { int newPosition = bufferLength - tokenStart ; if ( tokenStart == 0 ) { if ( buffer == null ) { buffer = new char [ 2048 ] ; } else if ( bufferLength == buffer . length ) { char [ ] newBuffer = new char [ buffer . length * 2 ] ; System . arraycopy ( buffer , 0 , newBuffer , 0 , bufferLength ) ; buffer = newBuffer ; } } else { System . arraycopy ( buffer , tokenStart , buffer , 0 , newPosition ) ; } bufferLength = newPosition ; bufferPosition = newPosition ; bufferStart += tokenStart ; tokenStart = 0 ; int charsRead = input . read ( buffer , newPosition , buffer . length - newPosition ) ; if ( charsRead == - 1 ) throw new IOException ( "read past eof" ) ; else bufferLength += charsRead ; } public final char BeginToken ( ) throws IOException { tokenStart = bufferPosition ; return readChar ( ) ; } public final void backup ( int amount ) { bufferPosition -= amount ; } public final String GetImage ( ) { return new String ( buffer , tokenStart , bufferPosition - tokenStart ) ; } public final char [ ] GetSuffix ( int len ) { char [ ] value = new char [ len ] ; System . arraycopy ( buffer , bufferPosition - len , value , 0 , len ) ; return value ; } public final void Done ( ) { try { input . close ( ) ; } catch ( IOException e ) { System . err . println ( "Caught: " + e + "; ignoring." ) ; } } public final int getColumn ( ) { return bufferStart + bufferPosition ; } public final int getLine ( ) { return 1 ; } public final int getEndColumn ( ) { return bufferStart + bufferPosition ; } public final int getEndLine ( ) { return 1 ; } public final int getBeginColumn ( ) { return bufferStart + tokenStart ; } public final int getBeginLine ( ) { return 1 ; } } 	0	['14', '1', '0', '2', '25', '3', '1', '1', '13', '0.602564103', '237', '0', '0', '0', '0.404761905', '0', '0', '15.5', '1', '0.9286', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . IndexInput ; import java . util . LinkedList ; import java . util . HashSet ; import java . util . Iterator ; import java . io . IOException ; final class CompoundFileWriter { private static final class FileEntry { String file ; long directoryOffset ; long dataOffset ; } private Directory directory ; private String fileName ; private HashSet ids ; private LinkedList entries ; private boolean merged = false ; public CompoundFileWriter ( Directory dir , String name ) { if ( dir == null ) throw new NullPointerException ( "directory cannot be null" ) ; if ( name == null ) throw new NullPointerException ( "name cannot be null" ) ; directory = dir ; fileName = name ; ids = new HashSet ( ) ; entries = new LinkedList ( ) ; } public Directory getDirectory ( ) { return directory ; } public String getName ( ) { return fileName ; } public void addFile ( String file ) { if ( merged ) throw new IllegalStateException ( "Can't add extensions after merge has been called" ) ; if ( file == null ) throw new NullPointerException ( "file cannot be null" ) ; if ( ! ids . add ( file ) ) throw new IllegalArgumentException ( "File " + file + " already added" ) ; FileEntry entry = new FileEntry ( ) ; entry . file = file ; entries . add ( entry ) ; } public void close ( ) throws IOException { if ( merged ) throw new IllegalStateException ( "Merge already performed" ) ; if ( entries . isEmpty ( ) ) throw new IllegalStateException ( "No entries to merge have been defined" ) ; merged = true ; IndexOutput os = null ; try { os = directory . createOutput ( fileName ) ; os . writeVInt ( entries . size ( ) ) ; Iterator it = entries . iterator ( ) ; while ( it . hasNext ( ) ) { FileEntry fe = ( FileEntry ) it . next ( ) ; fe . directoryOffset = os . getFilePointer ( ) ; os . writeLong ( 0 ) ; os . writeString ( fe . file ) ; } byte buffer [ ] = new byte [ 1024 ] ; it = entries . iterator ( ) ; while ( it . hasNext ( ) ) { FileEntry fe = ( FileEntry ) it . next ( ) ; fe . dataOffset = os . getFilePointer ( ) ; copyFile ( fe , os , buffer ) ; } it = entries . iterator ( ) ; while ( it . hasNext ( ) ) { FileEntry fe = ( FileEntry ) it . next ( ) ; os . seek ( fe . directoryOffset ) ; os . writeLong ( fe . dataOffset ) ; } IndexOutput tmp = os ; os = null ; tmp . close ( ) ; } finally { if ( os != null ) try { os . close ( ) ; } catch ( IOException e ) { } } } private void copyFile ( FileEntry source , IndexOutput os , byte buffer [ ] ) throws IOException { IndexInput is = null ; try { long startPtr = os . getFilePointer ( ) ; is = directory . openInput ( source . file ) ; long length = is . length ( ) ; long remainder = length ; int chunk = buffer . length ; while ( remainder > 0 ) { int len = ( int ) Math . min ( chunk , remainder ) ; is . readBytes ( buffer , 0 , len ) ; os . writeBytes ( buffer , len ) ; remainder -= len ; } if ( remainder != 0 ) throw new IOException ( "Non-zero remainder length after copying: " + remainder + " (id: " + source . file + ", length: " + length + ", buffer size: " + chunk + ")" ) ; long endPtr = os . getFilePointer ( ) ; long diff = endPtr - startPtr ; if ( diff != length ) throw new IOException ( "Difference in the output file offsets " + diff + " does not match the original file length " + length ) ; } finally { if ( is != null ) is . close ( ) ; } } } 	1	['6', '1', '0', '6', '39', '0', '1', '5', '5', '0.6', '337', '1', '1', '0', '0.333333333', '0', '0', '54.33333333', '4', '1.3333', '1']
package org . apache . lucene . search ; import java . io . IOException ; public class ReqExclScorer extends Scorer { private Scorer reqScorer , exclScorer ; public ReqExclScorer ( Scorer reqScorer , Scorer exclScorer ) { super ( null ) ; this . reqScorer = reqScorer ; this . exclScorer = exclScorer ; } private boolean firstTime = true ; public boolean next ( ) throws IOException { if ( firstTime ) { if ( ! exclScorer . next ( ) ) { exclScorer = null ; } firstTime = false ; } if ( reqScorer == null ) { return false ; } if ( ! reqScorer . next ( ) ) { reqScorer = null ; return false ; } if ( exclScorer == null ) { return true ; } return toNonExcluded ( ) ; } private boolean toNonExcluded ( ) throws IOException { int exclDoc = exclScorer . doc ( ) ; do { int reqDoc = reqScorer . doc ( ) ; if ( reqDoc < exclDoc ) { return true ; } else if ( reqDoc > exclDoc ) { if ( ! exclScorer . skipTo ( reqDoc ) ) { exclScorer = null ; return true ; } exclDoc = exclScorer . doc ( ) ; if ( exclDoc > reqDoc ) { return true ; } } } while ( reqScorer . next ( ) ) ; reqScorer = null ; return false ; } public int doc ( ) { return reqScorer . doc ( ) ; } public float score ( ) throws IOException { return reqScorer . score ( ) ; } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { firstTime = false ; if ( ! exclScorer . skipTo ( target ) ) { exclScorer = null ; } } if ( reqScorer == null ) { return false ; } if ( exclScorer == null ) { return reqScorer . skipTo ( target ) ; } if ( ! reqScorer . skipTo ( target ) ) { reqScorer = null ; return false ; } return toNonExcluded ( ) ; } public Explanation explain ( int doc ) throws IOException { Explanation res = new Explanation ( ) ; if ( exclScorer . skipTo ( doc ) && ( exclScorer . doc ( ) == doc ) ) { res . setDescription ( "excluded" ) ; } else { res . setDescription ( "not excluded" ) ; res . addDetail ( reqScorer . explain ( doc ) ) ; } return res ; } } 	0	['7', '2', '0', '4', '16', '0', '1', '3', '6', '0.333333333', '179', '1', '2', '0.571428571', '0.476190476', '1', '3', '24.14285714', '1', '0.8571', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . ArrayList ; import java . util . Collection ; import java . util . Enumeration ; import java . util . HashSet ; import java . util . Iterator ; import java . util . List ; import java . util . Map ; import java . util . Set ; import java . util . SortedMap ; import java . util . TreeMap ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . Field ; public class ParallelReader extends IndexReader { private List readers = new ArrayList ( ) ; private SortedMap fieldToReader = new TreeMap ( ) ; private List storedFieldReaders = new ArrayList ( ) ; private int maxDoc ; private int numDocs ; private boolean hasDeletions ; public ParallelReader ( ) throws IOException { super ( null ) ; } public void add ( IndexReader reader ) throws IOException { add ( reader , false ) ; } public void add ( IndexReader reader , boolean ignoreStoredFields ) throws IOException { if ( readers . size ( ) == 0 ) { this . maxDoc = reader . maxDoc ( ) ; this . numDocs = reader . numDocs ( ) ; this . hasDeletions = reader . hasDeletions ( ) ; } if ( reader . maxDoc ( ) != maxDoc ) throw new IllegalArgumentException ( "All readers must have same maxDoc: " + maxDoc + "!=" + reader . maxDoc ( ) ) ; if ( reader . numDocs ( ) != numDocs ) throw new IllegalArgumentException ( "All readers must have same numDocs: " + numDocs + "!=" + reader . numDocs ( ) ) ; Iterator i = reader . getFieldNames ( IndexReader . FieldOption . ALL ) . iterator ( ) ; while ( i . hasNext ( ) ) { String field = ( String ) i . next ( ) ; if ( fieldToReader . get ( field ) == null ) fieldToReader . put ( field , reader ) ; } if ( ! ignoreStoredFields ) storedFieldReaders . add ( reader ) ; readers . add ( reader ) ; } public int numDocs ( ) { return numDocs ; } public int maxDoc ( ) { return maxDoc ; } public boolean hasDeletions ( ) { return hasDeletions ; } public boolean isDeleted ( int n ) { if ( readers . size ( ) > 0 ) return ( ( IndexReader ) readers . get ( 0 ) ) . isDeleted ( n ) ; return false ; } protected void doDelete ( int n ) throws IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { ( ( IndexReader ) readers . get ( i ) ) . deleteDocument ( n ) ; } hasDeletions = true ; } protected void doUndeleteAll ( ) throws IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { ( ( IndexReader ) readers . get ( i ) ) . undeleteAll ( ) ; } hasDeletions = false ; } public Document document ( int n ) throws IOException { Document result = new Document ( ) ; for ( int i = 0 ; i < storedFieldReaders . size ( ) ; i ++ ) { IndexReader reader = ( IndexReader ) storedFieldReaders . get ( i ) ; Enumeration fields = reader . document ( n ) . fields ( ) ; while ( fields . hasMoreElements ( ) ) { result . add ( ( Field ) fields . nextElement ( ) ) ; } } return result ; } public TermFreqVector [ ] getTermFreqVectors ( int n ) throws IOException { ArrayList results = new ArrayList ( ) ; Iterator i = fieldToReader . entrySet ( ) . iterator ( ) ; while ( i . hasNext ( ) ) { Map . Entry e = ( Map . Entry ) i . next ( ) ; String field = ( String ) e . getKey ( ) ; IndexReader reader = ( IndexReader ) e . getValue ( ) ; TermFreqVector vector = reader . getTermFreqVector ( n , field ) ; if ( vector != null ) results . add ( vector ) ; } return ( TermFreqVector [ ] ) results . toArray ( new TermFreqVector [ results . size ( ) ] ) ; } public TermFreqVector getTermFreqVector ( int n , String field ) throws IOException { IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; return reader == null ? null : reader . getTermFreqVector ( n , field ) ; } public boolean hasNorms ( String field ) throws IOException { IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; return reader == null ? false : reader . hasNorms ( field ) ; } public byte [ ] norms ( String field ) throws IOException { IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; return reader == null ? null : reader . norms ( field ) ; } public void norms ( String field , byte [ ] result , int offset ) throws IOException { IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; if ( reader != null ) reader . norms ( field , result , offset ) ; } protected void doSetNorm ( int n , String field , byte value ) throws IOException { IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; if ( reader != null ) reader . doSetNorm ( n , field , value ) ; } public TermEnum terms ( ) throws IOException { return new ParallelTermEnum ( ) ; } public TermEnum terms ( Term term ) throws IOException { return new ParallelTermEnum ( term ) ; } public int docFreq ( Term term ) throws IOException { IndexReader reader = ( ( IndexReader ) fieldToReader . get ( term . field ( ) ) ) ; return reader == null ? 0 : reader . docFreq ( term ) ; } public TermDocs termDocs ( Term term ) throws IOException { return new ParallelTermDocs ( term ) ; } public TermDocs termDocs ( ) throws IOException { return new ParallelTermDocs ( ) ; } public TermPositions termPositions ( Term term ) throws IOException { return new ParallelTermPositions ( term ) ; } public TermPositions termPositions ( ) throws IOException { return new ParallelTermPositions ( ) ; } protected void doCommit ( ) throws IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) ( ( IndexReader ) readers . get ( i ) ) . commit ( ) ; } protected synchronized void doClose ( ) throws IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) ( ( IndexReader ) readers . get ( i ) ) . close ( ) ; } public Collection getFieldNames ( IndexReader . FieldOption fieldNames ) { Set fieldSet = new HashSet ( ) ; for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader reader = ( ( IndexReader ) readers . get ( i ) ) ; Collection names = reader . getFieldNames ( fieldNames ) ; fieldSet . addAll ( names ) ; } return fieldSet ; } private class ParallelTermEnum extends TermEnum { private String field ; private TermEnum termEnum ; public ParallelTermEnum ( ) throws IOException { field = ( String ) fieldToReader . firstKey ( ) ; if ( field != null ) termEnum = ( ( IndexReader ) fieldToReader . get ( field ) ) . terms ( ) ; } public ParallelTermEnum ( Term term ) throws IOException { field = term . field ( ) ; IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; if ( reader != null ) termEnum = reader . terms ( term ) ; } public boolean next ( ) throws IOException { if ( termEnum == null ) return false ; boolean next = termEnum . next ( ) ; if ( next && termEnum . term ( ) . field ( ) == field ) return true ; termEnum . close ( ) ; field = ( String ) fieldToReader . tailMap ( field ) . firstKey ( ) ; if ( field != null ) { termEnum = ( ( IndexReader ) fieldToReader . get ( field ) ) . terms ( ) ; return true ; } return false ; } public Term term ( ) { if ( termEnum == null ) return null ; return termEnum . term ( ) ; } public int docFreq ( ) { if ( termEnum == null ) return 0 ; return termEnum . docFreq ( ) ; } public void close ( ) throws IOException { if ( termEnum != null ) termEnum . close ( ) ; } } private class ParallelTermDocs implements TermDocs { protected TermDocs termDocs ; public ParallelTermDocs ( ) { } public ParallelTermDocs ( Term term ) throws IOException { seek ( term ) ; } public int doc ( ) { return termDocs . doc ( ) ; } public int freq ( ) { return termDocs . freq ( ) ; } public void seek ( Term term ) throws IOException { IndexReader reader = ( ( IndexReader ) fieldToReader . get ( term . field ( ) ) ) ; termDocs = reader != null ? reader . termDocs ( term ) : null ; } public void seek ( TermEnum termEnum ) throws IOException { seek ( termEnum . term ( ) ) ; } public boolean next ( ) throws IOException { if ( termDocs == null ) return false ; return termDocs . next ( ) ; } public int read ( final int [ ] docs , final int [ ] freqs ) throws IOException { if ( termDocs == null ) return 0 ; return termDocs . read ( docs , freqs ) ; } public boolean skipTo ( int target ) throws IOException { if ( termDocs == null ) return false ; return termDocs . skipTo ( target ) ; } public void close ( ) throws IOException { if ( termDocs != null ) termDocs . close ( ) ; } } private class ParallelTermPositions extends ParallelTermDocs implements TermPositions { public ParallelTermPositions ( ) { } public ParallelTermPositions ( Term term ) throws IOException { seek ( term ) ; } public void seek ( Term term ) throws IOException { IndexReader reader = ( ( IndexReader ) fieldToReader . get ( term . field ( ) ) ) ; termDocs = reader != null ? reader . termPositions ( term ) : null ; } public int nextPosition ( ) throws IOException { return ( ( TermPositions ) termDocs ) . nextPosition ( ) ; } } } 	1	['27', '2', '0', '13', '80', '193', '3', '13', '21', '0.814102564', '479', '1', '0', '0.679012346', '0.181481481', '1', '6', '16.51851852', '2', '1.037', '6']
package org . apache . lucene . analysis ; import java . io . IOException ; public final class PorterStemFilter extends TokenFilter { private PorterStemmer stemmer ; public PorterStemFilter ( TokenStream in ) { super ( in ) ; stemmer = new PorterStemmer ( ) ; } public final Token next ( ) throws IOException { Token token = input . next ( ) ; if ( token == null ) return null ; else { String s = stemmer . stem ( token . termText ) ; if ( s != token . termText ) token . termText = s ; return token ; } } } 	0	['2', '3', '0', '4', '6', '0', '0', '4', '2', '0', '35', '1', '1', '0.75', '0.75', '0', '0', '16', '1', '0.5', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; import java . io . IOException ; import java . util . BitSet ; import java . util . Set ; public class FilteredQuery extends Query { Query query ; Filter filter ; public FilteredQuery ( Query query , Filter filter ) { this . query = query ; this . filter = filter ; } protected Weight createWeight ( final Searcher searcher ) throws IOException { final Weight weight = query . createWeight ( searcher ) ; final Similarity similarity = query . getSimilarity ( searcher ) ; return new Weight ( ) { public float getValue ( ) { return weight . getValue ( ) ; } public float sumOfSquaredWeights ( ) throws IOException { return weight . sumOfSquaredWeights ( ) ; } public void normalize ( float v ) { weight . normalize ( v ) ; } public Explanation explain ( IndexReader ir , int i ) throws IOException { return weight . explain ( ir , i ) ; } public Query getQuery ( ) { return FilteredQuery . this ; } public Scorer scorer ( IndexReader indexReader ) throws IOException { final Scorer scorer = weight . scorer ( indexReader ) ; final BitSet bitset = filter . bits ( indexReader ) ; return new Scorer ( similarity ) { public boolean next ( ) throws IOException { do { if ( ! scorer . next ( ) ) { return false ; } } while ( ! bitset . get ( scorer . doc ( ) ) ) ; return true ; } public int doc ( ) { return scorer . doc ( ) ; } public boolean skipTo ( int i ) throws IOException { if ( ! scorer . skipTo ( i ) ) { return false ; } while ( ! bitset . get ( scorer . doc ( ) ) ) { int nextFiltered = bitset . nextSetBit ( scorer . doc ( ) + 1 ) ; if ( nextFiltered == - 1 ) { return false ; } else if ( ! scorer . skipTo ( nextFiltered ) ) { return false ; } } return true ; } public float score ( ) throws IOException { return scorer . score ( ) ; } public Explanation explain ( int i ) throws IOException { Explanation exp = scorer . explain ( i ) ; if ( bitset . get ( i ) ) exp . setDescription ( "allowed by filter: " + exp . getDescription ( ) ) ; else exp . setDescription ( "removed by filter: " + exp . getDescription ( ) ) ; return exp ; } } ; } } ; } public Query rewrite ( IndexReader reader ) throws IOException { Query rewritten = query . rewrite ( reader ) ; if ( rewritten != query ) { FilteredQuery clone = ( FilteredQuery ) this . clone ( ) ; clone . query = rewritten ; return clone ; } else { return this ; } } public Query getQuery ( ) { return query ; } public Filter getFilter ( ) { return filter ; } public void extractTerms ( Set terms ) { getQuery ( ) . extractTerms ( terms ) ; } public String toString ( String s ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "filtered(" ) ; buffer . append ( query . toString ( s ) ) ; buffer . append ( ")->" ) ; buffer . append ( filter ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( o instanceof FilteredQuery ) { FilteredQuery fq = ( FilteredQuery ) o ; return ( query . equals ( fq . query ) && filter . equals ( fq . filter ) ) ; } return false ; } public int hashCode ( ) { return query . hashCode ( ) ^ filter . hashCode ( ) ; } } 	1	['9', '2', '0', '8', '25', '0', '1', '8', '8', '0.3125', '133', '0', '2', '0.6', '0.222222222', '2', '4', '13.55555556', '4', '1.2222', '4']
package org . apache . lucene . analysis ; import java . io . File ; import java . io . FileReader ; import java . io . IOException ; import java . io . Reader ; import java . io . BufferedReader ; import java . util . HashSet ; import java . util . Hashtable ; import java . util . Iterator ; public class WordlistLoader { public static HashSet getWordSet ( File wordfile ) throws IOException { HashSet result = new HashSet ( ) ; FileReader reader = null ; try { reader = new FileReader ( wordfile ) ; result = getWordSet ( reader ) ; } finally { if ( reader != null ) reader . close ( ) ; } return result ; } public static HashSet getWordSet ( Reader reader ) throws IOException { HashSet result = new HashSet ( ) ; BufferedReader br = null ; try { if ( reader instanceof BufferedReader ) { br = ( BufferedReader ) reader ; } else { br = new BufferedReader ( reader ) ; } String word = null ; while ( ( word = br . readLine ( ) ) != null ) { result . add ( word . trim ( ) ) ; } } finally { if ( br != null ) br . close ( ) ; } return result ; } private static Hashtable makeWordTable ( HashSet wordSet ) { Hashtable table = new Hashtable ( ) ; for ( Iterator iter = wordSet . iterator ( ) ; iter . hasNext ( ) ; ) { String word = ( String ) iter . next ( ) ; table . put ( word , word ) ; } return table ; } } 	0	['4', '1', '0', '2', '18', '6', '2', '0', '3', '2', '102', '0', '0', '0', '0.25', '0', '0', '24.5', '2', '1', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . io . Serializable ; import java . util . Collection ; import java . util . Iterator ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . IndexWriter ; import org . apache . lucene . document . Field ; import org . apache . lucene . util . SmallFloat ; public abstract class Similarity implements Serializable { private static Similarity defaultImpl = new DefaultSimilarity ( ) ; public static void setDefault ( Similarity similarity ) { Similarity . defaultImpl = similarity ; } public static Similarity getDefault ( ) { return Similarity . defaultImpl ; } private static final float [ ] NORM_TABLE = new float [ 256 ] ; static { for ( int i = 0 ; i < 256 ; i ++ ) NORM_TABLE [ i ] = SmallFloat . byte315ToFloat ( ( byte ) i ) ; } public static float decodeNorm ( byte b ) { return NORM_TABLE [ b & 0xFF ] ; } public static float [ ] getNormDecoder ( ) { return NORM_TABLE ; } public abstract float lengthNorm ( String fieldName , int numTokens ) ; public abstract float queryNorm ( float sumOfSquaredWeights ) ; public static byte encodeNorm ( float f ) { return SmallFloat . floatToByte315 ( f ) ; } public float tf ( int freq ) { return tf ( ( float ) freq ) ; } public abstract float sloppyFreq ( int distance ) ; public abstract float tf ( float freq ) ; public float idf ( Term term , Searcher searcher ) throws IOException { return idf ( searcher . docFreq ( term ) , searcher . maxDoc ( ) ) ; } public float idf ( Collection terms , Searcher searcher ) throws IOException { float idf = 0.0f ; Iterator i = terms . iterator ( ) ; while ( i . hasNext ( ) ) { idf += idf ( ( Term ) i . next ( ) , searcher ) ; } return idf ; } public abstract float idf ( int docFreq , int numDocs ) ; public abstract float coord ( int overlap , int maxOverlap ) ; } 	1	['16', '1', '2', '42', '25', '108', '40', '4', '15', '0.866666667', '91', '1', '1', '0', '0.185185185', '0', '0', '4.5625', '1', '0.875', '3']
package org . apache . lucene . analysis ; import java . io . IOException ; public final class LowerCaseFilter extends TokenFilter { public LowerCaseFilter ( TokenStream in ) { super ( in ) ; } public final Token next ( ) throws IOException { Token t = input . next ( ) ; if ( t == null ) return null ; t . termText = t . termText . toLowerCase ( ) ; return t ; } } 	0	['2', '3', '0', '4', '5', '1', '1', '3', '2', '2', '21', '0', '0', '0.75', '0.75', '0', '0', '9.5', '1', '0.5', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class Lock { public static long LOCK_POLL_INTERVAL = 1000 ; public abstract boolean obtain ( ) throws IOException ; public boolean obtain ( long lockWaitTimeout ) throws IOException { boolean locked = obtain ( ) ; int maxSleepCount = ( int ) ( lockWaitTimeout / LOCK_POLL_INTERVAL ) ; int sleepCount = 0 ; while ( ! locked ) { if ( sleepCount ++ == maxSleepCount ) { throw new IOException ( "Lock obtain timed out: " + this . toString ( ) ) ; } try { Thread . sleep ( LOCK_POLL_INTERVAL ) ; } catch ( InterruptedException e ) { throw new IOException ( e . toString ( ) ) ; } locked = obtain ( ) ; } return locked ; } public abstract void release ( ) ; public abstract boolean isLocked ( ) ; public abstract static class With { private Lock lock ; private long lockWaitTimeout ; public With ( Lock lock , long lockWaitTimeout ) { this . lock = lock ; this . lockWaitTimeout = lockWaitTimeout ; } protected abstract Object doBody ( ) throws IOException ; public Object run ( ) throws IOException { boolean locked = false ; try { locked = lock . obtain ( lockWaitTimeout ) ; return doBody ( ) ; } finally { if ( locked ) lock . release ( ) ; } } } } 	1	['6', '1', '2', '16', '14', '13', '16', '0', '5', '0.8', '58', '0', '0', '0', '0.6', '0', '0', '8.5', '1', '0.6667', '2']
package org . apache . lucene . queryParser ; public interface CharStream { char readChar ( ) throws java . io . IOException ; int getEndColumn ( ) ; int getEndLine ( ) ; int getBeginColumn ( ) ; int getBeginLine ( ) ; void backup ( int amount ) ; char BeginToken ( ) throws java . io . IOException ; String GetImage ( ) ; char [ ] GetSuffix ( int len ) ; void Done ( ) ; } 	0	['10', '1', '0', '3', '10', '45', '3', '0', '10', '2', '10', '0', '0', '0', '0.6', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . document . Document ; public class Hit implements java . io . Serializable { private Document doc = null ; private boolean resolved = false ; private Hits hits = null ; private int hitNumber ; Hit ( Hits hits , int hitNumber ) { this . hits = hits ; this . hitNumber = hitNumber ; } public Document getDocument ( ) throws IOException { if ( ! resolved ) fetchTheHit ( ) ; return doc ; } public float getScore ( ) throws IOException { return hits . score ( hitNumber ) ; } public int getId ( ) throws IOException { return hits . id ( hitNumber ) ; } private void fetchTheHit ( ) throws IOException { doc = hits . doc ( hitNumber ) ; resolved = true ; } public float getBoost ( ) throws IOException { return getDocument ( ) . getBoost ( ) ; } public String get ( String name ) throws IOException { return getDocument ( ) . get ( name ) ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "Hit<" ) ; buffer . append ( hits . toString ( ) ) ; buffer . append ( " [" ) ; buffer . append ( hitNumber ) ; buffer . append ( "] " ) ; if ( resolved ) { buffer . append ( "resolved" ) ; } else { buffer . append ( "unresolved" ) ; } buffer . append ( ">" ) ; return buffer . toString ( ) ; } } 	1	['8', '1', '0', '3', '19', '2', '1', '2', '6', '0.178571429', '116', '1', '2', '0', '0.34375', '0', '0', '13', '2', '1', '1']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; public abstract class SortComparator implements SortComparatorSource { public ScoreDocComparator newComparator ( final IndexReader reader , final String fieldname ) throws IOException { final String field = fieldname . intern ( ) ; final Comparable [ ] cachedValues = FieldCache . DEFAULT . getCustom ( reader , field , SortComparator . this ) ; return new ScoreDocComparator ( ) { public int compare ( ScoreDoc i , ScoreDoc j ) { return cachedValues [ i . doc ] . compareTo ( cachedValues [ j . doc ] ) ; } public Comparable sortValue ( ScoreDoc i ) { return cachedValues [ i . doc ] ; } public int sortType ( ) { return SortField . CUSTOM ; } } ; } protected abstract Comparable getComparable ( String termtext ) ; } 	0	['3', '1', '0', '6', '7', '3', '3', '5', '2', '2', '21', '0', '0', '0', '0.666666667', '0', '0', '6', '1', '0.6667', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; import java . util . ArrayList ; import java . util . Iterator ; import java . util . Collection ; import java . util . Set ; public class DisjunctionMaxQuery extends Query { private ArrayList disjuncts = new ArrayList ( ) ; private float tieBreakerMultiplier = 0.0f ; public DisjunctionMaxQuery ( float tieBreakerMultiplier ) { this . tieBreakerMultiplier = tieBreakerMultiplier ; } public DisjunctionMaxQuery ( Collection disjuncts , float tieBreakerMultiplier ) { this . tieBreakerMultiplier = tieBreakerMultiplier ; add ( disjuncts ) ; } public void add ( Query query ) { disjuncts . add ( query ) ; } public void add ( Collection disjuncts ) { this . disjuncts . addAll ( disjuncts ) ; } public Iterator iterator ( ) { return disjuncts . iterator ( ) ; } private class DisjunctionMaxWeight implements Weight { private Searcher searcher ; private ArrayList weights = new ArrayList ( ) ; public DisjunctionMaxWeight ( Searcher searcher ) throws IOException { this . searcher = searcher ; for ( int i = 0 ; i < disjuncts . size ( ) ; i ++ ) weights . add ( ( ( Query ) disjuncts . get ( i ) ) . createWeight ( searcher ) ) ; } public Query getQuery ( ) { return DisjunctionMaxQuery . this ; } public float getValue ( ) { return getBoost ( ) ; } public float sumOfSquaredWeights ( ) throws IOException { float max = 0.0f , sum = 0.0f ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { float sub = ( ( Weight ) weights . get ( i ) ) . sumOfSquaredWeights ( ) ; sum += sub ; max = Math . max ( max , sub ) ; } return ( ( ( sum - max ) * tieBreakerMultiplier * tieBreakerMultiplier ) + max ) * getBoost ( ) * getBoost ( ) ; } public void normalize ( float norm ) { norm *= getBoost ( ) ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) ( ( Weight ) weights . get ( i ) ) . normalize ( norm ) ; } public Scorer scorer ( IndexReader reader ) throws IOException { DisjunctionMaxScorer result = new DisjunctionMaxScorer ( tieBreakerMultiplier , getSimilarity ( searcher ) ) ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { Weight w = ( Weight ) weights . get ( i ) ; Scorer subScorer = w . scorer ( reader ) ; if ( subScorer == null ) return null ; result . add ( subScorer ) ; } return result ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { if ( disjuncts . size ( ) == 1 ) return ( ( Weight ) weights . get ( 0 ) ) . explain ( reader , doc ) ; Explanation result = new Explanation ( ) ; float max = 0.0f , sum = 0.0f ; result . setDescription ( tieBreakerMultiplier == 0.0f ? "max of:" : "max plus " + tieBreakerMultiplier + " times others of:" ) ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { Explanation e = ( ( Weight ) weights . get ( i ) ) . explain ( reader , doc ) ; if ( e . getValue ( ) > 0 ) { result . addDetail ( e ) ; sum += e . getValue ( ) ; max = Math . max ( max , e . getValue ( ) ) ; } } result . setValue ( max + ( sum - max ) * tieBreakerMultiplier ) ; return result ; } } protected Weight createWeight ( Searcher searcher ) throws IOException { return new DisjunctionMaxWeight ( searcher ) ; } public Query rewrite ( IndexReader reader ) throws IOException { if ( disjuncts . size ( ) == 1 ) { Query singleton = ( Query ) disjuncts . get ( 0 ) ; Query result = singleton . rewrite ( reader ) ; if ( getBoost ( ) != 1.0f ) { if ( result == singleton ) result = ( Query ) result . clone ( ) ; result . setBoost ( getBoost ( ) * result . getBoost ( ) ) ; } return result ; } DisjunctionMaxQuery clone = null ; for ( int i = 0 ; i < disjuncts . size ( ) ; i ++ ) { Query clause = ( Query ) disjuncts . get ( i ) ; Query rewrite = clause . rewrite ( reader ) ; if ( rewrite != clause ) { if ( clone == null ) clone = ( DisjunctionMaxQuery ) this . clone ( ) ; clone . disjuncts . set ( i , rewrite ) ; } } if ( clone != null ) return clone ; else return this ; } public Object clone ( ) { DisjunctionMaxQuery clone = ( DisjunctionMaxQuery ) super . clone ( ) ; clone . disjuncts = ( ArrayList ) this . disjuncts . clone ( ) ; return clone ; } public void extractTerms ( Set terms ) { for ( int i = 0 ; i < disjuncts . size ( ) ; i ++ ) { ( ( Query ) disjuncts . get ( i ) ) . extractTerms ( terms ) ; } } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "(" ) ; for ( int i = 0 ; i < disjuncts . size ( ) ; i ++ ) { Query subquery = ( Query ) disjuncts . get ( i ) ; if ( subquery instanceof BooleanQuery ) { buffer . append ( "(" ) ; buffer . append ( subquery . toString ( field ) ) ; buffer . append ( ")" ) ; } else buffer . append ( subquery . toString ( field ) ) ; if ( i != disjuncts . size ( ) - 1 ) buffer . append ( " | " ) ; } buffer . append ( ")" ) ; if ( tieBreakerMultiplier != 0.0f ) { buffer . append ( "~" ) ; buffer . append ( tieBreakerMultiplier ) ; } if ( getBoost ( ) != 1.0 ) { buffer . append ( "^" ) ; buffer . append ( getBoost ( ) ) ; } return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof DisjunctionMaxQuery ) ) return false ; DisjunctionMaxQuery other = ( DisjunctionMaxQuery ) o ; return this . getBoost ( ) == other . getBoost ( ) && this . tieBreakerMultiplier == other . tieBreakerMultiplier && this . disjuncts . equals ( other . disjuncts ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) + Float . floatToIntBits ( tieBreakerMultiplier ) + disjuncts . hashCode ( ) ; } } 	1	['14', '2', '0', '6', '38', '0', '1', '6', '11', '0.384615385', '318', '1', '0', '0.5', '0.171428571', '2', '5', '21.57142857', '6', '1.5714', '1']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . TermDocs ; final class TermScorer extends Scorer { private Weight weight ; private TermDocs termDocs ; private byte [ ] norms ; private float weightValue ; private int doc ; private final int [ ] docs = new int [ 32 ] ; private final int [ ] freqs = new int [ 32 ] ; private int pointer ; private int pointerMax ; private static final int SCORE_CACHE_SIZE = 32 ; private float [ ] scoreCache = new float [ SCORE_CACHE_SIZE ] ; TermScorer ( Weight weight , TermDocs td , Similarity similarity , byte [ ] norms ) { super ( similarity ) ; this . weight = weight ; this . termDocs = td ; this . norms = norms ; this . weightValue = weight . getValue ( ) ; for ( int i = 0 ; i < SCORE_CACHE_SIZE ; i ++ ) scoreCache [ i ] = getSimilarity ( ) . tf ( i ) * weightValue ; } public void score ( HitCollector hc ) throws IOException { next ( ) ; score ( hc , Integer . MAX_VALUE ) ; } protected boolean score ( HitCollector c , int end ) throws IOException { Similarity similarity = getSimilarity ( ) ; float [ ] normDecoder = Similarity . getNormDecoder ( ) ; while ( doc < end ) { int f = freqs [ pointer ] ; float score = f < SCORE_CACHE_SIZE ? scoreCache [ f ] : similarity . tf ( f ) * weightValue ; score *= normDecoder [ norms [ doc ] & 0xFF ] ; c . collect ( doc , score ) ; if ( ++ pointer >= pointerMax ) { pointerMax = termDocs . read ( docs , freqs ) ; if ( pointerMax != 0 ) { pointer = 0 ; } else { termDocs . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } } doc = docs [ pointer ] ; } return true ; } public int doc ( ) { return doc ; } public boolean next ( ) throws IOException { pointer ++ ; if ( pointer >= pointerMax ) { pointerMax = termDocs . read ( docs , freqs ) ; if ( pointerMax != 0 ) { pointer = 0 ; } else { termDocs . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } } doc = docs [ pointer ] ; return true ; } public float score ( ) { int f = freqs [ pointer ] ; float raw = f < SCORE_CACHE_SIZE ? scoreCache [ f ] : getSimilarity ( ) . tf ( f ) * weightValue ; return raw * Similarity . decodeNorm ( norms [ doc ] ) ; } public boolean skipTo ( int target ) throws IOException { for ( pointer ++ ; pointer < pointerMax ; pointer ++ ) { if ( docs [ pointer ] >= target ) { doc = docs [ pointer ] ; return true ; } } boolean result = termDocs . skipTo ( target ) ; if ( result ) { pointerMax = 1 ; pointer = 0 ; docs [ pointer ] = doc = termDocs . doc ( ) ; freqs [ pointer ] = termDocs . freq ( ) ; } else { doc = Integer . MAX_VALUE ; } return result ; } public Explanation explain ( int doc ) throws IOException { TermQuery query = ( TermQuery ) weight . getQuery ( ) ; Explanation tfExplanation = new Explanation ( ) ; int tf = 0 ; while ( pointer < pointerMax ) { if ( docs [ pointer ] == doc ) tf = freqs [ pointer ] ; pointer ++ ; } if ( tf == 0 ) { while ( termDocs . next ( ) ) { if ( termDocs . doc ( ) == doc ) { tf = termDocs . freq ( ) ; } } } termDocs . close ( ) ; tfExplanation . setValue ( getSimilarity ( ) . tf ( tf ) ) ; tfExplanation . setDescription ( "tf(termFreq(" + query . getTerm ( ) + ")=" + tf + ")" ) ; return tfExplanation ; } public String toString ( ) { return "scorer(" + weight + ")" ; } } 	0	['9', '2', '0', '10', '32', '0', '1', '9', '7', '0.545454545', '409', '1', '2', '0.5', '0.285714286', '1', '3', '43.22222222', '2', '1', '0']
package org . apache . lucene . index ; import java . io . ByteArrayOutputStream ; import java . io . IOException ; import java . util . Enumeration ; import java . util . zip . Deflater ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . Field ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; final class FieldsWriter { static final byte FIELD_IS_TOKENIZED = 0x1 ; static final byte FIELD_IS_BINARY = 0x2 ; static final byte FIELD_IS_COMPRESSED = 0x4 ; private FieldInfos fieldInfos ; private IndexOutput fieldsStream ; private IndexOutput indexStream ; FieldsWriter ( Directory d , String segment , FieldInfos fn ) throws IOException { fieldInfos = fn ; fieldsStream = d . createOutput ( segment + ".fdt" ) ; indexStream = d . createOutput ( segment + ".fdx" ) ; } final void close ( ) throws IOException { fieldsStream . close ( ) ; indexStream . close ( ) ; } final void addDocument ( Document doc ) throws IOException { indexStream . writeLong ( fieldsStream . getFilePointer ( ) ) ; int storedCount = 0 ; Enumeration fields = doc . fields ( ) ; while ( fields . hasMoreElements ( ) ) { Field field = ( Field ) fields . nextElement ( ) ; if ( field . isStored ( ) ) storedCount ++ ; } fieldsStream . writeVInt ( storedCount ) ; fields = doc . fields ( ) ; while ( fields . hasMoreElements ( ) ) { Field field = ( Field ) fields . nextElement ( ) ; if ( field . isStored ( ) ) { fieldsStream . writeVInt ( fieldInfos . fieldNumber ( field . name ( ) ) ) ; byte bits = 0 ; if ( field . isTokenized ( ) ) bits |= FieldsWriter . FIELD_IS_TOKENIZED ; if ( field . isBinary ( ) ) bits |= FieldsWriter . FIELD_IS_BINARY ; if ( field . isCompressed ( ) ) bits |= FieldsWriter . FIELD_IS_COMPRESSED ; fieldsStream . writeByte ( bits ) ; if ( field . isCompressed ( ) ) { byte [ ] data = null ; if ( field . isBinary ( ) ) { data = compress ( field . binaryValue ( ) ) ; } else { data = compress ( field . stringValue ( ) . getBytes ( "UTF-8" ) ) ; } final int len = data . length ; fieldsStream . writeVInt ( len ) ; fieldsStream . writeBytes ( data , len ) ; } else { if ( field . isBinary ( ) ) { byte [ ] data = field . binaryValue ( ) ; final int len = data . length ; fieldsStream . writeVInt ( len ) ; fieldsStream . writeBytes ( data , len ) ; } else { fieldsStream . writeString ( field . stringValue ( ) ) ; } } } } } private final byte [ ] compress ( byte [ ] input ) { Deflater compressor = new Deflater ( ) ; compressor . setLevel ( Deflater . BEST_COMPRESSION ) ; compressor . setInput ( input ) ; compressor . finish ( ) ; ByteArrayOutputStream bos = new ByteArrayOutputStream ( input . length ) ; byte [ ] buf = new byte [ 1024 ] ; while ( ! compressor . finished ( ) ) { int count = compressor . deflate ( buf ) ; bos . write ( buf , 0 , count ) ; } compressor . end ( ) ; return bos . toByteArray ( ) ; } } 	1	['4', '1', '0', '7', '38', '0', '2', '5', '0', '0.888888889', '224', '0.5', '3', '0', '0.375', '0', '0', '53.5', '2', '1', '2']
package org . apache . lucene . util ; public abstract class PriorityQueue { private Object [ ] heap ; private int size ; private int maxSize ; protected abstract boolean lessThan ( Object a , Object b ) ; protected final void initialize ( int maxSize ) { size = 0 ; int heapSize = maxSize + 1 ; heap = new Object [ heapSize ] ; this . maxSize = maxSize ; } public final void put ( Object element ) { size ++ ; heap [ size ] = element ; upHeap ( ) ; } public boolean insert ( Object element ) { if ( size < maxSize ) { put ( element ) ; return true ; } else if ( size > 0 && ! lessThan ( element , top ( ) ) ) { heap [ 1 ] = element ; adjustTop ( ) ; return true ; } else return false ; } public final Object top ( ) { if ( size > 0 ) return heap [ 1 ] ; else return null ; } public final Object pop ( ) { if ( size > 0 ) { Object result = heap [ 1 ] ; heap [ 1 ] = heap [ size ] ; heap [ size ] = null ; size -- ; downHeap ( ) ; return result ; } else return null ; } public final void adjustTop ( ) { downHeap ( ) ; } public final int size ( ) { return size ; } public final void clear ( ) { for ( int i = 0 ; i <= size ; i ++ ) heap [ i ] = null ; size = 0 ; } private final void upHeap ( ) { int i = size ; Object node = heap [ i ] ; int j = i > > > 1 ; while ( j > 0 && lessThan ( node , heap [ j ] ) ) { heap [ i ] = heap [ j ] ; i = j ; j = j > > > 1 ; } heap [ i ] = node ; } private final void downHeap ( ) { int i = 1 ; Object node = heap [ i ] ; int j = i << 1 ; int k = j + 1 ; if ( k <= size && lessThan ( heap [ k ] , heap [ j ] ) ) { j = k ; } while ( j <= size && lessThan ( heap [ j ] , node ) ) { heap [ i ] = heap [ j ] ; i = j ; j = i << 1 ; k = j + 1 ; if ( k <= size && lessThan ( heap [ k ] , heap [ j ] ) ) { j = k ; } } heap [ i ] = node ; } } 	0	['12', '1', '10', '13', '13', '0', '13', '0', '8', '0.454545455', '275', '1', '0', '0', '0.444444444', '0', '0', '21.66666667', '7', '2.0833', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexInput ; final class SegmentTermPositions extends SegmentTermDocs implements TermPositions { private IndexInput proxStream ; private int proxCount ; private int position ; SegmentTermPositions ( SegmentReader p ) { super ( p ) ; this . proxStream = ( IndexInput ) parent . proxStream . clone ( ) ; } final void seek ( TermInfo ti ) throws IOException { super . seek ( ti ) ; if ( ti != null ) proxStream . seek ( ti . proxPointer ) ; proxCount = 0 ; } public final void close ( ) throws IOException { super . close ( ) ; proxStream . close ( ) ; } public final int nextPosition ( ) throws IOException { proxCount -- ; return position += proxStream . readVInt ( ) ; } protected final void skippingDoc ( ) throws IOException { for ( int f = freq ; f > 0 ; f -- ) proxStream . readVInt ( ) ; } public final boolean next ( ) throws IOException { for ( int f = proxCount ; f > 0 ; f -- ) proxStream . readVInt ( ) ; if ( super . next ( ) ) { proxCount = freq ; position = 0 ; return true ; } return false ; } public final int read ( final int [ ] docs , final int [ ] freqs ) { throw new UnsupportedOperationException ( "TermPositions does not support processing multiple documents in one call. Use TermDocs instead." ) ; } protected void skipProx ( long proxPointer ) throws IOException { proxStream . seek ( proxPointer ) ; proxCount = 0 ; } } 	1	['8', '2', '0', '5', '17', '0', '1', '5', '4', '0.523809524', '108', '1', '1', '0.611111111', '0.3', '1', '3', '12.125', '1', '0.875', '4']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import java . io . IOException ; public final class FuzzyTermEnum extends FilteredTermEnum { private static final int TYPICAL_LONGEST_WORD_IN_INDEX = 19 ; private int [ ] [ ] d ; private float similarity ; private boolean endEnum = false ; private Term searchTerm = null ; private final String field ; private final String text ; private final String prefix ; private final float minimumSimilarity ; private final float scale_factor ; private final int [ ] maxDistances = new int [ TYPICAL_LONGEST_WORD_IN_INDEX ] ; public FuzzyTermEnum ( IndexReader reader , Term term ) throws IOException { this ( reader , term , FuzzyQuery . defaultMinSimilarity , FuzzyQuery . defaultPrefixLength ) ; } public FuzzyTermEnum ( IndexReader reader , Term term , float minSimilarity ) throws IOException { this ( reader , term , minSimilarity , FuzzyQuery . defaultPrefixLength ) ; } public FuzzyTermEnum ( IndexReader reader , Term term , final float minSimilarity , final int prefixLength ) throws IOException { super ( ) ; if ( minSimilarity >= 1.0f ) throw new IllegalArgumentException ( "minimumSimilarity cannot be greater than or equal to 1" ) ; else if ( minSimilarity < 0.0f ) throw new IllegalArgumentException ( "minimumSimilarity cannot be less than 0" ) ; if ( prefixLength < 0 ) throw new IllegalArgumentException ( "prefixLength cannot be less than 0" ) ; this . minimumSimilarity = minSimilarity ; this . scale_factor = 1.0f / ( 1.0f - minimumSimilarity ) ; this . searchTerm = term ; this . field = searchTerm . field ( ) ; final int fullSearchTermLength = searchTerm . text ( ) . length ( ) ; final int realPrefixLength = prefixLength > fullSearchTermLength ? fullSearchTermLength : prefixLength ; this . text = searchTerm . text ( ) . substring ( realPrefixLength ) ; this . prefix = searchTerm . text ( ) . substring ( 0 , realPrefixLength ) ; initializeMaxDistances ( ) ; this . d = initDistanceArray ( ) ; setEnum ( reader . terms ( new Term ( searchTerm . field ( ) , prefix ) ) ) ; } protected final boolean termCompare ( Term term ) { if ( field == term . field ( ) && term . text ( ) . startsWith ( prefix ) ) { final String target = term . text ( ) . substring ( prefix . length ( ) ) ; this . similarity = similarity ( target ) ; return ( similarity > minimumSimilarity ) ; } endEnum = true ; return false ; } public final float difference ( ) { return ( float ) ( ( similarity - minimumSimilarity ) * scale_factor ) ; } public final boolean endEnum ( ) { return endEnum ; } private static final int min ( int a , int b , int c ) { final int t = ( a < b ) ? a : b ; return ( t < c ) ? t : c ; } private final int [ ] [ ] initDistanceArray ( ) { return new int [ this . text . length ( ) + 1 ] [ TYPICAL_LONGEST_WORD_IN_INDEX ] ; } private synchronized final float similarity ( final String target ) { final int m = target . length ( ) ; final int n = text . length ( ) ; if ( n == 0 ) { return prefix . length ( ) == 0 ? 0.0f : 1.0f - ( ( float ) m / prefix . length ( ) ) ; } if ( m == 0 ) { return prefix . length ( ) == 0 ? 0.0f : 1.0f - ( ( float ) n / prefix . length ( ) ) ; } final int maxDistance = getMaxDistance ( m ) ; if ( maxDistance < Math . abs ( m - n ) ) { return 0.0f ; } if ( d [ 0 ] . length <= m ) { growDistanceArray ( m ) ; } for ( int i = 0 ; i <= n ; i ++ ) d [ i ] [ 0 ] = i ; for ( int j = 0 ; j <= m ; j ++ ) d [ 0 ] [ j ] = j ; for ( int i = 1 ; i <= n ; i ++ ) { int bestPossibleEditDistance = m ; final char s_i = text . charAt ( i - 1 ) ; for ( int j = 1 ; j <= m ; j ++ ) { if ( s_i != target . charAt ( j - 1 ) ) { d [ i ] [ j ] = min ( d [ i - 1 ] [ j ] , d [ i ] [ j - 1 ] , d [ i - 1 ] [ j - 1 ] ) + 1 ; } else { d [ i ] [ j ] = min ( d [ i - 1 ] [ j ] + 1 , d [ i ] [ j - 1 ] + 1 , d [ i - 1 ] [ j - 1 ] ) ; } bestPossibleEditDistance = Math . min ( bestPossibleEditDistance , d [ i ] [ j ] ) ; } if ( i > maxDistance && bestPossibleEditDistance > maxDistance ) { return 0.0f ; } } return 1.0f - ( ( float ) d [ n ] [ m ] / ( float ) ( prefix . length ( ) + Math . min ( n , m ) ) ) ; } private void growDistanceArray ( int m ) { for ( int i = 0 ; i < d . length ; i ++ ) { d [ i ] = new int [ m + 1 ] ; } } private final int getMaxDistance ( int m ) { return ( m < maxDistances . length ) ? maxDistances [ m ] : calculateMaxDistance ( m ) ; } private void initializeMaxDistances ( ) { for ( int i = 0 ; i < maxDistances . length ; i ++ ) { maxDistances [ i ] = calculateMaxDistance ( i ) ; } } private int calculateMaxDistance ( int m ) { return ( int ) ( ( 1 - minimumSimilarity ) * ( Math . min ( text . length ( ) , m ) + prefix . length ( ) ) ) ; } public void close ( ) throws IOException { super . close ( ) ; } } 	0	['14', '3', '0', '5', '29', '53', '1', '4', '6', '0.692307692', '514', '1', '1', '0.541666667', '0.333333333', '1', '4', '34.92857143', '14', '2.2857', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . * ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . Field ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . Directory ; import org . apache . lucene . util . BitVector ; import org . apache . lucene . search . DefaultSimilarity ; class SegmentReader extends IndexReader { private String segment ; FieldInfos fieldInfos ; private FieldsReader fieldsReader ; TermInfosReader tis ; TermVectorsReader termVectorsReaderOrig = null ; ThreadLocal termVectorsLocal = new ThreadLocal ( ) ; BitVector deletedDocs = null ; private boolean deletedDocsDirty = false ; private boolean normsDirty = false ; private boolean undeleteAll = false ; IndexInput freqStream ; IndexInput proxStream ; CompoundFileReader cfsReader = null ; private class Norm { public Norm ( IndexInput in , int number ) { this . in = in ; this . number = number ; } private IndexInput in ; private byte [ ] bytes ; private boolean dirty ; private int number ; private void reWrite ( ) throws IOException { IndexOutput out = directory ( ) . createOutput ( segment + ".tmp" ) ; try { out . writeBytes ( bytes , maxDoc ( ) ) ; } finally { out . close ( ) ; } String fileName ; if ( cfsReader == null ) fileName = segment + ".f" + number ; else { fileName = segment + ".s" + number ; } directory ( ) . renameFile ( segment + ".tmp" , fileName ) ; this . dirty = false ; } } private Hashtable norms = new Hashtable ( ) ; private static Class IMPL ; static { try { String name = System . getProperty ( "org.apache.lucene.SegmentReader.class" , SegmentReader . class . getName ( ) ) ; IMPL = Class . forName ( name ) ; } catch ( ClassNotFoundException e ) { throw new RuntimeException ( "cannot load SegmentReader class: " + e , e ) ; } catch ( SecurityException se ) { try { IMPL = Class . forName ( SegmentReader . class . getName ( ) ) ; } catch ( ClassNotFoundException e ) { throw new RuntimeException ( "cannot load default SegmentReader class: " + e , e ) ; } } } protected SegmentReader ( ) { super ( null ) ; } public static SegmentReader get ( SegmentInfo si ) throws IOException { return get ( si . dir , si , null , false , false ) ; } public static SegmentReader get ( SegmentInfos sis , SegmentInfo si , boolean closeDir ) throws IOException { return get ( si . dir , si , sis , closeDir , true ) ; } public static SegmentReader get ( Directory dir , SegmentInfo si , SegmentInfos sis , boolean closeDir , boolean ownDir ) throws IOException { SegmentReader instance ; try { instance = ( SegmentReader ) IMPL . newInstance ( ) ; } catch ( Exception e ) { throw new RuntimeException ( "cannot load SegmentReader class: " + e , e ) ; } instance . init ( dir , sis , closeDir , ownDir ) ; instance . initialize ( si ) ; return instance ; } private void initialize ( SegmentInfo si ) throws IOException { segment = si . name ; Directory cfsDir = directory ( ) ; if ( directory ( ) . fileExists ( segment + ".cfs" ) ) { cfsReader = new CompoundFileReader ( directory ( ) , segment + ".cfs" ) ; cfsDir = cfsReader ; } fieldInfos = new FieldInfos ( cfsDir , segment + ".fnm" ) ; fieldsReader = new FieldsReader ( cfsDir , segment , fieldInfos ) ; tis = new TermInfosReader ( cfsDir , segment , fieldInfos ) ; if ( hasDeletions ( si ) ) deletedDocs = new BitVector ( directory ( ) , segment + ".del" ) ; freqStream = cfsDir . openInput ( segment + ".frq" ) ; proxStream = cfsDir . openInput ( segment + ".prx" ) ; openNorms ( cfsDir ) ; if ( fieldInfos . hasVectors ( ) ) { termVectorsReaderOrig = new TermVectorsReader ( cfsDir , segment , fieldInfos ) ; } } protected void finalize ( ) { termVectorsLocal . set ( null ) ; super . finalize ( ) ; } protected void doCommit ( ) throws IOException { if ( deletedDocsDirty ) { deletedDocs . write ( directory ( ) , segment + ".tmp" ) ; directory ( ) . renameFile ( segment + ".tmp" , segment + ".del" ) ; } if ( undeleteAll && directory ( ) . fileExists ( segment + ".del" ) ) { directory ( ) . deleteFile ( segment + ".del" ) ; } if ( normsDirty ) { Enumeration values = norms . elements ( ) ; while ( values . hasMoreElements ( ) ) { Norm norm = ( Norm ) values . nextElement ( ) ; if ( norm . dirty ) { norm . reWrite ( ) ; } } } deletedDocsDirty = false ; normsDirty = false ; undeleteAll = false ; } protected void doClose ( ) throws IOException { fieldsReader . close ( ) ; tis . close ( ) ; if ( freqStream != null ) freqStream . close ( ) ; if ( proxStream != null ) proxStream . close ( ) ; closeNorms ( ) ; if ( termVectorsReaderOrig != null ) termVectorsReaderOrig . close ( ) ; if ( cfsReader != null ) cfsReader . close ( ) ; } static boolean hasDeletions ( SegmentInfo si ) throws IOException { return si . dir . fileExists ( si . name + ".del" ) ; } public boolean hasDeletions ( ) { return deletedDocs != null ; } static boolean usesCompoundFile ( SegmentInfo si ) throws IOException { return si . dir . fileExists ( si . name + ".cfs" ) ; } static boolean hasSeparateNorms ( SegmentInfo si ) throws IOException { String [ ] result = si . dir . list ( ) ; String pattern = si . name + ".s" ; int patternLength = pattern . length ( ) ; for ( int i = 0 ; i < result . length ; i ++ ) { if ( result [ i ] . startsWith ( pattern ) && Character . isDigit ( result [ i ] . charAt ( patternLength ) ) ) return true ; } return false ; } protected void doDelete ( int docNum ) { if ( deletedDocs == null ) deletedDocs = new BitVector ( maxDoc ( ) ) ; deletedDocsDirty = true ; undeleteAll = false ; deletedDocs . set ( docNum ) ; } protected void doUndeleteAll ( ) { deletedDocs = null ; deletedDocsDirty = false ; undeleteAll = true ; } Vector files ( ) throws IOException { Vector files = new Vector ( 16 ) ; for ( int i = 0 ; i < IndexFileNames . INDEX_EXTENSIONS . length ; i ++ ) { String name = segment + "." + IndexFileNames . INDEX_EXTENSIONS [ i ] ; if ( directory ( ) . fileExists ( name ) ) files . addElement ( name ) ; } for ( int i = 0 ; i < fieldInfos . size ( ) ; i ++ ) { FieldInfo fi = fieldInfos . fieldInfo ( i ) ; if ( fi . isIndexed && ! fi . omitNorms ) { String name ; if ( cfsReader == null ) name = segment + ".f" + i ; else name = segment + ".s" + i ; if ( directory ( ) . fileExists ( name ) ) files . addElement ( name ) ; } } return files ; } public TermEnum terms ( ) { return tis . terms ( ) ; } public TermEnum terms ( Term t ) throws IOException { return tis . terms ( t ) ; } public synchronized Document document ( int n ) throws IOException { if ( isDeleted ( n ) ) throw new IllegalArgumentException ( "attempt to access a deleted document" ) ; return fieldsReader . doc ( n ) ; } public synchronized boolean isDeleted ( int n ) { return ( deletedDocs != null && deletedDocs . get ( n ) ) ; } public TermDocs termDocs ( ) throws IOException { return new SegmentTermDocs ( this ) ; } public TermPositions termPositions ( ) throws IOException { return new SegmentTermPositions ( this ) ; } public int docFreq ( Term t ) throws IOException { TermInfo ti = tis . get ( t ) ; if ( ti != null ) return ti . docFreq ; else return 0 ; } public int numDocs ( ) { int n = maxDoc ( ) ; if ( deletedDocs != null ) n -= deletedDocs . count ( ) ; return n ; } public int maxDoc ( ) { return fieldsReader . size ( ) ; } public Collection getFieldNames ( IndexReader . FieldOption fieldOption ) { Set fieldSet = new HashSet ( ) ; for ( int i = 0 ; i < fieldInfos . size ( ) ; i ++ ) { FieldInfo fi = fieldInfos . fieldInfo ( i ) ; if ( fieldOption == IndexReader . FieldOption . ALL ) { fieldSet . add ( fi . name ) ; } else if ( ! fi . isIndexed && fieldOption == IndexReader . FieldOption . UNINDEXED ) { fieldSet . add ( fi . name ) ; } else if ( fi . isIndexed && fieldOption == IndexReader . FieldOption . INDEXED ) { fieldSet . add ( fi . name ) ; } else if ( fi . isIndexed && fi . storeTermVector == false && fieldOption == IndexReader . FieldOption . INDEXED_NO_TERMVECTOR ) { fieldSet . add ( fi . name ) ; } else if ( fi . storeTermVector == true && fi . storePositionWithTermVector == false && fi . storeOffsetWithTermVector == false && fieldOption == IndexReader . FieldOption . TERMVECTOR ) { fieldSet . add ( fi . name ) ; } else if ( fi . isIndexed && fi . storeTermVector && fieldOption == IndexReader . FieldOption . INDEXED_WITH_TERMVECTOR ) { fieldSet . add ( fi . name ) ; } else if ( fi . storePositionWithTermVector && fi . storeOffsetWithTermVector == false && fieldOption == IndexReader . FieldOption . TERMVECTOR_WITH_POSITION ) { fieldSet . add ( fi . name ) ; } else if ( fi . storeOffsetWithTermVector && fi . storePositionWithTermVector == false && fieldOption == IndexReader . FieldOption . TERMVECTOR_WITH_OFFSET ) { fieldSet . add ( fi . name ) ; } else if ( ( fi . storeOffsetWithTermVector && fi . storePositionWithTermVector ) && fieldOption == IndexReader . FieldOption . TERMVECTOR_WITH_POSITION_OFFSET ) { fieldSet . add ( fi . name ) ; } } return fieldSet ; } public synchronized boolean hasNorms ( String field ) { return norms . containsKey ( field ) ; } static byte [ ] createFakeNorms ( int size ) { byte [ ] ones = new byte [ size ] ; Arrays . fill ( ones , DefaultSimilarity . encodeNorm ( 1.0f ) ) ; return ones ; } private byte [ ] ones ; private byte [ ] fakeNorms ( ) { if ( ones == null ) ones = createFakeNorms ( maxDoc ( ) ) ; return ones ; } protected synchronized byte [ ] getNorms ( String field ) throws IOException { Norm norm = ( Norm ) norms . get ( field ) ; if ( norm == null ) return null ; if ( norm . bytes == null ) { byte [ ] bytes = new byte [ maxDoc ( ) ] ; norms ( field , bytes , 0 ) ; norm . bytes = bytes ; } return norm . bytes ; } public synchronized byte [ ] norms ( String field ) throws IOException { byte [ ] bytes = getNorms ( field ) ; if ( bytes == null ) bytes = fakeNorms ( ) ; return bytes ; } protected void doSetNorm ( int doc , String field , byte value ) throws IOException { Norm norm = ( Norm ) norms . get ( field ) ; if ( norm == null ) return ; norm . dirty = true ; normsDirty = true ; norms ( field ) [ doc ] = value ; } public synchronized void norms ( String field , byte [ ] bytes , int offset ) throws IOException { Norm norm = ( Norm ) norms . get ( field ) ; if ( norm == null ) { System . arraycopy ( fakeNorms ( ) , 0 , bytes , offset , maxDoc ( ) ) ; return ; } if ( norm . bytes != null ) { System . arraycopy ( norm . bytes , 0 , bytes , offset , maxDoc ( ) ) ; return ; } IndexInput normStream = ( IndexInput ) norm . in . clone ( ) ; try { normStream . seek ( 0 ) ; normStream . readBytes ( bytes , offset , maxDoc ( ) ) ; } finally { normStream . close ( ) ; } } private void openNorms ( Directory cfsDir ) throws IOException { for ( int i = 0 ; i < fieldInfos . size ( ) ; i ++ ) { FieldInfo fi = fieldInfos . fieldInfo ( i ) ; if ( fi . isIndexed && ! fi . omitNorms ) { String fileName = segment + ".s" + fi . number ; Directory d = directory ( ) ; if ( ! d . fileExists ( fileName ) ) { fileName = segment + ".f" + fi . number ; d = cfsDir ; } norms . put ( fi . name , new Norm ( d . openInput ( fileName ) , fi . number ) ) ; } } } private void closeNorms ( ) throws IOException { synchronized ( norms ) { Enumeration enumerator = norms . elements ( ) ; while ( enumerator . hasMoreElements ( ) ) { Norm norm = ( Norm ) enumerator . nextElement ( ) ; norm . in . close ( ) ; } } } private TermVectorsReader getTermVectorsReader ( ) { TermVectorsReader tvReader = ( TermVectorsReader ) termVectorsLocal . get ( ) ; if ( tvReader == null ) { tvReader = ( TermVectorsReader ) termVectorsReaderOrig . clone ( ) ; termVectorsLocal . set ( tvReader ) ; } return tvReader ; } public TermFreqVector getTermFreqVector ( int docNumber , String field ) throws IOException { FieldInfo fi = fieldInfos . fieldInfo ( field ) ; if ( fi == null || ! fi . storeTermVector || termVectorsReaderOrig == null ) return null ; TermVectorsReader termVectorsReader = getTermVectorsReader ( ) ; if ( termVectorsReader == null ) return null ; return termVectorsReader . get ( docNumber , field ) ; } public TermFreqVector [ ] getTermFreqVectors ( int docNumber ) throws IOException { if ( termVectorsReaderOrig == null ) return null ; TermVectorsReader termVectorsReader = getTermVectorsReader ( ) ; if ( termVectorsReader == null ) return null ; return termVectorsReader . get ( docNumber ) ; } } 	1	['40', '2', '0', '29', '123', '576', '6', '26', '19', '0.883861237', '1200', '0.470588235', '8', '0.591397849', '0.136752137', '1', '5', '28.575', '26', '1.75', '11']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Query ; import org . apache . lucene . util . ToStringUtils ; public class SpanFirstQuery extends SpanQuery { private SpanQuery match ; private int end ; public SpanFirstQuery ( SpanQuery match , int end ) { this . match = match ; this . end = end ; } public SpanQuery getMatch ( ) { return match ; } public int getEnd ( ) { return end ; } public String getField ( ) { return match . getField ( ) ; } public Collection getTerms ( ) { return match . getTerms ( ) ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "spanFirst(" ) ; buffer . append ( match . toString ( field ) ) ; buffer . append ( ", " ) ; buffer . append ( end ) ; buffer . append ( ")" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public void extractTerms ( Set terms ) { match . extractTerms ( terms ) ; } public Spans getSpans ( final IndexReader reader ) throws IOException { return new Spans ( ) { private Spans spans = match . getSpans ( reader ) ; public boolean next ( ) throws IOException { while ( spans . next ( ) ) { if ( end ( ) <= end ) return true ; } return false ; } public boolean skipTo ( int target ) throws IOException { if ( ! spans . skipTo ( target ) ) return false ; if ( spans . end ( ) <= end ) return true ; return next ( ) ; } public int doc ( ) { return spans . doc ( ) ; } public int start ( ) { return spans . start ( ) ; } public int end ( ) { return spans . end ( ) ; } public String toString ( ) { return "spans(" + SpanFirstQuery . this . toString ( ) + ")" ; } } ; } public Query rewrite ( IndexReader reader ) throws IOException { SpanFirstQuery clone = null ; SpanQuery rewritten = ( SpanQuery ) match . rewrite ( reader ) ; if ( rewritten != match ) { clone = ( SpanFirstQuery ) this . clone ( ) ; clone . match = rewritten ; } if ( clone != null ) { return clone ; } else { return this ; } } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof SpanFirstQuery ) ) return false ; SpanFirstQuery other = ( SpanFirstQuery ) o ; return this . end == other . end && this . match . equals ( other . match ) && this . getBoost ( ) == other . getBoost ( ) ; } public int hashCode ( ) { int h = match . hashCode ( ) ; h ^= ( h << 8 ) | ( h > > > 25 ) ; h ^= Float . floatToRawIntBits ( getBoost ( ) ) ^ end ; return h ; } } 	0	['13', '3', '0', '6', '30', '0', '1', '6', '11', '0.416666667', '176', '1', '1', '0.571428571', '0.192307692', '2', '2', '12.38461538', '6', '1.3077', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . PriorityQueue ; import java . io . IOException ; import java . util . WeakHashMap ; import java . util . HashMap ; import java . util . Map ; import java . util . Locale ; import java . text . Collator ; public class FieldSortedHitQueue extends PriorityQueue { public FieldSortedHitQueue ( IndexReader reader , SortField [ ] fields , int size ) throws IOException { final int n = fields . length ; comparators = new ScoreDocComparator [ n ] ; this . fields = new SortField [ n ] ; for ( int i = 0 ; i < n ; ++ i ) { String fieldname = fields [ i ] . getField ( ) ; comparators [ i ] = getCachedComparator ( reader , fieldname , fields [ i ] . getType ( ) , fields [ i ] . getLocale ( ) , fields [ i ] . getFactory ( ) ) ; if ( comparators [ i ] . sortType ( ) == SortField . STRING ) { this . fields [ i ] = new SortField ( fieldname , fields [ i ] . getLocale ( ) , fields [ i ] . getReverse ( ) ) ; } else { this . fields [ i ] = new SortField ( fieldname , comparators [ i ] . sortType ( ) , fields [ i ] . getReverse ( ) ) ; } } initialize ( size ) ; } protected ScoreDocComparator [ ] comparators ; protected SortField [ ] fields ; protected float maxscore = Float . NEGATIVE_INFINITY ; public float getMaxScore ( ) { return maxscore ; } public boolean insert ( FieldDoc fdoc ) { maxscore = Math . max ( maxscore , fdoc . score ) ; return super . insert ( fdoc ) ; } public boolean insert ( Object fdoc ) { return insert ( ( FieldDoc ) fdoc ) ; } protected boolean lessThan ( final Object a , final Object b ) { final ScoreDoc docA = ( ScoreDoc ) a ; final ScoreDoc docB = ( ScoreDoc ) b ; final int n = comparators . length ; int c = 0 ; for ( int i = 0 ; i < n && c == 0 ; ++ i ) { c = ( fields [ i ] . reverse ) ? comparators [ i ] . compare ( docB , docA ) : comparators [ i ] . compare ( docA , docB ) ; } if ( c == 0 ) return docA . doc > docB . doc ; return c > 0 ; } FieldDoc fillFields ( final FieldDoc doc ) { final int n = comparators . length ; final Comparable [ ] fields = new Comparable [ n ] ; for ( int i = 0 ; i < n ; ++ i ) fields [ i ] = comparators [ i ] . sortValue ( doc ) ; doc . fields = fields ; return doc ; } SortField [ ] getFields ( ) { return fields ; } static final Map Comparators = new WeakHashMap ( ) ; static ScoreDocComparator lookup ( IndexReader reader , String field , int type , Locale locale , Object factory ) { FieldCacheImpl . Entry entry = ( factory != null ) ? new FieldCacheImpl . Entry ( field , factory ) : new FieldCacheImpl . Entry ( field , type , locale ) ; synchronized ( Comparators ) { HashMap readerCache = ( HashMap ) Comparators . get ( reader ) ; if ( readerCache == null ) return null ; return ( ScoreDocComparator ) readerCache . get ( entry ) ; } } static Object store ( IndexReader reader , String field , int type , Locale locale , Object factory , Object value ) { FieldCacheImpl . Entry entry = ( factory != null ) ? new FieldCacheImpl . Entry ( field , factory ) : new FieldCacheImpl . Entry ( field , type , locale ) ; synchronized ( Comparators ) { HashMap readerCache = ( HashMap ) Comparators . get ( reader ) ; if ( readerCache == null ) { readerCache = new HashMap ( ) ; Comparators . put ( reader , readerCache ) ; } return readerCache . put ( entry , value ) ; } } static ScoreDocComparator getCachedComparator ( IndexReader reader , String fieldname , int type , Locale locale , SortComparatorSource factory ) throws IOException { if ( type == SortField . DOC ) return ScoreDocComparator . INDEXORDER ; if ( type == SortField . SCORE ) return ScoreDocComparator . RELEVANCE ; ScoreDocComparator comparator = lookup ( reader , fieldname , type , locale , factory ) ; if ( comparator == null ) { switch ( type ) { case SortField . AUTO : comparator = comparatorAuto ( reader , fieldname ) ; break ; case SortField . INT : comparator = comparatorInt ( reader , fieldname ) ; break ; case SortField . FLOAT : comparator = comparatorFloat ( reader , fieldname ) ; break ; case SortField . STRING : if ( locale != null ) comparator = comparatorStringLocale ( reader , fieldname , locale ) ; else comparator = comparatorString ( reader , fieldname ) ; break ; case SortField . CUSTOM : comparator = factory . newComparator ( reader , fieldname ) ; break ; default : throw new RuntimeException ( "unknown field type: " + type ) ; } store ( reader , fieldname , type , locale , factory , comparator ) ; } return comparator ; } static ScoreDocComparator comparatorInt ( final IndexReader reader , final String fieldname ) throws IOException { final String field = fieldname . intern ( ) ; final int [ ] fieldOrder = FieldCache . DEFAULT . getInts ( reader , field ) ; return new ScoreDocComparator ( ) { public final int compare ( final ScoreDoc i , final ScoreDoc j ) { final int fi = fieldOrder [ i . doc ] ; final int fj = fieldOrder [ j . doc ] ; if ( fi < fj ) return - 1 ; if ( fi > fj ) return 1 ; return 0 ; } public Comparable sortValue ( final ScoreDoc i ) { return new Integer ( fieldOrder [ i . doc ] ) ; } public int sortType ( ) { return SortField . INT ; } } ; } static ScoreDocComparator comparatorFloat ( final IndexReader reader , final String fieldname ) throws IOException { final String field = fieldname . intern ( ) ; final float [ ] fieldOrder = FieldCache . DEFAULT . getFloats ( reader , field ) ; return new ScoreDocComparator ( ) { public final int compare ( final ScoreDoc i , final ScoreDoc j ) { final float fi = fieldOrder [ i . doc ] ; final float fj = fieldOrder [ j . doc ] ; if ( fi < fj ) return - 1 ; if ( fi > fj ) return 1 ; return 0 ; } public Comparable sortValue ( final ScoreDoc i ) { return new Float ( fieldOrder [ i . doc ] ) ; } public int sortType ( ) { return SortField . FLOAT ; } } ; } static ScoreDocComparator comparatorString ( final IndexReader reader , final String fieldname ) throws IOException { final String field = fieldname . intern ( ) ; final FieldCache . StringIndex index = FieldCache . DEFAULT . getStringIndex ( reader , field ) ; return new ScoreDocComparator ( ) { public final int compare ( final ScoreDoc i , final ScoreDoc j ) { final int fi = index . order [ i . doc ] ; final int fj = index . order [ j . doc ] ; if ( fi < fj ) return - 1 ; if ( fi > fj ) return 1 ; return 0 ; } public Comparable sortValue ( final ScoreDoc i ) { return index . lookup [ index . order [ i . doc ] ] ; } public int sortType ( ) { return SortField . STRING ; } } ; } static ScoreDocComparator comparatorStringLocale ( final IndexReader reader , final String fieldname , final Locale locale ) throws IOException { final Collator collator = Collator . getInstance ( locale ) ; final String field = fieldname . intern ( ) ; final String [ ] index = FieldCache . DEFAULT . getStrings ( reader , field ) ; return new ScoreDocComparator ( ) { public final int compare ( final ScoreDoc i , final ScoreDoc j ) { return collator . compare ( index [ i . doc ] , index [ j . doc ] ) ; } public Comparable sortValue ( final ScoreDoc i ) { return index [ i . doc ] ; } public int sortType ( ) { return SortField . STRING ; } } ; } static ScoreDocComparator comparatorAuto ( final IndexReader reader , final String fieldname ) throws IOException { final String field = fieldname . intern ( ) ; Object lookupArray = FieldCache . DEFAULT . getAuto ( reader , field ) ; if ( lookupArray instanceof FieldCache . StringIndex ) { return comparatorString ( reader , field ) ; } else if ( lookupArray instanceof int [ ] ) { return comparatorInt ( reader , field ) ; } else if ( lookupArray instanceof float [ ] ) { return comparatorFloat ( reader , field ) ; } else if ( lookupArray instanceof String [ ] ) { return comparatorString ( reader , field ) ; } else { throw new RuntimeException ( "unknown data type in field '" + field + "'" ) ; } } } 	1	['16', '2', '0', '15', '55', '98', '1', '14', '4', '0.833333333', '498', '0.75', '2', '0.44', '0.296296296', '1', '3', '29.875', '7', '1.5625', '2']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; public class TopFieldDocCollector extends TopDocCollector { public TopFieldDocCollector ( IndexReader reader , Sort sort , int numHits ) throws IOException { super ( numHits , new FieldSortedHitQueue ( reader , sort . fields , numHits ) ) ; } public void collect ( int doc , float score ) { if ( score > 0.0f ) { totalHits ++ ; hq . insert ( new FieldDoc ( doc , score ) ) ; } } public TopDocs topDocs ( ) { FieldSortedHitQueue fshq = ( FieldSortedHitQueue ) hq ; ScoreDoc [ ] scoreDocs = new ScoreDoc [ fshq . size ( ) ] ; for ( int i = fshq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = fshq . fillFields ( ( FieldDoc ) fshq . pop ( ) ) ; return new TopFieldDocs ( totalHits , scoreDocs , fshq . getFields ( ) , fshq . getMaxScore ( ) ) ; } } 	0	['3', '3', '0', '11', '13', '1', '1', '10', '3', '2', '70', '0', '0', '0.666666667', '0.533333333', '1', '3', '22.33333333', '2', '1.3333', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; import java . util . Map ; import java . util . HashMap ; public class PerFieldAnalyzerWrapper extends Analyzer { private Analyzer defaultAnalyzer ; private Map analyzerMap = new HashMap ( ) ; public PerFieldAnalyzerWrapper ( Analyzer defaultAnalyzer ) { this . defaultAnalyzer = defaultAnalyzer ; } public void addAnalyzer ( String fieldName , Analyzer analyzer ) { analyzerMap . put ( fieldName , analyzer ) ; } public TokenStream tokenStream ( String fieldName , Reader reader ) { Analyzer analyzer = ( Analyzer ) analyzerMap . get ( fieldName ) ; if ( analyzer == null ) { analyzer = defaultAnalyzer ; } return analyzer . tokenStream ( fieldName , reader ) ; } public String toString ( ) { return "PerFieldAnalyzerWrapper(" + analyzerMap + ", default=" + defaultAnalyzer + ")" ; } } 	1	['4', '2', '0', '2', '13', '0', '0', '2', '4', '0.166666667', '57', '1', '1', '0.4', '0.5625', '0', '0', '12.75', '2', '1', '1']
package org . apache . lucene . index ; import java . io . File ; import java . io . FilenameFilter ; public class IndexFileNameFilter implements FilenameFilter { public boolean accept ( File dir , String name ) { for ( int i = 0 ; i < IndexFileNames . INDEX_EXTENSIONS . length ; i ++ ) { if ( name . endsWith ( "." + IndexFileNames . INDEX_EXTENSIONS [ i ] ) ) return true ; } if ( name . equals ( IndexFileNames . DELETABLE ) ) return true ; else if ( name . equals ( IndexFileNames . SEGMENTS ) ) return true ; else if ( name . matches ( ".+\\.f\\d+" ) ) return true ; return false ; } } 	0	['2', '1', '0', '2', '9', '1', '1', '1', '2', '2', '48', '0', '0', '0', '0.666666667', '0', '0', '23', '6', '3', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; import java . io . IOException ; import java . util . Iterator ; import java . util . Set ; import java . util . Vector ; public class BooleanQuery extends Query { private static int maxClauseCount = 1024 ; public static class TooManyClauses extends RuntimeException { } public static int getMaxClauseCount ( ) { return maxClauseCount ; } public static void setMaxClauseCount ( int maxClauseCount ) { if ( maxClauseCount < 1 ) throw new IllegalArgumentException ( "maxClauseCount must be >= 1" ) ; BooleanQuery . maxClauseCount = maxClauseCount ; } private Vector clauses = new Vector ( ) ; private boolean disableCoord ; public BooleanQuery ( ) { } public BooleanQuery ( boolean disableCoord ) { this . disableCoord = disableCoord ; } public boolean isCoordDisabled ( ) { return disableCoord ; } public Similarity getSimilarity ( Searcher searcher ) { Similarity result = super . getSimilarity ( searcher ) ; if ( disableCoord ) { result = new SimilarityDelegator ( result ) { public float coord ( int overlap , int maxOverlap ) { return 1.0f ; } } ; } return result ; } public void setMinimumNumberShouldMatch ( int min ) { this . minNrShouldMatch = min ; } protected int minNrShouldMatch = 0 ; public int getMinimumNumberShouldMatch ( ) { return minNrShouldMatch ; } public void add ( Query query , BooleanClause . Occur occur ) { add ( new BooleanClause ( query , occur ) ) ; } public void add ( BooleanClause clause ) { if ( clauses . size ( ) >= maxClauseCount ) throw new TooManyClauses ( ) ; clauses . addElement ( clause ) ; } public BooleanClause [ ] getClauses ( ) { return ( BooleanClause [ ] ) clauses . toArray ( new BooleanClause [ 0 ] ) ; } private class BooleanWeight implements Weight { protected Similarity similarity ; protected Vector weights = new Vector ( ) ; public BooleanWeight ( Searcher searcher ) throws IOException { this . similarity = getSimilarity ( searcher ) ; for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . elementAt ( i ) ; weights . add ( c . getQuery ( ) . createWeight ( searcher ) ) ; } } public Query getQuery ( ) { return BooleanQuery . this ; } public float getValue ( ) { return getBoost ( ) ; } public float sumOfSquaredWeights ( ) throws IOException { float sum = 0.0f ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . elementAt ( i ) ; Weight w = ( Weight ) weights . elementAt ( i ) ; if ( ! c . isProhibited ( ) ) sum += w . sumOfSquaredWeights ( ) ; } sum *= getBoost ( ) * getBoost ( ) ; return sum ; } public void normalize ( float norm ) { norm *= getBoost ( ) ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . elementAt ( i ) ; Weight w = ( Weight ) weights . elementAt ( i ) ; if ( ! c . isProhibited ( ) ) w . normalize ( norm ) ; } } public Scorer scorer ( IndexReader reader ) throws IOException { boolean allRequired = true ; boolean noneBoolean = true ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . elementAt ( i ) ; if ( ! c . isRequired ( ) ) allRequired = false ; if ( c . getQuery ( ) instanceof BooleanQuery ) noneBoolean = false ; } if ( allRequired && noneBoolean ) { ConjunctionScorer result = new ConjunctionScorer ( similarity ) ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { Weight w = ( Weight ) weights . elementAt ( i ) ; Scorer subScorer = w . scorer ( reader ) ; if ( subScorer == null ) return null ; result . add ( subScorer ) ; } return result ; } BooleanScorer result = new BooleanScorer ( similarity ) ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . elementAt ( i ) ; Weight w = ( Weight ) weights . elementAt ( i ) ; Scorer subScorer = w . scorer ( reader ) ; if ( subScorer != null ) result . add ( subScorer , c . isRequired ( ) , c . isProhibited ( ) ) ; else if ( c . isRequired ( ) ) return null ; } return result ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { Explanation sumExpl = new Explanation ( ) ; sumExpl . setDescription ( "sum of:" ) ; int coord = 0 ; int maxCoord = 0 ; float sum = 0.0f ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . elementAt ( i ) ; Weight w = ( Weight ) weights . elementAt ( i ) ; Explanation e = w . explain ( reader , doc ) ; if ( ! c . isProhibited ( ) ) maxCoord ++ ; if ( e . getValue ( ) > 0 ) { if ( ! c . isProhibited ( ) ) { sumExpl . addDetail ( e ) ; sum += e . getValue ( ) ; coord ++ ; } else { return new Explanation ( 0.0f , "match prohibited" ) ; } } else if ( c . isRequired ( ) ) { return new Explanation ( 0.0f , "match required" ) ; } } sumExpl . setValue ( sum ) ; if ( coord == 1 ) sumExpl = sumExpl . getDetails ( ) [ 0 ] ; float coordFactor = similarity . coord ( coord , maxCoord ) ; if ( coordFactor == 1.0f ) return sumExpl ; else { Explanation result = new Explanation ( ) ; result . setDescription ( "product of:" ) ; result . addDetail ( sumExpl ) ; result . addDetail ( new Explanation ( coordFactor , "coord(" + coord + "/" + maxCoord + ")" ) ) ; result . setValue ( sum * coordFactor ) ; return result ; } } } private class BooleanWeight2 extends BooleanWeight { public BooleanWeight2 ( Searcher searcher ) throws IOException { super ( searcher ) ; } public Scorer scorer ( IndexReader reader ) throws IOException { BooleanScorer2 result = new BooleanScorer2 ( similarity , minNrShouldMatch ) ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . elementAt ( i ) ; Weight w = ( Weight ) weights . elementAt ( i ) ; Scorer subScorer = w . scorer ( reader ) ; if ( subScorer != null ) result . add ( subScorer , c . isRequired ( ) , c . isProhibited ( ) ) ; else if ( c . isRequired ( ) ) return null ; } return result ; } } private static boolean useScorer14 = false ; public static void setUseScorer14 ( boolean use14 ) { useScorer14 = use14 ; } public static boolean getUseScorer14 ( ) { return useScorer14 ; } protected Weight createWeight ( Searcher searcher ) throws IOException { if ( 0 < minNrShouldMatch ) { return new BooleanWeight2 ( searcher ) ; } return getUseScorer14 ( ) ? ( Weight ) new BooleanWeight ( searcher ) : ( Weight ) new BooleanWeight2 ( searcher ) ; } public Query rewrite ( IndexReader reader ) throws IOException { if ( clauses . size ( ) == 1 ) { BooleanClause c = ( BooleanClause ) clauses . elementAt ( 0 ) ; if ( ! c . isProhibited ( ) ) { Query query = c . getQuery ( ) . rewrite ( reader ) ; if ( getBoost ( ) != 1.0f ) { if ( query == c . getQuery ( ) ) query = ( Query ) query . clone ( ) ; query . setBoost ( getBoost ( ) * query . getBoost ( ) ) ; } return query ; } } BooleanQuery clone = null ; for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . elementAt ( i ) ; Query query = c . getQuery ( ) . rewrite ( reader ) ; if ( query != c . getQuery ( ) ) { if ( clone == null ) clone = ( BooleanQuery ) this . clone ( ) ; clone . clauses . setElementAt ( new BooleanClause ( query , c . getOccur ( ) ) , i ) ; } } if ( clone != null ) { return clone ; } else return this ; } public void extractTerms ( Set terms ) { for ( Iterator i = clauses . iterator ( ) ; i . hasNext ( ) ; ) { BooleanClause clause = ( BooleanClause ) i . next ( ) ; clause . getQuery ( ) . extractTerms ( terms ) ; } } public Object clone ( ) { BooleanQuery clone = ( BooleanQuery ) super . clone ( ) ; clone . clauses = ( Vector ) this . clauses . clone ( ) ; return clone ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; boolean needParens = ( getBoost ( ) != 1.0 ) || ( getMinimumNumberShouldMatch ( ) > 0 ) ; if ( needParens ) { buffer . append ( "(" ) ; } for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . elementAt ( i ) ; if ( c . isProhibited ( ) ) buffer . append ( "-" ) ; else if ( c . isRequired ( ) ) buffer . append ( "+" ) ; Query subQuery = c . getQuery ( ) ; if ( subQuery instanceof BooleanQuery ) { buffer . append ( "(" ) ; buffer . append ( c . getQuery ( ) . toString ( field ) ) ; buffer . append ( ")" ) ; } else buffer . append ( c . getQuery ( ) . toString ( field ) ) ; if ( i != clauses . size ( ) - 1 ) buffer . append ( " " ) ; } if ( needParens ) { buffer . append ( ")" ) ; } if ( getMinimumNumberShouldMatch ( ) > 0 ) { buffer . append ( '~' ) ; buffer . append ( getMinimumNumberShouldMatch ( ) ) ; } if ( getBoost ( ) != 1.0f ) { buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; } return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof BooleanQuery ) ) return false ; BooleanQuery other = ( BooleanQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && this . clauses . equals ( other . clauses ) && this . getMinimumNumberShouldMatch ( ) == other . getMinimumNumberShouldMatch ( ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ clauses . hashCode ( ) + getMinimumNumberShouldMatch ( ) ; } } 	1	['22', '2', '0', '20', '60', '79', '12', '12', '19', '0.742857143', '439', '1', '0', '0.387096774', '0.119047619', '2', '6', '18.72727273', '12', '1.7273', '4']
package org . apache . lucene . analysis ; import java . io . Reader ; public final class LowerCaseTokenizer extends LetterTokenizer { public LowerCaseTokenizer ( Reader in ) { super ( in ) ; } protected char normalize ( char c ) { return Character . toLowerCase ( c ) ; } } 	0	['2', '5', '0', '3', '4', '1', '2', '1', '1', '2', '9', '0', '0', '0.875', '0.666666667', '1', '1', '3.5', '1', '0.5', '0']
package org . apache . lucene . index ; public final class Term implements Comparable , java . io . Serializable { String field ; String text ; public Term ( String fld , String txt ) { this ( fld , txt , true ) ; } Term ( String fld , String txt , boolean intern ) { field = intern ? fld . intern ( ) : fld ; text = txt ; } public final String field ( ) { return field ; } public final String text ( ) { return text ; } public Term createTerm ( String text ) { return new Term ( field , text , false ) ; } public final boolean equals ( Object o ) { if ( o == null ) return false ; Term other = ( Term ) o ; return field == other . field && text . equals ( other . text ) ; } public final int hashCode ( ) { return field . hashCode ( ) + text . hashCode ( ) ; } public int compareTo ( Object other ) { return compareTo ( ( Term ) other ) ; } public final int compareTo ( Term other ) { if ( field == other . field ) return text . compareTo ( other . text ) ; else return field . compareTo ( other . field ) ; } final void set ( String fld , String txt ) { field = fld ; text = txt ; } public final String toString ( ) { return field + ":" + text ; } private void readObject ( java . io . ObjectInputStream in ) throws java . io . IOException , ClassNotFoundException { in . defaultReadObject ( ) ; field = field . intern ( ) ; } } 	1	['12', '1', '0', '58', '21', '0', '58', '0', '9', '0.136363636', '128', '0', '0', '0', '0.291666667', '1', '1', '9.5', '4', '1.1667', '1']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermEnum ; public abstract class FilteredTermEnum extends TermEnum { private Term currentTerm = null ; private TermEnum actualEnum = null ; public FilteredTermEnum ( ) { } protected abstract boolean termCompare ( Term term ) ; public abstract float difference ( ) ; protected abstract boolean endEnum ( ) ; protected void setEnum ( TermEnum actualEnum ) throws IOException { this . actualEnum = actualEnum ; Term term = actualEnum . term ( ) ; if ( term != null && termCompare ( term ) ) currentTerm = term ; else next ( ) ; } public int docFreq ( ) { if ( actualEnum == null ) return - 1 ; return actualEnum . docFreq ( ) ; } public boolean next ( ) throws IOException { if ( actualEnum == null ) return false ; currentTerm = null ; while ( currentTerm == null ) { if ( endEnum ( ) ) return false ; if ( actualEnum . next ( ) ) { Term term = actualEnum . term ( ) ; if ( termCompare ( term ) ) { currentTerm = term ; return true ; } } else return false ; } currentTerm = null ; return false ; } public Term term ( ) { return currentTerm ; } public void close ( ) throws IOException { actualEnum . close ( ) ; currentTerm = null ; actualEnum = null ; } } 	0	['9', '2', '2', '7', '14', '8', '5', '2', '6', '0.5', '103', '1', '2', '0.384615385', '0.407407407', '1', '2', '10.22222222', '2', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; import java . util . BitSet ; import java . util . Set ; public class ConstantScoreQuery extends Query { protected final Filter filter ; public ConstantScoreQuery ( Filter filter ) { this . filter = filter ; } public Query rewrite ( IndexReader reader ) throws IOException { return this ; } public void extractTerms ( Set terms ) { } protected class ConstantWeight implements Weight { private Similarity similarity ; private float queryNorm ; private float queryWeight ; public ConstantWeight ( Searcher searcher ) { this . similarity = getSimilarity ( searcher ) ; } public Query getQuery ( ) { return ConstantScoreQuery . this ; } public float getValue ( ) { return queryWeight ; } public float sumOfSquaredWeights ( ) throws IOException { queryWeight = getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float norm ) { this . queryNorm = norm ; queryWeight *= this . queryNorm ; } public Scorer scorer ( IndexReader reader ) throws IOException { return new ConstantScorer ( similarity , reader , this ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { ConstantScorer cs = ( ConstantScorer ) scorer ( reader ) ; boolean exists = cs . bits . get ( doc ) ; Explanation result = new Explanation ( ) ; if ( exists ) { result . setDescription ( "ConstantScoreQuery(" + filter + "), product of:" ) ; result . setValue ( queryWeight ) ; result . addDetail ( new Explanation ( getBoost ( ) , "boost" ) ) ; result . addDetail ( new Explanation ( queryNorm , "queryNorm" ) ) ; } else { result . setDescription ( "ConstantScoreQuery(" + filter + ") doesn't match id " + doc ) ; result . setValue ( 0 ) ; } return result ; } } protected class ConstantScorer extends Scorer { final BitSet bits ; final float theScore ; int doc = - 1 ; public ConstantScorer ( Similarity similarity , IndexReader reader , Weight w ) throws IOException { super ( similarity ) ; theScore = w . getValue ( ) ; bits = filter . bits ( reader ) ; } public boolean next ( ) throws IOException { doc = bits . nextSetBit ( doc + 1 ) ; return doc >= 0 ; } public int doc ( ) { return doc ; } public float score ( ) throws IOException { return theScore ; } public boolean skipTo ( int target ) throws IOException { doc = bits . nextSetBit ( target ) ; return doc >= 0 ; } public Explanation explain ( int doc ) throws IOException { throw new UnsupportedOperationException ( ) ; } } protected Weight createWeight ( Searcher searcher ) { return new ConstantScoreQuery . ConstantWeight ( searcher ) ; } public String toString ( String field ) { return "ConstantScore(" + filter . toString ( ) + ( getBoost ( ) == 1.0 ? ")" : "^" + getBoost ( ) ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof ConstantScoreQuery ) ) return false ; ConstantScoreQuery other = ( ConstantScoreQuery ) o ; return this . getBoost ( ) == other . getBoost ( ) && filter . equals ( other . filter ) ; } public int hashCode ( ) { return filter . hashCode ( ) + Float . floatToIntBits ( getBoost ( ) ) ; } } 	1	['7', '2', '0', '8', '18', '9', '3', '6', '6', '0.5', '89', '1', '1', '0.666666667', '0.265306122', '2', '3', '11.57142857', '5', '1.5714', '1']
package org . apache . lucene . index ; import java . io . IOException ; public interface TermDocs { void seek ( Term term ) throws IOException ; void seek ( TermEnum termEnum ) throws IOException ; int doc ( ) ; int freq ( ) ; boolean next ( ) throws IOException ; int read ( int [ ] docs , int [ ] freqs ) throws IOException ; boolean skipTo ( int target ) throws IOException ; void close ( ) throws IOException ; } 	0	['8', '1', '0', '19', '8', '28', '17', '2', '8', '2', '8', '0', '0', '0', '0.3', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . document . Document ; import org . apache . lucene . index . Term ; import java . io . IOException ; import java . rmi . Naming ; import java . rmi . RMISecurityManager ; import java . rmi . RemoteException ; import java . rmi . server . UnicastRemoteObject ; public class RemoteSearchable extends UnicastRemoteObject implements Searchable { private Searchable local ; public RemoteSearchable ( Searchable local ) throws RemoteException { super ( ) ; this . local = local ; } public void search ( Weight weight , Filter filter , HitCollector results ) throws IOException { local . search ( weight , filter , results ) ; } public void close ( ) throws IOException { local . close ( ) ; } public int docFreq ( Term term ) throws IOException { return local . docFreq ( term ) ; } public int [ ] docFreqs ( Term [ ] terms ) throws IOException { return local . docFreqs ( terms ) ; } public int maxDoc ( ) throws IOException { return local . maxDoc ( ) ; } public TopDocs search ( Weight weight , Filter filter , int n ) throws IOException { return local . search ( weight , filter , n ) ; } public TopFieldDocs search ( Weight weight , Filter filter , int n , Sort sort ) throws IOException { return local . search ( weight , filter , n , sort ) ; } public Document doc ( int i ) throws IOException { return local . doc ( i ) ; } public Query rewrite ( Query original ) throws IOException { return local . rewrite ( original ) ; } public Explanation explain ( Weight weight , int doc ) throws IOException { return local . explain ( weight , doc ) ; } public static void main ( String args [ ] ) throws Exception { String indexName = null ; if ( args != null && args . length == 1 ) indexName = args [ 0 ] ; if ( indexName == null ) { System . out . println ( "Usage: org.apache.lucene.search.RemoteSearchable <index>" ) ; return ; } if ( System . getSecurityManager ( ) == null ) { System . setSecurityManager ( new RMISecurityManager ( ) ) ; } Searchable local = new IndexSearcher ( indexName ) ; RemoteSearchable impl = new RemoteSearchable ( local ) ; Naming . rebind ( "//localhost/Searchable" , impl ) ; } } 	1	['12', '4', '0', '12', '29', '0', '0', '12', '12', '0', '113', '1', '1', '0.620689655', '0.21969697', '0', '0', '8.333333333', '1', '0.9167', '2']
package org . apache . lucene . index ; final class TermInfo { int docFreq = 0 ; long freqPointer = 0 ; long proxPointer = 0 ; int skipOffset ; TermInfo ( ) { } TermInfo ( int df , long fp , long pp ) { docFreq = df ; freqPointer = fp ; proxPointer = pp ; } TermInfo ( TermInfo ti ) { docFreq = ti . docFreq ; freqPointer = ti . freqPointer ; proxPointer = ti . proxPointer ; skipOffset = ti . skipOffset ; } final void set ( int docFreq , long freqPointer , long proxPointer , int skipOffset ) { this . docFreq = docFreq ; this . freqPointer = freqPointer ; this . proxPointer = proxPointer ; this . skipOffset = skipOffset ; } final void set ( TermInfo ti ) { docFreq = ti . docFreq ; freqPointer = ti . freqPointer ; proxPointer = ti . proxPointer ; skipOffset = ti . skipOffset ; } } 	0	['5', '1', '0', '8', '6', '0', '8', '0', '0', '0.125', '100', '0', '0', '0', '0.55', '0', '0', '18.2', '1', '0.4', '0']
package org . apache . lucene . index ; import org . apache . lucene . document . Document ; import java . io . IOException ; import java . util . Collection ; public class FilterIndexReader extends IndexReader { public static class FilterTermDocs implements TermDocs { protected TermDocs in ; public FilterTermDocs ( TermDocs in ) { this . in = in ; } public void seek ( Term term ) throws IOException { in . seek ( term ) ; } public void seek ( TermEnum termEnum ) throws IOException { in . seek ( termEnum ) ; } public int doc ( ) { return in . doc ( ) ; } public int freq ( ) { return in . freq ( ) ; } public boolean next ( ) throws IOException { return in . next ( ) ; } public int read ( int [ ] docs , int [ ] freqs ) throws IOException { return in . read ( docs , freqs ) ; } public boolean skipTo ( int i ) throws IOException { return in . skipTo ( i ) ; } public void close ( ) throws IOException { in . close ( ) ; } } public static class FilterTermPositions extends FilterTermDocs implements TermPositions { public FilterTermPositions ( TermPositions in ) { super ( in ) ; } public int nextPosition ( ) throws IOException { return ( ( TermPositions ) this . in ) . nextPosition ( ) ; } } public static class FilterTermEnum extends TermEnum { protected TermEnum in ; public FilterTermEnum ( TermEnum in ) { this . in = in ; } public boolean next ( ) throws IOException { return in . next ( ) ; } public Term term ( ) { return in . term ( ) ; } public int docFreq ( ) { return in . docFreq ( ) ; } public void close ( ) throws IOException { in . close ( ) ; } } protected IndexReader in ; public FilterIndexReader ( IndexReader in ) { super ( in . directory ( ) ) ; this . in = in ; } public TermFreqVector [ ] getTermFreqVectors ( int docNumber ) throws IOException { return in . getTermFreqVectors ( docNumber ) ; } public TermFreqVector getTermFreqVector ( int docNumber , String field ) throws IOException { return in . getTermFreqVector ( docNumber , field ) ; } public int numDocs ( ) { return in . numDocs ( ) ; } public int maxDoc ( ) { return in . maxDoc ( ) ; } public Document document ( int n ) throws IOException { return in . document ( n ) ; } public boolean isDeleted ( int n ) { return in . isDeleted ( n ) ; } public boolean hasDeletions ( ) { return in . hasDeletions ( ) ; } protected void doUndeleteAll ( ) throws IOException { in . undeleteAll ( ) ; } public boolean hasNorms ( String field ) throws IOException { return in . hasNorms ( field ) ; } public byte [ ] norms ( String f ) throws IOException { return in . norms ( f ) ; } public void norms ( String f , byte [ ] bytes , int offset ) throws IOException { in . norms ( f , bytes , offset ) ; } protected void doSetNorm ( int d , String f , byte b ) throws IOException { in . setNorm ( d , f , b ) ; } public TermEnum terms ( ) throws IOException { return in . terms ( ) ; } public TermEnum terms ( Term t ) throws IOException { return in . terms ( t ) ; } public int docFreq ( Term t ) throws IOException { return in . docFreq ( t ) ; } public TermDocs termDocs ( ) throws IOException { return in . termDocs ( ) ; } public TermPositions termPositions ( ) throws IOException { return in . termPositions ( ) ; } protected void doDelete ( int n ) throws IOException { in . deleteDocument ( n ) ; } protected void doCommit ( ) throws IOException { in . commit ( ) ; } protected void doClose ( ) throws IOException { in . close ( ) ; } public Collection getFieldNames ( IndexReader . FieldOption fieldNames ) { return in . getFieldNames ( fieldNames ) ; } public long getVersion ( ) { return in . getVersion ( ) ; } public boolean isCurrent ( ) throws IOException { return in . isCurrent ( ) ; } } 	1	['24', '2', '0', '9', '49', '0', '0', '9', '19', '0', '142', '1', '1', '0.705128205', '0.21875', '1', '5', '4.875', '1', '0.9583', '4']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . util . PriorityQueue ; final class SegmentMergeQueue extends PriorityQueue { SegmentMergeQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object a , Object b ) { SegmentMergeInfo stiA = ( SegmentMergeInfo ) a ; SegmentMergeInfo stiB = ( SegmentMergeInfo ) b ; int comparison = stiA . term . compareTo ( stiB . term ) ; if ( comparison == 0 ) return stiA . base < stiB . base ; else return comparison < 0 ; } final void close ( ) throws IOException { while ( top ( ) != null ) ( ( SegmentMergeInfo ) pop ( ) ) . close ( ) ; } } 	0	['3', '2', '0', '5', '9', '3', '2', '3', '0', '2', '47', '0', '0', '0.846153846', '0.555555556', '1', '3', '14.66666667', '4', '1.6667', '0']
package org . apache . lucene . queryParser ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . search . BooleanClause ; import org . apache . lucene . search . BooleanQuery ; import org . apache . lucene . search . MultiPhraseQuery ; import org . apache . lucene . search . PhraseQuery ; import org . apache . lucene . search . Query ; import java . util . Vector ; public class MultiFieldQueryParser extends QueryParser { private String [ ] fields ; public MultiFieldQueryParser ( String [ ] fields , Analyzer analyzer ) { super ( null , analyzer ) ; this . fields = fields ; } protected Query getFieldQuery ( String field , String queryText , int slop ) throws ParseException { if ( field == null ) { Vector clauses = new Vector ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { Query q = super . getFieldQuery ( fields [ i ] , queryText ) ; if ( q != null ) { if ( q instanceof PhraseQuery ) { ( ( PhraseQuery ) q ) . setSlop ( slop ) ; } if ( q instanceof MultiPhraseQuery ) { ( ( MultiPhraseQuery ) q ) . setSlop ( slop ) ; } clauses . add ( new BooleanClause ( q , BooleanClause . Occur . SHOULD ) ) ; } } if ( clauses . size ( ) == 0 ) return null ; return getBooleanQuery ( clauses , true ) ; } return super . getFieldQuery ( field , queryText ) ; } protected Query getFieldQuery ( String field , String queryText ) throws ParseException { return getFieldQuery ( field , queryText , 0 ) ; } protected Query getFuzzyQuery ( String field , String termStr , float minSimilarity ) throws ParseException { if ( field == null ) { Vector clauses = new Vector ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( super . getFuzzyQuery ( fields [ i ] , termStr , minSimilarity ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getFuzzyQuery ( field , termStr , minSimilarity ) ; } protected Query getPrefixQuery ( String field , String termStr ) throws ParseException { if ( field == null ) { Vector clauses = new Vector ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( super . getPrefixQuery ( fields [ i ] , termStr ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getPrefixQuery ( field , termStr ) ; } protected Query getWildcardQuery ( String field , String termStr ) throws ParseException { if ( field == null ) { Vector clauses = new Vector ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( super . getWildcardQuery ( fields [ i ] , termStr ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getWildcardQuery ( field , termStr ) ; } protected Query getRangeQuery ( String field , String part1 , String part2 , boolean inclusive ) throws ParseException { if ( field == null ) { Vector clauses = new Vector ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( super . getRangeQuery ( fields [ i ] , part1 , part2 , inclusive ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getRangeQuery ( field , part1 , part2 , inclusive ) ; } public static Query parse ( String [ ] queries , String [ ] fields , Analyzer analyzer ) throws ParseException { if ( queries . length != fields . length ) throw new IllegalArgumentException ( "queries.length != fields.length" ) ; BooleanQuery bQuery = new BooleanQuery ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { QueryParser qp = new QueryParser ( fields [ i ] , analyzer ) ; Query q = qp . parse ( queries [ i ] ) ; bQuery . add ( q , BooleanClause . Occur . SHOULD ) ; } return bQuery ; } public static Query parse ( String query , String [ ] fields , BooleanClause . Occur [ ] flags , Analyzer analyzer ) throws ParseException { if ( fields . length != flags . length ) throw new IllegalArgumentException ( "fields.length != flags.length" ) ; BooleanQuery bQuery = new BooleanQuery ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { QueryParser qp = new QueryParser ( fields [ i ] , analyzer ) ; Query q = qp . parse ( query ) ; bQuery . add ( q , flags [ i ] ) ; } return bQuery ; } public static Query parse ( String [ ] queries , String [ ] fields , BooleanClause . Occur [ ] flags , Analyzer analyzer ) throws ParseException { if ( ! ( queries . length == fields . length && queries . length == flags . length ) ) throw new IllegalArgumentException ( "queries, fields, and flags array have have different length" ) ; BooleanQuery bQuery = new BooleanQuery ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { QueryParser qp = new QueryParser ( fields [ i ] , analyzer ) ; Query q = qp . parse ( queries [ i ] ) ; bQuery . add ( q , flags [ i ] ) ; } return bQuery ; } } 	1	['10', '2', '0', '9', '27', '15', '0', '9', '4', '0.333333333', '382', '1', '0', '0.842105263', '0.3375', '1', '6', '37.1', '1', '0.9', '1']
package org . apache . lucene . util ; public class ToStringUtils { public static String boost ( float boost ) { if ( boost != 1.0f ) { return "^" + Float . toString ( boost ) ; } else return "" ; } } 	0	['2', '1', '0', '15', '7', '1', '15', '0', '2', '2', '21', '0', '0', '0', '0.5', '0', '0', '9.5', '2', '1', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . BitSet ; import org . apache . lucene . store . Directory ; import org . apache . lucene . document . Document ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; public class IndexSearcher extends Searcher { IndexReader reader ; private boolean closeReader ; public IndexSearcher ( String path ) throws IOException { this ( IndexReader . open ( path ) , true ) ; } public IndexSearcher ( Directory directory ) throws IOException { this ( IndexReader . open ( directory ) , true ) ; } public IndexSearcher ( IndexReader r ) { this ( r , false ) ; } private IndexSearcher ( IndexReader r , boolean closeReader ) { reader = r ; this . closeReader = closeReader ; } public IndexReader getIndexReader ( ) { return reader ; } public void close ( ) throws IOException { if ( closeReader ) reader . close ( ) ; } public int docFreq ( Term term ) throws IOException { return reader . docFreq ( term ) ; } public Document doc ( int i ) throws IOException { return reader . document ( i ) ; } public int maxDoc ( ) throws IOException { return reader . maxDoc ( ) ; } public TopDocs search ( Weight weight , Filter filter , final int nDocs ) throws IOException { if ( nDocs <= 0 ) throw new IllegalArgumentException ( "nDocs must be > 0" ) ; TopDocCollector collector = new TopDocCollector ( nDocs ) ; search ( weight , filter , collector ) ; return collector . topDocs ( ) ; } public TopFieldDocs search ( Weight weight , Filter filter , final int nDocs , Sort sort ) throws IOException { TopFieldDocCollector collector = new TopFieldDocCollector ( reader , sort , nDocs ) ; search ( weight , filter , collector ) ; return ( TopFieldDocs ) collector . topDocs ( ) ; } public void search ( Weight weight , Filter filter , final HitCollector results ) throws IOException { HitCollector collector = results ; if ( filter != null ) { final BitSet bits = filter . bits ( reader ) ; collector = new HitCollector ( ) { public final void collect ( int doc , float score ) { if ( bits . get ( doc ) ) { results . collect ( doc , score ) ; } } } ; } Scorer scorer = weight . scorer ( reader ) ; if ( scorer == null ) return ; scorer . score ( collector ) ; } public Query rewrite ( Query original ) throws IOException { Query query = original ; for ( Query rewrittenQuery = query . rewrite ( reader ) ; rewrittenQuery != query ; rewrittenQuery = query . rewrite ( reader ) ) { query = rewrittenQuery ; } return query ; } public Explanation explain ( Weight weight , int doc ) throws IOException { return weight . explain ( reader , doc ) ; } } 	1	['14', '2', '0', '19', '32', '1', '3', '17', '13', '0.346153846', '157', '0.5', '1', '0.6875', '0.202380952', '1', '3', '10.07142857', '1', '0.7143', '2']
package org . apache . lucene . search ; public class ScoreDoc implements java . io . Serializable { public float score ; public int doc ; public ScoreDoc ( int doc , float score ) { this . doc = doc ; this . score = score ; } } 	0	['1', '1', '1', '19', '2', '0', '19', '0', '1', '2', '12', '0', '0', '0', '1', '0', '0', '9', '0', '0', '0']
package org . apache . lucene . index ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . Field ; import org . apache . lucene . search . Similarity ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . FSDirectory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . Lock ; import java . io . File ; import java . io . FileOutputStream ; import java . io . IOException ; import java . util . Arrays ; import java . util . Collection ; public abstract class IndexReader { public static final class FieldOption { private String option ; private FieldOption ( ) { } private FieldOption ( String option ) { this . option = option ; } public String toString ( ) { return this . option ; } public static final FieldOption ALL = new FieldOption ( "ALL" ) ; public static final FieldOption INDEXED = new FieldOption ( "INDEXED" ) ; public static final FieldOption UNINDEXED = new FieldOption ( "UNINDEXED" ) ; public static final FieldOption INDEXED_WITH_TERMVECTOR = new FieldOption ( "INDEXED_WITH_TERMVECTOR" ) ; public static final FieldOption INDEXED_NO_TERMVECTOR = new FieldOption ( "INDEXED_NO_TERMVECTOR" ) ; public static final FieldOption TERMVECTOR = new FieldOption ( "TERMVECTOR" ) ; public static final FieldOption TERMVECTOR_WITH_POSITION = new FieldOption ( "TERMVECTOR_WITH_POSITION" ) ; public static final FieldOption TERMVECTOR_WITH_OFFSET = new FieldOption ( "TERMVECTOR_WITH_OFFSET" ) ; public static final FieldOption TERMVECTOR_WITH_POSITION_OFFSET = new FieldOption ( "TERMVECTOR_WITH_POSITION_OFFSET" ) ; } protected IndexReader ( Directory directory ) { this . directory = directory ; } IndexReader ( Directory directory , SegmentInfos segmentInfos , boolean closeDirectory ) { init ( directory , segmentInfos , closeDirectory , true ) ; } void init ( Directory directory , SegmentInfos segmentInfos , boolean closeDirectory , boolean directoryOwner ) { this . directory = directory ; this . segmentInfos = segmentInfos ; this . directoryOwner = directoryOwner ; this . closeDirectory = closeDirectory ; } private Directory directory ; private boolean directoryOwner ; private boolean closeDirectory ; private SegmentInfos segmentInfos ; private Lock writeLock ; private boolean stale ; private boolean hasChanges ; public static IndexReader open ( String path ) throws IOException { return open ( FSDirectory . getDirectory ( path , false ) , true ) ; } public static IndexReader open ( File path ) throws IOException { return open ( FSDirectory . getDirectory ( path , false ) , true ) ; } public static IndexReader open ( final Directory directory ) throws IOException { return open ( directory , false ) ; } private static IndexReader open ( final Directory directory , final boolean closeDirectory ) throws IOException { synchronized ( directory ) { return ( IndexReader ) new Lock . With ( directory . makeLock ( IndexWriter . COMMIT_LOCK_NAME ) , IndexWriter . COMMIT_LOCK_TIMEOUT ) { public Object doBody ( ) throws IOException { SegmentInfos infos = new SegmentInfos ( ) ; infos . read ( directory ) ; if ( infos . size ( ) == 1 ) { return SegmentReader . get ( infos , infos . info ( 0 ) , closeDirectory ) ; } IndexReader [ ] readers = new IndexReader [ infos . size ( ) ] ; for ( int i = 0 ; i < infos . size ( ) ; i ++ ) readers [ i ] = SegmentReader . get ( infos . info ( i ) ) ; return new MultiReader ( directory , infos , closeDirectory , readers ) ; } } . run ( ) ; } } public Directory directory ( ) { return directory ; } public static long lastModified ( String directory ) throws IOException { return lastModified ( new File ( directory ) ) ; } public static long lastModified ( File directory ) throws IOException { return FSDirectory . fileModified ( directory , IndexFileNames . SEGMENTS ) ; } public static long lastModified ( Directory directory ) throws IOException { return directory . fileModified ( IndexFileNames . SEGMENTS ) ; } public static long getCurrentVersion ( String directory ) throws IOException { return getCurrentVersion ( new File ( directory ) ) ; } public static long getCurrentVersion ( File directory ) throws IOException { Directory dir = FSDirectory . getDirectory ( directory , false ) ; long version = getCurrentVersion ( dir ) ; dir . close ( ) ; return version ; } public static long getCurrentVersion ( Directory directory ) throws IOException { synchronized ( directory ) { Lock commitLock = directory . makeLock ( IndexWriter . COMMIT_LOCK_NAME ) ; boolean locked = false ; try { locked = commitLock . obtain ( IndexWriter . COMMIT_LOCK_TIMEOUT ) ; return SegmentInfos . readCurrentVersion ( directory ) ; } finally { if ( locked ) { commitLock . release ( ) ; } } } } public long getVersion ( ) { return segmentInfos . getVersion ( ) ; } public boolean isCurrent ( ) throws IOException { synchronized ( directory ) { Lock commitLock = directory . makeLock ( IndexWriter . COMMIT_LOCK_NAME ) ; boolean locked = false ; try { locked = commitLock . obtain ( IndexWriter . COMMIT_LOCK_TIMEOUT ) ; return SegmentInfos . readCurrentVersion ( directory ) == segmentInfos . getVersion ( ) ; } finally { if ( locked ) { commitLock . release ( ) ; } } } } abstract public TermFreqVector [ ] getTermFreqVectors ( int docNumber ) throws IOException ; abstract public TermFreqVector getTermFreqVector ( int docNumber , String field ) throws IOException ; public static boolean indexExists ( String directory ) { return ( new File ( directory , IndexFileNames . SEGMENTS ) ) . exists ( ) ; } public static boolean indexExists ( File directory ) { return ( new File ( directory , IndexFileNames . SEGMENTS ) ) . exists ( ) ; } public static boolean indexExists ( Directory directory ) throws IOException { return directory . fileExists ( IndexFileNames . SEGMENTS ) ; } public abstract int numDocs ( ) ; public abstract int maxDoc ( ) ; public abstract Document document ( int n ) throws IOException ; public abstract boolean isDeleted ( int n ) ; public abstract boolean hasDeletions ( ) ; public boolean hasNorms ( String field ) throws IOException { return norms ( field ) != null ; } public abstract byte [ ] norms ( String field ) throws IOException ; public abstract void norms ( String field , byte [ ] bytes , int offset ) throws IOException ; public final synchronized void setNorm ( int doc , String field , byte value ) throws IOException { if ( directoryOwner ) aquireWriteLock ( ) ; doSetNorm ( doc , field , value ) ; hasChanges = true ; } protected abstract void doSetNorm ( int doc , String field , byte value ) throws IOException ; public void setNorm ( int doc , String field , float value ) throws IOException { setNorm ( doc , field , Similarity . encodeNorm ( value ) ) ; } public abstract TermEnum terms ( ) throws IOException ; public abstract TermEnum terms ( Term t ) throws IOException ; public abstract int docFreq ( Term t ) throws IOException ; public TermDocs termDocs ( Term term ) throws IOException { TermDocs termDocs = termDocs ( ) ; termDocs . seek ( term ) ; return termDocs ; } public abstract TermDocs termDocs ( ) throws IOException ; public TermPositions termPositions ( Term term ) throws IOException { TermPositions termPositions = termPositions ( ) ; termPositions . seek ( term ) ; return termPositions ; } public abstract TermPositions termPositions ( ) throws IOException ; private void aquireWriteLock ( ) throws IOException { if ( stale ) throw new IOException ( "IndexReader out of date and no longer valid for delete, undelete, or setNorm operations" ) ; if ( writeLock == null ) { Lock writeLock = directory . makeLock ( IndexWriter . WRITE_LOCK_NAME ) ; if ( ! writeLock . obtain ( IndexWriter . WRITE_LOCK_TIMEOUT ) ) throw new IOException ( "Index locked for write: " + writeLock ) ; this . writeLock = writeLock ; if ( SegmentInfos . readCurrentVersion ( directory ) > segmentInfos . getVersion ( ) ) { stale = true ; this . writeLock . release ( ) ; this . writeLock = null ; throw new IOException ( "IndexReader out of date and no longer valid for delete, undelete, or setNorm operations" ) ; } } } public final synchronized void deleteDocument ( int docNum ) throws IOException { if ( directoryOwner ) aquireWriteLock ( ) ; doDelete ( docNum ) ; hasChanges = true ; } protected abstract void doDelete ( int docNum ) throws IOException ; public final int deleteDocuments ( Term term ) throws IOException { TermDocs docs = termDocs ( term ) ; if ( docs == null ) return 0 ; int n = 0 ; try { while ( docs . next ( ) ) { deleteDocument ( docs . doc ( ) ) ; n ++ ; } } finally { docs . close ( ) ; } return n ; } public final synchronized void undeleteAll ( ) throws IOException { if ( directoryOwner ) aquireWriteLock ( ) ; doUndeleteAll ( ) ; hasChanges = true ; } protected abstract void doUndeleteAll ( ) throws IOException ; protected final synchronized void commit ( ) throws IOException { if ( hasChanges ) { if ( directoryOwner ) { synchronized ( directory ) { new Lock . With ( directory . makeLock ( IndexWriter . COMMIT_LOCK_NAME ) , IndexWriter . COMMIT_LOCK_TIMEOUT ) { public Object doBody ( ) throws IOException { doCommit ( ) ; segmentInfos . write ( directory ) ; return null ; } } . run ( ) ; } if ( writeLock != null ) { writeLock . release ( ) ; writeLock = null ; } } else doCommit ( ) ; } hasChanges = false ; } protected abstract void doCommit ( ) throws IOException ; public final synchronized void close ( ) throws IOException { commit ( ) ; doClose ( ) ; if ( closeDirectory ) directory . close ( ) ; } protected abstract void doClose ( ) throws IOException ; protected void finalize ( ) { if ( writeLock != null ) { writeLock . release ( ) ; writeLock = null ; } } public abstract Collection getFieldNames ( FieldOption fldOption ) ; public static boolean isLocked ( Directory directory ) throws IOException { return directory . makeLock ( IndexWriter . WRITE_LOCK_NAME ) . isLocked ( ) || directory . makeLock ( IndexWriter . COMMIT_LOCK_NAME ) . isLocked ( ) ; } public static boolean isLocked ( String directory ) throws IOException { Directory dir = FSDirectory . getDirectory ( directory , false ) ; boolean result = isLocked ( dir ) ; dir . close ( ) ; return result ; } public static void unlock ( Directory directory ) throws IOException { directory . makeLock ( IndexWriter . WRITE_LOCK_NAME ) . release ( ) ; directory . makeLock ( IndexWriter . COMMIT_LOCK_NAME ) . release ( ) ; } public static void main ( String [ ] args ) { String filename = null ; boolean extract = false ; for ( int i = 0 ; i < args . length ; ++ i ) { if ( args [ i ] . equals ( "-extract" ) ) { extract = true ; } else if ( filename == null ) { filename = args [ i ] ; } } if ( filename == null ) { System . out . println ( "Usage: org.apache.lucene.index.IndexReader [-extract] <cfsfile>" ) ; return ; } Directory dir = null ; CompoundFileReader cfr = null ; try { File file = new File ( filename ) ; String dirname = file . getAbsoluteFile ( ) . getParent ( ) ; filename = file . getName ( ) ; dir = FSDirectory . getDirectory ( dirname , false ) ; cfr = new CompoundFileReader ( dir , filename ) ; String [ ] files = cfr . list ( ) ; Arrays . sort ( files ) ; for ( int i = 0 ; i < files . length ; ++ i ) { long len = cfr . fileLength ( files [ i ] ) ; if ( extract ) { System . out . println ( "extract " + files [ i ] + " with " + len + " bytes to local directory..." ) ; IndexInput ii = cfr . openInput ( files [ i ] ) ; FileOutputStream f = new FileOutputStream ( files [ i ] ) ; byte [ ] buffer = new byte [ 1024 ] ; int chunk = buffer . length ; while ( len > 0 ) { final int bufLen = ( int ) Math . min ( chunk , len ) ; ii . readBytes ( buffer , 0 , bufLen ) ; f . write ( buffer , 0 , bufLen ) ; len -= bufLen ; } f . close ( ) ; ii . close ( ) ; } else System . out . println ( files [ i ] + ": " + len + " bytes" ) ; } } catch ( IOException ioe ) { ioe . printStackTrace ( ) ; } finally { try { if ( dir != null ) dir . close ( ) ; if ( cfr != null ) cfr . close ( ) ; } catch ( IOException ioe ) { ioe . printStackTrace ( ) ; } } } } 	1	['57', '1', '4', '80', '108', '1504', '66', '16', '43', '0.867346939', '724', '1', '3', '0', '0.115288221', '0', '0', '11.57894737', '12', '1.1754', '11']
package org . apache . lucene . analysis ; import java . io . IOException ; public final class LengthFilter extends TokenFilter { final int min ; final int max ; public LengthFilter ( TokenStream in , int min , int max ) { super ( in ) ; this . min = min ; this . max = max ; } public final Token next ( ) throws IOException { for ( Token token = input . next ( ) ; token != null ; token = input . next ( ) ) { int len = token . termText ( ) . length ( ) ; if ( len >= min && len <= max ) { return token ; } } return null ; } } 	0	['2', '3', '0', '3', '6', '0', '0', '3', '2', '0', '41', '0', '0', '0.75', '0.666666667', '0', '0', '18.5', '1', '0.5', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import java . io . IOException ; class TermVectorsReader implements Cloneable { private FieldInfos fieldInfos ; private IndexInput tvx ; private IndexInput tvd ; private IndexInput tvf ; private int size ; private int tvdFormat ; private int tvfFormat ; TermVectorsReader ( Directory d , String segment , FieldInfos fieldInfos ) throws IOException { if ( d . fileExists ( segment + TermVectorsWriter . TVX_EXTENSION ) ) { tvx = d . openInput ( segment + TermVectorsWriter . TVX_EXTENSION ) ; checkValidFormat ( tvx ) ; tvd = d . openInput ( segment + TermVectorsWriter . TVD_EXTENSION ) ; tvdFormat = checkValidFormat ( tvd ) ; tvf = d . openInput ( segment + TermVectorsWriter . TVF_EXTENSION ) ; tvfFormat = checkValidFormat ( tvf ) ; size = ( int ) tvx . length ( ) / 8 ; } this . fieldInfos = fieldInfos ; } private int checkValidFormat ( IndexInput in ) throws IOException { int format = in . readInt ( ) ; if ( format > TermVectorsWriter . FORMAT_VERSION ) { throw new IOException ( "Incompatible format version: " + format + " expected " + TermVectorsWriter . FORMAT_VERSION + " or less" ) ; } return format ; } void close ( ) throws IOException { IOException keep = null ; if ( tvx != null ) try { tvx . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tvd != null ) try { tvd . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tvf != null ) try { tvf . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( keep != null ) throw ( IOException ) keep . fillInStackTrace ( ) ; } int size ( ) { return size ; } TermFreqVector get ( int docNum , String field ) throws IOException { int fieldNumber = fieldInfos . fieldNumber ( field ) ; TermFreqVector result = null ; if ( tvx != null ) { tvx . seek ( ( docNum * 8L ) + TermVectorsWriter . FORMAT_SIZE ) ; long position = tvx . readLong ( ) ; tvd . seek ( position ) ; int fieldCount = tvd . readVInt ( ) ; int number = 0 ; int found = - 1 ; for ( int i = 0 ; i < fieldCount ; i ++ ) { if ( tvdFormat == TermVectorsWriter . FORMAT_VERSION ) number = tvd . readVInt ( ) ; else number += tvd . readVInt ( ) ; if ( number == fieldNumber ) found = i ; } if ( found != - 1 ) { position = 0 ; for ( int i = 0 ; i <= found ; i ++ ) position += tvd . readVLong ( ) ; result = readTermVector ( field , position ) ; } else { } } else { } return result ; } TermFreqVector [ ] get ( int docNum ) throws IOException { TermFreqVector [ ] result = null ; if ( tvx != null ) { tvx . seek ( ( docNum * 8L ) + TermVectorsWriter . FORMAT_SIZE ) ; long position = tvx . readLong ( ) ; tvd . seek ( position ) ; int fieldCount = tvd . readVInt ( ) ; if ( fieldCount != 0 ) { int number = 0 ; String [ ] fields = new String [ fieldCount ] ; for ( int i = 0 ; i < fieldCount ; i ++ ) { if ( tvdFormat == TermVectorsWriter . FORMAT_VERSION ) number = tvd . readVInt ( ) ; else number += tvd . readVInt ( ) ; fields [ i ] = fieldInfos . fieldName ( number ) ; } position = 0 ; long [ ] tvfPointers = new long [ fieldCount ] ; for ( int i = 0 ; i < fieldCount ; i ++ ) { position += tvd . readVLong ( ) ; tvfPointers [ i ] = position ; } result = readTermVectors ( fields , tvfPointers ) ; } } else { } return result ; } private SegmentTermVector [ ] readTermVectors ( String fields [ ] , long tvfPointers [ ] ) throws IOException { SegmentTermVector res [ ] = new SegmentTermVector [ fields . length ] ; for ( int i = 0 ; i < fields . length ; i ++ ) { res [ i ] = readTermVector ( fields [ i ] , tvfPointers [ i ] ) ; } return res ; } private SegmentTermVector readTermVector ( String field , long tvfPointer ) throws IOException { tvf . seek ( tvfPointer ) ; int numTerms = tvf . readVInt ( ) ; if ( numTerms == 0 ) return new SegmentTermVector ( field , null , null ) ; boolean storePositions ; boolean storeOffsets ; if ( tvfFormat == TermVectorsWriter . FORMAT_VERSION ) { byte bits = tvf . readByte ( ) ; storePositions = ( bits & TermVectorsWriter . STORE_POSITIONS_WITH_TERMVECTOR ) != 0 ; storeOffsets = ( bits & TermVectorsWriter . STORE_OFFSET_WITH_TERMVECTOR ) != 0 ; } else { tvf . readVInt ( ) ; storePositions = false ; storeOffsets = false ; } String terms [ ] = new String [ numTerms ] ; int termFreqs [ ] = new int [ numTerms ] ; int positions [ ] [ ] = null ; TermVectorOffsetInfo offsets [ ] [ ] = null ; if ( storePositions ) positions = new int [ numTerms ] [ ] ; if ( storeOffsets ) offsets = new TermVectorOffsetInfo [ numTerms ] [ ] ; int start = 0 ; int deltaLength = 0 ; int totalLength = 0 ; char [ ] buffer = new char [ 10 ] ; char [ ] previousBuffer = { } ; for ( int i = 0 ; i < numTerms ; i ++ ) { start = tvf . readVInt ( ) ; deltaLength = tvf . readVInt ( ) ; totalLength = start + deltaLength ; if ( buffer . length < totalLength ) { buffer = null ; buffer = new char [ totalLength ] ; if ( start > 0 ) System . arraycopy ( previousBuffer , 0 , buffer , 0 , start ) ; } tvf . readChars ( buffer , start , deltaLength ) ; terms [ i ] = new String ( buffer , 0 , totalLength ) ; previousBuffer = buffer ; int freq = tvf . readVInt ( ) ; termFreqs [ i ] = freq ; if ( storePositions ) { int [ ] pos = new int [ freq ] ; positions [ i ] = pos ; int prevPosition = 0 ; for ( int j = 0 ; j < freq ; j ++ ) { pos [ j ] = prevPosition + tvf . readVInt ( ) ; prevPosition = pos [ j ] ; } } if ( storeOffsets ) { TermVectorOffsetInfo [ ] offs = new TermVectorOffsetInfo [ freq ] ; offsets [ i ] = offs ; int prevOffset = 0 ; for ( int j = 0 ; j < freq ; j ++ ) { int startOffset = prevOffset + tvf . readVInt ( ) ; int endOffset = startOffset + tvf . readVInt ( ) ; offs [ j ] = new TermVectorOffsetInfo ( startOffset , endOffset ) ; prevOffset = endOffset ; } } } SegmentTermVector tv ; if ( storePositions || storeOffsets ) { tv = new SegmentTermPositionVector ( field , terms , termFreqs , positions , offsets ) ; } else { tv = new SegmentTermVector ( field , terms , termFreqs ) ; } return tv ; } protected Object clone ( ) { if ( tvx == null || tvd == null || tvf == null ) return null ; TermVectorsReader clone = null ; try { clone = ( TermVectorsReader ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . tvx = ( IndexInput ) tvx . clone ( ) ; clone . tvd = ( IndexInput ) tvd . clone ( ) ; clone . tvf = ( IndexInput ) tvf . clone ( ) ; return clone ; } } 	1	['9', '1', '0', '8', '36', '8', '1', '7', '0', '0.625', '648', '1', '4', '0', '0.24691358', '0', '0', '70.22222222', '4', '1.2222', '2']
package org . apache . lucene . queryParser ; public class ParseException extends Exception { public ParseException ( Token currentTokenVal , int [ ] [ ] expectedTokenSequencesVal , String [ ] tokenImageVal ) { super ( "" ) ; specialConstructor = true ; currentToken = currentTokenVal ; expectedTokenSequences = expectedTokenSequencesVal ; tokenImage = tokenImageVal ; } public ParseException ( ) { super ( ) ; specialConstructor = false ; } public ParseException ( String message ) { super ( message ) ; specialConstructor = false ; } protected boolean specialConstructor ; public Token currentToken ; public int [ ] [ ] expectedTokenSequences ; public String [ ] tokenImage ; public String getMessage ( ) { if ( ! specialConstructor ) { return super . getMessage ( ) ; } String expected = "" ; int maxSize = 0 ; for ( int i = 0 ; i < expectedTokenSequences . length ; i ++ ) { if ( maxSize < expectedTokenSequences [ i ] . length ) { maxSize = expectedTokenSequences [ i ] . length ; } for ( int j = 0 ; j < expectedTokenSequences [ i ] . length ; j ++ ) { expected += tokenImage [ expectedTokenSequences [ i ] [ j ] ] + " " ; } if ( expectedTokenSequences [ i ] [ expectedTokenSequences [ i ] . length - 1 ] != 0 ) { expected += "..." ; } expected += eol + "    " ; } String retval = "Encountered \"" ; Token tok = currentToken . next ; for ( int i = 0 ; i < maxSize ; i ++ ) { if ( i != 0 ) retval += " " ; if ( tok . kind == 0 ) { retval += tokenImage [ 0 ] ; break ; } retval += add_escapes ( tok . image ) ; tok = tok . next ; } retval += "\" at line " + currentToken . next . beginLine + ", column " + currentToken . next . beginColumn ; retval += "." + eol ; if ( expectedTokenSequences . length == 1 ) { retval += "Was expecting:" + eol + "    " ; } else { retval += "Was expecting one of:" + eol + "    " ; } retval += expected ; return retval ; } protected String eol = System . getProperty ( "line.separator" , "\n" ) ; protected String add_escapes ( String str ) { StringBuffer retval = new StringBuffer ( ) ; char ch ; for ( int i = 0 ; i < str . length ( ) ; i ++ ) { switch ( str . charAt ( i ) ) { case 0 : continue ; case '\b' : retval . append ( "\\b" ) ; continue ; case '\t' : retval . append ( "\\t" ) ; continue ; case '\n' : retval . append ( "\\n" ) ; continue ; case '\f' : retval . append ( "\\f" ) ; continue ; case '\r' : retval . append ( "\\r" ) ; continue ; case '\"' : retval . append ( "\\\"" ) ; continue ; case '\'' : retval . append ( "\\\'" ) ; continue ; case '\\' : retval . append ( "\\\\" ) ; continue ; default : if ( ( ch = str . charAt ( i ) ) < 0x20 || ch > 0x7e ) { String s = "0000" + Integer . toString ( ch , 16 ) ; retval . append ( "\\u" + s . substring ( s . length ( ) - 4 , s . length ( ) ) ) ; } else { retval . append ( ch ) ; } continue ; } } return retval . toString ( ) ; } } 	0	['5', '3', '0', '3', '18', '0', '2', '1', '4', '0.55', '387', '0.4', '1', '0.866666667', '0.4', '1', '1', '75.4', '14', '4.8', '0']
package org . apache . lucene . analysis . standard ; import java . io . * ; public class StandardTokenizerTokenManager implements StandardTokenizerConstants { public java . io . PrintStream debugStream = System . out ; public void setDebugStream ( java . io . PrintStream ds ) { debugStream = ds ; } private final int jjMoveStringLiteralDfa0_0 ( ) { return jjMoveNfa_0 ( 0 , 0 ) ; } private final void jjCheckNAdd ( int state ) { if ( jjrounds [ state ] != jjround ) { jjstateSet [ jjnewStateCnt ++ ] = state ; jjrounds [ state ] = jjround ; } } private final void jjAddStates ( int start , int end ) { do { jjstateSet [ jjnewStateCnt ++ ] = jjnextStates [ start ] ; } while ( start ++ != end ) ; } private final void jjCheckNAddTwoStates ( int state1 , int state2 ) { jjCheckNAdd ( state1 ) ; jjCheckNAdd ( state2 ) ; } private final void jjCheckNAddStates ( int start , int end ) { do { jjCheckNAdd ( jjnextStates [ start ] ) ; } while ( start ++ != end ) ; } private final void jjCheckNAddStates ( int start ) { jjCheckNAdd ( jjnextStates [ start ] ) ; jjCheckNAdd ( jjnextStates [ start + 1 ] ) ; } static final long [ ] jjbitVec0 = { 0x1ff0000000000000L , 0xffffffffffffc000L , 0xffffffffL , 0x600000000000000L } ; static final long [ ] jjbitVec2 = { 0x0L , 0xffffffffffffffffL , 0xffffffffffffffffL , 0xffffffffffffffffL } ; static final long [ ] jjbitVec3 = { 0xffffffffffffffffL , 0xffffffffffffffffL , 0xffffL , 0x0L } ; static final long [ ] jjbitVec4 = { 0xffffffffffffffffL , 0xffffffffffffffffL , 0x0L , 0x0L } ; static final long [ ] jjbitVec5 = { 0x3fffffffffffL , 0x0L , 0x0L , 0x0L } ; static final long [ ] jjbitVec6 = { 0x0L , 0x0L , 0xfffff00000000000L , 0x7fffffL } ; static final long [ ] jjbitVec7 = { 0xffffffffffffffffL , 0xffffffffffffffffL , 0xffffffffffffL , 0x0L } ; static final long [ ] jjbitVec8 = { 0xfffffffeL , 0x0L , 0x0L , 0x0L } ; static final long [ ] jjbitVec9 = { 0x0L , 0x0L , 0x0L , 0xff7fffffff7fffffL } ; static final long [ ] jjbitVec10 = { 0x1600L , 0x0L , 0x0L , 0x0L } ; static final long [ ] jjbitVec11 = { 0x0L , 0xffc000000000L , 0x0L , 0xffc000000000L } ; static final long [ ] jjbitVec12 = { 0x0L , 0x3ff00000000L , 0x0L , 0x3ff000000000000L } ; static final long [ ] jjbitVec13 = { 0x0L , 0xffc000000000L , 0x0L , 0xff8000000000L } ; static final long [ ] jjbitVec14 = { 0x0L , 0xffc000000000L , 0x0L , 0x0L } ; static final long [ ] jjbitVec15 = { 0x0L , 0x3ff0000L , 0x0L , 0x3ff0000L } ; static final long [ ] jjbitVec16 = { 0x0L , 0x3ffL , 0x0L , 0x0L } ; static final long [ ] jjbitVec17 = { 0xfffffffeL , 0x0L , 0xfffff00000000000L , 0x7fffffL } ; private final int jjMoveNfa_0 ( int startState , int curPos ) { int [ ] nextStates ; int startsAt = 0 ; jjnewStateCnt = 75 ; int i = 1 ; jjstateSet [ 0 ] = startState ; int j , kind = 0x7fffffff ; for ( ; ; ) { if ( ++ jjround == 0x7fffffff ) ReInitRounds ( ) ; if ( curChar < 64 ) { long l = 1L << curChar ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( ( 0x3ff000000000000L & l ) != 0L ) { if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; } if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 12 , 17 ) ; if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 18 , 23 ) ; break ; case 2 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 18 , 23 ) ; break ; case 3 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 3 , 4 ) ; break ; case 4 : case 5 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 5 , 6 ) ; break ; case 6 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 7 ) ; break ; case 7 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAdd ( 7 ) ; break ; case 8 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 8 , 9 ) ; break ; case 9 : case 10 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 10 , 11 ) ; break ; case 11 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 12 ) ; break ; case 12 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 12 , 13 ) ; break ; case 13 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 14 , 15 ) ; break ; case 14 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 14 , 15 ) ; break ; case 15 : case 16 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 11 , 16 ) ; break ; case 17 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 17 , 18 ) ; break ; case 18 : case 19 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 19 , 20 ) ; break ; case 20 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 21 ) ; break ; case 21 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 21 , 22 ) ; break ; case 22 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 23 , 24 ) ; break ; case 23 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 23 , 24 ) ; break ; case 24 : case 25 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 26 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 27 ) ; break ; case 27 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 22 , 27 ) ; break ; case 28 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 12 , 17 ) ; break ; case 29 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; break ; case 30 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAdd ( 30 ) ; break ; case 31 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 24 , 26 ) ; break ; case 32 : if ( ( 0x600000000000L & l ) != 0L ) jjCheckNAdd ( 33 ) ; break ; case 33 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 27 , 29 ) ; break ; case 35 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 35 , 36 ) ; break ; case 36 : if ( ( 0x600000000000L & l ) != 0L ) jjCheckNAdd ( 37 ) ; break ; case 37 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 5 ) kind = 5 ; jjCheckNAddTwoStates ( 36 , 37 ) ; break ; case 38 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 38 , 39 ) ; break ; case 39 : if ( curChar == 46 ) jjCheckNAdd ( 40 ) ; break ; case 40 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 6 ) kind = 6 ; jjCheckNAddTwoStates ( 39 , 40 ) ; break ; case 41 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 41 , 42 ) ; break ; case 42 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 43 , 44 ) ; break ; case 43 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 43 , 44 ) ; break ; case 44 : case 45 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAdd ( 45 ) ; break ; case 46 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 46 , 47 ) ; break ; case 47 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 48 , 49 ) ; break ; case 48 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 48 , 49 ) ; break ; case 49 : case 50 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 50 , 51 ) ; break ; case 51 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 52 ) ; break ; case 52 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 47 , 52 ) ; break ; case 53 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 53 , 54 ) ; break ; case 54 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 55 , 56 ) ; break ; case 55 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 55 , 56 ) ; break ; case 56 : case 57 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 57 , 58 ) ; break ; case 58 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 59 ) ; break ; case 59 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 59 , 60 ) ; break ; case 60 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 61 , 62 ) ; break ; case 61 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 61 , 62 ) ; break ; case 62 : case 63 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 58 , 63 ) ; break ; case 66 : if ( curChar == 39 ) jjstateSet [ jjnewStateCnt ++ ] = 67 ; break ; case 69 : if ( curChar == 46 ) jjCheckNAdd ( 70 ) ; break ; case 71 : if ( curChar != 46 ) break ; if ( kind > 3 ) kind = 3 ; jjCheckNAdd ( 70 ) ; break ; case 73 : if ( curChar == 38 ) jjstateSet [ jjnewStateCnt ++ ] = 74 ; break ; default : break ; } } while ( i != startsAt ) ; } else if ( curChar < 128 ) { long l = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 30 , 35 ) ; if ( ( 0x7fffffe07fffffeL & l ) != 0L ) { if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; } if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 18 , 23 ) ; break ; case 2 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 18 , 23 ) ; break ; case 3 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 3 , 4 ) ; break ; case 5 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjAddStates ( 36 , 37 ) ; break ; case 6 : if ( curChar == 95 ) jjCheckNAdd ( 7 ) ; break ; case 7 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAdd ( 7 ) ; break ; case 8 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 8 , 9 ) ; break ; case 10 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 10 , 11 ) ; break ; case 11 : if ( curChar == 95 ) jjCheckNAdd ( 12 ) ; break ; case 12 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 12 , 13 ) ; break ; case 13 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 14 , 15 ) ; break ; case 14 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 14 , 15 ) ; break ; case 16 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 11 , 16 ) ; break ; case 17 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 17 , 18 ) ; break ; case 19 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjAddStates ( 38 , 39 ) ; break ; case 20 : if ( curChar == 95 ) jjCheckNAdd ( 21 ) ; break ; case 21 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 21 , 22 ) ; break ; case 22 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 23 , 24 ) ; break ; case 23 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 23 , 24 ) ; break ; case 25 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjAddStates ( 40 , 41 ) ; break ; case 26 : if ( curChar == 95 ) jjCheckNAdd ( 27 ) ; break ; case 27 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 22 , 27 ) ; break ; case 29 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; break ; case 30 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAdd ( 30 ) ; break ; case 31 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 24 , 26 ) ; break ; case 32 : if ( curChar == 95 ) jjCheckNAdd ( 33 ) ; break ; case 33 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 27 , 29 ) ; break ; case 34 : if ( curChar == 64 ) jjCheckNAdd ( 35 ) ; break ; case 35 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 35 , 36 ) ; break ; case 37 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 5 ) kind = 5 ; jjCheckNAddTwoStates ( 36 , 37 ) ; break ; case 38 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 38 , 39 ) ; break ; case 40 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 6 ) kind = 6 ; jjCheckNAddTwoStates ( 39 , 40 ) ; break ; case 41 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 41 , 42 ) ; break ; case 42 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 43 , 44 ) ; break ; case 43 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 43 , 44 ) ; break ; case 45 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjstateSet [ jjnewStateCnt ++ ] = 45 ; break ; case 46 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 46 , 47 ) ; break ; case 47 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 48 , 49 ) ; break ; case 48 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 48 , 49 ) ; break ; case 50 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjAddStates ( 42 , 43 ) ; break ; case 51 : if ( curChar == 95 ) jjCheckNAdd ( 52 ) ; break ; case 52 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 47 , 52 ) ; break ; case 53 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 53 , 54 ) ; break ; case 54 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 55 , 56 ) ; break ; case 55 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 55 , 56 ) ; break ; case 57 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 57 , 58 ) ; break ; case 58 : if ( curChar == 95 ) jjCheckNAdd ( 59 ) ; break ; case 59 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 59 , 60 ) ; break ; case 60 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 61 , 62 ) ; break ; case 61 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 61 , 62 ) ; break ; case 63 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 58 , 63 ) ; break ; case 64 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 30 , 35 ) ; break ; case 65 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 65 , 66 ) ; break ; case 67 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 2 ) kind = 2 ; jjCheckNAddTwoStates ( 66 , 67 ) ; break ; case 68 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 68 , 69 ) ; break ; case 70 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjAddStates ( 44 , 45 ) ; break ; case 72 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 72 , 73 ) ; break ; case 73 : if ( curChar == 64 ) jjCheckNAdd ( 74 ) ; break ; case 74 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 4 ) kind = 4 ; jjCheckNAdd ( 74 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else { int hiByte = ( int ) ( curChar > > 8 ) ; int i1 = hiByte > > 6 ; long l1 = 1L << ( hiByte & 077 ) ; int i2 = ( curChar & 0xff ) > > 6 ; long l2 = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) { if ( kind > 12 ) kind = 12 ; } if ( jjCanMove_1 ( hiByte , i1 , i2 , l1 , l2 ) ) { if ( kind > 13 ) kind = 13 ; } if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 18 , 23 ) ; if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 12 , 17 ) ; if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) { if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; } if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 30 , 35 ) ; break ; case 1 : if ( jjCanMove_1 ( hiByte , i1 , i2 , l1 , l2 ) && kind > 13 ) kind = 13 ; break ; case 2 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 18 , 23 ) ; break ; case 3 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 3 , 4 ) ; break ; case 4 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 5 , 6 ) ; break ; case 5 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 5 , 6 ) ; break ; case 7 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjstateSet [ jjnewStateCnt ++ ] = 7 ; break ; case 8 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 8 , 9 ) ; break ; case 9 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 10 , 11 ) ; break ; case 10 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 10 , 11 ) ; break ; case 12 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 46 , 47 ) ; break ; case 14 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 48 , 49 ) ; break ; case 15 : if ( ! jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 11 , 16 ) ; break ; case 16 : if ( ! jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 11 , 16 ) ; break ; case 17 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 17 , 18 ) ; break ; case 18 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 19 , 20 ) ; break ; case 19 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 19 , 20 ) ; break ; case 21 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 21 , 22 ) ; break ; case 23 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 50 , 51 ) ; break ; case 24 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 25 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 27 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 22 , 27 ) ; break ; case 28 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 12 , 17 ) ; break ; case 29 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; break ; case 30 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAdd ( 30 ) ; break ; case 31 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 24 , 26 ) ; break ; case 33 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 27 , 29 ) ; break ; case 35 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 35 , 36 ) ; break ; case 37 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 5 ) kind = 5 ; jjCheckNAddTwoStates ( 36 , 37 ) ; break ; case 38 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 38 , 39 ) ; break ; case 40 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 6 ) kind = 6 ; jjCheckNAddTwoStates ( 39 , 40 ) ; break ; case 41 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 41 , 42 ) ; break ; case 43 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 52 , 53 ) ; break ; case 44 : if ( ! jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAdd ( 45 ) ; break ; case 45 : if ( ! jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAdd ( 45 ) ; break ; case 46 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 46 , 47 ) ; break ; case 48 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 54 , 55 ) ; break ; case 49 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 50 , 51 ) ; break ; case 50 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 50 , 51 ) ; break ; case 52 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 47 , 52 ) ; break ; case 53 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 53 , 54 ) ; break ; case 55 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 56 , 57 ) ; break ; case 56 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 57 , 58 ) ; break ; case 57 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 57 , 58 ) ; break ; case 59 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 58 , 59 ) ; break ; case 61 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 60 , 61 ) ; break ; case 62 : if ( ! jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 58 , 63 ) ; break ; case 63 : if ( ! jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 58 , 63 ) ; break ; case 64 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 30 , 35 ) ; break ; case 65 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 65 , 66 ) ; break ; case 67 : if ( ! jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 2 ) kind = 2 ; jjCheckNAddTwoStates ( 66 , 67 ) ; break ; case 68 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 68 , 69 ) ; break ; case 70 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 44 , 45 ) ; break ; case 72 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 72 , 73 ) ; break ; case 74 : if ( ! jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 4 ) kind = 4 ; jjstateSet [ jjnewStateCnt ++ ] = 74 ; break ; default : break ; } } while ( i != startsAt ) ; } if ( kind != 0x7fffffff ) { jjmatchedKind = kind ; jjmatchedPos = curPos ; kind = 0x7fffffff ; } ++ curPos ; if ( ( i = jjnewStateCnt ) == ( startsAt = 75 - ( jjnewStateCnt = startsAt ) ) ) return curPos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return curPos ; } } } static final int [ ] jjnextStates = { 30 , 31 , 32 , 34 , 38 , 39 , 41 , 42 , 46 , 47 , 53 , 54 , 5 , 6 , 10 , 11 , 19 , 20 , 3 , 4 , 8 , 9 , 17 , 18 , 31 , 32 , 34 , 32 , 33 , 34 , 65 , 66 , 68 , 69 , 72 , 73 , 5 , 6 , 19 , 20 , 25 , 26 , 50 , 51 , 70 , 71 , 12 , 13 , 14 , 15 , 23 , 24 , 43 , 44 , 48 , 49 , 55 , 56 , 59 , 60 , 61 , 62 , } ; private static final boolean jjCanMove_0 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 48 : return ( ( jjbitVec2 [ i2 ] & l2 ) != 0L ) ; case 49 : return ( ( jjbitVec3 [ i2 ] & l2 ) != 0L ) ; case 51 : return ( ( jjbitVec4 [ i2 ] & l2 ) != 0L ) ; case 61 : return ( ( jjbitVec5 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec0 [ i1 ] & l1 ) != 0L ) return true ; return false ; } } private static final boolean jjCanMove_1 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 215 : return ( ( jjbitVec7 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec6 [ i1 ] & l1 ) != 0L ) return true ; return false ; } } private static final boolean jjCanMove_2 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 0 : return ( ( jjbitVec9 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec8 [ i1 ] & l1 ) != 0L ) return true ; return false ; } } private static final boolean jjCanMove_3 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 6 : return ( ( jjbitVec12 [ i2 ] & l2 ) != 0L ) ; case 11 : return ( ( jjbitVec13 [ i2 ] & l2 ) != 0L ) ; case 13 : return ( ( jjbitVec14 [ i2 ] & l2 ) != 0L ) ; case 14 : return ( ( jjbitVec15 [ i2 ] & l2 ) != 0L ) ; case 16 : return ( ( jjbitVec16 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec10 [ i1 ] & l1 ) != 0L ) if ( ( jjbitVec11 [ i2 ] & l2 ) == 0L ) return false ; else return true ; return false ; } } private static final boolean jjCanMove_4 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 0 : return ( ( jjbitVec9 [ i2 ] & l2 ) != 0L ) ; case 215 : return ( ( jjbitVec7 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec17 [ i1 ] & l1 ) != 0L ) return true ; return false ; } } public static final String [ ] jjstrLiteralImages = { "" , null , null , null , null , null , null , null , null , null , null , null , null , null , null , null , } ; public static final String [ ] lexStateNames = { "DEFAULT" , } ; static final long [ ] jjtoToken = { 0x30ffL , } ; static final long [ ] jjtoSkip = { 0x8000L , } ; protected CharStream input_stream ; private final int [ ] jjrounds = new int [ 75 ] ; private final int [ ] jjstateSet = new int [ 150 ] ; protected char curChar ; public StandardTokenizerTokenManager ( CharStream stream ) { input_stream = stream ; } public StandardTokenizerTokenManager ( CharStream stream , int lexState ) { this ( stream ) ; SwitchTo ( lexState ) ; } public void ReInit ( CharStream stream ) { jjmatchedPos = jjnewStateCnt = 0 ; curLexState = defaultLexState ; input_stream = stream ; ReInitRounds ( ) ; } private final void ReInitRounds ( ) { int i ; jjround = 0x80000001 ; for ( i = 75 ; i -- > 0 ; ) jjrounds [ i ] = 0x80000000 ; } public void ReInit ( CharStream stream , int lexState ) { ReInit ( stream ) ; SwitchTo ( lexState ) ; } public void SwitchTo ( int lexState ) { if ( lexState >= 1 || lexState < 0 ) throw new TokenMgrError ( "Error: Ignoring invalid lexical state : " + lexState + ". State unchanged." , TokenMgrError . INVALID_LEXICAL_STATE ) ; else curLexState = lexState ; } protected Token jjFillToken ( ) { Token t = Token . newToken ( jjmatchedKind ) ; t . kind = jjmatchedKind ; String im = jjstrLiteralImages [ jjmatchedKind ] ; t . image = ( im == null ) ? input_stream . GetImage ( ) : im ; t . beginLine = input_stream . getBeginLine ( ) ; t . beginColumn = input_stream . getBeginColumn ( ) ; t . endLine = input_stream . getEndLine ( ) ; t . endColumn = input_stream . getEndColumn ( ) ; return t ; } int curLexState = 0 ; int defaultLexState = 0 ; int jjnewStateCnt ; int jjround ; int jjmatchedPos ; int jjmatchedKind ; public Token getNextToken ( ) { int kind ; Token specialToken = null ; Token matchedToken ; int curPos = 0 ; EOFLoop : for ( ; ; ) { try { curChar = input_stream . BeginToken ( ) ; } catch ( java . io . IOException e ) { jjmatchedKind = 0 ; matchedToken = jjFillToken ( ) ; return matchedToken ; } jjmatchedKind = 0x7fffffff ; jjmatchedPos = 0 ; curPos = jjMoveStringLiteralDfa0_0 ( ) ; if ( jjmatchedPos == 0 && jjmatchedKind > 15 ) { jjmatchedKind = 15 ; } if ( jjmatchedKind != 0x7fffffff ) { if ( jjmatchedPos + 1 < curPos ) input_stream . backup ( curPos - jjmatchedPos - 1 ) ; if ( ( jjtoToken [ jjmatchedKind > > 6 ] & ( 1L << ( jjmatchedKind & 077 ) ) ) != 0L ) { matchedToken = jjFillToken ( ) ; return matchedToken ; } else { continue EOFLoop ; } } int error_line = input_stream . getEndLine ( ) ; int error_column = input_stream . getEndColumn ( ) ; String error_after = null ; boolean EOFSeen = false ; try { input_stream . readChar ( ) ; input_stream . backup ( 1 ) ; } catch ( java . io . IOException e1 ) { EOFSeen = true ; error_after = curPos <= 1 ? "" : input_stream . GetImage ( ) ; if ( curChar == '\n' || curChar == '\r' ) { error_line ++ ; error_column = 0 ; } else error_column ++ ; } if ( ! EOFSeen ) { input_stream . backup ( 1 ) ; error_after = curPos <= 1 ? "" : input_stream . GetImage ( ) ; } throw new TokenMgrError ( EOFSeen , curLexState , error_line , error_column , error_after , curChar , TokenMgrError . LEXICAL_ERROR ) ; } } } 	1	['22', '1', '0', '5', '38', '153', '1', '4', '7', '0.848484848', '3709', '0.121212121', '1', '0', '0.380952381', '0', '0', '166.0909091', '236', '13.7727', '1']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; public interface Weight extends java . io . Serializable { Query getQuery ( ) ; float getValue ( ) ; float sumOfSquaredWeights ( ) throws IOException ; void normalize ( float norm ) ; Scorer scorer ( IndexReader reader ) throws IOException ; Explanation explain ( IndexReader reader , int doc ) throws IOException ; } 	0	['6', '1', '0', '40', '6', '15', '37', '4', '6', '2', '6', '0', '0', '0', '0.416666667', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . analysis ; public class ISOLatin1AccentFilter extends TokenFilter { public ISOLatin1AccentFilter ( TokenStream input ) { super ( input ) ; } public final Token next ( ) throws java . io . IOException { final Token t = input . next ( ) ; if ( t == null ) return null ; return new Token ( removeAccents ( t . termText ( ) ) , t . startOffset ( ) , t . endOffset ( ) , t . type ( ) ) ; } public final static String removeAccents ( String input ) { final StringBuffer output = new StringBuffer ( ) ; for ( int i = 0 ; i < input . length ( ) ; i ++ ) { switch ( input . charAt ( i ) ) { case '' : case '' : case '' : case '' : case '' : case '' : output . append ( "A" ) ; break ; case '' : output . append ( "AE" ) ; break ; case '' : output . append ( "C" ) ; break ; case '' : case '' : case '' : case '' : output . append ( "E" ) ; break ; case '' : case '' : case '' : case '' : output . append ( "I" ) ; break ; case '' : output . append ( "D" ) ; break ; case '' : output . append ( "N" ) ; break ; case '' : case '' : case '' : case '' : case '' : case '' : output . append ( "O" ) ; break ; case '' : output . append ( "OE" ) ; break ; case '' : output . append ( "TH" ) ; break ; case '' : case '' : case '' : case '' : output . append ( "U" ) ; break ; case '' : case '' : output . append ( "Y" ) ; break ; case '' : case '' : case '' : case '' : case '' : case '' : output . append ( "a" ) ; break ; case '' : output . append ( "ae" ) ; break ; case '' : output . append ( "c" ) ; break ; case '' : case '' : case '' : case '' : output . append ( "e" ) ; break ; case '' : case '' : case '' : case '' : output . append ( "i" ) ; break ; case '' : output . append ( "d" ) ; break ; case '' : output . append ( "n" ) ; break ; case '' : case '' : case '' : case '' : case '' : case '' : output . append ( "o" ) ; break ; case '' : output . append ( "oe" ) ; break ; case '' : output . append ( "ss" ) ; break ; case '' : output . append ( "th" ) ; break ; case '' : case '' : case '' : case '' : output . append ( "u" ) ; break ; case '' : case '' : output . append ( "y" ) ; break ; default : output . append ( input . charAt ( i ) ) ; break ; } } return output . toString ( ) ; } } 	1	['3', '3', '0', '3', '16', '3', '0', '3', '3', '2', '178', '0', '0', '0.6', '0.444444444', '0', '0', '58.33333333', '3', '1.3333', '1']
package org . apache . lucene . search ; import java . io . IOException ; public abstract class Scorer { private Similarity similarity ; protected Scorer ( Similarity similarity ) { this . similarity = similarity ; } public Similarity getSimilarity ( ) { return this . similarity ; } public void score ( HitCollector hc ) throws IOException { while ( next ( ) ) { hc . collect ( doc ( ) , score ( ) ) ; } } protected boolean score ( HitCollector hc , int max ) throws IOException { while ( doc ( ) < max ) { hc . collect ( doc ( ) , score ( ) ) ; if ( ! next ( ) ) return false ; } return true ; } public abstract boolean next ( ) throws IOException ; public abstract int doc ( ) ; public abstract float score ( ) throws IOException ; public abstract boolean skipTo ( int target ) throws IOException ; public abstract Explanation explain ( int doc ) throws IOException ; } 	0	['9', '1', '15', '33', '11', '34', '30', '3', '7', '0.875', '47', '1', '1', '0', '0.416666667', '0', '0', '4.111111111', '1', '0.8889', '0']
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; final class PhraseQueue extends PriorityQueue { PhraseQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object o1 , Object o2 ) { PhrasePositions pp1 = ( PhrasePositions ) o1 ; PhrasePositions pp2 = ( PhrasePositions ) o2 ; if ( pp1 . doc == pp2 . doc ) return pp1 . position < pp2 . position ; else return pp1 . doc < pp2 . doc ; } } 	1	['2', '2', '0', '5', '4', '1', '3', '2', '0', '2', '37', '0', '0', '0.916666667', '0.666666667', '1', '3', '17.5', '4', '2', '1']
package org . apache . lucene . analysis ; import java . io . File ; import java . io . IOException ; import java . io . Reader ; import java . util . Set ; public final class StopAnalyzer extends Analyzer { private Set stopWords ; public static final String [ ] ENGLISH_STOP_WORDS = { "a" , "an" , "and" , "are" , "as" , "at" , "be" , "but" , "by" , "for" , "if" , "in" , "into" , "is" , "it" , "no" , "not" , "of" , "on" , "or" , "s" , "such" , "t" , "that" , "the" , "their" , "then" , "there" , "these" , "they" , "this" , "to" , "was" , "will" , "with" } ; public StopAnalyzer ( ) { stopWords = StopFilter . makeStopSet ( ENGLISH_STOP_WORDS ) ; } public StopAnalyzer ( Set stopWords ) { this . stopWords = stopWords ; } public StopAnalyzer ( String [ ] stopWords ) { this . stopWords = StopFilter . makeStopSet ( stopWords ) ; } public StopAnalyzer ( File stopwordsFile ) throws IOException { stopWords = WordlistLoader . getWordSet ( stopwordsFile ) ; } public StopAnalyzer ( Reader stopwords ) throws IOException { stopWords = WordlistLoader . getWordSet ( stopwords ) ; } public TokenStream tokenStream ( String fieldName , Reader reader ) { return new StopFilter ( new LowerCaseTokenizer ( reader ) , stopWords ) ; } } 	0	['7', '2', '0', '6', '13', '0', '1', '5', '6', '0.5', '197', '0.5', '0', '0.666666667', '0.333333333', '0', '0', '26.85714286', '1', '0.1429', '0']
package org . apache . lucene . search ; import java . util . ArrayList ; public class Explanation implements java . io . Serializable { private float value ; private String description ; private ArrayList details ; public Explanation ( ) { } public Explanation ( float value , String description ) { this . value = value ; this . description = description ; } public float getValue ( ) { return value ; } public void setValue ( float value ) { this . value = value ; } public String getDescription ( ) { return description ; } public void setDescription ( String description ) { this . description = description ; } public Explanation [ ] getDetails ( ) { if ( details == null ) return null ; return ( Explanation [ ] ) details . toArray ( new Explanation [ 0 ] ) ; } public void addDetail ( Explanation detail ) { if ( details == null ) details = new ArrayList ( ) ; details . add ( detail ) ; } public String toString ( ) { return toString ( 0 ) ; } private String toString ( int depth ) { StringBuffer buffer = new StringBuffer ( ) ; for ( int i = 0 ; i < depth ; i ++ ) { buffer . append ( "  " ) ; } buffer . append ( getValue ( ) ) ; buffer . append ( " = " ) ; buffer . append ( getDescription ( ) ) ; buffer . append ( "\n" ) ; Explanation [ ] details = getDetails ( ) ; if ( details != null ) { for ( int i = 0 ; i < details . length ; i ++ ) { buffer . append ( details [ i ] . toString ( depth + 1 ) ) ; } } return buffer . toString ( ) ; } public String toHtml ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "<ul>\n" ) ; buffer . append ( "<li>" ) ; buffer . append ( getValue ( ) ) ; buffer . append ( " = " ) ; buffer . append ( getDescription ( ) ) ; buffer . append ( "</li>\n" ) ; Explanation [ ] details = getDetails ( ) ; if ( details != null ) { for ( int i = 0 ; i < details . length ; i ++ ) { buffer . append ( details [ i ] . toHtml ( ) ) ; } } buffer . append ( "</ul>\n" ) ; return buffer . toString ( ) ; } } 	1	['11', '1', '0', '33', '19', '41', '33', '0', '10', '0.633333333', '186', '1', '0', '0', '0.309090909', '0', '0', '15.63636364', '4', '1.4545', '2']
package org . apache . lucene . search ; import java . io . Serializable ; import java . util . Locale ; public class SortField implements Serializable { public static final int SCORE = 0 ; public static final int DOC = 1 ; public static final int AUTO = 2 ; public static final int STRING = 3 ; public static final int INT = 4 ; public static final int FLOAT = 5 ; public static final int CUSTOM = 9 ; public static final SortField FIELD_SCORE = new SortField ( null , SCORE ) ; public static final SortField FIELD_DOC = new SortField ( null , DOC ) ; private String field ; private int type = AUTO ; private Locale locale ; boolean reverse = false ; private SortComparatorSource factory ; public SortField ( String field ) { this . field = field . intern ( ) ; } public SortField ( String field , boolean reverse ) { this . field = field . intern ( ) ; this . reverse = reverse ; } public SortField ( String field , int type ) { this . field = ( field != null ) ? field . intern ( ) : field ; this . type = type ; } public SortField ( String field , int type , boolean reverse ) { this . field = ( field != null ) ? field . intern ( ) : field ; this . type = type ; this . reverse = reverse ; } public SortField ( String field , Locale locale ) { this . field = field . intern ( ) ; this . type = STRING ; this . locale = locale ; } public SortField ( String field , Locale locale , boolean reverse ) { this . field = field . intern ( ) ; this . type = STRING ; this . locale = locale ; this . reverse = reverse ; } public SortField ( String field , SortComparatorSource comparator ) { this . field = ( field != null ) ? field . intern ( ) : field ; this . type = CUSTOM ; this . factory = comparator ; } public SortField ( String field , SortComparatorSource comparator , boolean reverse ) { this . field = ( field != null ) ? field . intern ( ) : field ; this . type = CUSTOM ; this . reverse = reverse ; this . factory = comparator ; } public String getField ( ) { return field ; } public int getType ( ) { return type ; } public Locale getLocale ( ) { return locale ; } public boolean getReverse ( ) { return reverse ; } public SortComparatorSource getFactory ( ) { return factory ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; switch ( type ) { case SCORE : buffer . append ( "<score>" ) ; break ; case DOC : buffer . append ( "<doc>" ) ; break ; case CUSTOM : buffer . append ( "<custom:\"" + field + "\": " + factory + ">" ) ; break ; default : buffer . append ( "\"" + field + "\"" ) ; break ; } if ( locale != null ) buffer . append ( "(" + locale + ")" ) ; if ( reverse ) buffer . append ( '!' ) ; return buffer . toString ( ) ; } } 	0	['15', '1', '0', '9', '22', '0', '8', '1', '14', '0.852040816', '297', '0.285714286', '3', '0', '0.380952381', '0', '0', '17.86666667', '7', '0.8', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . util . BitVector ; import org . apache . lucene . store . IndexInput ; class SegmentTermDocs implements TermDocs { protected SegmentReader parent ; protected IndexInput freqStream ; protected int count ; protected int df ; protected BitVector deletedDocs ; int doc = 0 ; int freq ; private int skipInterval ; private int numSkips ; private int skipCount ; private IndexInput skipStream ; private int skipDoc ; private long freqPointer ; private long proxPointer ; private long skipPointer ; private boolean haveSkipped ; protected SegmentTermDocs ( SegmentReader parent ) { this . parent = parent ; this . freqStream = ( IndexInput ) parent . freqStream . clone ( ) ; this . deletedDocs = parent . deletedDocs ; this . skipInterval = parent . tis . getSkipInterval ( ) ; } public void seek ( Term term ) throws IOException { TermInfo ti = parent . tis . get ( term ) ; seek ( ti ) ; } public void seek ( TermEnum termEnum ) throws IOException { TermInfo ti ; if ( termEnum instanceof SegmentTermEnum && ( ( SegmentTermEnum ) termEnum ) . fieldInfos == parent . fieldInfos ) ti = ( ( SegmentTermEnum ) termEnum ) . termInfo ( ) ; else ti = parent . tis . get ( termEnum . term ( ) ) ; seek ( ti ) ; } void seek ( TermInfo ti ) throws IOException { count = 0 ; if ( ti == null ) { df = 0 ; } else { df = ti . docFreq ; doc = 0 ; skipDoc = 0 ; skipCount = 0 ; numSkips = df / skipInterval ; freqPointer = ti . freqPointer ; proxPointer = ti . proxPointer ; skipPointer = freqPointer + ti . skipOffset ; freqStream . seek ( freqPointer ) ; haveSkipped = false ; } } public void close ( ) throws IOException { freqStream . close ( ) ; if ( skipStream != null ) skipStream . close ( ) ; } public final int doc ( ) { return doc ; } public final int freq ( ) { return freq ; } protected void skippingDoc ( ) throws IOException { } public boolean next ( ) throws IOException { while ( true ) { if ( count == df ) return false ; int docCode = freqStream . readVInt ( ) ; doc += docCode > > > 1 ; if ( ( docCode & 1 ) != 0 ) freq = 1 ; else freq = freqStream . readVInt ( ) ; count ++ ; if ( deletedDocs == null || ! deletedDocs . get ( doc ) ) break ; skippingDoc ( ) ; } return true ; } public int read ( final int [ ] docs , final int [ ] freqs ) throws IOException { final int length = docs . length ; int i = 0 ; while ( i < length && count < df ) { final int docCode = freqStream . readVInt ( ) ; doc += docCode > > > 1 ; if ( ( docCode & 1 ) != 0 ) freq = 1 ; else freq = freqStream . readVInt ( ) ; count ++ ; if ( deletedDocs == null || ! deletedDocs . get ( doc ) ) { docs [ i ] = doc ; freqs [ i ] = freq ; ++ i ; } } return i ; } protected void skipProx ( long proxPointer ) throws IOException { } public boolean skipTo ( int target ) throws IOException { if ( df >= skipInterval ) { if ( skipStream == null ) skipStream = ( IndexInput ) freqStream . clone ( ) ; if ( ! haveSkipped ) { skipStream . seek ( skipPointer ) ; haveSkipped = true ; } int lastSkipDoc = skipDoc ; long lastFreqPointer = freqStream . getFilePointer ( ) ; long lastProxPointer = - 1 ; int numSkipped = - 1 - ( count % skipInterval ) ; while ( target > skipDoc ) { lastSkipDoc = skipDoc ; lastFreqPointer = freqPointer ; lastProxPointer = proxPointer ; if ( skipDoc != 0 && skipDoc >= doc ) numSkipped += skipInterval ; if ( skipCount >= numSkips ) break ; skipDoc += skipStream . readVInt ( ) ; freqPointer += skipStream . readVInt ( ) ; proxPointer += skipStream . readVInt ( ) ; skipCount ++ ; } if ( lastFreqPointer > freqStream . getFilePointer ( ) ) { freqStream . seek ( lastFreqPointer ) ; skipProx ( lastProxPointer ) ; doc = lastSkipDoc ; count += numSkipped ; } } do { if ( ! next ( ) ) return false ; } while ( target > doc ) ; return true ; } } 	1	['12', '1', '1', '11', '23', '16', '2', '10', '8', '0.670454545', '420', '0.875', '4', '0', '0.197916667', '0', '0', '32.66666667', '1', '0.9167', '2']
package org . apache . lucene . queryParser ; import java . io . * ; public final class FastCharStream implements CharStream { char [ ] buffer = null ; int bufferLength = 0 ; int bufferPosition = 0 ; int tokenStart = 0 ; int bufferStart = 0 ; Reader input ; public FastCharStream ( Reader r ) { input = r ; } public final char readChar ( ) throws IOException { if ( bufferPosition >= bufferLength ) refill ( ) ; return buffer [ bufferPosition ++ ] ; } private final void refill ( ) throws IOException { int newPosition = bufferLength - tokenStart ; if ( tokenStart == 0 ) { if ( buffer == null ) { buffer = new char [ 2048 ] ; } else if ( bufferLength == buffer . length ) { char [ ] newBuffer = new char [ buffer . length * 2 ] ; System . arraycopy ( buffer , 0 , newBuffer , 0 , bufferLength ) ; buffer = newBuffer ; } } else { System . arraycopy ( buffer , tokenStart , buffer , 0 , newPosition ) ; } bufferLength = newPosition ; bufferPosition = newPosition ; bufferStart += tokenStart ; tokenStart = 0 ; int charsRead = input . read ( buffer , newPosition , buffer . length - newPosition ) ; if ( charsRead == - 1 ) throw new IOException ( "read past eof" ) ; else bufferLength += charsRead ; } public final char BeginToken ( ) throws IOException { tokenStart = bufferPosition ; return readChar ( ) ; } public final void backup ( int amount ) { bufferPosition -= amount ; } public final String GetImage ( ) { return new String ( buffer , tokenStart , bufferPosition - tokenStart ) ; } public final char [ ] GetSuffix ( int len ) { char [ ] value = new char [ len ] ; System . arraycopy ( buffer , bufferPosition - len , value , 0 , len ) ; return value ; } public final void Done ( ) { try { input . close ( ) ; } catch ( IOException e ) { System . err . println ( "Caught: " + e + "; ignoring." ) ; } } public final int getColumn ( ) { return bufferStart + bufferPosition ; } public final int getLine ( ) { return 1 ; } public final int getEndColumn ( ) { return bufferStart + bufferPosition ; } public final int getEndLine ( ) { return 1 ; } public final int getBeginColumn ( ) { return bufferStart + tokenStart ; } public final int getBeginLine ( ) { return 1 ; } } 	0	['14', '1', '0', '2', '25', '3', '1', '1', '13', '0.602564103', '237', '0', '0', '0', '0.404761905', '0', '0', '15.5', '1', '0.9286', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . BufferedIndexInput ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . Lock ; import java . util . HashMap ; import java . io . IOException ; class CompoundFileReader extends Directory { private static final class FileEntry { long offset ; long length ; } private Directory directory ; private String fileName ; private IndexInput stream ; private HashMap entries = new HashMap ( ) ; public CompoundFileReader ( Directory dir , String name ) throws IOException { directory = dir ; fileName = name ; boolean success = false ; try { stream = dir . openInput ( name ) ; int count = stream . readVInt ( ) ; FileEntry entry = null ; for ( int i = 0 ; i < count ; i ++ ) { long offset = stream . readLong ( ) ; String id = stream . readString ( ) ; if ( entry != null ) { entry . length = offset - entry . offset ; } entry = new FileEntry ( ) ; entry . offset = offset ; entries . put ( id , entry ) ; } if ( entry != null ) { entry . length = stream . length ( ) - entry . offset ; } success = true ; } finally { if ( ! success && ( stream != null ) ) { try { stream . close ( ) ; } catch ( IOException e ) { } } } } public Directory getDirectory ( ) { return directory ; } public String getName ( ) { return fileName ; } public synchronized void close ( ) throws IOException { if ( stream == null ) throw new IOException ( "Already closed" ) ; entries . clear ( ) ; stream . close ( ) ; stream = null ; } public synchronized IndexInput openInput ( String id ) throws IOException { if ( stream == null ) throw new IOException ( "Stream closed" ) ; FileEntry entry = ( FileEntry ) entries . get ( id ) ; if ( entry == null ) throw new IOException ( "No sub-file with id " + id + " found" ) ; return new CSIndexInput ( stream , entry . offset , entry . length ) ; } public String [ ] list ( ) { String res [ ] = new String [ entries . size ( ) ] ; return ( String [ ] ) entries . keySet ( ) . toArray ( res ) ; } public boolean fileExists ( String name ) { return entries . containsKey ( name ) ; } public long fileModified ( String name ) throws IOException { return directory . fileModified ( fileName ) ; } public void touchFile ( String name ) throws IOException { directory . touchFile ( fileName ) ; } public void deleteFile ( String name ) { throw new UnsupportedOperationException ( ) ; } public void renameFile ( String from , String to ) { throw new UnsupportedOperationException ( ) ; } public long fileLength ( String name ) throws IOException { FileEntry e = ( FileEntry ) entries . get ( name ) ; if ( e == null ) throw new IOException ( "File " + name + " does not exist" ) ; return e . length ; } public IndexOutput createOutput ( String name ) { throw new UnsupportedOperationException ( ) ; } public Lock makeLock ( String name ) { throw new UnsupportedOperationException ( ) ; } static final class CSIndexInput extends BufferedIndexInput { IndexInput base ; long fileOffset ; long length ; CSIndexInput ( final IndexInput base , final long fileOffset , final long length ) { this . base = base ; this . fileOffset = fileOffset ; this . length = length ; } protected void readInternal ( byte [ ] b , int offset , int len ) throws IOException { synchronized ( base ) { long start = getFilePointer ( ) ; if ( start + len > length ) throw new IOException ( "read past EOF" ) ; base . seek ( fileOffset + start ) ; base . readBytes ( b , offset , len ) ; } } protected void seekInternal ( long pos ) { } public void close ( ) { } public long length ( ) { return length ; } } } 	1	['14', '2', '0', '10', '38', '43', '3', '7', '14', '0.75', '246', '1', '2', '0.458333333', '0.595238095', '0', '0', '16.28571429', '1', '0.9286', '1']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; public class WildcardTermEnum extends FilteredTermEnum { Term searchTerm ; String field = "" ; String text = "" ; String pre = "" ; int preLen = 0 ; boolean endEnum = false ; public WildcardTermEnum ( IndexReader reader , Term term ) throws IOException { super ( ) ; searchTerm = term ; field = searchTerm . field ( ) ; text = searchTerm . text ( ) ; int sidx = text . indexOf ( WILDCARD_STRING ) ; int cidx = text . indexOf ( WILDCARD_CHAR ) ; int idx = sidx ; if ( idx == - 1 ) { idx = cidx ; } else if ( cidx >= 0 ) { idx = Math . min ( idx , cidx ) ; } pre = searchTerm . text ( ) . substring ( 0 , idx ) ; preLen = pre . length ( ) ; text = text . substring ( preLen ) ; setEnum ( reader . terms ( new Term ( searchTerm . field ( ) , pre ) ) ) ; } protected final boolean termCompare ( Term term ) { if ( field == term . field ( ) ) { String searchText = term . text ( ) ; if ( searchText . startsWith ( pre ) ) { return wildcardEquals ( text , 0 , searchText , preLen ) ; } } endEnum = true ; return false ; } public final float difference ( ) { return 1.0f ; } public final boolean endEnum ( ) { return endEnum ; } public static final char WILDCARD_STRING = '*' ; public static final char WILDCARD_CHAR = '?' ; public static final boolean wildcardEquals ( String pattern , int patternIdx , String string , int stringIdx ) { int p = patternIdx ; for ( int s = stringIdx ; ; ++ p , ++ s ) { boolean sEnd = ( s >= string . length ( ) ) ; boolean pEnd = ( p >= pattern . length ( ) ) ; if ( sEnd ) { boolean justWildcardsLeft = true ; int wildcardSearchPos = p ; while ( wildcardSearchPos < pattern . length ( ) && justWildcardsLeft ) { char wildchar = pattern . charAt ( wildcardSearchPos ) ; if ( wildchar != WILDCARD_CHAR && wildchar != WILDCARD_STRING ) { justWildcardsLeft = false ; } else { if ( wildchar == WILDCARD_CHAR ) { return false ; } wildcardSearchPos ++ ; } } if ( justWildcardsLeft ) { return true ; } } if ( sEnd || pEnd ) { break ; } if ( pattern . charAt ( p ) == WILDCARD_CHAR ) { continue ; } if ( pattern . charAt ( p ) == WILDCARD_STRING ) { ++ p ; for ( int i = string . length ( ) ; i >= s ; -- i ) { if ( wildcardEquals ( pattern , p , string , i ) ) { return true ; } } break ; } if ( pattern . charAt ( p ) != string . charAt ( s ) ) { break ; } } return false ; } public void close ( ) throws IOException { super . close ( ) ; searchTerm = null ; field = null ; text = null ; } } 	0	['6', '3', '0', '5', '20', '5', '1', '4', '5', '0.825', '247', '0', '1', '0.722222222', '0.333333333', '1', '4', '38.83333333', '16', '3.6667', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . List ; import java . util . ArrayList ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . PriorityQueue ; class NearSpans implements Spans { private SpanNearQuery query ; private List ordered = new ArrayList ( ) ; private int slop ; private boolean inOrder ; private SpansCell first ; private SpansCell last ; private int totalLength ; private CellQueue queue ; private SpansCell max ; private boolean more = true ; private boolean firstTime = true ; private class CellQueue extends PriorityQueue { public CellQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object o1 , Object o2 ) { SpansCell spans1 = ( SpansCell ) o1 ; SpansCell spans2 = ( SpansCell ) o2 ; if ( spans1 . doc ( ) == spans2 . doc ( ) ) { if ( spans1 . start ( ) == spans2 . start ( ) ) { if ( spans1 . end ( ) == spans2 . end ( ) ) { return spans1 . index > spans2 . index ; } else { return spans1 . end ( ) < spans2 . end ( ) ; } } else { return spans1 . start ( ) < spans2 . start ( ) ; } } else { return spans1 . doc ( ) < spans2 . doc ( ) ; } } } private class SpansCell implements Spans { private Spans spans ; private SpansCell next ; private int length = - 1 ; private int index ; public SpansCell ( Spans spans , int index ) { this . spans = spans ; this . index = index ; } public boolean next ( ) throws IOException { if ( length != - 1 ) totalLength -= length ; boolean more = spans . next ( ) ; if ( more ) { length = end ( ) - start ( ) ; totalLength += length ; if ( max == null || doc ( ) > max . doc ( ) || ( doc ( ) == max . doc ( ) && end ( ) > max . end ( ) ) ) max = this ; } return more ; } public boolean skipTo ( int target ) throws IOException { if ( length != - 1 ) totalLength -= length ; boolean more = spans . skipTo ( target ) ; if ( more ) { length = end ( ) - start ( ) ; totalLength += length ; if ( max == null || doc ( ) > max . doc ( ) || ( doc ( ) == max . doc ( ) && end ( ) > max . end ( ) ) ) max = this ; } return more ; } public int doc ( ) { return spans . doc ( ) ; } public int start ( ) { return spans . start ( ) ; } public int end ( ) { return spans . end ( ) ; } public String toString ( ) { return spans . toString ( ) + "#" + index ; } } public NearSpans ( SpanNearQuery query , IndexReader reader ) throws IOException { this . query = query ; this . slop = query . getSlop ( ) ; this . inOrder = query . isInOrder ( ) ; SpanQuery [ ] clauses = query . getClauses ( ) ; queue = new CellQueue ( clauses . length ) ; for ( int i = 0 ; i < clauses . length ; i ++ ) { SpansCell cell = new SpansCell ( clauses [ i ] . getSpans ( reader ) , i ) ; ordered . add ( cell ) ; } } public boolean next ( ) throws IOException { if ( firstTime ) { initList ( true ) ; listToQueue ( ) ; firstTime = false ; } else if ( more ) { more = min ( ) . next ( ) ; if ( more ) queue . adjustTop ( ) ; } while ( more ) { boolean queueStale = false ; if ( min ( ) . doc ( ) != max . doc ( ) ) { queueToList ( ) ; queueStale = true ; } while ( more && first . doc ( ) < last . doc ( ) ) { more = first . skipTo ( last . doc ( ) ) ; firstToLast ( ) ; queueStale = true ; } if ( ! more ) return false ; if ( queueStale ) { listToQueue ( ) ; queueStale = false ; } if ( atMatch ( ) ) return true ; if ( inOrder && checkSlop ( ) ) { more = firstNonOrderedNextToPartialList ( ) ; if ( more ) { partialListToQueue ( ) ; } } else { more = min ( ) . next ( ) ; if ( more ) { queue . adjustTop ( ) ; } } } return false ; } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { initList ( false ) ; for ( SpansCell cell = first ; more && cell != null ; cell = cell . next ) { more = cell . skipTo ( target ) ; } if ( more ) { listToQueue ( ) ; } firstTime = false ; } else { while ( more && min ( ) . doc ( ) < target ) { more = min ( ) . skipTo ( target ) ; if ( more ) queue . adjustTop ( ) ; } } if ( more ) { if ( atMatch ( ) ) return true ; return next ( ) ; } return false ; } private SpansCell min ( ) { return ( SpansCell ) queue . top ( ) ; } public int doc ( ) { return min ( ) . doc ( ) ; } public int start ( ) { return min ( ) . start ( ) ; } public int end ( ) { return max . end ( ) ; } public String toString ( ) { return "spans(" + query . toString ( ) + ")@" + ( firstTime ? "START" : ( more ? ( doc ( ) + ":" + start ( ) + "-" + end ( ) ) : "END" ) ) ; } private void initList ( boolean next ) throws IOException { for ( int i = 0 ; more && i < ordered . size ( ) ; i ++ ) { SpansCell cell = ( SpansCell ) ordered . get ( i ) ; if ( next ) more = cell . next ( ) ; if ( more ) { addToList ( cell ) ; } } } private void addToList ( SpansCell cell ) { if ( last != null ) { last . next = cell ; } else first = cell ; last = cell ; cell . next = null ; } private void firstToLast ( ) { last . next = first ; last = first ; first = first . next ; last . next = null ; } private void queueToList ( ) { last = first = null ; while ( queue . top ( ) != null ) { addToList ( ( SpansCell ) queue . pop ( ) ) ; } } private boolean firstNonOrderedNextToPartialList ( ) throws IOException { last = first = null ; int orderedIndex = 0 ; while ( queue . top ( ) != null ) { SpansCell cell = ( SpansCell ) queue . pop ( ) ; addToList ( cell ) ; if ( cell . index == orderedIndex ) { orderedIndex ++ ; } else { return cell . next ( ) ; } } throw new RuntimeException ( "Unexpected: ordered" ) ; } private void listToQueue ( ) { queue . clear ( ) ; partialListToQueue ( ) ; } private void partialListToQueue ( ) { for ( SpansCell cell = first ; cell != null ; cell = cell . next ) { queue . put ( cell ) ; } } private boolean atMatch ( ) { return ( min ( ) . doc ( ) == max . doc ( ) ) && checkSlop ( ) && ( ! inOrder || matchIsOrdered ( ) ) ; } private boolean checkSlop ( ) { int matchLength = max . end ( ) - min ( ) . start ( ) ; return ( matchLength - totalLength ) <= slop ; } private boolean matchIsOrdered ( ) { int lastStart = - 1 ; for ( int i = 0 ; i < ordered . size ( ) ; i ++ ) { int start = ( ( SpansCell ) ordered . get ( i ) ) . start ( ) ; if ( ! ( start > lastStart ) ) return false ; lastStart = start ; } return true ; } } 	1	['22', '1', '0', '6', '52', '95', '3', '6', '7', '0.748917749', '563', '1', '5', '0', '0.194805195', '0', '0', '24.09090909', '5', '1.5', '1']
package org . apache . lucene . search ; import java . io . Serializable ; public class Sort implements Serializable { public static final Sort RELEVANCE = new Sort ( ) ; public static final Sort INDEXORDER = new Sort ( SortField . FIELD_DOC ) ; SortField [ ] fields ; public Sort ( ) { this ( new SortField [ ] { SortField . FIELD_SCORE , SortField . FIELD_DOC } ) ; } public Sort ( String field ) { setSort ( field , false ) ; } public Sort ( String field , boolean reverse ) { setSort ( field , reverse ) ; } public Sort ( String [ ] fields ) { setSort ( fields ) ; } public Sort ( SortField field ) { setSort ( field ) ; } public Sort ( SortField [ ] fields ) { setSort ( fields ) ; } public final void setSort ( String field ) { setSort ( field , false ) ; } public void setSort ( String field , boolean reverse ) { SortField [ ] nfields = new SortField [ ] { new SortField ( field , SortField . AUTO , reverse ) , SortField . FIELD_DOC } ; fields = nfields ; } public void setSort ( String [ ] fieldnames ) { final int n = fieldnames . length ; SortField [ ] nfields = new SortField [ n ] ; for ( int i = 0 ; i < n ; ++ i ) { nfields [ i ] = new SortField ( fieldnames [ i ] , SortField . AUTO ) ; } fields = nfields ; } public void setSort ( SortField field ) { this . fields = new SortField [ ] { field } ; } public void setSort ( SortField [ ] fields ) { this . fields = fields ; } public SortField [ ] getSort ( ) { return fields ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { buffer . append ( fields [ i ] . toString ( ) ) ; if ( ( i + 1 ) < fields . length ) buffer . append ( ',' ) ; } return buffer . toString ( ) ; } } 	0	['14', '1', '0', '12', '22', '61', '11', '1', '13', '0.692307692', '175', '0', '3', '0', '0.320512821', '0', '0', '11.28571429', '3', '0.7143', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . Vector ; import java . util . Iterator ; import org . apache . lucene . document . Document ; public final class Hits { private Weight weight ; private Searcher searcher ; private Filter filter = null ; private Sort sort = null ; private int length ; private Vector hitDocs = new Vector ( ) ; private HitDoc first ; private HitDoc last ; private int numDocs = 0 ; private int maxDocs = 200 ; Hits ( Searcher s , Query q , Filter f ) throws IOException { weight = q . weight ( s ) ; searcher = s ; filter = f ; getMoreDocs ( 50 ) ; } Hits ( Searcher s , Query q , Filter f , Sort o ) throws IOException { weight = q . weight ( s ) ; searcher = s ; filter = f ; sort = o ; getMoreDocs ( 50 ) ; } private final void getMoreDocs ( int min ) throws IOException { if ( hitDocs . size ( ) > min ) { min = hitDocs . size ( ) ; } int n = min * 2 ; TopDocs topDocs = ( sort == null ) ? searcher . search ( weight , filter , n ) : searcher . search ( weight , filter , n , sort ) ; length = topDocs . totalHits ; ScoreDoc [ ] scoreDocs = topDocs . scoreDocs ; float scoreNorm = 1.0f ; if ( length > 0 && topDocs . getMaxScore ( ) > 1.0f ) { scoreNorm = 1.0f / topDocs . getMaxScore ( ) ; } int end = scoreDocs . length < length ? scoreDocs . length : length ; for ( int i = hitDocs . size ( ) ; i < end ; i ++ ) { hitDocs . addElement ( new HitDoc ( scoreDocs [ i ] . score * scoreNorm , scoreDocs [ i ] . doc ) ) ; } } public final int length ( ) { return length ; } public final Document doc ( int n ) throws IOException { HitDoc hitDoc = hitDoc ( n ) ; remove ( hitDoc ) ; addToFront ( hitDoc ) ; if ( numDocs > maxDocs ) { HitDoc oldLast = last ; remove ( last ) ; oldLast . doc = null ; } if ( hitDoc . doc == null ) { hitDoc . doc = searcher . doc ( hitDoc . id ) ; } return hitDoc . doc ; } public final float score ( int n ) throws IOException { return hitDoc ( n ) . score ; } public final int id ( int n ) throws IOException { return hitDoc ( n ) . id ; } public Iterator iterator ( ) { return new HitIterator ( this ) ; } private final HitDoc hitDoc ( int n ) throws IOException { if ( n >= length ) { throw new IndexOutOfBoundsException ( "Not a valid hit number: " + n ) ; } if ( n >= hitDocs . size ( ) ) { getMoreDocs ( n ) ; } return ( HitDoc ) hitDocs . elementAt ( n ) ; } private final void addToFront ( HitDoc hitDoc ) { if ( first == null ) { last = hitDoc ; } else { first . prev = hitDoc ; } hitDoc . next = first ; first = hitDoc ; hitDoc . prev = null ; numDocs ++ ; } private final void remove ( HitDoc hitDoc ) { if ( hitDoc . doc == null ) { return ; } if ( hitDoc . next == null ) { last = hitDoc . prev ; } else { hitDoc . next . prev = hitDoc . prev ; } if ( hitDoc . prev == null ) { first = hitDoc . next ; } else { hitDoc . prev . next = hitDoc . next ; } numDocs -- ; } } final class HitDoc { float score ; int id ; Document doc = null ; HitDoc next ; HitDoc prev ; HitDoc ( float s , int i ) { score = s ; id = i ; } } 	1	['11', '1', '0', '12', '28', '19', '3', '11', '5', '0.64', '338', '1', '6', '0', '0.324675325', '0', '0', '28.81818182', '4', '1.1818', '1']
package org . apache . lucene . search ; public class DefaultSimilarity extends Similarity { public float lengthNorm ( String fieldName , int numTerms ) { return ( float ) ( 1.0 / Math . sqrt ( numTerms ) ) ; } public float queryNorm ( float sumOfSquaredWeights ) { return ( float ) ( 1.0 / Math . sqrt ( sumOfSquaredWeights ) ) ; } public float tf ( float freq ) { return ( float ) Math . sqrt ( freq ) ; } public float sloppyFreq ( int distance ) { return 1.0f / ( distance + 1 ) ; } public float idf ( int docFreq , int numDocs ) { return ( float ) ( Math . log ( numDocs / ( double ) ( docFreq + 1 ) ) + 1.0 ) ; } public float coord ( int overlap , int maxOverlap ) { return overlap / ( float ) maxOverlap ; } } 	0	['7', '2', '0', '3', '10', '21', '3', '1', '7', '2', '54', '0', '0', '0.7', '0.5', '1', '2', '6.714285714', '1', '0.8571', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . ArrayList ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermPositions ; import org . apache . lucene . util . ToStringUtils ; public class SpanTermQuery extends SpanQuery { private Term term ; public SpanTermQuery ( Term term ) { this . term = term ; } public Term getTerm ( ) { return term ; } public String getField ( ) { return term . field ( ) ; } public Collection getTerms ( ) { Collection terms = new ArrayList ( ) ; terms . add ( term ) ; return terms ; } public void extractTerms ( Set terms ) { terms . add ( term ) ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( term . field ( ) . equals ( field ) ) buffer . append ( term . text ( ) ) ; else buffer . append ( term . toString ( ) ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof SpanTermQuery ) ) return false ; SpanTermQuery other = ( SpanTermQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && this . term . equals ( other . term ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ term . hashCode ( ) ^ 0xD23FE494 ; } public Spans getSpans ( final IndexReader reader ) throws IOException { return new Spans ( ) { private TermPositions positions = reader . termPositions ( term ) ; private int doc = - 1 ; private int freq ; private int count ; private int position ; public boolean next ( ) throws IOException { if ( count == freq ) { if ( ! positions . next ( ) ) { doc = Integer . MAX_VALUE ; return false ; } doc = positions . doc ( ) ; freq = positions . freq ( ) ; count = 0 ; } position = positions . nextPosition ( ) ; count ++ ; return true ; } public boolean skipTo ( int target ) throws IOException { if ( doc >= target ) { return true ; } if ( ! positions . skipTo ( target ) ) { doc = Integer . MAX_VALUE ; return false ; } doc = positions . doc ( ) ; freq = positions . freq ( ) ; count = 0 ; position = positions . nextPosition ( ) ; count ++ ; return true ; } public int doc ( ) { return doc ; } public int start ( ) { return position ; } public int end ( ) { return position + 1 ; } public String toString ( ) { return "spans(" + SpanTermQuery . this . toString ( ) + ")@" + ( doc == - 1 ? "START" : ( doc == Integer . MAX_VALUE ) ? "END" : doc + "-" + position ) ; } } ; } } 	1	['10', '3', '0', '6', '27', '0', '1', '6', '9', '0.111111111', '116', '1', '1', '0.64', '0.214285714', '2', '2', '10.5', '4', '1.3', '1']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; public class ConstantScoreRangeQuery extends Query { private final String fieldName ; private final String lowerVal ; private final String upperVal ; private final boolean includeLower ; private final boolean includeUpper ; public ConstantScoreRangeQuery ( String fieldName , String lowerVal , String upperVal , boolean includeLower , boolean includeUpper ) { if ( lowerVal == null ) { includeLower = true ; } else if ( includeLower && lowerVal . equals ( "" ) ) { lowerVal = null ; } if ( upperVal == null ) { includeUpper = true ; } this . fieldName = fieldName . intern ( ) ; this . lowerVal = lowerVal ; this . upperVal = upperVal ; this . includeLower = includeLower ; this . includeUpper = includeUpper ; } public String getField ( ) { return fieldName ; } public String getLowerVal ( ) { return lowerVal ; } public String getUpperVal ( ) { return upperVal ; } public boolean includesLower ( ) { return includeLower ; } public boolean includesUpper ( ) { return includeUpper ; } public Query rewrite ( IndexReader reader ) throws IOException { RangeFilter rangeFilt = new RangeFilter ( fieldName , lowerVal != null ? lowerVal : "" , upperVal , lowerVal == "" ? false : includeLower , upperVal == null ? false : includeUpper ) ; Query q = new ConstantScoreQuery ( rangeFilt ) ; q . setBoost ( getBoost ( ) ) ; return q ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! getField ( ) . equals ( field ) ) { buffer . append ( getField ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( includeLower ? '[' : '{' ) ; buffer . append ( lowerVal != null ? lowerVal : "*" ) ; buffer . append ( " TO " ) ; buffer . append ( upperVal != null ? upperVal : "*" ) ; buffer . append ( includeUpper ? ']' : '}' ) ; if ( getBoost ( ) != 1.0f ) { buffer . append ( "^" ) ; buffer . append ( Float . toString ( getBoost ( ) ) ) ; } return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof ConstantScoreRangeQuery ) ) return false ; ConstantScoreRangeQuery other = ( ConstantScoreRangeQuery ) o ; if ( this . fieldName != other . fieldName || this . includeLower != other . includeLower || this . includeUpper != other . includeUpper ) { return false ; } if ( this . lowerVal != null ? ! this . lowerVal . equals ( other . lowerVal ) : other . lowerVal != null ) return false ; if ( this . upperVal != null ? ! this . upperVal . equals ( other . upperVal ) : other . upperVal != null ) return false ; return this . getBoost ( ) == other . getBoost ( ) ; } public int hashCode ( ) { int h = Float . floatToIntBits ( getBoost ( ) ) ^ fieldName . hashCode ( ) ; h ^= lowerVal != null ? lowerVal . hashCode ( ) : 0x965a965a ; h ^= ( h << 17 ) | ( h > > > 16 ) ; h ^= ( upperVal != null ? ( upperVal . hashCode ( ) ) : 0x5a695a69 ) ; h ^= ( includeLower ? 0x665599aa : 0 ) ^ ( includeUpper ? 0x99aa5566 : 0 ) ; return h ; } } 	0	['10', '2', '0', '5', '24', '0', '0', '5', '10', '0.444444444', '313', '1', '0', '0.571428571', '0.3', '2', '3', '29.8', '13', '3.1', '0']
package org . apache . lucene . store ; class RAMInputStream extends BufferedIndexInput implements Cloneable { private RAMFile file ; private long pointer = 0 ; private long length ; public RAMInputStream ( RAMFile f ) { file = f ; length = file . length ; } public void readInternal ( byte [ ] dest , int destOffset , int len ) { int remainder = len ; long start = pointer ; while ( remainder != 0 ) { int bufferNumber = ( int ) ( start / BUFFER_SIZE ) ; int bufferOffset = ( int ) ( start % BUFFER_SIZE ) ; int bytesInBuffer = BUFFER_SIZE - bufferOffset ; int bytesToCopy = bytesInBuffer >= remainder ? remainder : bytesInBuffer ; byte [ ] buffer = ( byte [ ] ) file . buffers . elementAt ( bufferNumber ) ; System . arraycopy ( buffer , bufferOffset , dest , destOffset , bytesToCopy ) ; destOffset += bytesToCopy ; start += bytesToCopy ; remainder -= bytesToCopy ; } pointer += len ; } public void close ( ) { } public void seekInternal ( long pos ) { pointer = pos ; } public long length ( ) { return length ; } } 	1	['5', '3', '0', '3', '8', '2', '1', '2', '5', '0.666666667', '93', '1', '1', '0.84', '0.36', '1', '5', '17', '3', '1.2', '2']
package org . apache . lucene . analysis ; import java . io . IOException ; public abstract class TokenFilter extends TokenStream { protected TokenStream input ; protected TokenFilter ( TokenStream input ) { this . input = input ; } public void close ( ) throws IOException { input . close ( ) ; } } 	0	['2', '2', '6', '7', '4', '0', '6', '1', '1', '0', '13', '1', '1', '0.666666667', '0.75', '0', '0', '5', '1', '0.5', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . document . Document ; public abstract class Searcher implements Searchable { public final Hits search ( Query query ) throws IOException { return search ( query , ( Filter ) null ) ; } public Hits search ( Query query , Filter filter ) throws IOException { return new Hits ( this , query , filter ) ; } public Hits search ( Query query , Sort sort ) throws IOException { return new Hits ( this , query , null , sort ) ; } public Hits search ( Query query , Filter filter , Sort sort ) throws IOException { return new Hits ( this , query , filter , sort ) ; } public TopFieldDocs search ( Query query , Filter filter , int n , Sort sort ) throws IOException { return search ( createWeight ( query ) , filter , n , sort ) ; } public void search ( Query query , HitCollector results ) throws IOException { search ( query , ( Filter ) null , results ) ; } public void search ( Query query , Filter filter , HitCollector results ) throws IOException { search ( createWeight ( query ) , filter , results ) ; } public TopDocs search ( Query query , Filter filter , int n ) throws IOException { return search ( createWeight ( query ) , filter , n ) ; } public Explanation explain ( Query query , int doc ) throws IOException { return explain ( createWeight ( query ) , doc ) ; } private Similarity similarity = Similarity . getDefault ( ) ; public void setSimilarity ( Similarity similarity ) { this . similarity = similarity ; } public Similarity getSimilarity ( ) { return this . similarity ; } protected Weight createWeight ( Query query ) throws IOException { return query . weight ( this ) ; } public int [ ] docFreqs ( Term [ ] terms ) throws IOException { int [ ] result = new int [ terms . length ] ; for ( int i = 0 ; i < terms . length ; i ++ ) { result [ i ] = docFreq ( terms [ i ] ) ; } return result ; } abstract public void search ( Weight weight , Filter filter , HitCollector results ) throws IOException ; abstract public void close ( ) throws IOException ; abstract public int docFreq ( Term term ) throws IOException ; abstract public int maxDoc ( ) throws IOException ; abstract public TopDocs search ( Weight weight , Filter filter , int n ) throws IOException ; abstract public Document doc ( int i ) throws IOException ; abstract public Query rewrite ( Query query ) throws IOException ; abstract public Explanation explain ( Weight weight , int doc ) throws IOException ; abstract public TopFieldDocs search ( Weight weight , Filter filter , int n , Sort sort ) throws IOException ; } 	1	['23', '1', '3', '34', '28', '247', '24', '13', '22', '0.909090909', '131', '1', '1', '0', '0.273913043', '0', '0', '4.652173913', '1', '0.9565', '1']
package org . apache . lucene . store ; import java . io . IOException ; import java . io . File ; import java . io . RandomAccessFile ; import java . nio . ByteBuffer ; import java . nio . channels . FileChannel ; import java . nio . channels . FileChannel . MapMode ; public class MMapDirectory extends FSDirectory { private static class MMapIndexInput extends IndexInput { private ByteBuffer buffer ; private final long length ; private MMapIndexInput ( RandomAccessFile raf ) throws IOException { this . length = raf . length ( ) ; this . buffer = raf . getChannel ( ) . map ( MapMode . READ_ONLY , 0 , length ) ; } public byte readByte ( ) throws IOException { return buffer . get ( ) ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { buffer . get ( b , offset , len ) ; } public long getFilePointer ( ) { return buffer . position ( ) ; } public void seek ( long pos ) throws IOException { buffer . position ( ( int ) pos ) ; } public long length ( ) { return length ; } public Object clone ( ) { MMapIndexInput clone = ( MMapIndexInput ) super . clone ( ) ; clone . buffer = buffer . duplicate ( ) ; return clone ; } public void close ( ) throws IOException { } } private static class MultiMMapIndexInput extends IndexInput { private ByteBuffer [ ] buffers ; private int [ ] bufSizes ; private final long length ; private int curBufIndex ; private final int maxBufSize ; private ByteBuffer curBuf ; private int curAvail ; public MultiMMapIndexInput ( RandomAccessFile raf , int maxBufSize ) throws IOException { this . length = raf . length ( ) ; this . maxBufSize = maxBufSize ; if ( maxBufSize <= 0 ) throw new IllegalArgumentException ( "Non positive maxBufSize: " + maxBufSize ) ; if ( ( length / maxBufSize ) > Integer . MAX_VALUE ) throw new IllegalArgumentException ( "RandomAccessFile too big for maximum buffer size: " + raf . toString ( ) ) ; int nrBuffers = ( int ) ( length / maxBufSize ) ; if ( ( nrBuffers * maxBufSize ) < length ) nrBuffers ++ ; this . buffers = new ByteBuffer [ nrBuffers ] ; this . bufSizes = new int [ nrBuffers ] ; long bufferStart = 0 ; FileChannel rafc = raf . getChannel ( ) ; for ( int bufNr = 0 ; bufNr < nrBuffers ; bufNr ++ ) { int bufSize = ( length > ( bufferStart + maxBufSize ) ) ? maxBufSize : ( int ) ( length - bufferStart ) ; this . buffers [ bufNr ] = rafc . map ( MapMode . READ_ONLY , bufferStart , bufSize ) ; this . bufSizes [ bufNr ] = bufSize ; bufferStart += bufSize ; } seek ( 0L ) ; } public byte readByte ( ) throws IOException { if ( curAvail == 0 ) { curBufIndex ++ ; curBuf = buffers [ curBufIndex ] ; curBuf . position ( 0 ) ; curAvail = bufSizes [ curBufIndex ] ; } curAvail -- ; return curBuf . get ( ) ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { while ( len > curAvail ) { curBuf . get ( b , offset , curAvail ) ; len -= curAvail ; offset += curAvail ; curBufIndex ++ ; curBuf = buffers [ curBufIndex ] ; curBuf . position ( 0 ) ; curAvail = bufSizes [ curBufIndex ] ; } curBuf . get ( b , offset , len ) ; curAvail -= len ; } public long getFilePointer ( ) { return ( curBufIndex * ( long ) maxBufSize ) + curBuf . position ( ) ; } public void seek ( long pos ) throws IOException { curBufIndex = ( int ) ( pos / maxBufSize ) ; curBuf = buffers [ curBufIndex ] ; int bufOffset = ( int ) ( pos - ( curBufIndex * maxBufSize ) ) ; curBuf . position ( bufOffset ) ; curAvail = bufSizes [ curBufIndex ] - bufOffset ; } public long length ( ) { return length ; } public Object clone ( ) { MultiMMapIndexInput clone = ( MultiMMapIndexInput ) super . clone ( ) ; clone . buffers = new ByteBuffer [ buffers . length ] ; for ( int bufNr = 0 ; bufNr < buffers . length ; bufNr ++ ) { clone . buffers [ bufNr ] = buffers [ bufNr ] . duplicate ( ) ; } try { clone . seek ( getFilePointer ( ) ) ; } catch ( IOException ioe ) { RuntimeException newException = new RuntimeException ( ioe ) ; newException . initCause ( ioe ) ; throw newException ; } ; return clone ; } public void close ( ) throws IOException { } } private final int MAX_BBUF = Integer . MAX_VALUE ; public IndexInput openInput ( String name ) throws IOException { File f = new File ( getFile ( ) , name ) ; RandomAccessFile raf = new RandomAccessFile ( f , "r" ) ; try { return ( raf . length ( ) <= MAX_BBUF ) ? ( IndexInput ) new MMapIndexInput ( raf ) : ( IndexInput ) new MultiMMapIndexInput ( raf , MAX_BBUF ) ; } finally { raf . close ( ) ; } } } 	0	['2', '3', '0', '5', '10', '1', '0', '5', '2', '1', '48', '1', '0', '0.972222222', '0.75', '0', '0', '22.5', '1', '0.5', '0']
package org . apache . lucene . search ; import java . util . List ; import java . util . Iterator ; import java . io . IOException ; import org . apache . lucene . util . PriorityQueue ; class DisjunctionSumScorer extends Scorer { private final int nrScorers ; protected final List subScorers ; private final int minimumNrMatchers ; private ScorerQueue scorerQueue = null ; private int currentDoc = - 1 ; protected int nrMatchers = - 1 ; private float currentScore = Float . NaN ; public DisjunctionSumScorer ( List subScorers , int minimumNrMatchers ) { super ( null ) ; nrScorers = subScorers . size ( ) ; if ( minimumNrMatchers <= 0 ) { throw new IllegalArgumentException ( "Minimum nr of matchers must be positive" ) ; } if ( nrScorers <= 1 ) { throw new IllegalArgumentException ( "There must be at least 2 subScorers" ) ; } this . minimumNrMatchers = minimumNrMatchers ; this . subScorers = subScorers ; } public DisjunctionSumScorer ( List subScorers ) { this ( subScorers , 1 ) ; } private void initScorerQueue ( ) throws IOException { Iterator si = subScorers . iterator ( ) ; scorerQueue = new ScorerQueue ( nrScorers ) ; while ( si . hasNext ( ) ) { Scorer se = ( Scorer ) si . next ( ) ; if ( se . next ( ) ) { scorerQueue . insert ( se ) ; } } } private class ScorerQueue extends PriorityQueue { ScorerQueue ( int size ) { initialize ( size ) ; } protected boolean lessThan ( Object o1 , Object o2 ) { return ( ( Scorer ) o1 ) . doc ( ) < ( ( Scorer ) o2 ) . doc ( ) ; } } public boolean next ( ) throws IOException { if ( scorerQueue == null ) { initScorerQueue ( ) ; } if ( scorerQueue . size ( ) < minimumNrMatchers ) { return false ; } else { return advanceAfterCurrent ( ) ; } } protected boolean advanceAfterCurrent ( ) throws IOException { do { Scorer top = ( Scorer ) scorerQueue . top ( ) ; currentDoc = top . doc ( ) ; currentScore = top . score ( ) ; nrMatchers = 1 ; do { if ( top . next ( ) ) { scorerQueue . adjustTop ( ) ; } else { scorerQueue . pop ( ) ; if ( scorerQueue . size ( ) < ( minimumNrMatchers - nrMatchers ) ) { return false ; } if ( scorerQueue . size ( ) == 0 ) { break ; } } top = ( Scorer ) scorerQueue . top ( ) ; if ( top . doc ( ) != currentDoc ) { break ; } else { currentScore += top . score ( ) ; nrMatchers ++ ; } } while ( true ) ; if ( nrMatchers >= minimumNrMatchers ) { return true ; } else if ( scorerQueue . size ( ) < minimumNrMatchers ) { return false ; } } while ( true ) ; } public float score ( ) throws IOException { return currentScore ; } public int doc ( ) { return currentDoc ; } public int nrMatchers ( ) { return nrMatchers ; } public boolean skipTo ( int target ) throws IOException { if ( scorerQueue == null ) { initScorerQueue ( ) ; } if ( scorerQueue . size ( ) < minimumNrMatchers ) { return false ; } if ( target <= currentDoc ) { return true ; } do { Scorer top = ( Scorer ) scorerQueue . top ( ) ; if ( top . doc ( ) >= target ) { return advanceAfterCurrent ( ) ; } else if ( top . skipTo ( target ) ) { scorerQueue . adjustTop ( ) ; } else { scorerQueue . pop ( ) ; if ( scorerQueue . size ( ) < minimumNrMatchers ) { return false ; } } } while ( true ) ; } public Explanation explain ( int doc ) throws IOException { Explanation res = new Explanation ( ) ; res . setDescription ( "At least " + minimumNrMatchers + " of" ) ; Iterator ssi = subScorers . iterator ( ) ; while ( ssi . hasNext ( ) ) { res . addDetail ( ( ( Scorer ) ssi . next ( ) ) . explain ( doc ) ) ; } return res ; } } 	1	['10', '2', '1', '6', '34', '1', '3', '4', '8', '0.46031746', '288', '1', '1', '0.5', '0.5', '1', '3', '27.1', '1', '0.8', '1']
package org . apache . lucene . index ; import java . io . IOException ; final class SegmentMergeInfo { Term term ; int base ; TermEnum termEnum ; IndexReader reader ; private TermPositions postings ; private int [ ] docMap ; SegmentMergeInfo ( int b , TermEnum te , IndexReader r ) throws IOException { base = b ; reader = r ; termEnum = te ; term = te . term ( ) ; } int [ ] getDocMap ( ) { if ( docMap == null ) { if ( reader . hasDeletions ( ) ) { int maxDoc = reader . maxDoc ( ) ; docMap = new int [ maxDoc ] ; int j = 0 ; for ( int i = 0 ; i < maxDoc ; i ++ ) { if ( reader . isDeleted ( i ) ) docMap [ i ] = - 1 ; else docMap [ i ] = j ++ ; } } } return docMap ; } TermPositions getPositions ( ) throws IOException { if ( postings == null ) { postings = reader . termPositions ( ) ; } return postings ; } final boolean next ( ) throws IOException { if ( termEnum . next ( ) ) { term = termEnum . term ( ) ; return true ; } else { term = null ; return false ; } } final void close ( ) throws IOException { termEnum . close ( ) ; if ( postings != null ) { postings . close ( ) ; } } } 	0	['5', '1', '0', '7', '14', '0', '3', '4', '0', '0.75', '108', '0.333333333', '4', '0', '0.4', '0', '0', '19.4', '5', '1.6', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class BufferedIndexOutput extends IndexOutput { static final int BUFFER_SIZE = 1024 ; private final byte [ ] buffer = new byte [ BUFFER_SIZE ] ; private long bufferStart = 0 ; private int bufferPosition = 0 ; public void writeByte ( byte b ) throws IOException { if ( bufferPosition >= BUFFER_SIZE ) flush ( ) ; buffer [ bufferPosition ++ ] = b ; } public void writeBytes ( byte [ ] b , int length ) throws IOException { int bytesLeft = BUFFER_SIZE - bufferPosition ; if ( bytesLeft >= length ) { System . arraycopy ( b , 0 , buffer , bufferPosition , length ) ; bufferPosition += length ; if ( BUFFER_SIZE - bufferPosition == 0 ) flush ( ) ; } else { if ( length > BUFFER_SIZE ) { if ( bufferPosition > 0 ) flush ( ) ; flushBuffer ( b , length ) ; bufferStart += length ; } else { int pos = 0 ; int pieceLength ; while ( pos < length ) { pieceLength = ( length - pos < bytesLeft ) ? length - pos : bytesLeft ; System . arraycopy ( b , pos , buffer , bufferPosition , pieceLength ) ; pos += pieceLength ; bufferPosition += pieceLength ; bytesLeft = BUFFER_SIZE - bufferPosition ; if ( bytesLeft == 0 ) { flush ( ) ; bytesLeft = BUFFER_SIZE ; } } } } } public void flush ( ) throws IOException { flushBuffer ( buffer , bufferPosition ) ; bufferStart += bufferPosition ; bufferPosition = 0 ; } protected abstract void flushBuffer ( byte [ ] b , int len ) throws IOException ; public void close ( ) throws IOException { flush ( ) ; } public long getFilePointer ( ) { return bufferStart + bufferPosition ; } public void seek ( long pos ) throws IOException { flush ( ) ; bufferStart = pos ; } public abstract long length ( ) throws IOException ; } 	1	['9', '2', '2', '3', '11', '8', '2', '1', '8', '0.5', '175', '0.75', '0', '0.619047619', '0.333333333', '1', '3', '18', '1', '0.8889', '2']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . Set ; import java . util . Vector ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermPositions ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; public class PhraseQuery extends Query { private String field ; private Vector terms = new Vector ( ) ; private Vector positions = new Vector ( ) ; private int slop = 0 ; public PhraseQuery ( ) { } public void setSlop ( int s ) { slop = s ; } public int getSlop ( ) { return slop ; } public void add ( Term term ) { int position = 0 ; if ( positions . size ( ) > 0 ) position = ( ( Integer ) positions . lastElement ( ) ) . intValue ( ) + 1 ; add ( term , position ) ; } public void add ( Term term , int position ) { if ( terms . size ( ) == 0 ) field = term . field ( ) ; else if ( term . field ( ) != field ) throw new IllegalArgumentException ( "All phrase terms must be in the same field: " + term ) ; terms . addElement ( term ) ; positions . addElement ( new Integer ( position ) ) ; } public Term [ ] getTerms ( ) { return ( Term [ ] ) terms . toArray ( new Term [ 0 ] ) ; } public int [ ] getPositions ( ) { int [ ] result = new int [ positions . size ( ) ] ; for ( int i = 0 ; i < positions . size ( ) ; i ++ ) result [ i ] = ( ( Integer ) positions . elementAt ( i ) ) . intValue ( ) ; return result ; } private class PhraseWeight implements Weight { private Similarity similarity ; private float value ; private float idf ; private float queryNorm ; private float queryWeight ; public PhraseWeight ( Searcher searcher ) throws IOException { this . similarity = getSimilarity ( searcher ) ; idf = similarity . idf ( terms , searcher ) ; } public String toString ( ) { return "weight(" + PhraseQuery . this + ")" ; } public Query getQuery ( ) { return PhraseQuery . this ; } public float getValue ( ) { return value ; } public float sumOfSquaredWeights ( ) { queryWeight = idf * getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float queryNorm ) { this . queryNorm = queryNorm ; queryWeight *= queryNorm ; value = queryWeight * idf ; } public Scorer scorer ( IndexReader reader ) throws IOException { if ( terms . size ( ) == 0 ) return null ; TermPositions [ ] tps = new TermPositions [ terms . size ( ) ] ; for ( int i = 0 ; i < terms . size ( ) ; i ++ ) { TermPositions p = reader . termPositions ( ( Term ) terms . elementAt ( i ) ) ; if ( p == null ) return null ; tps [ i ] = p ; } if ( slop == 0 ) return new ExactPhraseScorer ( this , tps , getPositions ( ) , similarity , reader . norms ( field ) ) ; else return new SloppyPhraseScorer ( this , tps , getPositions ( ) , similarity , slop , reader . norms ( field ) ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { Explanation result = new Explanation ( ) ; result . setDescription ( "weight(" + getQuery ( ) + " in " + doc + "), product of:" ) ; StringBuffer docFreqs = new StringBuffer ( ) ; StringBuffer query = new StringBuffer ( ) ; query . append ( '\"' ) ; for ( int i = 0 ; i < terms . size ( ) ; i ++ ) { if ( i != 0 ) { docFreqs . append ( " " ) ; query . append ( " " ) ; } Term term = ( Term ) terms . elementAt ( i ) ; docFreqs . append ( term . text ( ) ) ; docFreqs . append ( "=" ) ; docFreqs . append ( reader . docFreq ( term ) ) ; query . append ( term . text ( ) ) ; } query . append ( '\"' ) ; Explanation idfExpl = new Explanation ( idf , "idf(" + field + ": " + docFreqs + ")" ) ; Explanation queryExpl = new Explanation ( ) ; queryExpl . setDescription ( "queryWeight(" + getQuery ( ) + "), product of:" ) ; Explanation boostExpl = new Explanation ( getBoost ( ) , "boost" ) ; if ( getBoost ( ) != 1.0f ) queryExpl . addDetail ( boostExpl ) ; queryExpl . addDetail ( idfExpl ) ; Explanation queryNormExpl = new Explanation ( queryNorm , "queryNorm" ) ; queryExpl . addDetail ( queryNormExpl ) ; queryExpl . setValue ( boostExpl . getValue ( ) * idfExpl . getValue ( ) * queryNormExpl . getValue ( ) ) ; result . addDetail ( queryExpl ) ; Explanation fieldExpl = new Explanation ( ) ; fieldExpl . setDescription ( "fieldWeight(" + field + ":" + query + " in " + doc + "), product of:" ) ; Explanation tfExpl = scorer ( reader ) . explain ( doc ) ; fieldExpl . addDetail ( tfExpl ) ; fieldExpl . addDetail ( idfExpl ) ; Explanation fieldNormExpl = new Explanation ( ) ; byte [ ] fieldNorms = reader . norms ( field ) ; float fieldNorm = fieldNorms != null ? Similarity . decodeNorm ( fieldNorms [ doc ] ) : 0.0f ; fieldNormExpl . setValue ( fieldNorm ) ; fieldNormExpl . setDescription ( "fieldNorm(field=" + field + ", doc=" + doc + ")" ) ; fieldExpl . addDetail ( fieldNormExpl ) ; fieldExpl . setValue ( tfExpl . getValue ( ) * idfExpl . getValue ( ) * fieldNormExpl . getValue ( ) ) ; result . addDetail ( fieldExpl ) ; result . setValue ( queryExpl . getValue ( ) * fieldExpl . getValue ( ) ) ; if ( queryExpl . getValue ( ) == 1.0f ) return fieldExpl ; return result ; } } protected Weight createWeight ( Searcher searcher ) throws IOException { if ( terms . size ( ) == 1 ) { Term term = ( Term ) terms . elementAt ( 0 ) ; Query termQuery = new TermQuery ( term ) ; termQuery . setBoost ( getBoost ( ) ) ; return termQuery . createWeight ( searcher ) ; } return new PhraseWeight ( searcher ) ; } public void extractTerms ( Set queryTerms ) { queryTerms . addAll ( terms ) ; } public String toString ( String f ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! field . equals ( f ) ) { buffer . append ( field ) ; buffer . append ( ":" ) ; } buffer . append ( "\"" ) ; for ( int i = 0 ; i < terms . size ( ) ; i ++ ) { buffer . append ( ( ( Term ) terms . elementAt ( i ) ) . text ( ) ) ; if ( i != terms . size ( ) - 1 ) buffer . append ( " " ) ; } buffer . append ( "\"" ) ; if ( slop != 0 ) { buffer . append ( "~" ) ; buffer . append ( slop ) ; } buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof PhraseQuery ) ) return false ; PhraseQuery other = ( PhraseQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && ( this . slop == other . slop ) && this . terms . equals ( other . terms ) && this . positions . equals ( other . positions ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ slop ^ terms . hashCode ( ) ^ positions . hashCode ( ) ; } } 	0	['15', '2', '0', '9', '43', '0', '3', '7', '11', '0.589285714', '302', '1', '0', '0.461538462', '0.191666667', '2', '3', '18.86666667', '6', '1.8', '0']
package org . apache . lucene . document ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Hits ; import org . apache . lucene . search . Similarity ; import org . apache . lucene . util . Parameter ; import java . io . Reader ; import java . io . Serializable ; public final class Field implements Serializable { private String name = "body" ; private Object fieldsData = null ; private boolean storeTermVector = false ; private boolean storeOffsetWithTermVector = false ; private boolean storePositionWithTermVector = false ; private boolean omitNorms = false ; private boolean isStored = false ; private boolean isIndexed = true ; private boolean isTokenized = true ; private boolean isBinary = false ; private boolean isCompressed = false ; private float boost = 1.0f ; public static final class Store extends Parameter implements Serializable { private Store ( String name ) { super ( name ) ; } public static final Store COMPRESS = new Store ( "COMPRESS" ) ; public static final Store YES = new Store ( "YES" ) ; public static final Store NO = new Store ( "NO" ) ; } public static final class Index extends Parameter implements Serializable { private Index ( String name ) { super ( name ) ; } public static final Index NO = new Index ( "NO" ) ; public static final Index TOKENIZED = new Index ( "TOKENIZED" ) ; public static final Index UN_TOKENIZED = new Index ( "UN_TOKENIZED" ) ; public static final Index NO_NORMS = new Index ( "NO_NORMS" ) ; } public static final class TermVector extends Parameter implements Serializable { private TermVector ( String name ) { super ( name ) ; } public static final TermVector NO = new TermVector ( "NO" ) ; public static final TermVector YES = new TermVector ( "YES" ) ; public static final TermVector WITH_POSITIONS = new TermVector ( "WITH_POSITIONS" ) ; public static final TermVector WITH_OFFSETS = new TermVector ( "WITH_OFFSETS" ) ; public static final TermVector WITH_POSITIONS_OFFSETS = new TermVector ( "WITH_POSITIONS_OFFSETS" ) ; } public void setBoost ( float boost ) { this . boost = boost ; } public float getBoost ( ) { return boost ; } public String name ( ) { return name ; } public String stringValue ( ) { return fieldsData instanceof String ? ( String ) fieldsData : null ; } public Reader readerValue ( ) { return fieldsData instanceof Reader ? ( Reader ) fieldsData : null ; } public byte [ ] binaryValue ( ) { return fieldsData instanceof byte [ ] ? ( byte [ ] ) fieldsData : null ; } public Field ( String name , String value , Store store , Index index ) { this ( name , value , store , index , TermVector . NO ) ; } public Field ( String name , String value , Store store , Index index , TermVector termVector ) { if ( name == null ) throw new NullPointerException ( "name cannot be null" ) ; if ( value == null ) throw new NullPointerException ( "value cannot be null" ) ; if ( name . length ( ) == 0 && value . length ( ) == 0 ) throw new IllegalArgumentException ( "name and value cannot both be empty" ) ; if ( index == Index . NO && store == Store . NO ) throw new IllegalArgumentException ( "it doesn't make sense to have a field that " + "is neither indexed nor stored" ) ; if ( index == Index . NO && termVector != TermVector . NO ) throw new IllegalArgumentException ( "cannot store term vector information " + "for a field that is not indexed" ) ; this . name = name . intern ( ) ; this . fieldsData = value ; if ( store == Store . YES ) { this . isStored = true ; this . isCompressed = false ; } else if ( store == Store . COMPRESS ) { this . isStored = true ; this . isCompressed = true ; } else if ( store == Store . NO ) { this . isStored = false ; this . isCompressed = false ; } else throw new IllegalArgumentException ( "unknown store parameter " + store ) ; if ( index == Index . NO ) { this . isIndexed = false ; this . isTokenized = false ; } else if ( index == Index . TOKENIZED ) { this . isIndexed = true ; this . isTokenized = true ; } else if ( index == Index . UN_TOKENIZED ) { this . isIndexed = true ; this . isTokenized = false ; } else if ( index == Index . NO_NORMS ) { this . isIndexed = true ; this . isTokenized = false ; this . omitNorms = true ; } else { throw new IllegalArgumentException ( "unknown index parameter " + index ) ; } this . isBinary = false ; setStoreTermVector ( termVector ) ; } public Field ( String name , Reader reader ) { this ( name , reader , TermVector . NO ) ; } public Field ( String name , Reader reader , TermVector termVector ) { if ( name == null ) throw new NullPointerException ( "name cannot be null" ) ; if ( reader == null ) throw new NullPointerException ( "reader cannot be null" ) ; this . name = name . intern ( ) ; this . fieldsData = reader ; this . isStored = false ; this . isCompressed = false ; this . isIndexed = true ; this . isTokenized = true ; this . isBinary = false ; setStoreTermVector ( termVector ) ; } public Field ( String name , byte [ ] value , Store store ) { if ( name == null ) throw new IllegalArgumentException ( "name cannot be null" ) ; if ( value == null ) throw new IllegalArgumentException ( "value cannot be null" ) ; this . name = name . intern ( ) ; this . fieldsData = value ; if ( store == Store . YES ) { this . isStored = true ; this . isCompressed = false ; } else if ( store == Store . COMPRESS ) { this . isStored = true ; this . isCompressed = true ; } else if ( store == Store . NO ) throw new IllegalArgumentException ( "binary values can't be unstored" ) ; else throw new IllegalArgumentException ( "unknown store parameter " + store ) ; this . isIndexed = false ; this . isTokenized = false ; this . isBinary = true ; setStoreTermVector ( TermVector . NO ) ; } private void setStoreTermVector ( TermVector termVector ) { if ( termVector == TermVector . NO ) { this . storeTermVector = false ; this . storePositionWithTermVector = false ; this . storeOffsetWithTermVector = false ; } else if ( termVector == TermVector . YES ) { this . storeTermVector = true ; this . storePositionWithTermVector = false ; this . storeOffsetWithTermVector = false ; } else if ( termVector == TermVector . WITH_POSITIONS ) { this . storeTermVector = true ; this . storePositionWithTermVector = true ; this . storeOffsetWithTermVector = false ; } else if ( termVector == TermVector . WITH_OFFSETS ) { this . storeTermVector = true ; this . storePositionWithTermVector = false ; this . storeOffsetWithTermVector = true ; } else if ( termVector == TermVector . WITH_POSITIONS_OFFSETS ) { this . storeTermVector = true ; this . storePositionWithTermVector = true ; this . storeOffsetWithTermVector = true ; } else { throw new IllegalArgumentException ( "unknown termVector parameter " + termVector ) ; } } public final boolean isStored ( ) { return isStored ; } public final boolean isIndexed ( ) { return isIndexed ; } public final boolean isTokenized ( ) { return isTokenized ; } public final boolean isCompressed ( ) { return isCompressed ; } public final boolean isTermVectorStored ( ) { return storeTermVector ; } public boolean isStoreOffsetWithTermVector ( ) { return storeOffsetWithTermVector ; } public boolean isStorePositionWithTermVector ( ) { return storePositionWithTermVector ; } public final boolean isBinary ( ) { return isBinary ; } public boolean getOmitNorms ( ) { return omitNorms ; } public void setOmitNorms ( boolean omitNorms ) { this . omitNorms = omitNorms ; } public final String toString ( ) { StringBuffer result = new StringBuffer ( ) ; if ( isStored ) { result . append ( "stored" ) ; if ( isCompressed ) result . append ( "/compressed" ) ; else result . append ( "/uncompressed" ) ; } if ( isIndexed ) { if ( result . length ( ) > 0 ) result . append ( "," ) ; result . append ( "indexed" ) ; } if ( isTokenized ) { if ( result . length ( ) > 0 ) result . append ( "," ) ; result . append ( "tokenized" ) ; } if ( storeTermVector ) { if ( result . length ( ) > 0 ) result . append ( "," ) ; result . append ( "termVector" ) ; } if ( storeOffsetWithTermVector ) { if ( result . length ( ) > 0 ) result . append ( "," ) ; result . append ( "termVectorOffsets" ) ; } if ( storePositionWithTermVector ) { if ( result . length ( ) > 0 ) result . append ( "," ) ; result . append ( "termVectorPosition" ) ; } if ( isBinary ) { if ( result . length ( ) > 0 ) result . append ( "," ) ; result . append ( "binary" ) ; } if ( omitNorms ) { result . append ( ",omitNorms" ) ; } result . append ( '<' ) ; result . append ( name ) ; result . append ( ':' ) ; if ( fieldsData != null ) { result . append ( fieldsData ) ; } result . append ( '>' ) ; return result . toString ( ) ; } } 	1	['23', '1', '0', '9', '34', '93', '6', '3', '22', '0.704545455', '726', '1', '0', '0', '0.198067633', '0', '0', '30.04347826', '17', '1.8261', '1']
package org . apache . lucene . queryParser ; public class Token { public int kind ; public int beginLine , beginColumn , endLine , endColumn ; public String image ; public Token next ; public Token specialToken ; public String toString ( ) { return image ; } public static final Token newToken ( int ofKind ) { switch ( ofKind ) { default : return new Token ( ) ; } } } 	0	['3', '1', '0', '4', '4', '3', '4', '0', '3', '1.4375', '23', '0', '2', '0', '0.5', '0', '0', '4', '2', '1', '0']
package org . apache . lucene . store ; import java . io . File ; import java . io . FileInputStream ; import java . io . FileOutputStream ; import java . io . IOException ; import java . io . RandomAccessFile ; import java . security . MessageDigest ; import java . security . NoSuchAlgorithmException ; import java . util . Hashtable ; import org . apache . lucene . index . IndexFileNameFilter ; public class FSDirectory extends Directory { private static final Hashtable DIRECTORIES = new Hashtable ( ) ; private static boolean disableLocks = false ; public static void setDisableLocks ( boolean doDisableLocks ) { FSDirectory . disableLocks = doDisableLocks ; } public static boolean getDisableLocks ( ) { return FSDirectory . disableLocks ; } public static final String LOCK_DIR = System . getProperty ( "org.apache.lucene.lockDir" , System . getProperty ( "java.io.tmpdir" ) ) ; private static Class IMPL ; static { try { String name = System . getProperty ( "org.apache.lucene.FSDirectory.class" , FSDirectory . class . getName ( ) ) ; IMPL = Class . forName ( name ) ; } catch ( ClassNotFoundException e ) { throw new RuntimeException ( "cannot load FSDirectory class: " + e . toString ( ) , e ) ; } catch ( SecurityException se ) { try { IMPL = Class . forName ( FSDirectory . class . getName ( ) ) ; } catch ( ClassNotFoundException e ) { throw new RuntimeException ( "cannot load default FSDirectory class: " + e . toString ( ) , e ) ; } } } private static MessageDigest DIGESTER ; static { try { DIGESTER = MessageDigest . getInstance ( "MD5" ) ; } catch ( NoSuchAlgorithmException e ) { throw new RuntimeException ( e . toString ( ) , e ) ; } } private byte [ ] buffer = null ; public static FSDirectory getDirectory ( String path , boolean create ) throws IOException { return getDirectory ( new File ( path ) , create ) ; } public static FSDirectory getDirectory ( File file , boolean create ) throws IOException { file = new File ( file . getCanonicalPath ( ) ) ; FSDirectory dir ; synchronized ( DIRECTORIES ) { dir = ( FSDirectory ) DIRECTORIES . get ( file ) ; if ( dir == null ) { try { dir = ( FSDirectory ) IMPL . newInstance ( ) ; } catch ( Exception e ) { throw new RuntimeException ( "cannot load FSDirectory class: " + e . toString ( ) , e ) ; } dir . init ( file , create ) ; DIRECTORIES . put ( file , dir ) ; } else if ( create ) { dir . create ( ) ; } } synchronized ( dir ) { dir . refCount ++ ; } return dir ; } private File directory = null ; private int refCount ; private File lockDir ; protected FSDirectory ( ) { } ; private void init ( File path , boolean create ) throws IOException { directory = path ; if ( LOCK_DIR == null ) { lockDir = directory ; } else { lockDir = new File ( LOCK_DIR ) ; } if ( ! lockDir . exists ( ) ) { if ( ! lockDir . mkdirs ( ) ) throw new IOException ( "Cannot create directory: " + lockDir . getAbsolutePath ( ) ) ; } else if ( ! lockDir . isDirectory ( ) ) { throw new IOException ( "Found regular file where directory expected: " + lockDir . getAbsolutePath ( ) ) ; } if ( create ) { create ( ) ; } if ( ! directory . isDirectory ( ) ) throw new IOException ( path + " not a directory" ) ; } private synchronized void create ( ) throws IOException { if ( ! directory . exists ( ) ) if ( ! directory . mkdirs ( ) ) throw new IOException ( "Cannot create directory: " + directory ) ; if ( ! directory . isDirectory ( ) ) throw new IOException ( directory + " not a directory" ) ; String [ ] files = directory . list ( new IndexFileNameFilter ( ) ) ; if ( files == null ) throw new IOException ( "Cannot read directory " + directory . getAbsolutePath ( ) ) ; for ( int i = 0 ; i < files . length ; i ++ ) { File file = new File ( directory , files [ i ] ) ; if ( ! file . delete ( ) ) throw new IOException ( "Cannot delete " + file ) ; } String lockPrefix = getLockPrefix ( ) . toString ( ) ; files = lockDir . list ( ) ; if ( files == null ) throw new IOException ( "Cannot read lock directory " + lockDir . getAbsolutePath ( ) ) ; for ( int i = 0 ; i < files . length ; i ++ ) { if ( ! files [ i ] . startsWith ( lockPrefix ) ) continue ; File lockFile = new File ( lockDir , files [ i ] ) ; if ( ! lockFile . delete ( ) ) throw new IOException ( "Cannot delete " + lockFile ) ; } } public String [ ] list ( ) { return directory . list ( ) ; } public boolean fileExists ( String name ) { File file = new File ( directory , name ) ; return file . exists ( ) ; } public long fileModified ( String name ) { File file = new File ( directory , name ) ; return file . lastModified ( ) ; } public static long fileModified ( File directory , String name ) { File file = new File ( directory , name ) ; return file . lastModified ( ) ; } public void touchFile ( String name ) { File file = new File ( directory , name ) ; file . setLastModified ( System . currentTimeMillis ( ) ) ; } public long fileLength ( String name ) { File file = new File ( directory , name ) ; return file . length ( ) ; } public void deleteFile ( String name ) throws IOException { File file = new File ( directory , name ) ; if ( ! file . delete ( ) ) throw new IOException ( "Cannot delete " + file ) ; } public synchronized void renameFile ( String from , String to ) throws IOException { File old = new File ( directory , from ) ; File nu = new File ( directory , to ) ; if ( nu . exists ( ) ) if ( ! nu . delete ( ) ) throw new IOException ( "Cannot delete " + nu ) ; if ( ! old . renameTo ( nu ) ) { java . io . InputStream in = null ; java . io . OutputStream out = null ; try { in = new FileInputStream ( old ) ; out = new FileOutputStream ( nu ) ; if ( buffer == null ) { buffer = new byte [ 1024 ] ; } int len ; while ( ( len = in . read ( buffer ) ) >= 0 ) { out . write ( buffer , 0 , len ) ; } old . delete ( ) ; } catch ( IOException ioe ) { IOException newExc = new IOException ( "Cannot rename " + old + " to " + nu ) ; newExc . initCause ( ioe ) ; throw newExc ; } finally { if ( in != null ) { try { in . close ( ) ; } catch ( IOException e ) { throw new RuntimeException ( "Cannot close input stream: " + e . toString ( ) , e ) ; } } if ( out != null ) { try { out . close ( ) ; } catch ( IOException e ) { throw new RuntimeException ( "Cannot close output stream: " + e . toString ( ) , e ) ; } } } } } public IndexOutput createOutput ( String name ) throws IOException { File file = new File ( directory , name ) ; if ( file . exists ( ) && ! file . delete ( ) ) throw new IOException ( "Cannot overwrite: " + file ) ; return new FSIndexOutput ( file ) ; } public IndexInput openInput ( String name ) throws IOException { return new FSIndexInput ( new File ( directory , name ) ) ; } private static final char [ ] HEX_DIGITS = { '0' , '1' , '2' , '3' , '4' , '5' , '6' , '7' , '8' , '9' , 'a' , 'b' , 'c' , 'd' , 'e' , 'f' } ; public Lock makeLock ( String name ) { StringBuffer buf = getLockPrefix ( ) ; buf . append ( "-" ) ; buf . append ( name ) ; final File lockFile = new File ( lockDir , buf . toString ( ) ) ; return new Lock ( ) { public boolean obtain ( ) throws IOException { if ( disableLocks ) return true ; if ( ! lockDir . exists ( ) ) { if ( ! lockDir . mkdirs ( ) ) { throw new IOException ( "Cannot create lock directory: " + lockDir ) ; } } return lockFile . createNewFile ( ) ; } public void release ( ) { if ( disableLocks ) return ; lockFile . delete ( ) ; } public boolean isLocked ( ) { if ( disableLocks ) return false ; return lockFile . exists ( ) ; } public String toString ( ) { return "Lock@" + lockFile ; } } ; } private StringBuffer getLockPrefix ( ) { String dirName ; try { dirName = directory . getCanonicalPath ( ) ; } catch ( IOException e ) { throw new RuntimeException ( e . toString ( ) , e ) ; } byte digest [ ] ; synchronized ( DIGESTER ) { digest = DIGESTER . digest ( dirName . getBytes ( ) ) ; } StringBuffer buf = new StringBuffer ( ) ; buf . append ( "lucene-" ) ; for ( int i = 0 ; i < digest . length ; i ++ ) { int b = digest [ i ] ; buf . append ( HEX_DIGITS [ ( b > > 4 ) & 0xf ] ) ; buf . append ( HEX_DIGITS [ b & 0xf ] ) ; } return buf ; } public synchronized void close ( ) { if ( -- refCount <= 0 ) { synchronized ( DIRECTORIES ) { DIRECTORIES . remove ( directory ) ; } } } public File getFile ( ) { return directory ; } public String toString ( ) { return this . getClass ( ) . getName ( ) + "@" + directory ; } } class FSIndexInput extends BufferedIndexInput { private class Descriptor extends RandomAccessFile { public long position ; public Descriptor ( File file , String mode ) throws IOException { super ( file , mode ) ; } } private Descriptor file = null ; boolean isClone ; private long length ; public FSIndexInput ( File path ) throws IOException { file = new Descriptor ( path , "r" ) ; length = file . length ( ) ; } protected void readInternal ( byte [ ] b , int offset , int len ) throws IOException { synchronized ( file ) { long position = getFilePointer ( ) ; if ( position != file . position ) { file . seek ( position ) ; file . position = position ; } int total = 0 ; do { int i = file . read ( b , offset + total , len - total ) ; if ( i == - 1 ) throw new IOException ( "read past EOF" ) ; file . position += i ; total += i ; } while ( total < len ) ; } } public void close ( ) throws IOException { if ( ! isClone ) file . close ( ) ; } protected void seekInternal ( long position ) { } public long length ( ) { return length ; } protected void finalize ( ) throws IOException { close ( ) ; } public Object clone ( ) { FSIndexInput clone = ( FSIndexInput ) super . clone ( ) ; clone . isClone = true ; return clone ; } boolean isFDValid ( ) throws IOException { return file . getFD ( ) . valid ( ) ; } } class FSIndexOutput extends BufferedIndexOutput { RandomAccessFile file = null ; public FSIndexOutput ( File path ) throws IOException { file = new RandomAccessFile ( path , "rw" ) ; } public void flushBuffer ( byte [ ] b , int size ) throws IOException { file . write ( b , 0 , size ) ; } public void close ( ) throws IOException { super . close ( ) ; file . close ( ) ; } public void seek ( long pos ) throws IOException { super . seek ( pos ) ; file . seek ( pos ) ; } public long length ( ) throws IOException { return file . length ( ) ; } protected void finalize ( ) throws IOException { file . close ( ) ; } } 	1	['26', '2', '1', '13', '80', '53', '6', '8', '18', '0.854545455', '938', '0.818181818', '0', '0.314285714', '0.296', '0', '0', '34.65384615', '2', '1', '14']
package org . apache . lucene . util ; public abstract class StringHelper { public static final int stringDifference ( String s1 , String s2 ) { int len1 = s1 . length ( ) ; int len2 = s2 . length ( ) ; int len = len1 < len2 ? len1 : len2 ; for ( int i = 0 ; i < len ; i ++ ) { if ( s1 . charAt ( i ) != s2 . charAt ( i ) ) { return i ; } } return len ; } private StringHelper ( ) { } } 	0	['2', '1', '0', '2', '5', '1', '2', '0', '1', '2', '36', '0', '0', '0', '0.5', '0', '0', '17', '4', '2', '0']
package org . apache . lucene . search ; import org . apache . lucene . util . Parameter ; public class BooleanClause implements java . io . Serializable { public static final class Occur extends Parameter implements java . io . Serializable { private Occur ( String name ) { super ( name ) ; } public String toString ( ) { if ( this == MUST ) return "+" ; if ( this == MUST_NOT ) return "-" ; return "" ; } public static final Occur MUST = new Occur ( "MUST" ) ; public static final Occur SHOULD = new Occur ( "SHOULD" ) ; public static final Occur MUST_NOT = new Occur ( "MUST_NOT" ) ; } private Query query ; private Occur occur = Occur . SHOULD ; public BooleanClause ( Query query , Occur occur ) { this . query = query ; this . occur = occur ; } public Occur getOccur ( ) { return occur ; } public void setOccur ( Occur occur ) { this . occur = occur ; } public Query getQuery ( ) { return query ; } public void setQuery ( Query query ) { this . query = query ; } public boolean isProhibited ( ) { return Occur . MUST_NOT . equals ( occur ) ; } public boolean isRequired ( ) { return Occur . MUST . equals ( occur ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof BooleanClause ) ) return false ; BooleanClause other = ( BooleanClause ) o ; return this . query . equals ( other . query ) && this . occur . equals ( other . occur ) ; } public int hashCode ( ) { return query . hashCode ( ) ^ ( Occur . MUST . equals ( occur ) ? 1 : 0 ) ^ ( Occur . MUST_NOT . equals ( occur ) ? 2 : 0 ) ; } public String toString ( ) { return occur . toString ( ) + query . toString ( ) ; } } 	1	['10', '1', '0', '7', '18', '0', '6', '2', '10', '0.333333333', '107', '1', '2', '0', '0.375', '1', '1', '9.5', '4', '1.4', '1']
package org . apache . lucene . analysis ; import java . io . Reader ; public abstract class Analyzer { public abstract TokenStream tokenStream ( String fieldName , Reader reader ) ; public int getPositionIncrementGap ( String fieldName ) { return 0 ; } } 	0	['3', '1', '6', '13', '4', '3', '12', '1', '3', '2', '8', '0', '0', '0', '0.666666667', '0', '0', '1.666666667', '1', '0.6667', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . util . PriorityQueue ; import org . apache . lucene . util . ToStringUtils ; import java . io . IOException ; public class FuzzyQuery extends MultiTermQuery { public final static float defaultMinSimilarity = 0.5f ; public final static int defaultPrefixLength = 0 ; private float minimumSimilarity ; private int prefixLength ; public FuzzyQuery ( Term term , float minimumSimilarity , int prefixLength ) throws IllegalArgumentException { super ( term ) ; if ( minimumSimilarity >= 1.0f ) throw new IllegalArgumentException ( "minimumSimilarity >= 1" ) ; else if ( minimumSimilarity < 0.0f ) throw new IllegalArgumentException ( "minimumSimilarity < 0" ) ; if ( prefixLength < 0 ) throw new IllegalArgumentException ( "prefixLength < 0" ) ; this . minimumSimilarity = minimumSimilarity ; this . prefixLength = prefixLength ; } public FuzzyQuery ( Term term , float minimumSimilarity ) throws IllegalArgumentException { this ( term , minimumSimilarity , defaultPrefixLength ) ; } public FuzzyQuery ( Term term ) { this ( term , defaultMinSimilarity , defaultPrefixLength ) ; } public float getMinSimilarity ( ) { return minimumSimilarity ; } public int getPrefixLength ( ) { return prefixLength ; } protected FilteredTermEnum getEnum ( IndexReader reader ) throws IOException { return new FuzzyTermEnum ( reader , getTerm ( ) , minimumSimilarity , prefixLength ) ; } public Query rewrite ( IndexReader reader ) throws IOException { FilteredTermEnum enumerator = getEnum ( reader ) ; int maxClauseCount = BooleanQuery . getMaxClauseCount ( ) ; ScoreTermQueue stQueue = new ScoreTermQueue ( maxClauseCount ) ; try { do { float minScore = 0.0f ; float score = 0.0f ; Term t = enumerator . term ( ) ; if ( t != null ) { score = enumerator . difference ( ) ; if ( stQueue . size ( ) < maxClauseCount || score > minScore ) { stQueue . insert ( new ScoreTerm ( t , score ) ) ; minScore = ( ( ScoreTerm ) stQueue . top ( ) ) . score ; } } } while ( enumerator . next ( ) ) ; } finally { enumerator . close ( ) ; } BooleanQuery query = new BooleanQuery ( true ) ; int size = stQueue . size ( ) ; for ( int i = 0 ; i < size ; i ++ ) { ScoreTerm st = ( ScoreTerm ) stQueue . pop ( ) ; TermQuery tq = new TermQuery ( st . term ) ; tq . setBoost ( getBoost ( ) * st . score ) ; query . add ( tq , BooleanClause . Occur . SHOULD ) ; } return query ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; Term term = getTerm ( ) ; if ( ! term . field ( ) . equals ( field ) ) { buffer . append ( term . field ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( term . text ( ) ) ; buffer . append ( '~' ) ; buffer . append ( Float . toString ( minimumSimilarity ) ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } protected static class ScoreTerm { public Term term ; public float score ; public ScoreTerm ( Term term , float score ) { this . term = term ; this . score = score ; } } protected static class ScoreTermQueue extends PriorityQueue { public ScoreTermQueue ( int size ) { initialize ( size ) ; } protected boolean lessThan ( Object a , Object b ) { ScoreTerm termA = ( ScoreTerm ) a ; ScoreTerm termB = ( ScoreTerm ) b ; if ( termA . score == termB . score ) return termA . term . compareTo ( termB . term ) > 0 ; else return termA . score < termB . score ; } } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof FuzzyQuery ) ) return false ; if ( ! super . equals ( o ) ) return false ; final FuzzyQuery fuzzyQuery = ( FuzzyQuery ) o ; if ( minimumSimilarity != fuzzyQuery . minimumSimilarity ) return false ; if ( prefixLength != fuzzyQuery . prefixLength ) return false ; return true ; } public int hashCode ( ) { int result = super . hashCode ( ) ; result = 29 * result + minimumSimilarity != + 0.0f ? Float . floatToIntBits ( minimumSimilarity ) : 0 ; result = 29 * result + prefixLength ; return result ; } } 	1	['10', '3', '0', '13', '42', '7', '1', '12', '9', '0.638888889', '280', '0.5', '0', '0.72', '0.285714286', '3', '4', '26.6', '6', '1.4', '1']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexInput ; final class TermBuffer implements Cloneable { private static final char [ ] NO_CHARS = new char [ 0 ] ; private String field ; private char [ ] text = NO_CHARS ; private int textLength ; private Term term ; public final int compareTo ( TermBuffer other ) { if ( field == other . field ) return compareChars ( text , textLength , other . text , other . textLength ) ; else return field . compareTo ( other . field ) ; } private static final int compareChars ( char [ ] v1 , int len1 , char [ ] v2 , int len2 ) { int end = Math . min ( len1 , len2 ) ; for ( int k = 0 ; k < end ; k ++ ) { char c1 = v1 [ k ] ; char c2 = v2 [ k ] ; if ( c1 != c2 ) { return c1 - c2 ; } } return len1 - len2 ; } private final void setTextLength ( int newLength ) { if ( text . length < newLength ) { char [ ] newText = new char [ newLength ] ; System . arraycopy ( text , 0 , newText , 0 , textLength ) ; text = newText ; } textLength = newLength ; } public final void read ( IndexInput input , FieldInfos fieldInfos ) throws IOException { this . term = null ; int start = input . readVInt ( ) ; int length = input . readVInt ( ) ; int totalLength = start + length ; setTextLength ( totalLength ) ; input . readChars ( this . text , start , length ) ; this . field = fieldInfos . fieldName ( input . readVInt ( ) ) ; } public final void set ( Term term ) { if ( term == null ) { reset ( ) ; return ; } setTextLength ( term . text ( ) . length ( ) ) ; term . text ( ) . getChars ( 0 , term . text ( ) . length ( ) , text , 0 ) ; this . field = term . field ( ) ; this . term = term ; } public final void set ( TermBuffer other ) { setTextLength ( other . textLength ) ; System . arraycopy ( other . text , 0 , text , 0 , textLength ) ; this . field = other . field ; this . term = other . term ; } public void reset ( ) { this . field = null ; this . textLength = 0 ; this . term = null ; } public Term toTerm ( ) { if ( field == null ) return null ; if ( term == null ) term = new Term ( field , new String ( text , 0 , textLength ) , false ) ; return term ; } protected Object clone ( ) { TermBuffer clone = null ; try { clone = ( TermBuffer ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . text = new char [ text . length ] ; System . arraycopy ( text , 0 , clone . text , 0 , textLength ) ; return clone ; } } 	0	['11', '1', '0', '4', '25', '0', '1', '3', '6', '0.52', '241', '1', '1', '0', '0.242857143', '0', '0', '20.45454545', '3', '1.4545', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import org . apache . lucene . store . BufferedIndexInput ; import org . apache . lucene . store . IndexInput ; abstract class MultiLevelSkipListReader { private int maxNumberOfSkipLevels ; private int numberOfSkipLevels ; private int numberOfLevelsToBuffer = 1 ; private int docCount ; private boolean haveSkipped ; private IndexInput [ ] skipStream ; private long skipPointer [ ] ; private int skipInterval [ ] ; private int [ ] numSkipped ; private int [ ] skipDoc ; private int lastDoc ; private long [ ] childPointer ; private long lastChildPointer ; private boolean inputIsBuffered ; public MultiLevelSkipListReader ( IndexInput skipStream , int maxSkipLevels , int skipInterval ) { this . skipStream = new IndexInput [ maxSkipLevels ] ; this . skipPointer = new long [ maxSkipLevels ] ; this . childPointer = new long [ maxSkipLevels ] ; this . numSkipped = new int [ maxSkipLevels ] ; this . maxNumberOfSkipLevels = maxSkipLevels ; this . skipInterval = new int [ maxSkipLevels ] ; this . skipStream [ 0 ] = skipStream ; this . inputIsBuffered = ( skipStream instanceof BufferedIndexInput ) ; this . skipInterval [ 0 ] = skipInterval ; for ( int i = 1 ; i < maxSkipLevels ; i ++ ) { this . skipInterval [ i ] = this . skipInterval [ i - 1 ] * skipInterval ; } skipDoc = new int [ maxSkipLevels ] ; } int getDoc ( ) { return lastDoc ; } int skipTo ( int target ) throws IOException { if ( ! haveSkipped ) { loadSkipLevels ( ) ; haveSkipped = true ; } int level = 0 ; while ( level < numberOfSkipLevels - 1 && target > skipDoc [ level + 1 ] ) { level ++ ; } while ( level >= 0 ) { if ( target > skipDoc [ level ] ) { if ( ! loadNextSkip ( level ) ) { continue ; } } else { if ( level > 0 && lastChildPointer > skipStream [ level - 1 ] . getFilePointer ( ) ) { seekChild ( level - 1 ) ; } level -- ; } } return numSkipped [ 0 ] - skipInterval [ 0 ] - 1 ; } private boolean loadNextSkip ( int level ) throws IOException { setLastSkipData ( level ) ; numSkipped [ level ] += skipInterval [ level ] ; if ( numSkipped [ level ] > docCount ) { skipDoc [ level ] = Integer . MAX_VALUE ; if ( numberOfSkipLevels > level ) numberOfSkipLevels = level ; return false ; } skipDoc [ level ] += readSkipData ( level , skipStream [ level ] ) ; if ( level != 0 ) { childPointer [ level ] = skipStream [ level ] . readVLong ( ) + skipPointer [ level - 1 ] ; } return true ; } protected void seekChild ( int level ) throws IOException { skipStream [ level ] . seek ( lastChildPointer ) ; numSkipped [ level ] = numSkipped [ level + 1 ] - skipInterval [ level + 1 ] ; skipDoc [ level ] = lastDoc ; if ( level > 0 ) { childPointer [ level ] = skipStream [ level ] . readVLong ( ) + skipPointer [ level - 1 ] ; } } void close ( ) throws IOException { for ( int i = 1 ; i < skipStream . length ; i ++ ) { if ( skipStream [ i ] != null ) { skipStream [ i ] . close ( ) ; } } } void init ( long skipPointer , int df ) { this . skipPointer [ 0 ] = skipPointer ; this . docCount = df ; Arrays . fill ( skipDoc , 0 ) ; Arrays . fill ( numSkipped , 0 ) ; haveSkipped = false ; for ( int i = 1 ; i < numberOfSkipLevels ; i ++ ) { skipStream [ 0 ] = null ; } } private void loadSkipLevels ( ) throws IOException { numberOfSkipLevels = docCount == 0 ? 0 : ( int ) Math . floor ( Math . log ( docCount ) / Math . log ( skipInterval [ 0 ] ) ) ; if ( numberOfSkipLevels > maxNumberOfSkipLevels ) { numberOfSkipLevels = maxNumberOfSkipLevels ; } skipStream [ 0 ] . seek ( skipPointer [ 0 ] ) ; int toBuffer = numberOfLevelsToBuffer ; for ( int i = numberOfSkipLevels - 1 ; i > 0 ; i -- ) { long length = skipStream [ 0 ] . readVLong ( ) ; skipPointer [ i ] = skipStream [ 0 ] . getFilePointer ( ) ; if ( toBuffer > 0 ) { skipStream [ i ] = new SkipBuffer ( skipStream [ 0 ] , ( int ) length ) ; toBuffer -- ; } else { skipStream [ i ] = ( IndexInput ) skipStream [ 0 ] . clone ( ) ; if ( inputIsBuffered && length < BufferedIndexInput . BUFFER_SIZE ) { ( ( BufferedIndexInput ) skipStream [ i ] ) . setBufferSize ( ( int ) length ) ; } skipStream [ 0 ] . seek ( skipStream [ 0 ] . getFilePointer ( ) + length ) ; } } skipPointer [ 0 ] = skipStream [ 0 ] . getFilePointer ( ) ; } protected abstract int readSkipData ( int level , IndexInput skipStream ) throws IOException ; protected void setLastSkipData ( int level ) { lastDoc = skipDoc [ level ] ; lastChildPointer = childPointer [ level ] ; } private final static class SkipBuffer extends IndexInput { private byte [ ] data ; private long pointer ; private int pos ; SkipBuffer ( IndexInput input , int length ) throws IOException { data = new byte [ length ] ; pointer = input . getFilePointer ( ) ; input . readBytes ( data , 0 , length ) ; } public void close ( ) throws IOException { data = null ; } public long getFilePointer ( ) { return pointer + pos ; } public long length ( ) { return data . length ; } public byte readByte ( ) throws IOException { return data [ pos ++ ] ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { System . arraycopy ( data , pos , b , offset , len ) ; pos += len ; } public void seek ( long pos ) throws IOException { this . pos = ( int ) ( pos - pointer ) ; } } } 	1	['10', '1', '1', '4', '21', '0', '1', '3', '1', '0.619047619', '477', '1', '1', '0', '0.5', '0', '0', '45.3', '2', '1', '1']
package org . apache . lucene . index ; public class TermVectorOffsetInfo { public static final TermVectorOffsetInfo [ ] EMPTY_OFFSET_INFO = new TermVectorOffsetInfo [ 0 ] ; private int startOffset ; private int endOffset ; public TermVectorOffsetInfo ( ) { } public TermVectorOffsetInfo ( int startOffset , int endOffset ) { this . endOffset = endOffset ; this . startOffset = startOffset ; } public int getEndOffset ( ) { return endOffset ; } public void setEndOffset ( int endOffset ) { this . endOffset = endOffset ; } public int getStartOffset ( ) { return startOffset ; } public void setStartOffset ( int startOffset ) { this . startOffset = startOffset ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof TermVectorOffsetInfo ) ) return false ; final TermVectorOffsetInfo termVectorOffsetInfo = ( TermVectorOffsetInfo ) o ; if ( endOffset != termVectorOffsetInfo . endOffset ) return false ; if ( startOffset != termVectorOffsetInfo . startOffset ) return false ; return true ; } public int hashCode ( ) { int result ; result = startOffset ; result = 29 * result + endOffset ; return result ; } } 	0	['9', '1', '0', '7', '10', '2', '7', '0', '8', '0.666666667', '83', '0.666666667', '1', '0', '0.5', '1', '1', '7.888888889', '5', '1.1111', '0']
package org . apache . lucene . store ; import java . io . IOException ; import java . io . FileNotFoundException ; import java . io . File ; import java . io . Serializable ; import java . util . HashMap ; import java . util . Iterator ; import java . util . Set ; public class RAMDirectory extends Directory implements Serializable { private static final long serialVersionUID = 1l ; HashMap fileMap = new HashMap ( ) ; long sizeInBytes = 0 ; public RAMDirectory ( ) { setLockFactory ( new SingleInstanceLockFactory ( ) ) ; } public RAMDirectory ( Directory dir ) throws IOException { this ( dir , false ) ; } private RAMDirectory ( Directory dir , boolean closeDir ) throws IOException { this ( ) ; Directory . copy ( dir , this , closeDir ) ; } public RAMDirectory ( File dir ) throws IOException { this ( FSDirectory . getDirectory ( dir ) , true ) ; } public RAMDirectory ( String dir ) throws IOException { this ( FSDirectory . getDirectory ( dir ) , true ) ; } public synchronized final String [ ] list ( ) { ensureOpen ( ) ; Set fileNames = fileMap . keySet ( ) ; String [ ] result = new String [ fileNames . size ( ) ] ; int i = 0 ; Iterator it = fileNames . iterator ( ) ; while ( it . hasNext ( ) ) result [ i ++ ] = ( String ) it . next ( ) ; return result ; } public final boolean fileExists ( String name ) { ensureOpen ( ) ; RAMFile file ; synchronized ( this ) { file = ( RAMFile ) fileMap . get ( name ) ; } return file != null ; } public final long fileModified ( String name ) throws IOException { ensureOpen ( ) ; RAMFile file ; synchronized ( this ) { file = ( RAMFile ) fileMap . get ( name ) ; } if ( file == null ) throw new FileNotFoundException ( name ) ; return file . getLastModified ( ) ; } public void touchFile ( String name ) throws IOException { ensureOpen ( ) ; RAMFile file ; synchronized ( this ) { file = ( RAMFile ) fileMap . get ( name ) ; } if ( file == null ) throw new FileNotFoundException ( name ) ; long ts2 , ts1 = System . currentTimeMillis ( ) ; do { try { Thread . sleep ( 0 , 1 ) ; } catch ( InterruptedException e ) { } ts2 = System . currentTimeMillis ( ) ; } while ( ts1 == ts2 ) ; file . setLastModified ( ts2 ) ; } public final long fileLength ( String name ) throws IOException { ensureOpen ( ) ; RAMFile file ; synchronized ( this ) { file = ( RAMFile ) fileMap . get ( name ) ; } if ( file == null ) throw new FileNotFoundException ( name ) ; return file . getLength ( ) ; } public synchronized final long sizeInBytes ( ) { ensureOpen ( ) ; return sizeInBytes ; } public synchronized void deleteFile ( String name ) throws IOException { ensureOpen ( ) ; RAMFile file = ( RAMFile ) fileMap . get ( name ) ; if ( file != null ) { fileMap . remove ( name ) ; file . directory = null ; sizeInBytes -= file . sizeInBytes ; } else throw new FileNotFoundException ( name ) ; } public synchronized final void renameFile ( String from , String to ) throws IOException { ensureOpen ( ) ; RAMFile fromFile = ( RAMFile ) fileMap . get ( from ) ; if ( fromFile == null ) throw new FileNotFoundException ( from ) ; RAMFile toFile = ( RAMFile ) fileMap . get ( to ) ; if ( toFile != null ) { sizeInBytes -= toFile . sizeInBytes ; toFile . directory = null ; } fileMap . remove ( from ) ; fileMap . put ( to , fromFile ) ; } public IndexOutput createOutput ( String name ) { ensureOpen ( ) ; RAMFile file = new RAMFile ( this ) ; synchronized ( this ) { RAMFile existing = ( RAMFile ) fileMap . get ( name ) ; if ( existing != null ) { sizeInBytes -= existing . sizeInBytes ; existing . directory = null ; } fileMap . put ( name , file ) ; } return new RAMOutputStream ( file ) ; } public IndexInput openInput ( String name ) throws IOException { ensureOpen ( ) ; RAMFile file ; synchronized ( this ) { file = ( RAMFile ) fileMap . get ( name ) ; } if ( file == null ) throw new FileNotFoundException ( name ) ; return new RAMInputStream ( file ) ; } public void close ( ) { fileMap = null ; } protected final void ensureOpen ( ) throws AlreadyClosedException { if ( fileMap == null ) { throw new AlreadyClosedException ( "this RAMDirectory is closed" ) ; } } } 	1	['17', '2', '0', '11', '42', '0', '2', '10', '15', '0.5625', '393', '0.333333333', '0', '0.586206897', '0.352941176', '1', '5', '21.94117647', '2', '0.8824', '2']
package org . apache . lucene . analysis ; import java . io . Reader ; public class KeywordAnalyzer extends Analyzer { public TokenStream tokenStream ( String fieldName , final Reader reader ) { return new KeywordTokenizer ( reader ) ; } } 	0	['2', '2', '0', '3', '4', '1', '0', '3', '2', '2', '10', '0', '0', '0.666666667', '0.666666667', '0', '0', '4', '1', '0.5', '0']
package org . apache . lucene . index ; import java . io . IOException ; public interface TermPositions extends TermDocs { int nextPosition ( ) throws IOException ; int getPayloadLength ( ) ; byte [ ] getPayload ( byte [ ] data , int offset ) throws IOException ; public boolean isPayloadAvailable ( ) ; } 	1	['4', '1', '0', '23', '4', '6', '22', '1', '4', '2', '4', '0', '0', '0', '0.5', '0', '0', '0', '1', '1', '1']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Query ; import org . apache . lucene . search . Weight ; import org . apache . lucene . search . Searcher ; public abstract class SpanQuery extends Query { public abstract Spans getSpans ( IndexReader reader ) throws IOException ; public abstract String getField ( ) ; public abstract Collection getTerms ( ) ; protected Weight createWeight ( Searcher searcher ) throws IOException { return new SpanWeight ( this , searcher ) ; } } 	0	['5', '2', '5', '17', '7', '10', '12', '6', '4', '2', '14', '0', '0', '0.75', '0.466666667', '1', '1', '1.8', '1', '0.8', '0']
package org . apache . lucene . search . function ; import org . apache . lucene . search . Explanation ; public abstract class DocValues { private int nVals ; public DocValues ( int nVals ) { this . nVals = nVals ; } private DocValues ( ) { } public abstract float floatVal ( int doc ) ; public int intVal ( int doc ) { return ( int ) floatVal ( doc ) ; } public long longVal ( int doc ) { return ( long ) floatVal ( doc ) ; } public double doubleVal ( int doc ) { return ( double ) floatVal ( doc ) ; } public String strVal ( int doc ) { return Float . toString ( floatVal ( doc ) ) ; } public abstract String toString ( int doc ) ; public Explanation explain ( int doc ) { return new Explanation ( floatVal ( doc ) , toString ( doc ) ) ; } Object getInnerArray ( ) { return new Object [ 0 ] ; } private float minVal ; private float maxVal ; private float avgVal ; private boolean computed = false ; private void compute ( ) { if ( computed ) { return ; } minVal = Float . MAX_VALUE ; maxVal = 0 ; float sum = 0 ; for ( int i = 0 ; i < nVals ; i ++ ) { float val = floatVal ( i ) ; sum += val ; minVal = Math . min ( minVal , val ) ; maxVal = Math . max ( maxVal , val ) ; } avgVal = sum / nVals ; computed = true ; } public float getMinValue ( ) { compute ( ) ; return minVal ; } public float getMaxValue ( ) { compute ( ) ; return maxVal ; } public float getAverageValue ( ) { compute ( ) ; return avgVal ; } } 	1	['14', '1', '6', '16', '19', '79', '15', '1', '11', '0.723076923', '133', '1', '0', '0', '0.785714286', '0', '0', '8.142857143', '3', '1', '1']
package org . apache . lucene . analysis . standard ; public interface CharStream { char readChar ( ) throws java . io . IOException ; int getColumn ( ) ; int getLine ( ) ; int getEndColumn ( ) ; int getEndLine ( ) ; int getBeginColumn ( ) ; int getBeginLine ( ) ; void backup ( int amount ) ; char BeginToken ( ) throws java . io . IOException ; String GetImage ( ) ; char [ ] GetSuffix ( int len ) ; void Done ( ) ; } 	0	['12', '1', '0', '3', '12', '66', '3', '0', '12', '2', '12', '0', '0', '0', '0.583333333', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermDocs ; import org . apache . lucene . index . TermEnum ; import java . io . IOException ; import java . util . Locale ; import java . util . Map ; import java . util . WeakHashMap ; import java . util . HashMap ; class FieldCacheImpl implements FieldCache { abstract static class Cache { private final Map readerCache = new WeakHashMap ( ) ; protected abstract Object createValue ( IndexReader reader , Object key ) throws IOException ; public Object get ( IndexReader reader , Object key ) throws IOException { Map innerCache ; Object value ; synchronized ( readerCache ) { innerCache = ( Map ) readerCache . get ( reader ) ; if ( innerCache == null ) { innerCache = new HashMap ( ) ; readerCache . put ( reader , innerCache ) ; value = null ; } else { value = innerCache . get ( key ) ; } if ( value == null ) { value = new CreationPlaceholder ( ) ; innerCache . put ( key , value ) ; } } if ( value instanceof CreationPlaceholder ) { synchronized ( value ) { CreationPlaceholder progress = ( CreationPlaceholder ) value ; if ( progress . value == null ) { progress . value = createValue ( reader , key ) ; synchronized ( readerCache ) { innerCache . put ( key , progress . value ) ; } } return progress . value ; } } return value ; } } static final class CreationPlaceholder { Object value ; } static class Entry { final String field ; final int type ; final Object custom ; final Locale locale ; Entry ( String field , int type , Locale locale ) { this . field = field . intern ( ) ; this . type = type ; this . custom = null ; this . locale = locale ; } Entry ( String field , Object custom ) { this . field = field . intern ( ) ; this . type = SortField . CUSTOM ; this . custom = custom ; this . locale = null ; } public boolean equals ( Object o ) { if ( o instanceof Entry ) { Entry other = ( Entry ) o ; if ( other . field == field && other . type == type ) { if ( other . locale == null ? locale == null : other . locale . equals ( locale ) ) { if ( other . custom == null ) { if ( custom == null ) return true ; } else if ( other . custom . equals ( custom ) ) { return true ; } } } } return false ; } public int hashCode ( ) { return field . hashCode ( ) ^ type ^ ( custom == null ? 0 : custom . hashCode ( ) ) ^ ( locale == null ? 0 : locale . hashCode ( ) ) ; } } private static final ByteParser BYTE_PARSER = new ByteParser ( ) { public byte parseByte ( String value ) { return Byte . parseByte ( value ) ; } } ; private static final ShortParser SHORT_PARSER = new ShortParser ( ) { public short parseShort ( String value ) { return Short . parseShort ( value ) ; } } ; private static final IntParser INT_PARSER = new IntParser ( ) { public int parseInt ( String value ) { return Integer . parseInt ( value ) ; } } ; private static final FloatParser FLOAT_PARSER = new FloatParser ( ) { public float parseFloat ( String value ) { return Float . parseFloat ( value ) ; } } ; public byte [ ] getBytes ( IndexReader reader , String field ) throws IOException { return getBytes ( reader , field , BYTE_PARSER ) ; } public byte [ ] getBytes ( IndexReader reader , String field , ByteParser parser ) throws IOException { return ( byte [ ] ) bytesCache . get ( reader , new Entry ( field , parser ) ) ; } Cache bytesCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object entryKey ) throws IOException { Entry entry = ( Entry ) entryKey ; String field = entry . field ; ByteParser parser = ( ByteParser ) entry . custom ; final byte [ ] retArray = new byte [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field , "" ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; byte termval = parser . parseByte ( term . text ( ) ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } return retArray ; } } ; public short [ ] getShorts ( IndexReader reader , String field ) throws IOException { return getShorts ( reader , field , SHORT_PARSER ) ; } public short [ ] getShorts ( IndexReader reader , String field , ShortParser parser ) throws IOException { return ( short [ ] ) shortsCache . get ( reader , new Entry ( field , parser ) ) ; } Cache shortsCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object entryKey ) throws IOException { Entry entry = ( Entry ) entryKey ; String field = entry . field ; ShortParser parser = ( ShortParser ) entry . custom ; final short [ ] retArray = new short [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field , "" ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; short termval = parser . parseShort ( term . text ( ) ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } return retArray ; } } ; public int [ ] getInts ( IndexReader reader , String field ) throws IOException { return getInts ( reader , field , INT_PARSER ) ; } public int [ ] getInts ( IndexReader reader , String field , IntParser parser ) throws IOException { return ( int [ ] ) intsCache . get ( reader , new Entry ( field , parser ) ) ; } Cache intsCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object entryKey ) throws IOException { Entry entry = ( Entry ) entryKey ; String field = entry . field ; IntParser parser = ( IntParser ) entry . custom ; final int [ ] retArray = new int [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field , "" ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; int termval = parser . parseInt ( term . text ( ) ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } return retArray ; } } ; public float [ ] getFloats ( IndexReader reader , String field ) throws IOException { return getFloats ( reader , field , FLOAT_PARSER ) ; } public float [ ] getFloats ( IndexReader reader , String field , FloatParser parser ) throws IOException { return ( float [ ] ) floatsCache . get ( reader , new Entry ( field , parser ) ) ; } Cache floatsCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object entryKey ) throws IOException { Entry entry = ( Entry ) entryKey ; String field = entry . field ; FloatParser parser = ( FloatParser ) entry . custom ; final float [ ] retArray = new float [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field , "" ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; float termval = parser . parseFloat ( term . text ( ) ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } return retArray ; } } ; public String [ ] getStrings ( IndexReader reader , String field ) throws IOException { return ( String [ ] ) stringsCache . get ( reader , field ) ; } Cache stringsCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object fieldKey ) throws IOException { String field = ( ( String ) fieldKey ) . intern ( ) ; final String [ ] retArray = new String [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field , "" ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; String termval = term . text ( ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } return retArray ; } } ; public StringIndex getStringIndex ( IndexReader reader , String field ) throws IOException { return ( StringIndex ) stringsIndexCache . get ( reader , field ) ; } Cache stringsIndexCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object fieldKey ) throws IOException { String field = ( ( String ) fieldKey ) . intern ( ) ; final int [ ] retArray = new int [ reader . maxDoc ( ) ] ; String [ ] mterms = new String [ reader . maxDoc ( ) + 1 ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field , "" ) ) ; int t = 0 ; mterms [ t ++ ] = null ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; if ( t >= mterms . length ) throw new RuntimeException ( "there are more terms than " + "documents in field \"" + field + "\", but it's impossible to sort on " + "tokenized fields" ) ; mterms [ t ] = term . text ( ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = t ; } t ++ ; } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } if ( t == 0 ) { mterms = new String [ 1 ] ; } else if ( t < mterms . length ) { String [ ] terms = new String [ t ] ; System . arraycopy ( mterms , 0 , terms , 0 , t ) ; mterms = terms ; } StringIndex value = new StringIndex ( retArray , mterms ) ; return value ; } } ; public Object getAuto ( IndexReader reader , String field ) throws IOException { return autoCache . get ( reader , field ) ; } Cache autoCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object fieldKey ) throws IOException { String field = ( ( String ) fieldKey ) . intern ( ) ; TermEnum enumerator = reader . terms ( new Term ( field , "" ) ) ; try { Term term = enumerator . term ( ) ; if ( term == null ) { throw new RuntimeException ( "no terms in field " + field + " - cannot determine sort type" ) ; } Object ret = null ; if ( term . field ( ) == field ) { String termtext = term . text ( ) . trim ( ) ; try { Integer . parseInt ( termtext ) ; ret = getInts ( reader , field ) ; } catch ( NumberFormatException nfe1 ) { try { Float . parseFloat ( termtext ) ; ret = getFloats ( reader , field ) ; } catch ( NumberFormatException nfe2 ) { ret = getStringIndex ( reader , field ) ; } } } else { throw new RuntimeException ( "field \"" + field + "\" does not appear to be indexed" ) ; } return ret ; } finally { enumerator . close ( ) ; } } } ; public Comparable [ ] getCustom ( IndexReader reader , String field , SortComparator comparator ) throws IOException { return ( Comparable [ ] ) customCache . get ( reader , new Entry ( field , comparator ) ) ; } Cache customCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object entryKey ) throws IOException { Entry entry = ( Entry ) entryKey ; String field = entry . field ; SortComparator comparator = ( SortComparator ) entry . custom ; final Comparable [ ] retArray = new Comparable [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field , "" ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; Comparable termval = comparator . getComparable ( term . text ( ) ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } return retArray ; } } ; } 	1	['14', '1', '0', '22', '29', '67', '9', '22', '12', '0.897435897', '199', '0.333333333', '12', '0', '0.403846154', '0', '0', '12.35714286', '1', '0.8571', '4']
package org . apache . lucene . analysis . standard ; import java . io . * ; public final class FastCharStream implements CharStream { char [ ] buffer = null ; int bufferLength = 0 ; int bufferPosition = 0 ; int tokenStart = 0 ; int bufferStart = 0 ; Reader input ; public FastCharStream ( Reader r ) { input = r ; } public final char readChar ( ) throws IOException { if ( bufferPosition >= bufferLength ) refill ( ) ; return buffer [ bufferPosition ++ ] ; } private final void refill ( ) throws IOException { int newPosition = bufferLength - tokenStart ; if ( tokenStart == 0 ) { if ( buffer == null ) { buffer = new char [ 2048 ] ; } else if ( bufferLength == buffer . length ) { char [ ] newBuffer = new char [ buffer . length * 2 ] ; System . arraycopy ( buffer , 0 , newBuffer , 0 , bufferLength ) ; buffer = newBuffer ; } } else { System . arraycopy ( buffer , tokenStart , buffer , 0 , newPosition ) ; } bufferLength = newPosition ; bufferPosition = newPosition ; bufferStart += tokenStart ; tokenStart = 0 ; int charsRead = input . read ( buffer , newPosition , buffer . length - newPosition ) ; if ( charsRead == - 1 ) throw new IOException ( "read past eof" ) ; else bufferLength += charsRead ; } public final char BeginToken ( ) throws IOException { tokenStart = bufferPosition ; return readChar ( ) ; } public final void backup ( int amount ) { bufferPosition -= amount ; } public final String GetImage ( ) { return new String ( buffer , tokenStart , bufferPosition - tokenStart ) ; } public final char [ ] GetSuffix ( int len ) { char [ ] value = new char [ len ] ; System . arraycopy ( buffer , bufferPosition - len , value , 0 , len ) ; return value ; } public final void Done ( ) { try { input . close ( ) ; } catch ( IOException e ) { System . err . println ( "Caught: " + e + "; ignoring." ) ; } } public final int getColumn ( ) { return bufferStart + bufferPosition ; } public final int getLine ( ) { return 1 ; } public final int getEndColumn ( ) { return bufferStart + bufferPosition ; } public final int getEndLine ( ) { return 1 ; } public final int getBeginColumn ( ) { return bufferStart + tokenStart ; } public final int getBeginLine ( ) { return 1 ; } } 	0	['14', '1', '0', '2', '25', '3', '1', '1', '13', '0.602564103', '237', '0', '0', '0', '0.404761905', '0', '0', '15.5', '1', '0.9286', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . IndexInput ; import java . io . IOException ; import java . util . List ; import java . util . ArrayList ; final class SegmentInfo { static final int NO = - 1 ; static final int YES = 1 ; static final int CHECK_DIR = 0 ; static final int WITHOUT_GEN = 0 ; public String name ; public int docCount ; public Directory dir ; private boolean preLockless ; private long delGen ; private long [ ] normGen ; private byte isCompoundFile ; private boolean hasSingleNormFile ; private List files ; public SegmentInfo ( String name , int docCount , Directory dir ) { this . name = name ; this . docCount = docCount ; this . dir = dir ; delGen = NO ; isCompoundFile = CHECK_DIR ; preLockless = true ; hasSingleNormFile = false ; } public SegmentInfo ( String name , int docCount , Directory dir , boolean isCompoundFile , boolean hasSingleNormFile ) { this ( name , docCount , dir ) ; this . isCompoundFile = ( byte ) ( isCompoundFile ? YES : NO ) ; this . hasSingleNormFile = hasSingleNormFile ; preLockless = false ; } void reset ( SegmentInfo src ) { files = null ; name = src . name ; docCount = src . docCount ; dir = src . dir ; preLockless = src . preLockless ; delGen = src . delGen ; if ( src . normGen == null ) { normGen = null ; } else { normGen = new long [ src . normGen . length ] ; System . arraycopy ( src . normGen , 0 , normGen , 0 , src . normGen . length ) ; } isCompoundFile = src . isCompoundFile ; hasSingleNormFile = src . hasSingleNormFile ; } SegmentInfo ( Directory dir , int format , IndexInput input ) throws IOException { this . dir = dir ; name = input . readString ( ) ; docCount = input . readInt ( ) ; if ( format <= SegmentInfos . FORMAT_LOCKLESS ) { delGen = input . readLong ( ) ; if ( format <= SegmentInfos . FORMAT_SINGLE_NORM_FILE ) { hasSingleNormFile = ( 1 == input . readByte ( ) ) ; } else { hasSingleNormFile = false ; } int numNormGen = input . readInt ( ) ; if ( numNormGen == NO ) { normGen = null ; } else { normGen = new long [ numNormGen ] ; for ( int j = 0 ; j < numNormGen ; j ++ ) { normGen [ j ] = input . readLong ( ) ; } } isCompoundFile = input . readByte ( ) ; preLockless = ( isCompoundFile == CHECK_DIR ) ; } else { delGen = CHECK_DIR ; normGen = null ; isCompoundFile = CHECK_DIR ; preLockless = true ; hasSingleNormFile = false ; } } void setNumFields ( int numFields ) { if ( normGen == null ) { normGen = new long [ numFields ] ; if ( preLockless ) { } else { for ( int i = 0 ; i < numFields ; i ++ ) { normGen [ i ] = NO ; } } } } boolean hasDeletions ( ) throws IOException { if ( delGen == NO ) { return false ; } else if ( delGen >= YES ) { return true ; } else { return dir . fileExists ( getDelFileName ( ) ) ; } } void advanceDelGen ( ) { if ( delGen == NO ) { delGen = YES ; } else { delGen ++ ; } files = null ; } void clearDelGen ( ) { delGen = NO ; files = null ; } public Object clone ( ) { SegmentInfo si = new SegmentInfo ( name , docCount , dir ) ; si . isCompoundFile = isCompoundFile ; si . delGen = delGen ; si . preLockless = preLockless ; si . hasSingleNormFile = hasSingleNormFile ; if ( normGen != null ) { si . normGen = ( long [ ] ) normGen . clone ( ) ; } return si ; } String getDelFileName ( ) { if ( delGen == NO ) { return null ; } else { return IndexFileNames . fileNameFromGeneration ( name , "." + IndexFileNames . DELETES_EXTENSION , delGen ) ; } } boolean hasSeparateNorms ( int fieldNumber ) throws IOException { if ( ( normGen == null && preLockless ) || ( normGen != null && normGen [ fieldNumber ] == CHECK_DIR ) ) { String fileName = name + ".s" + fieldNumber ; return dir . fileExists ( fileName ) ; } else if ( normGen == null || normGen [ fieldNumber ] == NO ) { return false ; } else { return true ; } } boolean hasSeparateNorms ( ) throws IOException { if ( normGen == null ) { if ( ! preLockless ) { return false ; } else { String [ ] result = dir . list ( ) ; if ( result == null ) throw new IOException ( "cannot read directory " + dir + ": list() returned null" ) ; String pattern ; pattern = name + ".s" ; int patternLength = pattern . length ( ) ; for ( int i = 0 ; i < result . length ; i ++ ) { if ( result [ i ] . startsWith ( pattern ) && Character . isDigit ( result [ i ] . charAt ( patternLength ) ) ) return true ; } return false ; } } else { for ( int i = 0 ; i < normGen . length ; i ++ ) { if ( normGen [ i ] >= YES ) { return true ; } } for ( int i = 0 ; i < normGen . length ; i ++ ) { if ( normGen [ i ] == CHECK_DIR ) { if ( hasSeparateNorms ( i ) ) { return true ; } } } } return false ; } void advanceNormGen ( int fieldIndex ) { if ( normGen [ fieldIndex ] == NO ) { normGen [ fieldIndex ] = YES ; } else { normGen [ fieldIndex ] ++ ; } files = null ; } String getNormFileName ( int number ) throws IOException { String prefix ; long gen ; if ( normGen == null ) { gen = CHECK_DIR ; } else { gen = normGen [ number ] ; } if ( hasSeparateNorms ( number ) ) { prefix = ".s" ; return IndexFileNames . fileNameFromGeneration ( name , prefix + number , gen ) ; } if ( hasSingleNormFile ) { prefix = "." + IndexFileNames . NORMS_EXTENSION ; return IndexFileNames . fileNameFromGeneration ( name , prefix , WITHOUT_GEN ) ; } prefix = ".f" ; return IndexFileNames . fileNameFromGeneration ( name , prefix + number , WITHOUT_GEN ) ; } void setUseCompoundFile ( boolean isCompoundFile ) { if ( isCompoundFile ) { this . isCompoundFile = YES ; } else { this . isCompoundFile = NO ; } files = null ; } boolean getUseCompoundFile ( ) throws IOException { if ( isCompoundFile == NO ) { return false ; } else if ( isCompoundFile == YES ) { return true ; } else { return dir . fileExists ( name + "." + IndexFileNames . COMPOUND_FILE_EXTENSION ) ; } } void write ( IndexOutput output ) throws IOException { output . writeString ( name ) ; output . writeInt ( docCount ) ; output . writeLong ( delGen ) ; output . writeByte ( ( byte ) ( hasSingleNormFile ? 1 : 0 ) ) ; if ( normGen == null ) { output . writeInt ( NO ) ; } else { output . writeInt ( normGen . length ) ; for ( int j = 0 ; j < normGen . length ; j ++ ) { output . writeLong ( normGen [ j ] ) ; } } output . writeByte ( isCompoundFile ) ; } public List files ( ) throws IOException { if ( files != null ) { return files ; } files = new ArrayList ( ) ; boolean useCompoundFile = getUseCompoundFile ( ) ; if ( useCompoundFile ) { files . add ( name + "." + IndexFileNames . COMPOUND_FILE_EXTENSION ) ; } else { for ( int i = 0 ; i < IndexFileNames . INDEX_EXTENSIONS_IN_COMPOUND_FILE . length ; i ++ ) { String ext = IndexFileNames . INDEX_EXTENSIONS_IN_COMPOUND_FILE [ i ] ; String fileName = name + "." + ext ; if ( dir . fileExists ( fileName ) ) { files . add ( fileName ) ; } } } String delFileName = IndexFileNames . fileNameFromGeneration ( name , "." + IndexFileNames . DELETES_EXTENSION , delGen ) ; if ( delFileName != null && ( delGen >= YES || dir . fileExists ( delFileName ) ) ) { files . add ( delFileName ) ; } if ( normGen != null ) { for ( int i = 0 ; i < normGen . length ; i ++ ) { long gen = normGen [ i ] ; if ( gen >= YES ) { files . add ( IndexFileNames . fileNameFromGeneration ( name , "." + IndexFileNames . SEPARATE_NORMS_EXTENSION + i , gen ) ) ; } else if ( NO == gen ) { if ( ! hasSingleNormFile && ! useCompoundFile ) { String fileName = name + "." + IndexFileNames . PLAIN_NORMS_EXTENSION + i ; if ( dir . fileExists ( fileName ) ) { files . add ( fileName ) ; } } } else if ( CHECK_DIR == gen ) { String fileName = null ; if ( useCompoundFile ) { fileName = name + "." + IndexFileNames . SEPARATE_NORMS_EXTENSION + i ; } else if ( ! hasSingleNormFile ) { fileName = name + "." + IndexFileNames . PLAIN_NORMS_EXTENSION + i ; } if ( fileName != null && dir . fileExists ( fileName ) ) { files . add ( fileName ) ; } } } } else if ( preLockless || ( ! hasSingleNormFile && ! useCompoundFile ) ) { String prefix ; if ( useCompoundFile ) prefix = name + "." + IndexFileNames . SEPARATE_NORMS_EXTENSION ; else prefix = name + "." + IndexFileNames . PLAIN_NORMS_EXTENSION ; int prefixLength = prefix . length ( ) ; String [ ] allFiles = dir . list ( ) ; if ( allFiles == null ) throw new IOException ( "cannot read directory " + dir + ": list() returned null" ) ; for ( int i = 0 ; i < allFiles . length ; i ++ ) { String fileName = allFiles [ i ] ; if ( fileName . length ( ) > prefixLength && Character . isDigit ( fileName . charAt ( prefixLength ) ) && fileName . startsWith ( prefix ) ) { files . add ( fileName ) ; } } } return files ; } } 	1	['18', '1', '0', '12', '44', '0', '8', '4', '4', '0.678733032', '981', '0.461538462', '1', '0', '0.243055556', '0', '0', '52.77777778', '4', '1.3333', '3']
package org . apache . lucene . document ; public class LoadFirstFieldSelector implements FieldSelector { public FieldSelectorResult accept ( String fieldName ) { return FieldSelectorResult . LOAD_AND_BREAK ; } } 	0	['2', '1', '0', '2', '3', '1', '0', '2', '2', '2', '7', '0', '0', '0', '0.75', '0', '0', '2.5', '1', '0.5', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class BufferedIndexInput extends IndexInput { public static final int BUFFER_SIZE = 1024 ; private int bufferSize = BUFFER_SIZE ; private byte [ ] buffer ; private long bufferStart = 0 ; private int bufferLength = 0 ; private int bufferPosition = 0 ; public byte readByte ( ) throws IOException { if ( bufferPosition >= bufferLength ) refill ( ) ; return buffer [ bufferPosition ++ ] ; } public BufferedIndexInput ( ) { } public BufferedIndexInput ( int bufferSize ) { checkBufferSize ( bufferSize ) ; this . bufferSize = bufferSize ; } public void setBufferSize ( int newSize ) { assert buffer == null || bufferSize == buffer . length ; if ( newSize != bufferSize ) { checkBufferSize ( newSize ) ; bufferSize = newSize ; if ( buffer != null ) { byte [ ] newBuffer = new byte [ newSize ] ; final int leftInBuffer = bufferLength - bufferPosition ; final int numToCopy ; if ( leftInBuffer > newSize ) numToCopy = newSize ; else numToCopy = leftInBuffer ; System . arraycopy ( buffer , bufferPosition , newBuffer , 0 , numToCopy ) ; bufferStart += bufferPosition ; bufferPosition = 0 ; bufferLength = numToCopy ; buffer = newBuffer ; } } } public int getBufferSize ( ) { return bufferSize ; } private void checkBufferSize ( int bufferSize ) { if ( bufferSize <= 0 ) throw new IllegalArgumentException ( "bufferSize must be greater than 0 (got " + bufferSize + ")" ) ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { if ( len <= ( bufferLength - bufferPosition ) ) { if ( len > 0 ) System . arraycopy ( buffer , bufferPosition , b , offset , len ) ; bufferPosition += len ; } else { int available = bufferLength - bufferPosition ; if ( available > 0 ) { System . arraycopy ( buffer , bufferPosition , b , offset , available ) ; offset += available ; len -= available ; bufferPosition += available ; } if ( len < bufferSize ) { refill ( ) ; if ( bufferLength < len ) { System . arraycopy ( buffer , 0 , b , offset , bufferLength ) ; throw new IOException ( "read past EOF" ) ; } else { System . arraycopy ( buffer , 0 , b , offset , len ) ; bufferPosition = len ; } } else { long after = bufferStart + bufferPosition + len ; if ( after > length ( ) ) throw new IOException ( "read past EOF" ) ; readInternal ( b , offset , len ) ; bufferStart = after ; bufferPosition = 0 ; bufferLength = 0 ; } } } private void refill ( ) throws IOException { long start = bufferStart + bufferPosition ; long end = start + bufferSize ; if ( end > length ( ) ) end = length ( ) ; bufferLength = ( int ) ( end - start ) ; if ( bufferLength <= 0 ) throw new IOException ( "read past EOF" ) ; if ( buffer == null ) { buffer = new byte [ bufferSize ] ; seekInternal ( bufferStart ) ; } readInternal ( buffer , 0 , bufferLength ) ; bufferStart = start ; bufferPosition = 0 ; } protected abstract void readInternal ( byte [ ] b , int offset , int length ) throws IOException ; public long getFilePointer ( ) { return bufferStart + bufferPosition ; } public void seek ( long pos ) throws IOException { if ( pos >= bufferStart && pos < ( bufferStart + bufferLength ) ) bufferPosition = ( int ) ( pos - bufferStart ) ; else { bufferStart = pos ; bufferPosition = 0 ; bufferLength = 0 ; seekInternal ( pos ) ; } } protected abstract void seekInternal ( long pos ) throws IOException ; public Object clone ( ) { BufferedIndexInput clone = ( BufferedIndexInput ) super . clone ( ) ; clone . buffer = null ; clone . bufferLength = 0 ; clone . bufferPosition = 0 ; clone . bufferStart = getFilePointer ( ) ; return clone ; } } 	1	['15', '2', '2', '4', '30', '21', '3', '1', '9', '0.705357143', '438', '0.625', '0', '0.538461538', '0.328571429', '1', '4', '27.66666667', '7', '1.2667', '3']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import org . apache . lucene . store . IndexInput ; class DefaultSkipListReader extends MultiLevelSkipListReader { private boolean currentFieldStoresPayloads ; private long freqPointer [ ] ; private long proxPointer [ ] ; private int payloadLength [ ] ; private long lastFreqPointer ; private long lastProxPointer ; private int lastPayloadLength ; DefaultSkipListReader ( IndexInput skipStream , int maxSkipLevels , int skipInterval ) { super ( skipStream , maxSkipLevels , skipInterval ) ; freqPointer = new long [ maxSkipLevels ] ; proxPointer = new long [ maxSkipLevels ] ; payloadLength = new int [ maxSkipLevels ] ; } void init ( long skipPointer , long freqBasePointer , long proxBasePointer , int df , boolean storesPayloads ) { super . init ( skipPointer , df ) ; this . currentFieldStoresPayloads = storesPayloads ; lastFreqPointer = freqBasePointer ; lastProxPointer = proxBasePointer ; Arrays . fill ( freqPointer , freqBasePointer ) ; Arrays . fill ( proxPointer , proxBasePointer ) ; Arrays . fill ( payloadLength , 0 ) ; } long getFreqPointer ( ) { return lastFreqPointer ; } long getProxPointer ( ) { return lastProxPointer ; } int getPayloadLength ( ) { return lastPayloadLength ; } protected void seekChild ( int level ) throws IOException { super . seekChild ( level ) ; freqPointer [ level ] = lastFreqPointer ; proxPointer [ level ] = lastProxPointer ; payloadLength [ level ] = lastPayloadLength ; } protected void setLastSkipData ( int level ) { super . setLastSkipData ( level ) ; lastFreqPointer = freqPointer [ level ] ; lastProxPointer = proxPointer [ level ] ; lastPayloadLength = payloadLength [ level ] ; } protected int readSkipData ( int level , IndexInput skipStream ) throws IOException { int delta ; if ( currentFieldStoresPayloads ) { delta = skipStream . readVInt ( ) ; if ( ( delta & 1 ) != 0 ) { payloadLength [ level ] = skipStream . readVInt ( ) ; } delta >>>= 1 ; } else { delta = skipStream . readVInt ( ) ; } freqPointer [ level ] += skipStream . readVInt ( ) ; proxPointer [ level ] += skipStream . readVInt ( ) ; return delta ; } } 	0	['8', '2', '0', '3', '15', '0', '1', '2', '0', '0.571428571', '158', '1', '0', '0.5625', '0.425', '1', '3', '17.875', '1', '0.875', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . Directory ; import org . apache . lucene . util . StringHelper ; final class TermInfosWriter { public static final int FORMAT = - 3 ; private FieldInfos fieldInfos ; private IndexOutput output ; private Term lastTerm = new Term ( "" , "" ) ; private TermInfo lastTi = new TermInfo ( ) ; private long size = 0 ; int indexInterval = 128 ; int skipInterval = 16 ; int maxSkipLevels = 10 ; private long lastIndexPointer = 0 ; private boolean isIndex = false ; private TermInfosWriter other = null ; TermInfosWriter ( Directory directory , String segment , FieldInfos fis , int interval ) throws IOException { initialize ( directory , segment , fis , interval , false ) ; other = new TermInfosWriter ( directory , segment , fis , interval , true ) ; other . other = this ; } private TermInfosWriter ( Directory directory , String segment , FieldInfos fis , int interval , boolean isIndex ) throws IOException { initialize ( directory , segment , fis , interval , isIndex ) ; } private void initialize ( Directory directory , String segment , FieldInfos fis , int interval , boolean isi ) throws IOException { indexInterval = interval ; fieldInfos = fis ; isIndex = isi ; output = directory . createOutput ( segment + ( isIndex ? ".tii" : ".tis" ) ) ; output . writeInt ( FORMAT ) ; output . writeLong ( 0 ) ; output . writeInt ( indexInterval ) ; output . writeInt ( skipInterval ) ; output . writeInt ( maxSkipLevels ) ; } final void add ( Term term , TermInfo ti ) throws CorruptIndexException , IOException { if ( ! isIndex && term . compareTo ( lastTerm ) <= 0 ) throw new CorruptIndexException ( "term out of order (\"" + term + "\".compareTo(\"" + lastTerm + "\") <= 0)" ) ; if ( ti . freqPointer < lastTi . freqPointer ) throw new CorruptIndexException ( "freqPointer out of order (" + ti . freqPointer + " < " + lastTi . freqPointer + ")" ) ; if ( ti . proxPointer < lastTi . proxPointer ) throw new CorruptIndexException ( "proxPointer out of order (" + ti . proxPointer + " < " + lastTi . proxPointer + ")" ) ; if ( ! isIndex && size % indexInterval == 0 ) other . add ( lastTerm , lastTi ) ; writeTerm ( term ) ; output . writeVInt ( ti . docFreq ) ; output . writeVLong ( ti . freqPointer - lastTi . freqPointer ) ; output . writeVLong ( ti . proxPointer - lastTi . proxPointer ) ; if ( ti . docFreq >= skipInterval ) { output . writeVInt ( ti . skipOffset ) ; } if ( isIndex ) { output . writeVLong ( other . output . getFilePointer ( ) - lastIndexPointer ) ; lastIndexPointer = other . output . getFilePointer ( ) ; } lastTi . set ( ti ) ; size ++ ; } private final void writeTerm ( Term term ) throws IOException { int start = StringHelper . stringDifference ( lastTerm . text , term . text ) ; int length = term . text . length ( ) - start ; output . writeVInt ( start ) ; output . writeVInt ( length ) ; output . writeChars ( term . text , start , length ) ; output . writeVInt ( fieldInfos . fieldNumber ( term . field ) ) ; lastTerm = term ; } final void close ( ) throws IOException { output . seek ( 4 ) ; output . writeLong ( size ) ; output . close ( ) ; if ( ! isIndex ) other . close ( ) ; } } 	1	['6', '1', '0', '9', '29', '0', '2', '7', '0', '0.45', '395', '0.666666667', '5', '0', '0.479166667', '0', '0', '62.83333333', '1', '0.6667', '3']
package org . apache . lucene . search ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . FieldSelector ; import org . apache . lucene . index . CorruptIndexException ; import org . apache . lucene . index . Term ; import java . io . IOException ; import java . util . HashMap ; import java . util . HashSet ; import java . util . Map ; import java . util . Set ; public class MultiSearcher extends Searcher { private static class CachedDfSource extends Searcher { private Map dfMap ; private int maxDoc ; public CachedDfSource ( Map dfMap , int maxDoc , Similarity similarity ) { this . dfMap = dfMap ; this . maxDoc = maxDoc ; setSimilarity ( similarity ) ; } public int docFreq ( Term term ) { int df ; try { df = ( ( Integer ) dfMap . get ( term ) ) . intValue ( ) ; } catch ( NullPointerException e ) { throw new IllegalArgumentException ( "df for term " + term . text ( ) + " not available" ) ; } return df ; } public int [ ] docFreqs ( Term [ ] terms ) { int [ ] result = new int [ terms . length ] ; for ( int i = 0 ; i < terms . length ; i ++ ) { result [ i ] = docFreq ( terms [ i ] ) ; } return result ; } public int maxDoc ( ) { return maxDoc ; } public Query rewrite ( Query query ) { return query ; } public void close ( ) { throw new UnsupportedOperationException ( ) ; } public Document doc ( int i ) { throw new UnsupportedOperationException ( ) ; } public Document doc ( int i , FieldSelector fieldSelector ) { throw new UnsupportedOperationException ( ) ; } public Explanation explain ( Weight weight , int doc ) { throw new UnsupportedOperationException ( ) ; } public void search ( Weight weight , Filter filter , HitCollector results ) { throw new UnsupportedOperationException ( ) ; } public TopDocs search ( Weight weight , Filter filter , int n ) { throw new UnsupportedOperationException ( ) ; } public TopFieldDocs search ( Weight weight , Filter filter , int n , Sort sort ) { throw new UnsupportedOperationException ( ) ; } } private Searchable [ ] searchables ; private int [ ] starts ; private int maxDoc = 0 ; public MultiSearcher ( Searchable [ ] searchables ) throws IOException { this . searchables = searchables ; starts = new int [ searchables . length + 1 ] ; for ( int i = 0 ; i < searchables . length ; i ++ ) { starts [ i ] = maxDoc ; maxDoc += searchables [ i ] . maxDoc ( ) ; } starts [ searchables . length ] = maxDoc ; } public Searchable [ ] getSearchables ( ) { return searchables ; } protected int [ ] getStarts ( ) { return starts ; } public void close ( ) throws IOException { for ( int i = 0 ; i < searchables . length ; i ++ ) searchables [ i ] . close ( ) ; } public int docFreq ( Term term ) throws IOException { int docFreq = 0 ; for ( int i = 0 ; i < searchables . length ; i ++ ) docFreq += searchables [ i ] . docFreq ( term ) ; return docFreq ; } public Document doc ( int n ) throws CorruptIndexException , IOException { int i = subSearcher ( n ) ; return searchables [ i ] . doc ( n - starts [ i ] ) ; } public Document doc ( int n , FieldSelector fieldSelector ) throws CorruptIndexException , IOException { int i = subSearcher ( n ) ; return searchables [ i ] . doc ( n - starts [ i ] , fieldSelector ) ; } public int subSearcher ( int n ) { int lo = 0 ; int hi = searchables . length - 1 ; while ( hi >= lo ) { int mid = ( lo + hi ) > > 1 ; int midValue = starts [ mid ] ; if ( n < midValue ) hi = mid - 1 ; else if ( n > midValue ) lo = mid + 1 ; else { while ( mid + 1 < searchables . length && starts [ mid + 1 ] == midValue ) { mid ++ ; } return mid ; } } return hi ; } public int subDoc ( int n ) { return n - starts [ subSearcher ( n ) ] ; } public int maxDoc ( ) throws IOException { return maxDoc ; } public TopDocs search ( Weight weight , Filter filter , int nDocs ) throws IOException { HitQueue hq = new HitQueue ( nDocs ) ; int totalHits = 0 ; for ( int i = 0 ; i < searchables . length ; i ++ ) { TopDocs docs = searchables [ i ] . search ( weight , filter , nDocs ) ; totalHits += docs . totalHits ; ScoreDoc [ ] scoreDocs = docs . scoreDocs ; for ( int j = 0 ; j < scoreDocs . length ; j ++ ) { ScoreDoc scoreDoc = scoreDocs [ j ] ; scoreDoc . doc += starts [ i ] ; if ( ! hq . insert ( scoreDoc ) ) break ; } } ScoreDoc [ ] scoreDocs = new ScoreDoc [ hq . size ( ) ] ; for ( int i = hq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = ( ScoreDoc ) hq . pop ( ) ; float maxScore = ( totalHits == 0 ) ? Float . NEGATIVE_INFINITY : scoreDocs [ 0 ] . score ; return new TopDocs ( totalHits , scoreDocs , maxScore ) ; } public TopFieldDocs search ( Weight weight , Filter filter , int n , Sort sort ) throws IOException { FieldDocSortedHitQueue hq = null ; int totalHits = 0 ; float maxScore = Float . NEGATIVE_INFINITY ; for ( int i = 0 ; i < searchables . length ; i ++ ) { TopFieldDocs docs = searchables [ i ] . search ( weight , filter , n , sort ) ; if ( hq == null ) hq = new FieldDocSortedHitQueue ( docs . fields , n ) ; totalHits += docs . totalHits ; maxScore = Math . max ( maxScore , docs . getMaxScore ( ) ) ; ScoreDoc [ ] scoreDocs = docs . scoreDocs ; for ( int j = 0 ; j < scoreDocs . length ; j ++ ) { ScoreDoc scoreDoc = scoreDocs [ j ] ; scoreDoc . doc += starts [ i ] ; if ( ! hq . insert ( scoreDoc ) ) break ; } } ScoreDoc [ ] scoreDocs = new ScoreDoc [ hq . size ( ) ] ; for ( int i = hq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = ( ScoreDoc ) hq . pop ( ) ; return new TopFieldDocs ( totalHits , scoreDocs , hq . getFields ( ) , maxScore ) ; } public void search ( Weight weight , Filter filter , final HitCollector results ) throws IOException { for ( int i = 0 ; i < searchables . length ; i ++ ) { final int start = starts [ i ] ; searchables [ i ] . search ( weight , filter , new HitCollector ( ) { public void collect ( int doc , float score ) { results . collect ( doc + start , score ) ; } } ) ; } } public Query rewrite ( Query original ) throws IOException { Query [ ] queries = new Query [ searchables . length ] ; for ( int i = 0 ; i < searchables . length ; i ++ ) { queries [ i ] = searchables [ i ] . rewrite ( original ) ; } return queries [ 0 ] . combine ( queries ) ; } public Explanation explain ( Weight weight , int doc ) throws IOException { int i = subSearcher ( doc ) ; return searchables [ i ] . explain ( weight , doc - starts [ i ] ) ; } protected Weight createWeight ( Query original ) throws IOException { Query rewrittenQuery = rewrite ( original ) ; Set terms = new HashSet ( ) ; rewrittenQuery . extractTerms ( terms ) ; Term [ ] allTermsArray = new Term [ terms . size ( ) ] ; terms . toArray ( allTermsArray ) ; int [ ] aggregatedDfs = new int [ terms . size ( ) ] ; for ( int i = 0 ; i < searchables . length ; i ++ ) { int [ ] dfs = searchables [ i ] . docFreqs ( allTermsArray ) ; for ( int j = 0 ; j < aggregatedDfs . length ; j ++ ) { aggregatedDfs [ j ] += dfs [ j ] ; } } HashMap dfMap = new HashMap ( ) ; for ( int i = 0 ; i < allTermsArray . length ; i ++ ) { dfMap . put ( allTermsArray [ i ] , new Integer ( aggregatedDfs [ i ] ) ) ; } int numDocs = maxDoc ( ) ; CachedDfSource cacheSim = new CachedDfSource ( dfMap , numDocs , getSimilarity ( ) ) ; return rewrittenQuery . weight ( cacheSim ) ; } } 	0	['16', '2', '1', '22', '53', '0', '2', '21', '14', '0.466666667', '577', '1', '1', '0.594594595', '0.23125', '1', '5', '34.875', '6', '1.25', '0']
package org . apache . lucene . search ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . FieldSelector ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . CorruptIndexException ; import java . io . IOException ; public interface Searchable extends java . rmi . Remote { void search ( Weight weight , Filter filter , HitCollector results ) throws IOException ; void close ( ) throws IOException ; int docFreq ( Term term ) throws IOException ; int [ ] docFreqs ( Term [ ] terms ) throws IOException ; int maxDoc ( ) throws IOException ; TopDocs search ( Weight weight , Filter filter , int n ) throws IOException ; Document doc ( int i ) throws CorruptIndexException , IOException ; Document doc ( int n , FieldSelector fieldSelector ) throws CorruptIndexException , IOException ; Query rewrite ( Query query ) throws IOException ; Explanation explain ( Weight weight , int doc ) throws IOException ; TopFieldDocs search ( Weight weight , Filter filter , int n , Sort sort ) throws IOException ; } 	1	['11', '1', '0', '18', '11', '55', '6', '12', '11', '2', '11', '0', '0', '0', '0.263636364', '0', '0', '0', '1', '1', '2']
package org . apache . lucene . analysis ; import java . io . BufferedReader ; import java . io . File ; import java . io . FileReader ; import java . io . IOException ; import java . io . Reader ; import java . util . HashMap ; import java . util . HashSet ; public class WordlistLoader { public static HashSet getWordSet ( File wordfile ) throws IOException { HashSet result = new HashSet ( ) ; FileReader reader = null ; try { reader = new FileReader ( wordfile ) ; result = getWordSet ( reader ) ; } finally { if ( reader != null ) reader . close ( ) ; } return result ; } public static HashSet getWordSet ( Reader reader ) throws IOException { HashSet result = new HashSet ( ) ; BufferedReader br = null ; try { if ( reader instanceof BufferedReader ) { br = ( BufferedReader ) reader ; } else { br = new BufferedReader ( reader ) ; } String word = null ; while ( ( word = br . readLine ( ) ) != null ) { result . add ( word . trim ( ) ) ; } } finally { if ( br != null ) br . close ( ) ; } return result ; } public static HashMap getStemDict ( File wordstemfile ) throws IOException { if ( wordstemfile == null ) throw new NullPointerException ( "wordstemfile may not be null" ) ; HashMap result = new HashMap ( ) ; BufferedReader br = null ; FileReader fr = null ; try { fr = new FileReader ( wordstemfile ) ; br = new BufferedReader ( fr ) ; String line ; while ( ( line = br . readLine ( ) ) != null ) { String [ ] wordstem = line . split ( "\t" , 2 ) ; result . put ( wordstem [ 0 ] , wordstem [ 1 ] ) ; } } finally { if ( fr != null ) fr . close ( ) ; if ( br != null ) br . close ( ) ; } return result ; } } 	0	['4', '1', '0', '2', '17', '6', '2', '0', '4', '2', '147', '0', '0', '0', '0.333333333', '0', '0', '35.75', '1', '0.75', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . * ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . MultipleTermPositions ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermPositions ; import org . apache . lucene . search . Query ; import org . apache . lucene . util . ToStringUtils ; public class MultiPhraseQuery extends Query { private String field ; private ArrayList termArrays = new ArrayList ( ) ; private Vector positions = new Vector ( ) ; private int slop = 0 ; public void setSlop ( int s ) { slop = s ; } public int getSlop ( ) { return slop ; } public void add ( Term term ) { add ( new Term [ ] { term } ) ; } public void add ( Term [ ] terms ) { int position = 0 ; if ( positions . size ( ) > 0 ) position = ( ( Integer ) positions . lastElement ( ) ) . intValue ( ) + 1 ; add ( terms , position ) ; } public void add ( Term [ ] terms , int position ) { if ( termArrays . size ( ) == 0 ) field = terms [ 0 ] . field ( ) ; for ( int i = 0 ; i < terms . length ; i ++ ) { if ( terms [ i ] . field ( ) != field ) { throw new IllegalArgumentException ( "All phrase terms must be in the same field (" + field + "): " + terms [ i ] ) ; } } termArrays . add ( terms ) ; positions . addElement ( new Integer ( position ) ) ; } public List getTermArrays ( ) { return Collections . unmodifiableList ( termArrays ) ; } public int [ ] getPositions ( ) { int [ ] result = new int [ positions . size ( ) ] ; for ( int i = 0 ; i < positions . size ( ) ; i ++ ) result [ i ] = ( ( Integer ) positions . elementAt ( i ) ) . intValue ( ) ; return result ; } public void extractTerms ( Set terms ) { for ( Iterator iter = termArrays . iterator ( ) ; iter . hasNext ( ) ; ) { Term [ ] arr = ( Term [ ] ) iter . next ( ) ; for ( int i = 0 ; i < arr . length ; i ++ ) { terms . add ( arr [ i ] ) ; } } } private class MultiPhraseWeight implements Weight { private Similarity similarity ; private float value ; private float idf ; private float queryNorm ; private float queryWeight ; public MultiPhraseWeight ( Searcher searcher ) throws IOException { this . similarity = getSimilarity ( searcher ) ; Iterator i = termArrays . iterator ( ) ; while ( i . hasNext ( ) ) { Term [ ] terms = ( Term [ ] ) i . next ( ) ; for ( int j = 0 ; j < terms . length ; j ++ ) { idf += getSimilarity ( searcher ) . idf ( terms [ j ] , searcher ) ; } } } public Query getQuery ( ) { return MultiPhraseQuery . this ; } public float getValue ( ) { return value ; } public float sumOfSquaredWeights ( ) { queryWeight = idf * getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float queryNorm ) { this . queryNorm = queryNorm ; queryWeight *= queryNorm ; value = queryWeight * idf ; } public Scorer scorer ( IndexReader reader ) throws IOException { if ( termArrays . size ( ) == 0 ) return null ; TermPositions [ ] tps = new TermPositions [ termArrays . size ( ) ] ; for ( int i = 0 ; i < tps . length ; i ++ ) { Term [ ] terms = ( Term [ ] ) termArrays . get ( i ) ; TermPositions p ; if ( terms . length > 1 ) p = new MultipleTermPositions ( reader , terms ) ; else p = reader . termPositions ( terms [ 0 ] ) ; if ( p == null ) return null ; tps [ i ] = p ; } if ( slop == 0 ) return new ExactPhraseScorer ( this , tps , getPositions ( ) , similarity , reader . norms ( field ) ) ; else return new SloppyPhraseScorer ( this , tps , getPositions ( ) , similarity , slop , reader . norms ( field ) ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { ComplexExplanation result = new ComplexExplanation ( ) ; result . setDescription ( "weight(" + getQuery ( ) + " in " + doc + "), product of:" ) ; Explanation idfExpl = new Explanation ( idf , "idf(" + getQuery ( ) + ")" ) ; Explanation queryExpl = new Explanation ( ) ; queryExpl . setDescription ( "queryWeight(" + getQuery ( ) + "), product of:" ) ; Explanation boostExpl = new Explanation ( getBoost ( ) , "boost" ) ; if ( getBoost ( ) != 1.0f ) queryExpl . addDetail ( boostExpl ) ; queryExpl . addDetail ( idfExpl ) ; Explanation queryNormExpl = new Explanation ( queryNorm , "queryNorm" ) ; queryExpl . addDetail ( queryNormExpl ) ; queryExpl . setValue ( boostExpl . getValue ( ) * idfExpl . getValue ( ) * queryNormExpl . getValue ( ) ) ; result . addDetail ( queryExpl ) ; ComplexExplanation fieldExpl = new ComplexExplanation ( ) ; fieldExpl . setDescription ( "fieldWeight(" + getQuery ( ) + " in " + doc + "), product of:" ) ; Explanation tfExpl = scorer ( reader ) . explain ( doc ) ; fieldExpl . addDetail ( tfExpl ) ; fieldExpl . addDetail ( idfExpl ) ; Explanation fieldNormExpl = new Explanation ( ) ; byte [ ] fieldNorms = reader . norms ( field ) ; float fieldNorm = fieldNorms != null ? Similarity . decodeNorm ( fieldNorms [ doc ] ) : 0.0f ; fieldNormExpl . setValue ( fieldNorm ) ; fieldNormExpl . setDescription ( "fieldNorm(field=" + field + ", doc=" + doc + ")" ) ; fieldExpl . addDetail ( fieldNormExpl ) ; fieldExpl . setMatch ( Boolean . valueOf ( tfExpl . isMatch ( ) ) ) ; fieldExpl . setValue ( tfExpl . getValue ( ) * idfExpl . getValue ( ) * fieldNormExpl . getValue ( ) ) ; result . addDetail ( fieldExpl ) ; result . setMatch ( fieldExpl . getMatch ( ) ) ; result . setValue ( queryExpl . getValue ( ) * fieldExpl . getValue ( ) ) ; if ( queryExpl . getValue ( ) == 1.0f ) return fieldExpl ; return result ; } } public Query rewrite ( IndexReader reader ) { if ( termArrays . size ( ) == 1 ) { Term [ ] terms = ( Term [ ] ) termArrays . get ( 0 ) ; BooleanQuery boq = new BooleanQuery ( true ) ; for ( int i = 0 ; i < terms . length ; i ++ ) { boq . add ( new TermQuery ( terms [ i ] ) , BooleanClause . Occur . SHOULD ) ; } boq . setBoost ( getBoost ( ) ) ; return boq ; } else { return this ; } } protected Weight createWeight ( Searcher searcher ) throws IOException { return new MultiPhraseWeight ( searcher ) ; } public final String toString ( String f ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! field . equals ( f ) ) { buffer . append ( field ) ; buffer . append ( ":" ) ; } buffer . append ( "\"" ) ; Iterator i = termArrays . iterator ( ) ; while ( i . hasNext ( ) ) { Term [ ] terms = ( Term [ ] ) i . next ( ) ; if ( terms . length > 1 ) { buffer . append ( "(" ) ; for ( int j = 0 ; j < terms . length ; j ++ ) { buffer . append ( terms [ j ] . text ( ) ) ; if ( j < terms . length - 1 ) buffer . append ( " " ) ; } buffer . append ( ")" ) ; } else { buffer . append ( terms [ 0 ] . text ( ) ) ; } if ( i . hasNext ( ) ) buffer . append ( " " ) ; } buffer . append ( "\"" ) ; if ( slop != 0 ) { buffer . append ( "~" ) ; buffer . append ( slop ) ; } buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof MultiPhraseQuery ) ) return false ; MultiPhraseQuery other = ( MultiPhraseQuery ) o ; return this . getBoost ( ) == other . getBoost ( ) && this . slop == other . slop && this . termArrays . equals ( other . termArrays ) && this . positions . equals ( other . positions ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ slop ^ termArrays . hashCode ( ) ^ positions . hashCode ( ) ^ 0x4AC65113 ; } } 	1	['17', '2', '0', '12', '55', '12', '3', '10', '13', '0.625', '407', '1', '0', '0.428571429', '0.158823529', '2', '4', '22.70588235', '7', '2.0588', '2']
package org . apache . lucene . search . spans ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermPositions ; import java . io . IOException ; public class TermSpans implements Spans { protected TermPositions positions ; protected Term term ; protected int doc ; protected int freq ; protected int count ; protected int position ; public TermSpans ( TermPositions positions , Term term ) throws IOException { this . positions = positions ; this . term = term ; doc = - 1 ; } public boolean next ( ) throws IOException { if ( count == freq ) { if ( ! positions . next ( ) ) { doc = Integer . MAX_VALUE ; return false ; } doc = positions . doc ( ) ; freq = positions . freq ( ) ; count = 0 ; } position = positions . nextPosition ( ) ; count ++ ; return true ; } public boolean skipTo ( int target ) throws IOException { if ( doc >= target ) { return true ; } if ( ! positions . skipTo ( target ) ) { doc = Integer . MAX_VALUE ; return false ; } doc = positions . doc ( ) ; freq = positions . freq ( ) ; count = 0 ; position = positions . nextPosition ( ) ; count ++ ; return true ; } public int doc ( ) { return doc ; } public int start ( ) { return position ; } public int end ( ) { return position + 1 ; } public String toString ( ) { return "spans(" + term . toString ( ) + ")@" + ( doc == - 1 ? "START" : ( doc == Integer . MAX_VALUE ) ? "END" : doc + "-" + position ) ; } public TermPositions getPositions ( ) { return positions ; } } 	0	['8', '1', '0', '6', '19', '0', '3', '3', '8', '0.666666667', '160', '1', '2', '0', '0.34375', '0', '0', '18.25', '3', '1.125', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class IndexOutput { public abstract void writeByte ( byte b ) throws IOException ; public void writeBytes ( byte [ ] b , int length ) throws IOException { writeBytes ( b , 0 , length ) ; } public abstract void writeBytes ( byte [ ] b , int offset , int length ) throws IOException ; public void writeInt ( int i ) throws IOException { writeByte ( ( byte ) ( i > > 24 ) ) ; writeByte ( ( byte ) ( i > > 16 ) ) ; writeByte ( ( byte ) ( i > > 8 ) ) ; writeByte ( ( byte ) i ) ; } public void writeVInt ( int i ) throws IOException { while ( ( i & ~ 0x7F ) != 0 ) { writeByte ( ( byte ) ( ( i & 0x7f ) | 0x80 ) ) ; i >>>= 7 ; } writeByte ( ( byte ) i ) ; } public void writeLong ( long i ) throws IOException { writeInt ( ( int ) ( i > > 32 ) ) ; writeInt ( ( int ) i ) ; } public void writeVLong ( long i ) throws IOException { while ( ( i & ~ 0x7F ) != 0 ) { writeByte ( ( byte ) ( ( i & 0x7f ) | 0x80 ) ) ; i >>>= 7 ; } writeByte ( ( byte ) i ) ; } public void writeString ( String s ) throws IOException { int length = s . length ( ) ; writeVInt ( length ) ; writeChars ( s , 0 , length ) ; } public void writeChars ( String s , int start , int length ) throws IOException { final int end = start + length ; for ( int i = start ; i < end ; i ++ ) { final int code = ( int ) s . charAt ( i ) ; if ( code >= 0x01 && code <= 0x7F ) writeByte ( ( byte ) code ) ; else if ( ( ( code >= 0x80 ) && ( code <= 0x7FF ) ) || code == 0 ) { writeByte ( ( byte ) ( 0xC0 | ( code > > 6 ) ) ) ; writeByte ( ( byte ) ( 0x80 | ( code & 0x3F ) ) ) ; } else { writeByte ( ( byte ) ( 0xE0 | ( code > > > 12 ) ) ) ; writeByte ( ( byte ) ( 0x80 | ( ( code > > 6 ) & 0x3F ) ) ) ; writeByte ( ( byte ) ( 0x80 | ( code & 0x3F ) ) ) ; } } } public abstract void flush ( ) throws IOException ; public abstract void close ( ) throws IOException ; public abstract long getFilePointer ( ) ; public abstract void seek ( long pos ) throws IOException ; public abstract long length ( ) throws IOException ; } 	1	['15', '1', '2', '19', '18', '105', '19', '0', '15', '2', '196', '0', '0', '0', '0.311111111', '0', '0', '12.06666667', '1', '0.9333', '2']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . List ; import java . util . ArrayList ; import java . util . Iterator ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Query ; import org . apache . lucene . util . ToStringUtils ; public class SpanNearQuery extends SpanQuery { private List clauses ; private int slop ; private boolean inOrder ; private String field ; public SpanNearQuery ( SpanQuery [ ] clauses , int slop , boolean inOrder ) { this . clauses = new ArrayList ( clauses . length ) ; for ( int i = 0 ; i < clauses . length ; i ++ ) { SpanQuery clause = clauses [ i ] ; if ( i == 0 ) { field = clause . getField ( ) ; } else if ( ! clause . getField ( ) . equals ( field ) ) { throw new IllegalArgumentException ( "Clauses must have same field." ) ; } this . clauses . add ( clause ) ; } this . slop = slop ; this . inOrder = inOrder ; } public SpanQuery [ ] getClauses ( ) { return ( SpanQuery [ ] ) clauses . toArray ( new SpanQuery [ clauses . size ( ) ] ) ; } public int getSlop ( ) { return slop ; } public boolean isInOrder ( ) { return inOrder ; } public String getField ( ) { return field ; } public Collection getTerms ( ) { Collection terms = new ArrayList ( ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; terms . addAll ( clause . getTerms ( ) ) ; } return terms ; } public void extractTerms ( Set terms ) { Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; clause . extractTerms ( terms ) ; } } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "spanNear([" ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; buffer . append ( clause . toString ( field ) ) ; if ( i . hasNext ( ) ) { buffer . append ( ", " ) ; } } buffer . append ( "], " ) ; buffer . append ( slop ) ; buffer . append ( ", " ) ; buffer . append ( inOrder ) ; buffer . append ( ")" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public Spans getSpans ( final IndexReader reader ) throws IOException { if ( clauses . size ( ) == 0 ) return new SpanOrQuery ( getClauses ( ) ) . getSpans ( reader ) ; if ( clauses . size ( ) == 1 ) return ( ( SpanQuery ) clauses . get ( 0 ) ) . getSpans ( reader ) ; return inOrder ? ( Spans ) new NearSpansOrdered ( this , reader ) : ( Spans ) new NearSpansUnordered ( this , reader ) ; } public Query rewrite ( IndexReader reader ) throws IOException { SpanNearQuery clone = null ; for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { SpanQuery c = ( SpanQuery ) clauses . get ( i ) ; SpanQuery query = ( SpanQuery ) c . rewrite ( reader ) ; if ( query != c ) { if ( clone == null ) clone = ( SpanNearQuery ) this . clone ( ) ; clone . clauses . set ( i , query ) ; } } if ( clone != null ) { return clone ; } else { return this ; } } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof SpanNearQuery ) ) return false ; final SpanNearQuery spanNearQuery = ( SpanNearQuery ) o ; if ( inOrder != spanNearQuery . inOrder ) return false ; if ( slop != spanNearQuery . slop ) return false ; if ( ! clauses . equals ( spanNearQuery . clauses ) ) return false ; return getBoost ( ) == spanNearQuery . getBoost ( ) ; } public int hashCode ( ) { int result ; result = clauses . hashCode ( ) ; result ^= ( result << 14 ) | ( result > > > 19 ) ; result += Float . floatToRawIntBits ( getBoost ( ) ) ; result += slop ; result ^= ( inOrder ? 0x99AFD3BD : 0 ) ; return result ; } } 	0	['12', '3', '0', '8', '47', '0', '2', '8', '12', '0.590909091', '353', '1', '0', '0.592592593', '0.208333333', '2', '2', '28.08333333', '7', '1.75', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; public final class LowerCaseFilter extends TokenFilter { public LowerCaseFilter ( TokenStream in ) { super ( in ) ; } public final Token next ( ) throws IOException { Token t = input . next ( ) ; if ( t == null ) return null ; t . termText = t . termText . toLowerCase ( ) ; return t ; } } 	1	['2', '3', '0', '4', '5', '1', '1', '3', '2', '2', '21', '0', '0', '0.8', '0.75', '0', '0', '9.5', '1', '0.5', '1']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . * ; final class PhrasePositions { int doc ; int position ; int count ; int offset ; TermPositions tp ; PhrasePositions next ; boolean repeats ; PhrasePositions ( TermPositions t , int o ) { tp = t ; offset = o ; } final boolean next ( ) throws IOException { if ( ! tp . next ( ) ) { tp . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } doc = tp . doc ( ) ; position = 0 ; return true ; } final boolean skipTo ( int target ) throws IOException { if ( ! tp . skipTo ( target ) ) { tp . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } doc = tp . doc ( ) ; position = 0 ; return true ; } final void firstPosition ( ) throws IOException { count = tp . freq ( ) ; nextPosition ( ) ; } final boolean nextPosition ( ) throws IOException { if ( count -- > 0 ) { position = tp . nextPosition ( ) - offset ; return true ; } else return false ; } } 	0	['5', '1', '0', '6', '12', '0', '5', '1', '0', '0.678571429', '95', '0', '2', '0', '0.533333333', '0', '0', '16.6', '1', '0.8', '0']
package org . apache . lucene . index ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . FieldSelector ; import org . apache . lucene . store . Directory ; import java . io . IOException ; import java . util . Collection ; import java . util . HashSet ; import java . util . Hashtable ; import java . util . Set ; public class MultiReader extends IndexReader { private IndexReader [ ] subReaders ; private int [ ] starts ; private Hashtable normsCache = new Hashtable ( ) ; private int maxDoc = 0 ; private int numDocs = - 1 ; private boolean hasDeletions = false ; public MultiReader ( IndexReader [ ] subReaders ) throws IOException { super ( subReaders . length == 0 ? null : subReaders [ 0 ] . directory ( ) ) ; initialize ( subReaders ) ; } MultiReader ( Directory directory , SegmentInfos sis , boolean closeDirectory , IndexReader [ ] subReaders ) { super ( directory , sis , closeDirectory ) ; initialize ( subReaders ) ; } private void initialize ( IndexReader [ ] subReaders ) { this . subReaders = subReaders ; starts = new int [ subReaders . length + 1 ] ; for ( int i = 0 ; i < subReaders . length ; i ++ ) { starts [ i ] = maxDoc ; maxDoc += subReaders [ i ] . maxDoc ( ) ; if ( subReaders [ i ] . hasDeletions ( ) ) hasDeletions = true ; } starts [ subReaders . length ] = maxDoc ; } public TermFreqVector [ ] getTermFreqVectors ( int n ) throws IOException { ensureOpen ( ) ; int i = readerIndex ( n ) ; return subReaders [ i ] . getTermFreqVectors ( n - starts [ i ] ) ; } public TermFreqVector getTermFreqVector ( int n , String field ) throws IOException { ensureOpen ( ) ; int i = readerIndex ( n ) ; return subReaders [ i ] . getTermFreqVector ( n - starts [ i ] , field ) ; } public synchronized int numDocs ( ) { if ( numDocs == - 1 ) { int n = 0 ; for ( int i = 0 ; i < subReaders . length ; i ++ ) n += subReaders [ i ] . numDocs ( ) ; numDocs = n ; } return numDocs ; } public int maxDoc ( ) { return maxDoc ; } public Document document ( int n , FieldSelector fieldSelector ) throws CorruptIndexException , IOException { ensureOpen ( ) ; int i = readerIndex ( n ) ; return subReaders [ i ] . document ( n - starts [ i ] , fieldSelector ) ; } public boolean isDeleted ( int n ) { int i = readerIndex ( n ) ; return subReaders [ i ] . isDeleted ( n - starts [ i ] ) ; } public boolean hasDeletions ( ) { return hasDeletions ; } protected void doDelete ( int n ) throws CorruptIndexException , IOException { numDocs = - 1 ; int i = readerIndex ( n ) ; subReaders [ i ] . deleteDocument ( n - starts [ i ] ) ; hasDeletions = true ; } protected void doUndeleteAll ( ) throws CorruptIndexException , IOException { for ( int i = 0 ; i < subReaders . length ; i ++ ) subReaders [ i ] . undeleteAll ( ) ; hasDeletions = false ; numDocs = - 1 ; } private int readerIndex ( int n ) { int lo = 0 ; int hi = subReaders . length - 1 ; while ( hi >= lo ) { int mid = ( lo + hi ) > > 1 ; int midValue = starts [ mid ] ; if ( n < midValue ) hi = mid - 1 ; else if ( n > midValue ) lo = mid + 1 ; else { while ( mid + 1 < subReaders . length && starts [ mid + 1 ] == midValue ) { mid ++ ; } return mid ; } } return hi ; } public boolean hasNorms ( String field ) throws IOException { ensureOpen ( ) ; for ( int i = 0 ; i < subReaders . length ; i ++ ) { if ( subReaders [ i ] . hasNorms ( field ) ) return true ; } return false ; } private byte [ ] ones ; private byte [ ] fakeNorms ( ) { if ( ones == null ) ones = SegmentReader . createFakeNorms ( maxDoc ( ) ) ; return ones ; } public synchronized byte [ ] norms ( String field ) throws IOException { ensureOpen ( ) ; byte [ ] bytes = ( byte [ ] ) normsCache . get ( field ) ; if ( bytes != null ) return bytes ; if ( ! hasNorms ( field ) ) return fakeNorms ( ) ; bytes = new byte [ maxDoc ( ) ] ; for ( int i = 0 ; i < subReaders . length ; i ++ ) subReaders [ i ] . norms ( field , bytes , starts [ i ] ) ; normsCache . put ( field , bytes ) ; return bytes ; } public synchronized void norms ( String field , byte [ ] result , int offset ) throws IOException { ensureOpen ( ) ; byte [ ] bytes = ( byte [ ] ) normsCache . get ( field ) ; if ( bytes == null && ! hasNorms ( field ) ) bytes = fakeNorms ( ) ; if ( bytes != null ) System . arraycopy ( bytes , 0 , result , offset , maxDoc ( ) ) ; for ( int i = 0 ; i < subReaders . length ; i ++ ) subReaders [ i ] . norms ( field , result , offset + starts [ i ] ) ; } protected void doSetNorm ( int n , String field , byte value ) throws CorruptIndexException , IOException { normsCache . remove ( field ) ; int i = readerIndex ( n ) ; subReaders [ i ] . setNorm ( n - starts [ i ] , field , value ) ; } public TermEnum terms ( ) throws IOException { ensureOpen ( ) ; return new MultiTermEnum ( subReaders , starts , null ) ; } public TermEnum terms ( Term term ) throws IOException { ensureOpen ( ) ; return new MultiTermEnum ( subReaders , starts , term ) ; } public int docFreq ( Term t ) throws IOException { ensureOpen ( ) ; int total = 0 ; for ( int i = 0 ; i < subReaders . length ; i ++ ) total += subReaders [ i ] . docFreq ( t ) ; return total ; } public TermDocs termDocs ( ) throws IOException { ensureOpen ( ) ; return new MultiTermDocs ( subReaders , starts ) ; } public TermPositions termPositions ( ) throws IOException { ensureOpen ( ) ; return new MultiTermPositions ( subReaders , starts ) ; } protected void doCommit ( ) throws IOException { for ( int i = 0 ; i < subReaders . length ; i ++ ) subReaders [ i ] . commit ( ) ; } void startCommit ( ) { super . startCommit ( ) ; for ( int i = 0 ; i < subReaders . length ; i ++ ) { subReaders [ i ] . startCommit ( ) ; } } void rollbackCommit ( ) { super . rollbackCommit ( ) ; for ( int i = 0 ; i < subReaders . length ; i ++ ) { subReaders [ i ] . rollbackCommit ( ) ; } } protected synchronized void doClose ( ) throws IOException { for ( int i = 0 ; i < subReaders . length ; i ++ ) subReaders [ i ] . close ( ) ; } public Collection getFieldNames ( IndexReader . FieldOption fieldNames ) { ensureOpen ( ) ; Set fieldSet = new HashSet ( ) ; for ( int i = 0 ; i < subReaders . length ; i ++ ) { IndexReader reader = subReaders [ i ] ; Collection names = reader . getFieldNames ( fieldNames ) ; fieldSet . addAll ( names ) ; } return fieldSet ; } } class MultiTermEnum extends TermEnum { private SegmentMergeQueue queue ; private Term term ; private int docFreq ; public MultiTermEnum ( IndexReader [ ] readers , int [ ] starts , Term t ) throws IOException { queue = new SegmentMergeQueue ( readers . length ) ; for ( int i = 0 ; i < readers . length ; i ++ ) { IndexReader reader = readers [ i ] ; TermEnum termEnum ; if ( t != null ) { termEnum = reader . terms ( t ) ; } else termEnum = reader . terms ( ) ; SegmentMergeInfo smi = new SegmentMergeInfo ( starts [ i ] , termEnum , reader ) ; if ( t == null ? smi . next ( ) : termEnum . term ( ) != null ) queue . put ( smi ) ; else smi . close ( ) ; } if ( t != null && queue . size ( ) > 0 ) { next ( ) ; } } public boolean next ( ) throws IOException { SegmentMergeInfo top = ( SegmentMergeInfo ) queue . top ( ) ; if ( top == null ) { term = null ; return false ; } term = top . term ; docFreq = 0 ; while ( top != null && term . compareTo ( top . term ) == 0 ) { queue . pop ( ) ; docFreq += top . termEnum . docFreq ( ) ; if ( top . next ( ) ) queue . put ( top ) ; else top . close ( ) ; top = ( SegmentMergeInfo ) queue . top ( ) ; } return true ; } public Term term ( ) { return term ; } public int docFreq ( ) { return docFreq ; } public void close ( ) throws IOException { queue . close ( ) ; } } class MultiTermDocs implements TermDocs { protected IndexReader [ ] readers ; protected int [ ] starts ; protected Term term ; protected int base = 0 ; protected int pointer = 0 ; private TermDocs [ ] readerTermDocs ; protected TermDocs current ; public MultiTermDocs ( IndexReader [ ] r , int [ ] s ) { readers = r ; starts = s ; readerTermDocs = new TermDocs [ r . length ] ; } public int doc ( ) { return base + current . doc ( ) ; } public int freq ( ) { return current . freq ( ) ; } public void seek ( Term term ) { this . term = term ; this . base = 0 ; this . pointer = 0 ; this . current = null ; } public void seek ( TermEnum termEnum ) throws IOException { seek ( termEnum . term ( ) ) ; } public boolean next ( ) throws IOException { for ( ; ; ) { if ( current != null && current . next ( ) ) { return true ; } else if ( pointer < readers . length ) { base = starts [ pointer ] ; current = termDocs ( pointer ++ ) ; } else { return false ; } } } public int read ( final int [ ] docs , final int [ ] freqs ) throws IOException { while ( true ) { while ( current == null ) { if ( pointer < readers . length ) { base = starts [ pointer ] ; current = termDocs ( pointer ++ ) ; } else { return 0 ; } } int end = current . read ( docs , freqs ) ; if ( end == 0 ) { current = null ; } else { final int b = base ; for ( int i = 0 ; i < end ; i ++ ) docs [ i ] += b ; return end ; } } } public boolean skipTo ( int target ) throws IOException { for ( ; ; ) { if ( current != null && current . skipTo ( target - base ) ) { return true ; } else if ( pointer < readers . length ) { base = starts [ pointer ] ; current = termDocs ( pointer ++ ) ; } else return false ; } } private TermDocs termDocs ( int i ) throws IOException { if ( term == null ) return null ; TermDocs result = readerTermDocs [ i ] ; if ( result == null ) result = readerTermDocs [ i ] = termDocs ( readers [ i ] ) ; result . seek ( term ) ; return result ; } protected TermDocs termDocs ( IndexReader reader ) throws IOException { return reader . termDocs ( ) ; } public void close ( ) throws IOException { for ( int i = 0 ; i < readerTermDocs . length ; i ++ ) { if ( readerTermDocs [ i ] != null ) readerTermDocs [ i ] . close ( ) ; } } } class MultiTermPositions extends MultiTermDocs implements TermPositions { public MultiTermPositions ( IndexReader [ ] r , int [ ] s ) { super ( r , s ) ; } protected TermDocs termDocs ( IndexReader reader ) throws IOException { return ( TermDocs ) reader . termPositions ( ) ; } public int nextPosition ( ) throws IOException { return ( ( TermPositions ) current ) . nextPosition ( ) ; } public int getPayloadLength ( ) { return ( ( TermPositions ) current ) . getPayloadLength ( ) ; } public byte [ ] getPayload ( byte [ ] data , int offset ) throws IOException { return ( ( TermPositions ) current ) . getPayload ( data , offset ) ; } public boolean isPayloadAvailable ( ) { return ( ( TermPositions ) current ) . isPayloadAvailable ( ) ; } } 	1	['28', '2', '0', '17', '61', '0', '1', '16', '17', '0.682539683', '664', '1', '1', '0.697674419', '0.157738095', '1', '9', '22.46428571', '6', '1.3929', '4']
package org . apache . lucene . document ; import java . io . Serializable ; public interface FieldSelector extends Serializable { FieldSelectorResult accept ( String fieldName ) ; } 	0	['1', '1', '0', '18', '1', '0', '17', '1', '1', '2', '1', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search . spans ; import org . apache . lucene . search . Explanation ; import org . apache . lucene . search . Scorer ; import org . apache . lucene . search . Similarity ; import org . apache . lucene . search . Weight ; import java . io . IOException ; public class SpanScorer extends Scorer { protected Spans spans ; protected Weight weight ; protected byte [ ] norms ; protected float value ; protected boolean firstTime = true ; protected boolean more = true ; protected int doc ; protected float freq ; protected SpanScorer ( Spans spans , Weight weight , Similarity similarity , byte [ ] norms ) throws IOException { super ( similarity ) ; this . spans = spans ; this . norms = norms ; this . weight = weight ; this . value = weight . getValue ( ) ; doc = - 1 ; } public boolean next ( ) throws IOException { if ( firstTime ) { more = spans . next ( ) ; firstTime = false ; } return setFreqCurrentDoc ( ) ; } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { more = spans . skipTo ( target ) ; firstTime = false ; } if ( ! more ) { return false ; } if ( spans . doc ( ) < target ) { more = spans . skipTo ( target ) ; } return setFreqCurrentDoc ( ) ; } protected boolean setFreqCurrentDoc ( ) throws IOException { if ( ! more ) { return false ; } doc = spans . doc ( ) ; freq = 0.0f ; while ( more && doc == spans . doc ( ) ) { int matchLength = spans . end ( ) - spans . start ( ) ; freq += getSimilarity ( ) . sloppyFreq ( matchLength ) ; more = spans . next ( ) ; } return more || ( freq != 0 ) ; } public int doc ( ) { return doc ; } public float score ( ) throws IOException { float raw = getSimilarity ( ) . tf ( freq ) * value ; return raw * Similarity . decodeNorm ( norms [ doc ] ) ; } public Explanation explain ( final int doc ) throws IOException { Explanation tfExplanation = new Explanation ( ) ; skipTo ( doc ) ; float phraseFreq = ( doc ( ) == doc ) ? freq : 0.0f ; tfExplanation . setValue ( getSimilarity ( ) . tf ( phraseFreq ) ) ; tfExplanation . setDescription ( "tf(phraseFreq=" + phraseFreq + ")" ) ; return tfExplanation ; } } 	1	['7', '2', '1', '7', '25', '0', '2', '5', '5', '0.520833333', '201', '1', '2', '0.571428571', '0.30952381', '1', '3', '26.57142857', '1', '0.8571', '1']
package org . apache . lucene . search . function ; public class FieldScoreQuery extends ValueSourceQuery { public static class Type { public static final Type BYTE = new Type ( "byte" ) ; public static final Type SHORT = new Type ( "short" ) ; public static final Type INT = new Type ( "int" ) ; public static final Type FLOAT = new Type ( "float" ) ; private String typeName ; private Type ( String name ) { this . typeName = name ; } public String toString ( ) { return getClass ( ) . getName ( ) + "::" + typeName ; } } public FieldScoreQuery ( String field , Type type ) { super ( getValueSource ( field , type ) ) ; } private static ValueSource getValueSource ( String field , Type type ) { if ( type == Type . BYTE ) { return new ByteFieldSource ( field ) ; } if ( type == Type . SHORT ) { return new ShortFieldSource ( field ) ; } if ( type == Type . INT ) { return new IntFieldSource ( field ) ; } if ( type == Type . FLOAT ) { return new FloatFieldSource ( field ) ; } throw new IllegalArgumentException ( type + " is not a known Field Score Query Type!" ) ; } } 	0	['2', '3', '0', '7', '12', '1', '0', '7', '1', '2', '52', '0', '0', '0.947368421', '0.833333333', '0', '0', '25', '5', '2.5', '0']
package org . apache . lucene . document ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Hits ; import org . apache . lucene . search . Searcher ; import java . util . * ; public final class Document implements java . io . Serializable { List fields = new Vector ( ) ; private float boost = 1.0f ; public Document ( ) { } public void setBoost ( float boost ) { this . boost = boost ; } public float getBoost ( ) { return boost ; } public final void add ( Fieldable field ) { fields . add ( field ) ; } public final void removeField ( String name ) { Iterator it = fields . iterator ( ) ; while ( it . hasNext ( ) ) { Fieldable field = ( Fieldable ) it . next ( ) ; if ( field . name ( ) . equals ( name ) ) { it . remove ( ) ; return ; } } } public final void removeFields ( String name ) { Iterator it = fields . iterator ( ) ; while ( it . hasNext ( ) ) { Fieldable field = ( Fieldable ) it . next ( ) ; if ( field . name ( ) . equals ( name ) ) { it . remove ( ) ; } } } public final Field getField ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Field field = ( Field ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) return field ; } return null ; } public Fieldable getFieldable ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) return field ; } return null ; } public final String get ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( ! field . isBinary ( ) ) ) return field . stringValue ( ) ; } return null ; } public final Enumeration fields ( ) { return ( ( Vector ) fields ) . elements ( ) ; } public final List getFields ( ) { return fields ; } public final Field [ ] getFields ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Field field = ( Field ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) { result . add ( field ) ; } } if ( result . size ( ) == 0 ) return null ; return ( Field [ ] ) result . toArray ( new Field [ result . size ( ) ] ) ; } public Fieldable [ ] getFieldables ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) { result . add ( field ) ; } } if ( result . size ( ) == 0 ) return null ; return ( Fieldable [ ] ) result . toArray ( new Fieldable [ result . size ( ) ] ) ; } public final String [ ] getValues ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( ! field . isBinary ( ) ) ) result . add ( field . stringValue ( ) ) ; } if ( result . size ( ) == 0 ) return null ; return ( String [ ] ) result . toArray ( new String [ result . size ( ) ] ) ; } public final byte [ ] [ ] getBinaryValues ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( field . isBinary ( ) ) ) result . add ( field . binaryValue ( ) ) ; } if ( result . size ( ) == 0 ) return null ; return ( byte [ ] [ ] ) result . toArray ( new byte [ result . size ( ) ] [ ] ) ; } public final byte [ ] getBinaryValue ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( field . isBinary ( ) ) ) return field . binaryValue ( ) ; } return null ; } public final String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "Document<" ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; buffer . append ( field . toString ( ) ) ; if ( i != fields . size ( ) - 1 ) buffer . append ( " " ) ; } buffer . append ( ">" ) ; return buffer . toString ( ) ; } } 	1	['17', '1', '0', '24', '39', '0', '22', '2', '17', '0.5', '415', '0.5', '0', '0', '0.426470588', '0', '0', '23.29411765', '5', '2.5882', '5']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . HashSet ; import java . util . Iterator ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; public abstract class Query implements java . io . Serializable , Cloneable { private float boost = 1.0f ; public void setBoost ( float b ) { boost = b ; } public float getBoost ( ) { return boost ; } public abstract String toString ( String field ) ; public String toString ( ) { return toString ( "" ) ; } protected Weight createWeight ( Searcher searcher ) throws IOException { throw new UnsupportedOperationException ( ) ; } public Weight weight ( Searcher searcher ) throws IOException { Query query = searcher . rewrite ( this ) ; Weight weight = query . createWeight ( searcher ) ; float sum = weight . sumOfSquaredWeights ( ) ; float norm = getSimilarity ( searcher ) . queryNorm ( sum ) ; weight . normalize ( norm ) ; return weight ; } public Query rewrite ( IndexReader reader ) throws IOException { return this ; } public Query combine ( Query [ ] queries ) { HashSet uniques = new HashSet ( ) ; for ( int i = 0 ; i < queries . length ; i ++ ) { Query query = queries [ i ] ; BooleanClause [ ] clauses = null ; boolean splittable = ( query instanceof BooleanQuery ) ; if ( splittable ) { BooleanQuery bq = ( BooleanQuery ) query ; splittable = bq . isCoordDisabled ( ) ; clauses = bq . getClauses ( ) ; for ( int j = 0 ; splittable && j < clauses . length ; j ++ ) { splittable = ( clauses [ j ] . getOccur ( ) == BooleanClause . Occur . SHOULD ) ; } } if ( splittable ) { for ( int j = 0 ; j < clauses . length ; j ++ ) { uniques . add ( clauses [ j ] . getQuery ( ) ) ; } } else { uniques . add ( query ) ; } } if ( uniques . size ( ) == 1 ) { return ( Query ) uniques . iterator ( ) . next ( ) ; } Iterator it = uniques . iterator ( ) ; BooleanQuery result = new BooleanQuery ( true ) ; while ( it . hasNext ( ) ) result . add ( ( Query ) it . next ( ) , BooleanClause . Occur . SHOULD ) ; return result ; } public void extractTerms ( Set terms ) { throw new UnsupportedOperationException ( ) ; } public static Query mergeBooleanQueries ( Query [ ] queries ) { HashSet allClauses = new HashSet ( ) ; for ( int i = 0 ; i < queries . length ; i ++ ) { BooleanClause [ ] clauses = ( ( BooleanQuery ) queries [ i ] ) . getClauses ( ) ; for ( int j = 0 ; j < clauses . length ; j ++ ) { allClauses . add ( clauses [ j ] ) ; } } boolean coordDisabled = queries . length == 0 ? false : ( ( BooleanQuery ) queries [ 0 ] ) . isCoordDisabled ( ) ; BooleanQuery result = new BooleanQuery ( coordDisabled ) ; Iterator i = allClauses . iterator ( ) ; while ( i . hasNext ( ) ) { result . add ( ( BooleanClause ) i . next ( ) ) ; } return result ; } public Similarity getSimilarity ( Searcher searcher ) { return searcher . getSimilarity ( ) ; } public Object clone ( ) { try { return ( Query ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { throw new RuntimeException ( "Clone not supported: " + e . getMessage ( ) ) ; } } } 	0	['13', '1', '15', '51', '39', '72', '48', '7', '12', '0.833333333', '249', '1', '0', '0', '0.230769231', '0', '0', '18.07692308', '9', '1.8462', '0']
package org . apache . lucene . queryParser ; public interface CharStream { char readChar ( ) throws java . io . IOException ; int getColumn ( ) ; int getLine ( ) ; int getEndColumn ( ) ; int getEndLine ( ) ; int getBeginColumn ( ) ; int getBeginLine ( ) ; void backup ( int amount ) ; char BeginToken ( ) throws java . io . IOException ; String GetImage ( ) ; char [ ] GetSuffix ( int len ) ; void Done ( ) ; } 	1	['12', '1', '0', '3', '12', '66', '3', '0', '12', '2', '12', '0', '0', '0', '0.583333333', '0', '0', '0', '1', '1', '2']
package org . apache . lucene . analysis ; import java . io . Reader ; public final class SimpleAnalyzer extends Analyzer { public TokenStream tokenStream ( String fieldName , Reader reader ) { return new LowerCaseTokenizer ( reader ) ; } } 	0	['2', '2', '0', '4', '4', '1', '1', '3', '2', '2', '10', '0', '0', '0.666666667', '0.666666667', '0', '0', '4', '1', '0.5', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . TermPositions ; import java . io . IOException ; import java . util . Arrays ; import java . util . Comparator ; import java . util . HashMap ; final class SloppyPhraseScorer extends PhraseScorer { private int slop ; private PhrasePositions repeats [ ] ; private boolean checkedRepeats ; SloppyPhraseScorer ( Weight weight , TermPositions [ ] tps , int [ ] offsets , Similarity similarity , int slop , byte [ ] norms ) { super ( weight , tps , offsets , similarity , norms ) ; this . slop = slop ; } protected final float phraseFreq ( ) throws IOException { int end = initPhrasePositions ( ) ; float freq = 0.0f ; boolean done = ( end < 0 ) ; while ( ! done ) { PhrasePositions pp = ( PhrasePositions ) pq . pop ( ) ; int start = pp . position ; int next = ( ( PhrasePositions ) pq . top ( ) ) . position ; boolean tpsDiffer = true ; for ( int pos = start ; pos <= next || ! tpsDiffer ; pos = pp . position ) { if ( pos <= next && tpsDiffer ) start = pos ; if ( ! pp . nextPosition ( ) ) { done = true ; break ; } tpsDiffer = ! pp . repeats || termPositionsDiffer ( pp ) ; } int matchLength = end - start ; if ( matchLength <= slop ) freq += getSimilarity ( ) . sloppyFreq ( matchLength ) ; if ( pp . position > end ) end = pp . position ; pq . put ( pp ) ; } return freq ; } private int initPhrasePositions ( ) throws IOException { int end = 0 ; if ( checkedRepeats && repeats == null ) { pq . clear ( ) ; for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) { pp . firstPosition ( ) ; if ( pp . position > end ) end = pp . position ; pq . put ( pp ) ; } return end ; } for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) pp . firstPosition ( ) ; if ( ! checkedRepeats ) { checkedRepeats = true ; HashMap m = null ; for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) { int tpPos = pp . position + pp . offset ; for ( PhrasePositions pp2 = pp . next ; pp2 != null ; pp2 = pp2 . next ) { int tpPos2 = pp2 . position + pp2 . offset ; if ( tpPos2 == tpPos ) { if ( m == null ) m = new HashMap ( ) ; pp . repeats = true ; pp2 . repeats = true ; m . put ( pp , null ) ; m . put ( pp2 , null ) ; } } } if ( m != null ) repeats = ( PhrasePositions [ ] ) m . keySet ( ) . toArray ( new PhrasePositions [ 0 ] ) ; } if ( repeats != null ) { Arrays . sort ( repeats , new Comparator ( ) { public int compare ( Object x , Object y ) { return ( ( PhrasePositions ) y ) . offset - ( ( PhrasePositions ) x ) . offset ; } } ) ; for ( int i = 0 ; i < repeats . length ; i ++ ) { PhrasePositions pp = repeats [ i ] ; while ( ! termPositionsDiffer ( pp ) ) { if ( ! pp . nextPosition ( ) ) return - 1 ; } } } pq . clear ( ) ; for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) { if ( pp . position > end ) end = pp . position ; pq . put ( pp ) ; } return end ; } private boolean termPositionsDiffer ( PhrasePositions pp ) { int tpPos = pp . position + pp . offset ; for ( int i = 0 ; i < repeats . length ; i ++ ) { PhrasePositions pp2 = repeats [ i ] ; if ( pp2 == pp ) continue ; int tpPos2 = pp2 . position + pp2 . offset ; if ( tpPos2 == tpPos ) return false ; } return true ; } } 	1	['4', '3', '0', '9', '19', '0', '3', '7', '0', '0.555555556', '326', '1', '1', '0.869565217', '0.34375', '1', '1', '79.75', '4', '1.5', '1']
package org . apache . lucene . index ; final class TermInfo { int docFreq = 0 ; long freqPointer = 0 ; long proxPointer = 0 ; int skipOffset ; TermInfo ( ) { } TermInfo ( int df , long fp , long pp ) { docFreq = df ; freqPointer = fp ; proxPointer = pp ; } TermInfo ( TermInfo ti ) { docFreq = ti . docFreq ; freqPointer = ti . freqPointer ; proxPointer = ti . proxPointer ; skipOffset = ti . skipOffset ; } final void set ( int docFreq , long freqPointer , long proxPointer , int skipOffset ) { this . docFreq = docFreq ; this . freqPointer = freqPointer ; this . proxPointer = proxPointer ; this . skipOffset = skipOffset ; } final void set ( TermInfo ti ) { docFreq = ti . docFreq ; freqPointer = ti . freqPointer ; proxPointer = ti . proxPointer ; skipOffset = ti . skipOffset ; } } 	0	['5', '1', '0', '8', '6', '0', '8', '0', '0', '0.125', '100', '0', '0', '0', '0.55', '0', '0', '18.2', '1', '0.4', '0']
package org . apache . lucene . index ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . IndexOutput ; import java . io . IOException ; import java . util . * ; final class FieldInfos { static final byte IS_INDEXED = 0x1 ; static final byte STORE_TERMVECTOR = 0x2 ; static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x4 ; static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x8 ; static final byte OMIT_NORMS = 0x10 ; static final byte STORE_PAYLOADS = 0x20 ; private ArrayList byNumber = new ArrayList ( ) ; private HashMap byName = new HashMap ( ) ; FieldInfos ( ) { } FieldInfos ( Directory d , String name ) throws IOException { IndexInput input = d . openInput ( name ) ; try { read ( input ) ; } finally { input . close ( ) ; } } public void add ( Document doc ) { List fields = doc . getFields ( ) ; Iterator fieldIterator = fields . iterator ( ) ; while ( fieldIterator . hasNext ( ) ) { Fieldable field = ( Fieldable ) fieldIterator . next ( ) ; add ( field . name ( ) , field . isIndexed ( ) , field . isTermVectorStored ( ) , field . isStorePositionWithTermVector ( ) , field . isStoreOffsetWithTermVector ( ) , field . getOmitNorms ( ) ) ; } } public void addIndexed ( Collection names , boolean storeTermVectors , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector ) { Iterator i = names . iterator ( ) ; while ( i . hasNext ( ) ) { add ( ( String ) i . next ( ) , true , storeTermVectors , storePositionWithTermVector , storeOffsetWithTermVector ) ; } } public void add ( Collection names , boolean isIndexed ) { Iterator i = names . iterator ( ) ; while ( i . hasNext ( ) ) { add ( ( String ) i . next ( ) , isIndexed ) ; } } public void add ( String name , boolean isIndexed ) { add ( name , isIndexed , false , false , false , false ) ; } public void add ( String name , boolean isIndexed , boolean storeTermVector ) { add ( name , isIndexed , storeTermVector , false , false , false ) ; } public void add ( String name , boolean isIndexed , boolean storeTermVector , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector ) { add ( name , isIndexed , storeTermVector , storePositionWithTermVector , storeOffsetWithTermVector , false ) ; } public void add ( String name , boolean isIndexed , boolean storeTermVector , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector , boolean omitNorms ) { add ( name , isIndexed , storeTermVector , storePositionWithTermVector , storeOffsetWithTermVector , omitNorms , false ) ; } public FieldInfo add ( String name , boolean isIndexed , boolean storeTermVector , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector , boolean omitNorms , boolean storePayloads ) { FieldInfo fi = fieldInfo ( name ) ; if ( fi == null ) { return addInternal ( name , isIndexed , storeTermVector , storePositionWithTermVector , storeOffsetWithTermVector , omitNorms , storePayloads ) ; } else { if ( fi . isIndexed != isIndexed ) { fi . isIndexed = true ; } if ( fi . storeTermVector != storeTermVector ) { fi . storeTermVector = true ; } if ( fi . storePositionWithTermVector != storePositionWithTermVector ) { fi . storePositionWithTermVector = true ; } if ( fi . storeOffsetWithTermVector != storeOffsetWithTermVector ) { fi . storeOffsetWithTermVector = true ; } if ( fi . omitNorms != omitNorms ) { fi . omitNorms = false ; } if ( fi . storePayloads != storePayloads ) { fi . storePayloads = true ; } } return fi ; } private FieldInfo addInternal ( String name , boolean isIndexed , boolean storeTermVector , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector , boolean omitNorms , boolean storePayloads ) { FieldInfo fi = new FieldInfo ( name , isIndexed , byNumber . size ( ) , storeTermVector , storePositionWithTermVector , storeOffsetWithTermVector , omitNorms , storePayloads ) ; byNumber . add ( fi ) ; byName . put ( name , fi ) ; return fi ; } public int fieldNumber ( String fieldName ) { try { FieldInfo fi = fieldInfo ( fieldName ) ; if ( fi != null ) return fi . number ; } catch ( IndexOutOfBoundsException ioobe ) { return - 1 ; } return - 1 ; } public FieldInfo fieldInfo ( String fieldName ) { return ( FieldInfo ) byName . get ( fieldName ) ; } public String fieldName ( int fieldNumber ) { try { return fieldInfo ( fieldNumber ) . name ; } catch ( NullPointerException npe ) { return "" ; } } public FieldInfo fieldInfo ( int fieldNumber ) { try { return ( FieldInfo ) byNumber . get ( fieldNumber ) ; } catch ( IndexOutOfBoundsException ioobe ) { return null ; } } public int size ( ) { return byNumber . size ( ) ; } public boolean hasVectors ( ) { boolean hasVectors = false ; for ( int i = 0 ; i < size ( ) ; i ++ ) { if ( fieldInfo ( i ) . storeTermVector ) { hasVectors = true ; break ; } } return hasVectors ; } public void write ( Directory d , String name ) throws IOException { IndexOutput output = d . createOutput ( name ) ; try { write ( output ) ; } finally { output . close ( ) ; } } public void write ( IndexOutput output ) throws IOException { output . writeVInt ( size ( ) ) ; for ( int i = 0 ; i < size ( ) ; i ++ ) { FieldInfo fi = fieldInfo ( i ) ; byte bits = 0x0 ; if ( fi . isIndexed ) bits |= IS_INDEXED ; if ( fi . storeTermVector ) bits |= STORE_TERMVECTOR ; if ( fi . storePositionWithTermVector ) bits |= STORE_POSITIONS_WITH_TERMVECTOR ; if ( fi . storeOffsetWithTermVector ) bits |= STORE_OFFSET_WITH_TERMVECTOR ; if ( fi . omitNorms ) bits |= OMIT_NORMS ; if ( fi . storePayloads ) bits |= STORE_PAYLOADS ; output . writeString ( fi . name ) ; output . writeByte ( bits ) ; } } private void read ( IndexInput input ) throws IOException { int size = input . readVInt ( ) ; for ( int i = 0 ; i < size ; i ++ ) { String name = input . readString ( ) . intern ( ) ; byte bits = input . readByte ( ) ; boolean isIndexed = ( bits & IS_INDEXED ) != 0 ; boolean storeTermVector = ( bits & STORE_TERMVECTOR ) != 0 ; boolean storePositionsWithTermVector = ( bits & STORE_POSITIONS_WITH_TERMVECTOR ) != 0 ; boolean storeOffsetWithTermVector = ( bits & STORE_OFFSET_WITH_TERMVECTOR ) != 0 ; boolean omitNorms = ( bits & OMIT_NORMS ) != 0 ; boolean storePayloads = ( bits & STORE_PAYLOADS ) != 0 ; addInternal ( name , isIndexed , storeTermVector , storePositionsWithTermVector , storeOffsetWithTermVector , omitNorms , storePayloads ) ; } } } 	1	['20', '1', '0', '18', '51', '164', '12', '6', '16', '0.940789474', '482', '0.25', '0', '0', '0.261111111', '0', '0', '22.7', '8', '1.5', '2']
package org . apache . lucene . index ; import java . io . IOException ; public class CorruptIndexException extends IOException { public CorruptIndexException ( String message ) { super ( message ) ; } } 	0	['1', '4', '0', '28', '2', '0', '28', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . IndexInput ; import java . util . LinkedList ; import java . util . HashSet ; import java . util . Iterator ; import java . io . IOException ; final class CompoundFileWriter { private static final class FileEntry { String file ; long directoryOffset ; long dataOffset ; } private Directory directory ; private String fileName ; private HashSet ids ; private LinkedList entries ; private boolean merged = false ; public CompoundFileWriter ( Directory dir , String name ) { if ( dir == null ) throw new NullPointerException ( "directory cannot be null" ) ; if ( name == null ) throw new NullPointerException ( "name cannot be null" ) ; directory = dir ; fileName = name ; ids = new HashSet ( ) ; entries = new LinkedList ( ) ; } public Directory getDirectory ( ) { return directory ; } public String getName ( ) { return fileName ; } public void addFile ( String file ) { if ( merged ) throw new IllegalStateException ( "Can't add extensions after merge has been called" ) ; if ( file == null ) throw new NullPointerException ( "file cannot be null" ) ; if ( ! ids . add ( file ) ) throw new IllegalArgumentException ( "File " + file + " already added" ) ; FileEntry entry = new FileEntry ( ) ; entry . file = file ; entries . add ( entry ) ; } public void close ( ) throws IOException { if ( merged ) throw new IllegalStateException ( "Merge already performed" ) ; if ( entries . isEmpty ( ) ) throw new IllegalStateException ( "No entries to merge have been defined" ) ; merged = true ; IndexOutput os = null ; try { os = directory . createOutput ( fileName ) ; os . writeVInt ( entries . size ( ) ) ; Iterator it = entries . iterator ( ) ; while ( it . hasNext ( ) ) { FileEntry fe = ( FileEntry ) it . next ( ) ; fe . directoryOffset = os . getFilePointer ( ) ; os . writeLong ( 0 ) ; os . writeString ( fe . file ) ; } byte buffer [ ] = new byte [ 16384 ] ; it = entries . iterator ( ) ; while ( it . hasNext ( ) ) { FileEntry fe = ( FileEntry ) it . next ( ) ; fe . dataOffset = os . getFilePointer ( ) ; copyFile ( fe , os , buffer ) ; } it = entries . iterator ( ) ; while ( it . hasNext ( ) ) { FileEntry fe = ( FileEntry ) it . next ( ) ; os . seek ( fe . directoryOffset ) ; os . writeLong ( fe . dataOffset ) ; } IndexOutput tmp = os ; os = null ; tmp . close ( ) ; } finally { if ( os != null ) try { os . close ( ) ; } catch ( IOException e ) { } } } private void copyFile ( FileEntry source , IndexOutput os , byte buffer [ ] ) throws IOException { IndexInput is = null ; try { long startPtr = os . getFilePointer ( ) ; is = directory . openInput ( source . file ) ; long length = is . length ( ) ; long remainder = length ; int chunk = buffer . length ; while ( remainder > 0 ) { int len = ( int ) Math . min ( chunk , remainder ) ; is . readBytes ( buffer , 0 , len ) ; os . writeBytes ( buffer , len ) ; remainder -= len ; } if ( remainder != 0 ) throw new IOException ( "Non-zero remainder length after copying: " + remainder + " (id: " + source . file + ", length: " + length + ", buffer size: " + chunk + ")" ) ; long endPtr = os . getFilePointer ( ) ; long diff = endPtr - startPtr ; if ( diff != length ) throw new IOException ( "Difference in the output file offsets " + diff + " does not match the original file length " + length ) ; } finally { if ( is != null ) is . close ( ) ; } } } 	1	['6', '1', '0', '6', '39', '0', '1', '5', '5', '0.6', '339', '1', '1', '0', '0.333333333', '0', '0', '54.66666667', '4', '1.3333', '3']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . util . PriorityQueue ; final class SegmentMergeQueue extends PriorityQueue { SegmentMergeQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object a , Object b ) { SegmentMergeInfo stiA = ( SegmentMergeInfo ) a ; SegmentMergeInfo stiB = ( SegmentMergeInfo ) b ; int comparison = stiA . term . compareTo ( stiB . term ) ; if ( comparison == 0 ) return stiA . base < stiB . base ; else return comparison < 0 ; } final void close ( ) throws IOException { while ( top ( ) != null ) ( ( SegmentMergeInfo ) pop ( ) ) . close ( ) ; } } 	0	['3', '2', '0', '5', '9', '3', '2', '3', '0', '2', '47', '0', '0', '0.846153846', '0.555555556', '1', '3', '14.66666667', '4', '1.6667', '0']
package org . apache . lucene . search ; import org . apache . lucene . search . Filter ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . TermEnum ; import org . apache . lucene . index . TermDocs ; import java . util . BitSet ; import java . io . IOException ; public class PrefixFilter extends Filter { protected final Term prefix ; public PrefixFilter ( Term prefix ) { this . prefix = prefix ; } public Term getPrefix ( ) { return prefix ; } public BitSet bits ( IndexReader reader ) throws IOException { final BitSet bitSet = new BitSet ( reader . maxDoc ( ) ) ; new PrefixGenerator ( prefix ) { public void handleDoc ( int doc ) { bitSet . set ( doc ) ; } } . generate ( reader ) ; return bitSet ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "PrefixFilter(" ) ; buffer . append ( prefix . toString ( ) ) ; buffer . append ( ")" ) ; return buffer . toString ( ) ; } } interface IdGenerator { public void generate ( IndexReader reader ) throws IOException ; public void handleDoc ( int doc ) ; } abstract class PrefixGenerator implements IdGenerator { protected final Term prefix ; PrefixGenerator ( Term prefix ) { this . prefix = prefix ; } public void generate ( IndexReader reader ) throws IOException { TermEnum enumerator = reader . terms ( prefix ) ; TermDocs termDocs = reader . termDocs ( ) ; try { String prefixText = prefix . text ( ) ; String prefixField = prefix . field ( ) ; do { Term term = enumerator . term ( ) ; if ( term != null && term . text ( ) . startsWith ( prefixText ) && term . field ( ) == prefixField ) { termDocs . seek ( term ) ; while ( termDocs . next ( ) ) { handleDoc ( termDocs . doc ( ) ) ; } } else { break ; } } while ( enumerator . next ( ) ) ; } finally { termDocs . close ( ) ; enumerator . close ( ) ; } } } 	1	['4', '2', '0', '4', '13', '0', '1', '4', '4', '0', '52', '1', '1', '0.25', '0.5', '0', '0', '11.75', '1', '0.75', '1']
package org . apache . lucene . search ; import java . util . List ; import java . util . Iterator ; import java . io . IOException ; import org . apache . lucene . util . ScorerDocQueue ; class DisjunctionSumScorer extends Scorer { private final int nrScorers ; protected final List subScorers ; private final int minimumNrMatchers ; private ScorerDocQueue scorerDocQueue = null ; private int queueSize = - 1 ; private int currentDoc = - 1 ; protected int nrMatchers = - 1 ; private float currentScore = Float . NaN ; public DisjunctionSumScorer ( List subScorers , int minimumNrMatchers ) { super ( null ) ; nrScorers = subScorers . size ( ) ; if ( minimumNrMatchers <= 0 ) { throw new IllegalArgumentException ( "Minimum nr of matchers must be positive" ) ; } if ( nrScorers <= 1 ) { throw new IllegalArgumentException ( "There must be at least 2 subScorers" ) ; } this . minimumNrMatchers = minimumNrMatchers ; this . subScorers = subScorers ; } public DisjunctionSumScorer ( List subScorers ) { this ( subScorers , 1 ) ; } private void initScorerDocQueue ( ) throws IOException { Iterator si = subScorers . iterator ( ) ; scorerDocQueue = new ScorerDocQueue ( nrScorers ) ; queueSize = 0 ; while ( si . hasNext ( ) ) { Scorer se = ( Scorer ) si . next ( ) ; if ( se . next ( ) ) { if ( scorerDocQueue . insert ( se ) ) { queueSize ++ ; } } } } public void score ( HitCollector hc ) throws IOException { while ( next ( ) ) { hc . collect ( currentDoc , currentScore ) ; } } protected boolean score ( HitCollector hc , int max ) throws IOException { while ( currentDoc < max ) { hc . collect ( currentDoc , currentScore ) ; if ( ! next ( ) ) { return false ; } } return true ; } public boolean next ( ) throws IOException { if ( scorerDocQueue == null ) { initScorerDocQueue ( ) ; } return ( scorerDocQueue . size ( ) >= minimumNrMatchers ) && advanceAfterCurrent ( ) ; } protected boolean advanceAfterCurrent ( ) throws IOException { do { currentDoc = scorerDocQueue . topDoc ( ) ; currentScore = scorerDocQueue . topScore ( ) ; nrMatchers = 1 ; do { if ( ! scorerDocQueue . topNextAndAdjustElsePop ( ) ) { if ( -- queueSize == 0 ) { break ; } } if ( scorerDocQueue . topDoc ( ) != currentDoc ) { break ; } currentScore += scorerDocQueue . topScore ( ) ; nrMatchers ++ ; } while ( true ) ; if ( nrMatchers >= minimumNrMatchers ) { return true ; } else if ( queueSize < minimumNrMatchers ) { return false ; } } while ( true ) ; } public float score ( ) throws IOException { return currentScore ; } public int doc ( ) { return currentDoc ; } public int nrMatchers ( ) { return nrMatchers ; } public boolean skipTo ( int target ) throws IOException { if ( scorerDocQueue == null ) { initScorerDocQueue ( ) ; } if ( queueSize < minimumNrMatchers ) { return false ; } if ( target <= currentDoc ) { return true ; } do { if ( scorerDocQueue . topDoc ( ) >= target ) { return advanceAfterCurrent ( ) ; } else if ( ! scorerDocQueue . topSkipToAndAdjustElsePop ( target ) ) { if ( -- queueSize < minimumNrMatchers ) { return false ; } } } while ( true ) ; } public Explanation explain ( int doc ) throws IOException { Explanation res = new Explanation ( ) ; Iterator ssi = subScorers . iterator ( ) ; float sumScore = 0.0f ; int nrMatches = 0 ; while ( ssi . hasNext ( ) ) { Explanation es = ( ( Scorer ) ssi . next ( ) ) . explain ( doc ) ; if ( es . getValue ( ) > 0.0f ) { sumScore += es . getValue ( ) ; nrMatches ++ ; } res . addDetail ( es ) ; } if ( nrMatchers >= minimumNrMatchers ) { res . setValue ( sumScore ) ; res . setDescription ( "sum over at least " + minimumNrMatchers + " of " + subScorers . size ( ) + ":" ) ; } else { res . setValue ( 0.0f ) ; res . setDescription ( nrMatches + " match(es) but at least " + minimumNrMatchers + " of " + subScorers . size ( ) + " needed" ) ; } return res ; } } 	0	['12', '2', '1', '7', '37', '0', '2', '5', '9', '0.454545455', '357', '1', '1', '0.444444444', '0.416666667', '1', '3', '28.08333333', '1', '0.8333', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . util . StringHelper ; import java . io . IOException ; import java . util . Vector ; final class TermVectorsWriter { static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x1 ; static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x2 ; static final int FORMAT_VERSION = 2 ; static final int FORMAT_SIZE = 4 ; static final String TVX_EXTENSION = ".tvx" ; static final String TVD_EXTENSION = ".tvd" ; static final String TVF_EXTENSION = ".tvf" ; private IndexOutput tvx = null , tvd = null , tvf = null ; private Vector fields = null ; private Vector terms = null ; private FieldInfos fieldInfos ; private TVField currentField = null ; private long currentDocPointer = - 1 ; public TermVectorsWriter ( Directory directory , String segment , FieldInfos fieldInfos ) throws IOException { tvx = directory . createOutput ( segment + TVX_EXTENSION ) ; tvx . writeInt ( FORMAT_VERSION ) ; tvd = directory . createOutput ( segment + TVD_EXTENSION ) ; tvd . writeInt ( FORMAT_VERSION ) ; tvf = directory . createOutput ( segment + TVF_EXTENSION ) ; tvf . writeInt ( FORMAT_VERSION ) ; this . fieldInfos = fieldInfos ; fields = new Vector ( fieldInfos . size ( ) ) ; terms = new Vector ( ) ; } public final void openDocument ( ) throws IOException { closeDocument ( ) ; currentDocPointer = tvd . getFilePointer ( ) ; } public final void closeDocument ( ) throws IOException { if ( isDocumentOpen ( ) ) { closeField ( ) ; writeDoc ( ) ; fields . clear ( ) ; currentDocPointer = - 1 ; } } public final boolean isDocumentOpen ( ) { return currentDocPointer != - 1 ; } public final void openField ( String field ) throws IOException { FieldInfo fieldInfo = fieldInfos . fieldInfo ( field ) ; openField ( fieldInfo . number , fieldInfo . storePositionWithTermVector , fieldInfo . storeOffsetWithTermVector ) ; } private void openField ( int fieldNumber , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector ) throws IOException { if ( ! isDocumentOpen ( ) ) throw new IllegalStateException ( "Cannot open field when no document is open." ) ; closeField ( ) ; currentField = new TVField ( fieldNumber , storePositionWithTermVector , storeOffsetWithTermVector ) ; } public final void closeField ( ) throws IOException { if ( isFieldOpen ( ) ) { writeField ( ) ; fields . add ( currentField ) ; terms . clear ( ) ; currentField = null ; } } public final boolean isFieldOpen ( ) { return currentField != null ; } public final void addTerm ( String termText , int freq ) { addTerm ( termText , freq , null , null ) ; } public final void addTerm ( String termText , int freq , int [ ] positions , TermVectorOffsetInfo [ ] offsets ) { if ( ! isDocumentOpen ( ) ) throw new IllegalStateException ( "Cannot add terms when document is not open" ) ; if ( ! isFieldOpen ( ) ) throw new IllegalStateException ( "Cannot add terms when field is not open" ) ; addTermInternal ( termText , freq , positions , offsets ) ; } private final void addTermInternal ( String termText , int freq , int [ ] positions , TermVectorOffsetInfo [ ] offsets ) { TVTerm term = new TVTerm ( ) ; term . termText = termText ; term . freq = freq ; term . positions = positions ; term . offsets = offsets ; terms . add ( term ) ; } public final void addAllDocVectors ( TermFreqVector [ ] vectors ) throws IOException { openDocument ( ) ; if ( vectors != null ) { for ( int i = 0 ; i < vectors . length ; i ++ ) { boolean storePositionWithTermVector = false ; boolean storeOffsetWithTermVector = false ; try { TermPositionVector tpVector = ( TermPositionVector ) vectors [ i ] ; if ( tpVector . size ( ) > 0 && tpVector . getTermPositions ( 0 ) != null ) storePositionWithTermVector = true ; if ( tpVector . size ( ) > 0 && tpVector . getOffsets ( 0 ) != null ) storeOffsetWithTermVector = true ; FieldInfo fieldInfo = fieldInfos . fieldInfo ( tpVector . getField ( ) ) ; openField ( fieldInfo . number , storePositionWithTermVector , storeOffsetWithTermVector ) ; for ( int j = 0 ; j < tpVector . size ( ) ; j ++ ) addTermInternal ( tpVector . getTerms ( ) [ j ] , tpVector . getTermFrequencies ( ) [ j ] , tpVector . getTermPositions ( j ) , tpVector . getOffsets ( j ) ) ; closeField ( ) ; } catch ( ClassCastException ignore ) { TermFreqVector tfVector = vectors [ i ] ; FieldInfo fieldInfo = fieldInfos . fieldInfo ( tfVector . getField ( ) ) ; openField ( fieldInfo . number , storePositionWithTermVector , storeOffsetWithTermVector ) ; for ( int j = 0 ; j < tfVector . size ( ) ; j ++ ) addTermInternal ( tfVector . getTerms ( ) [ j ] , tfVector . getTermFrequencies ( ) [ j ] , null , null ) ; closeField ( ) ; } } } closeDocument ( ) ; } final void close ( ) throws IOException { try { closeDocument ( ) ; } finally { IOException keep = null ; if ( tvx != null ) try { tvx . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tvd != null ) try { tvd . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tvf != null ) try { tvf . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( keep != null ) throw ( IOException ) keep . fillInStackTrace ( ) ; } } private void writeField ( ) throws IOException { currentField . tvfPointer = tvf . getFilePointer ( ) ; final int size = terms . size ( ) ; tvf . writeVInt ( size ) ; boolean storePositions = currentField . storePositions ; boolean storeOffsets = currentField . storeOffsets ; byte bits = 0x0 ; if ( storePositions ) bits |= STORE_POSITIONS_WITH_TERMVECTOR ; if ( storeOffsets ) bits |= STORE_OFFSET_WITH_TERMVECTOR ; tvf . writeByte ( bits ) ; String lastTermText = "" ; for ( int i = 0 ; i < size ; i ++ ) { TVTerm term = ( TVTerm ) terms . elementAt ( i ) ; int start = StringHelper . stringDifference ( lastTermText , term . termText ) ; int length = term . termText . length ( ) - start ; tvf . writeVInt ( start ) ; tvf . writeVInt ( length ) ; tvf . writeChars ( term . termText , start , length ) ; tvf . writeVInt ( term . freq ) ; lastTermText = term . termText ; if ( storePositions ) { if ( term . positions == null ) throw new IllegalStateException ( "Trying to write positions that are null!" ) ; int position = 0 ; for ( int j = 0 ; j < term . freq ; j ++ ) { tvf . writeVInt ( term . positions [ j ] - position ) ; position = term . positions [ j ] ; } } if ( storeOffsets ) { if ( term . offsets == null ) throw new IllegalStateException ( "Trying to write offsets that are null!" ) ; int position = 0 ; for ( int j = 0 ; j < term . freq ; j ++ ) { tvf . writeVInt ( term . offsets [ j ] . getStartOffset ( ) - position ) ; tvf . writeVInt ( term . offsets [ j ] . getEndOffset ( ) - term . offsets [ j ] . getStartOffset ( ) ) ; position = term . offsets [ j ] . getEndOffset ( ) ; } } } } private void writeDoc ( ) throws IOException { if ( isFieldOpen ( ) ) throw new IllegalStateException ( "Field is still open while writing document" ) ; tvx . writeLong ( currentDocPointer ) ; final int size = fields . size ( ) ; tvd . writeVInt ( size ) ; for ( int i = 0 ; i < size ; i ++ ) { TVField field = ( TVField ) fields . elementAt ( i ) ; tvd . writeVInt ( field . number ) ; } long lastFieldPointer = 0 ; for ( int i = 0 ; i < size ; i ++ ) { TVField field = ( TVField ) fields . elementAt ( i ) ; tvd . writeVLong ( field . tvfPointer - lastFieldPointer ) ; lastFieldPointer = field . tvfPointer ; } } private static class TVField { int number ; long tvfPointer = 0 ; boolean storePositions = false ; boolean storeOffsets = false ; TVField ( int number , boolean storePos , boolean storeOff ) { this . number = number ; storePositions = storePos ; storeOffsets = storeOff ; } } private static class TVTerm { String termText ; int freq = 0 ; int positions [ ] = null ; TermVectorOffsetInfo [ ] offsets = null ; } } 	1	['15', '1', '0', '13', '54', '41', '2', '11', '10', '0.804761905', '675', '0.533333333', '5', '0', '0.237037037', '0', '0', '43', '3', '1.2', '3']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexInput ; final class TermBuffer implements Cloneable { private static final char [ ] NO_CHARS = new char [ 0 ] ; private String field ; private char [ ] text = NO_CHARS ; private int textLength ; private Term term ; public final int compareTo ( TermBuffer other ) { if ( field == other . field ) return compareChars ( text , textLength , other . text , other . textLength ) ; else return field . compareTo ( other . field ) ; } private static final int compareChars ( char [ ] v1 , int len1 , char [ ] v2 , int len2 ) { int end = Math . min ( len1 , len2 ) ; for ( int k = 0 ; k < end ; k ++ ) { char c1 = v1 [ k ] ; char c2 = v2 [ k ] ; if ( c1 != c2 ) { return c1 - c2 ; } } return len1 - len2 ; } private final void setTextLength ( int newLength ) { if ( text . length < newLength ) { char [ ] newText = new char [ newLength ] ; System . arraycopy ( text , 0 , newText , 0 , textLength ) ; text = newText ; } textLength = newLength ; } public final void read ( IndexInput input , FieldInfos fieldInfos ) throws IOException { this . term = null ; int start = input . readVInt ( ) ; int length = input . readVInt ( ) ; int totalLength = start + length ; setTextLength ( totalLength ) ; input . readChars ( this . text , start , length ) ; this . field = fieldInfos . fieldName ( input . readVInt ( ) ) ; } public final void set ( Term term ) { if ( term == null ) { reset ( ) ; return ; } setTextLength ( term . text ( ) . length ( ) ) ; term . text ( ) . getChars ( 0 , term . text ( ) . length ( ) , text , 0 ) ; this . field = term . field ( ) ; this . term = term ; } public final void set ( TermBuffer other ) { setTextLength ( other . textLength ) ; System . arraycopy ( other . text , 0 , text , 0 , textLength ) ; this . field = other . field ; this . term = other . term ; } public void reset ( ) { this . field = null ; this . textLength = 0 ; this . term = null ; } public Term toTerm ( ) { if ( field == null ) return null ; if ( term == null ) term = new Term ( field , new String ( text , 0 , textLength ) , false ) ; return term ; } protected Object clone ( ) { TermBuffer clone = null ; try { clone = ( TermBuffer ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . text = new char [ text . length ] ; System . arraycopy ( text , 0 , clone . text , 0 , textLength ) ; return clone ; } } 	0	['11', '1', '0', '4', '25', '0', '1', '3', '6', '0.52', '241', '1', '1', '0', '0.242857143', '0', '0', '20.45454545', '3', '1.4545', '0']
package org . apache . lucene . index ; import org . apache . lucene . analysis . TokenStream ; import org . apache . lucene . document . * ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . AlreadyClosedException ; import org . apache . lucene . store . BufferedIndexInput ; import java . io . ByteArrayOutputStream ; import java . io . IOException ; import java . io . Reader ; import java . util . zip . DataFormatException ; import java . util . zip . Inflater ; final class FieldsReader { private final FieldInfos fieldInfos ; private final IndexInput cloneableFieldsStream ; private final IndexInput fieldsStream ; private final IndexInput indexStream ; private int size ; private boolean closed ; private ThreadLocal fieldsStreamTL = new ThreadLocal ( ) ; FieldsReader ( Directory d , String segment , FieldInfos fn ) throws IOException { this ( d , segment , fn , BufferedIndexInput . BUFFER_SIZE ) ; } FieldsReader ( Directory d , String segment , FieldInfos fn , int readBufferSize ) throws IOException { fieldInfos = fn ; cloneableFieldsStream = d . openInput ( segment + ".fdt" , readBufferSize ) ; fieldsStream = ( IndexInput ) cloneableFieldsStream . clone ( ) ; indexStream = d . openInput ( segment + ".fdx" , readBufferSize ) ; size = ( int ) ( indexStream . length ( ) / 8 ) ; } protected final void ensureOpen ( ) throws AlreadyClosedException { if ( closed ) { throw new AlreadyClosedException ( "this FieldsReader is closed" ) ; } } final void close ( ) throws IOException { if ( ! closed ) { fieldsStream . close ( ) ; cloneableFieldsStream . close ( ) ; indexStream . close ( ) ; IndexInput localFieldsStream = ( IndexInput ) fieldsStreamTL . get ( ) ; if ( localFieldsStream != null ) { localFieldsStream . close ( ) ; fieldsStreamTL . set ( null ) ; } closed = true ; } } final int size ( ) { return size ; } final Document doc ( int n , FieldSelector fieldSelector ) throws CorruptIndexException , IOException { indexStream . seek ( n * 8L ) ; long position = indexStream . readLong ( ) ; fieldsStream . seek ( position ) ; Document doc = new Document ( ) ; int numFields = fieldsStream . readVInt ( ) ; for ( int i = 0 ; i < numFields ; i ++ ) { int fieldNumber = fieldsStream . readVInt ( ) ; FieldInfo fi = fieldInfos . fieldInfo ( fieldNumber ) ; FieldSelectorResult acceptField = fieldSelector == null ? FieldSelectorResult . LOAD : fieldSelector . accept ( fi . name ) ; byte bits = fieldsStream . readByte ( ) ; boolean compressed = ( bits & FieldsWriter . FIELD_IS_COMPRESSED ) != 0 ; boolean tokenize = ( bits & FieldsWriter . FIELD_IS_TOKENIZED ) != 0 ; boolean binary = ( bits & FieldsWriter . FIELD_IS_BINARY ) != 0 ; if ( acceptField . equals ( FieldSelectorResult . LOAD ) ) { addField ( doc , fi , binary , compressed , tokenize ) ; } else if ( acceptField . equals ( FieldSelectorResult . LOAD_FOR_MERGE ) ) { addFieldForMerge ( doc , fi , binary , compressed , tokenize ) ; } else if ( acceptField . equals ( FieldSelectorResult . LOAD_AND_BREAK ) ) { addField ( doc , fi , binary , compressed , tokenize ) ; break ; } else if ( acceptField . equals ( FieldSelectorResult . LAZY_LOAD ) ) { addFieldLazy ( doc , fi , binary , compressed , tokenize ) ; } else if ( acceptField . equals ( FieldSelectorResult . SIZE ) ) { skipField ( binary , compressed , addFieldSize ( doc , fi , binary , compressed ) ) ; } else if ( acceptField . equals ( FieldSelectorResult . SIZE_AND_BREAK ) ) { addFieldSize ( doc , fi , binary , compressed ) ; break ; } else { skipField ( binary , compressed ) ; } } return doc ; } private void skipField ( boolean binary , boolean compressed ) throws IOException { skipField ( binary , compressed , fieldsStream . readVInt ( ) ) ; } private void skipField ( boolean binary , boolean compressed , int toRead ) throws IOException { if ( binary || compressed ) { long pointer = fieldsStream . getFilePointer ( ) ; fieldsStream . seek ( pointer + toRead ) ; } else { fieldsStream . skipChars ( toRead ) ; } } private void addFieldLazy ( Document doc , FieldInfo fi , boolean binary , boolean compressed , boolean tokenize ) throws IOException { if ( binary == true ) { int toRead = fieldsStream . readVInt ( ) ; long pointer = fieldsStream . getFilePointer ( ) ; if ( compressed ) { doc . add ( new LazyField ( fi . name , Field . Store . COMPRESS , toRead , pointer ) ) ; } else { doc . add ( new LazyField ( fi . name , Field . Store . YES , toRead , pointer ) ) ; } fieldsStream . seek ( pointer + toRead ) ; } else { Field . Store store = Field . Store . YES ; Field . Index index = getIndexType ( fi , tokenize ) ; Field . TermVector termVector = getTermVectorType ( fi ) ; Fieldable f ; if ( compressed ) { store = Field . Store . COMPRESS ; int toRead = fieldsStream . readVInt ( ) ; long pointer = fieldsStream . getFilePointer ( ) ; f = new LazyField ( fi . name , store , toRead , pointer ) ; fieldsStream . seek ( pointer + toRead ) ; f . setOmitNorms ( fi . omitNorms ) ; } else { int length = fieldsStream . readVInt ( ) ; long pointer = fieldsStream . getFilePointer ( ) ; fieldsStream . skipChars ( length ) ; f = new LazyField ( fi . name , store , index , termVector , length , pointer ) ; f . setOmitNorms ( fi . omitNorms ) ; } doc . add ( f ) ; } } private void addFieldForMerge ( Document doc , FieldInfo fi , boolean binary , boolean compressed , boolean tokenize ) throws IOException { Object data ; if ( binary || compressed ) { int toRead = fieldsStream . readVInt ( ) ; final byte [ ] b = new byte [ toRead ] ; fieldsStream . readBytes ( b , 0 , b . length ) ; data = b ; } else { data = fieldsStream . readString ( ) ; } doc . add ( new FieldForMerge ( data , fi , binary , compressed , tokenize ) ) ; } private void addField ( Document doc , FieldInfo fi , boolean binary , boolean compressed , boolean tokenize ) throws CorruptIndexException , IOException { if ( binary ) { int toRead = fieldsStream . readVInt ( ) ; final byte [ ] b = new byte [ toRead ] ; fieldsStream . readBytes ( b , 0 , b . length ) ; if ( compressed ) doc . add ( new Field ( fi . name , uncompress ( b ) , Field . Store . COMPRESS ) ) ; else doc . add ( new Field ( fi . name , b , Field . Store . YES ) ) ; } else { Field . Store store = Field . Store . YES ; Field . Index index = getIndexType ( fi , tokenize ) ; Field . TermVector termVector = getTermVectorType ( fi ) ; Fieldable f ; if ( compressed ) { store = Field . Store . COMPRESS ; int toRead = fieldsStream . readVInt ( ) ; final byte [ ] b = new byte [ toRead ] ; fieldsStream . readBytes ( b , 0 , b . length ) ; f = new Field ( fi . name , new String ( uncompress ( b ) , "UTF-8" ) , store , index , termVector ) ; f . setOmitNorms ( fi . omitNorms ) ; } else { f = new Field ( fi . name , fieldsStream . readString ( ) , store , index , termVector ) ; f . setOmitNorms ( fi . omitNorms ) ; } doc . add ( f ) ; } } private int addFieldSize ( Document doc , FieldInfo fi , boolean binary , boolean compressed ) throws IOException { int size = fieldsStream . readVInt ( ) , bytesize = binary || compressed ? size : 2 * size ; byte [ ] sizebytes = new byte [ 4 ] ; sizebytes [ 0 ] = ( byte ) ( bytesize > > > 24 ) ; sizebytes [ 1 ] = ( byte ) ( bytesize > > > 16 ) ; sizebytes [ 2 ] = ( byte ) ( bytesize > > > 8 ) ; sizebytes [ 3 ] = ( byte ) bytesize ; doc . add ( new Field ( fi . name , sizebytes , Field . Store . YES ) ) ; return size ; } private Field . TermVector getTermVectorType ( FieldInfo fi ) { Field . TermVector termVector = null ; if ( fi . storeTermVector ) { if ( fi . storeOffsetWithTermVector ) { if ( fi . storePositionWithTermVector ) { termVector = Field . TermVector . WITH_POSITIONS_OFFSETS ; } else { termVector = Field . TermVector . WITH_OFFSETS ; } } else if ( fi . storePositionWithTermVector ) { termVector = Field . TermVector . WITH_POSITIONS ; } else { termVector = Field . TermVector . YES ; } } else { termVector = Field . TermVector . NO ; } return termVector ; } private Field . Index getIndexType ( FieldInfo fi , boolean tokenize ) { Field . Index index ; if ( fi . isIndexed && tokenize ) index = Field . Index . TOKENIZED ; else if ( fi . isIndexed && ! tokenize ) index = Field . Index . UN_TOKENIZED ; else index = Field . Index . NO ; return index ; } private class LazyField extends AbstractField implements Fieldable { private int toRead ; private long pointer ; public LazyField ( String name , Field . Store store , int toRead , long pointer ) { super ( name , store , Field . Index . NO , Field . TermVector . NO ) ; this . toRead = toRead ; this . pointer = pointer ; lazy = true ; } public LazyField ( String name , Field . Store store , Field . Index index , Field . TermVector termVector , int toRead , long pointer ) { super ( name , store , index , termVector ) ; this . toRead = toRead ; this . pointer = pointer ; lazy = true ; } private IndexInput getFieldStream ( ) { IndexInput localFieldsStream = ( IndexInput ) fieldsStreamTL . get ( ) ; if ( localFieldsStream == null ) { localFieldsStream = ( IndexInput ) cloneableFieldsStream . clone ( ) ; fieldsStreamTL . set ( localFieldsStream ) ; } return localFieldsStream ; } public byte [ ] binaryValue ( ) { ensureOpen ( ) ; if ( fieldsData == null ) { final byte [ ] b = new byte [ toRead ] ; IndexInput localFieldsStream = getFieldStream ( ) ; try { localFieldsStream . seek ( pointer ) ; localFieldsStream . readBytes ( b , 0 , b . length ) ; if ( isCompressed == true ) { fieldsData = uncompress ( b ) ; } else { fieldsData = b ; } } catch ( IOException e ) { throw new FieldReaderException ( e ) ; } } return fieldsData instanceof byte [ ] ? ( byte [ ] ) fieldsData : null ; } public Reader readerValue ( ) { ensureOpen ( ) ; return fieldsData instanceof Reader ? ( Reader ) fieldsData : null ; } public TokenStream tokenStreamValue ( ) { ensureOpen ( ) ; return fieldsData instanceof TokenStream ? ( TokenStream ) fieldsData : null ; } public String stringValue ( ) { ensureOpen ( ) ; if ( fieldsData == null ) { IndexInput localFieldsStream = getFieldStream ( ) ; try { localFieldsStream . seek ( pointer ) ; if ( isCompressed ) { final byte [ ] b = new byte [ toRead ] ; localFieldsStream . readBytes ( b , 0 , b . length ) ; fieldsData = new String ( uncompress ( b ) , "UTF-8" ) ; } else { char [ ] chars = new char [ toRead ] ; localFieldsStream . readChars ( chars , 0 , toRead ) ; fieldsData = new String ( chars ) ; } } catch ( IOException e ) { throw new FieldReaderException ( e ) ; } } return fieldsData instanceof String ? ( String ) fieldsData : null ; } public long getPointer ( ) { ensureOpen ( ) ; return pointer ; } public void setPointer ( long pointer ) { ensureOpen ( ) ; this . pointer = pointer ; } public int getToRead ( ) { ensureOpen ( ) ; return toRead ; } public void setToRead ( int toRead ) { ensureOpen ( ) ; this . toRead = toRead ; } } private final byte [ ] uncompress ( final byte [ ] input ) throws CorruptIndexException , IOException { Inflater decompressor = new Inflater ( ) ; decompressor . setInput ( input ) ; ByteArrayOutputStream bos = new ByteArrayOutputStream ( input . length ) ; byte [ ] buf = new byte [ 1024 ] ; while ( ! decompressor . finished ( ) ) { try { int count = decompressor . inflate ( buf ) ; bos . write ( buf , 0 , count ) ; } catch ( DataFormatException e ) { CorruptIndexException newException = new CorruptIndexException ( "field data are in wrong format: " + e . toString ( ) ) ; newException . initCause ( e ) ; throw newException ; } } decompressor . end ( ) ; return bos . toByteArray ( ) ; } final static class FieldForMerge extends AbstractField { public String stringValue ( ) { return ( String ) this . fieldsData ; } public Reader readerValue ( ) { return null ; } public byte [ ] binaryValue ( ) { return ( byte [ ] ) this . fieldsData ; } public TokenStream tokenStreamValue ( ) { return null ; } public FieldForMerge ( Object value , FieldInfo fi , boolean binary , boolean compressed , boolean tokenize ) { this . isStored = true ; this . fieldsData = value ; this . isCompressed = compressed ; this . isBinary = binary ; this . isTokenized = tokenize ; this . name = fi . name . intern ( ) ; this . isIndexed = fi . isIndexed ; this . omitNorms = fi . omitNorms ; this . storeOffsetWithTermVector = fi . storeOffsetWithTermVector ; this . storePositionWithTermVector = fi . storePositionWithTermVector ; this . storeTermVector = fi . storeTermVector ; } } } 	1	['18', '1', '0', '17', '61', '69', '2', '16', '0', '0.806722689', '745', '1', '4', '0', '0.237373737', '0', '0', '40', '5', '1.3333', '11']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class BufferedIndexOutput extends IndexOutput { static final int BUFFER_SIZE = 16384 ; private final byte [ ] buffer = new byte [ BUFFER_SIZE ] ; private long bufferStart = 0 ; private int bufferPosition = 0 ; public void writeByte ( byte b ) throws IOException { if ( bufferPosition >= BUFFER_SIZE ) flush ( ) ; buffer [ bufferPosition ++ ] = b ; } public void writeBytes ( byte [ ] b , int offset , int length ) throws IOException { int bytesLeft = BUFFER_SIZE - bufferPosition ; if ( bytesLeft >= length ) { System . arraycopy ( b , offset , buffer , bufferPosition , length ) ; bufferPosition += length ; if ( BUFFER_SIZE - bufferPosition == 0 ) flush ( ) ; } else { if ( length > BUFFER_SIZE ) { if ( bufferPosition > 0 ) flush ( ) ; flushBuffer ( b , offset , length ) ; bufferStart += length ; } else { int pos = 0 ; int pieceLength ; while ( pos < length ) { pieceLength = ( length - pos < bytesLeft ) ? length - pos : bytesLeft ; System . arraycopy ( b , pos + offset , buffer , bufferPosition , pieceLength ) ; pos += pieceLength ; bufferPosition += pieceLength ; bytesLeft = BUFFER_SIZE - bufferPosition ; if ( bytesLeft == 0 ) { flush ( ) ; bytesLeft = BUFFER_SIZE ; } } } } } public void flush ( ) throws IOException { flushBuffer ( buffer , bufferPosition ) ; bufferStart += bufferPosition ; bufferPosition = 0 ; } private void flushBuffer ( byte [ ] b , int len ) throws IOException { flushBuffer ( b , 0 , len ) ; } protected abstract void flushBuffer ( byte [ ] b , int offset , int len ) throws IOException ; public void close ( ) throws IOException { flush ( ) ; } public long getFilePointer ( ) { return bufferStart + bufferPosition ; } public void seek ( long pos ) throws IOException { flush ( ) ; bufferStart = pos ; } public abstract long length ( ) throws IOException ; } 	0	['10', '2', '1', '2', '12', '17', '1', '1', '8', '0.555555556', '185', '0.75', '0', '0.608695652', '0.36', '1', '3', '17.1', '1', '0.9', '0']
package org . apache . lucene . util ; public abstract class PriorityQueue { private Object [ ] heap ; private int size ; private int maxSize ; protected abstract boolean lessThan ( Object a , Object b ) ; protected final void initialize ( int maxSize ) { size = 0 ; int heapSize = maxSize + 1 ; heap = new Object [ heapSize ] ; this . maxSize = maxSize ; } public final void put ( Object element ) { size ++ ; heap [ size ] = element ; upHeap ( ) ; } public boolean insert ( Object element ) { if ( size < maxSize ) { put ( element ) ; return true ; } else if ( size > 0 && ! lessThan ( element , top ( ) ) ) { heap [ 1 ] = element ; adjustTop ( ) ; return true ; } else return false ; } public final Object top ( ) { if ( size > 0 ) return heap [ 1 ] ; else return null ; } public final Object pop ( ) { if ( size > 0 ) { Object result = heap [ 1 ] ; heap [ 1 ] = heap [ size ] ; heap [ size ] = null ; size -- ; downHeap ( ) ; return result ; } else return null ; } public final void adjustTop ( ) { downHeap ( ) ; } public final int size ( ) { return size ; } public final void clear ( ) { for ( int i = 0 ; i <= size ; i ++ ) heap [ i ] = null ; size = 0 ; } private final void upHeap ( ) { int i = size ; Object node = heap [ i ] ; int j = i > > > 1 ; while ( j > 0 && lessThan ( node , heap [ j ] ) ) { heap [ i ] = heap [ j ] ; i = j ; j = j > > > 1 ; } heap [ i ] = node ; } private final void downHeap ( ) { int i = 1 ; Object node = heap [ i ] ; int j = i << 1 ; int k = j + 1 ; if ( k <= size && lessThan ( heap [ k ] , heap [ j ] ) ) { j = k ; } while ( j <= size && lessThan ( heap [ j ] , node ) ) { heap [ i ] = heap [ j ] ; i = j ; j = i << 1 ; k = j + 1 ; if ( k <= size && lessThan ( heap [ k ] , heap [ j ] ) ) { j = k ; } } heap [ i ] = node ; } } 	1	['12', '1', '9', '12', '13', '0', '12', '0', '8', '0.454545455', '275', '1', '0', '0', '0.444444444', '0', '0', '21.66666667', '7', '2.0833', '1']
package org . apache . lucene . index ; public class TermVectorOffsetInfo { public static final TermVectorOffsetInfo [ ] EMPTY_OFFSET_INFO = new TermVectorOffsetInfo [ 0 ] ; private int startOffset ; private int endOffset ; public TermVectorOffsetInfo ( ) { } public TermVectorOffsetInfo ( int startOffset , int endOffset ) { this . endOffset = endOffset ; this . startOffset = startOffset ; } public int getEndOffset ( ) { return endOffset ; } public void setEndOffset ( int endOffset ) { this . endOffset = endOffset ; } public int getStartOffset ( ) { return startOffset ; } public void setStartOffset ( int startOffset ) { this . startOffset = startOffset ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof TermVectorOffsetInfo ) ) return false ; final TermVectorOffsetInfo termVectorOffsetInfo = ( TermVectorOffsetInfo ) o ; if ( endOffset != termVectorOffsetInfo . endOffset ) return false ; if ( startOffset != termVectorOffsetInfo . startOffset ) return false ; return true ; } public int hashCode ( ) { int result ; result = startOffset ; result = 29 * result + endOffset ; return result ; } } 	0	['9', '1', '0', '7', '10', '2', '7', '0', '8', '0.666666667', '83', '0.666666667', '1', '0', '0.5', '1', '1', '7.888888889', '5', '1.1111', '0']
package org . apache . lucene . index ; import org . apache . lucene . util . PriorityQueue ; import java . io . IOException ; import java . util . Arrays ; import java . util . Iterator ; import java . util . LinkedList ; import java . util . List ; public class MultipleTermPositions implements TermPositions { private static final class TermPositionsQueue extends PriorityQueue { TermPositionsQueue ( List termPositions ) throws IOException { initialize ( termPositions . size ( ) ) ; Iterator i = termPositions . iterator ( ) ; while ( i . hasNext ( ) ) { TermPositions tp = ( TermPositions ) i . next ( ) ; if ( tp . next ( ) ) put ( tp ) ; } } final TermPositions peek ( ) { return ( TermPositions ) top ( ) ; } public final boolean lessThan ( Object a , Object b ) { return ( ( TermPositions ) a ) . doc ( ) < ( ( TermPositions ) b ) . doc ( ) ; } } private static final class IntQueue { private int _arraySize = 16 ; private int _index = 0 ; private int _lastIndex = 0 ; private int [ ] _array = new int [ _arraySize ] ; final void add ( int i ) { if ( _lastIndex == _arraySize ) growArray ( ) ; _array [ _lastIndex ++ ] = i ; } final int next ( ) { return _array [ _index ++ ] ; } final void sort ( ) { Arrays . sort ( _array , _index , _lastIndex ) ; } final void clear ( ) { _index = 0 ; _lastIndex = 0 ; } final int size ( ) { return ( _lastIndex - _index ) ; } private void growArray ( ) { int [ ] newArray = new int [ _arraySize * 2 ] ; System . arraycopy ( _array , 0 , newArray , 0 , _arraySize ) ; _array = newArray ; _arraySize *= 2 ; } } private int _doc ; private int _freq ; private TermPositionsQueue _termPositionsQueue ; private IntQueue _posList ; public MultipleTermPositions ( IndexReader indexReader , Term [ ] terms ) throws IOException { List termPositions = new LinkedList ( ) ; for ( int i = 0 ; i < terms . length ; i ++ ) termPositions . add ( indexReader . termPositions ( terms [ i ] ) ) ; _termPositionsQueue = new TermPositionsQueue ( termPositions ) ; _posList = new IntQueue ( ) ; } public final boolean next ( ) throws IOException { if ( _termPositionsQueue . size ( ) == 0 ) return false ; _posList . clear ( ) ; _doc = _termPositionsQueue . peek ( ) . doc ( ) ; TermPositions tp ; do { tp = _termPositionsQueue . peek ( ) ; for ( int i = 0 ; i < tp . freq ( ) ; i ++ ) _posList . add ( tp . nextPosition ( ) ) ; if ( tp . next ( ) ) _termPositionsQueue . adjustTop ( ) ; else { _termPositionsQueue . pop ( ) ; tp . close ( ) ; } } while ( _termPositionsQueue . size ( ) > 0 && _termPositionsQueue . peek ( ) . doc ( ) == _doc ) ; _posList . sort ( ) ; _freq = _posList . size ( ) ; return true ; } public final int nextPosition ( ) { return _posList . next ( ) ; } public final boolean skipTo ( int target ) throws IOException { while ( _termPositionsQueue . peek ( ) != null && target > _termPositionsQueue . peek ( ) . doc ( ) ) { TermPositions tp = ( TermPositions ) _termPositionsQueue . pop ( ) ; if ( tp . skipTo ( target ) ) _termPositionsQueue . put ( tp ) ; else tp . close ( ) ; } return next ( ) ; } public final int doc ( ) { return _doc ; } public final int freq ( ) { return _freq ; } public final void close ( ) throws IOException { while ( _termPositionsQueue . size ( ) > 0 ) ( ( TermPositions ) _termPositionsQueue . pop ( ) ) . close ( ) ; } public void seek ( Term arg0 ) throws IOException { throw new UnsupportedOperationException ( ) ; } public void seek ( TermEnum termEnum ) throws IOException { throw new UnsupportedOperationException ( ) ; } public int read ( int [ ] arg0 , int [ ] arg1 ) throws IOException { throw new UnsupportedOperationException ( ) ; } public int getPayloadLength ( ) { throw new UnsupportedOperationException ( ) ; } public byte [ ] getPayload ( byte [ ] data , int offset ) throws IOException { throw new UnsupportedOperationException ( ) ; } public boolean isPayloadAvailable ( ) { return false ; } } 	1	['13', '1', '0', '8', '36', '58', '1', '7', '13', '0.791666667', '191', '1', '2', '0', '0.201923077', '0', '0', '13.38461538', '1', '0.9231', '1']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . RAMOutputStream ; abstract class MultiLevelSkipListWriter { private int numberOfSkipLevels ; private int skipInterval ; private RAMOutputStream [ ] skipBuffer ; protected MultiLevelSkipListWriter ( int skipInterval , int maxSkipLevels , int df ) { this . skipInterval = skipInterval ; numberOfSkipLevels = df == 0 ? 0 : ( int ) Math . floor ( Math . log ( df ) / Math . log ( skipInterval ) ) ; if ( numberOfSkipLevels > maxSkipLevels ) { numberOfSkipLevels = maxSkipLevels ; } } protected void init ( ) { skipBuffer = new RAMOutputStream [ numberOfSkipLevels ] ; for ( int i = 0 ; i < numberOfSkipLevels ; i ++ ) { skipBuffer [ i ] = new RAMOutputStream ( ) ; } } protected void resetSkip ( ) { if ( skipBuffer == null ) { init ( ) ; } else { for ( int i = 0 ; i < skipBuffer . length ; i ++ ) { skipBuffer [ i ] . reset ( ) ; } } } protected abstract void writeSkipData ( int level , IndexOutput skipBuffer ) throws IOException ; void bufferSkip ( int df ) throws IOException { int numLevels ; for ( numLevels = 0 ; ( df % skipInterval ) == 0 && numLevels < numberOfSkipLevels ; df /= skipInterval ) { numLevels ++ ; } long childPointer = 0 ; for ( int level = 0 ; level < numLevels ; level ++ ) { writeSkipData ( level , skipBuffer [ level ] ) ; long newChildPointer = skipBuffer [ level ] . getFilePointer ( ) ; if ( level != 0 ) { skipBuffer [ level ] . writeVLong ( childPointer ) ; } childPointer = newChildPointer ; } } long writeSkip ( IndexOutput output ) throws IOException { long skipPointer = output . getFilePointer ( ) ; if ( skipBuffer == null || skipBuffer . length == 0 ) return skipPointer ; for ( int level = numberOfSkipLevels - 1 ; level > 0 ; level -- ) { long length = skipBuffer [ level ] . getFilePointer ( ) ; if ( length > 0 ) { output . writeVLong ( length ) ; skipBuffer [ level ] . writeTo ( output ) ; } } skipBuffer [ 0 ] . writeTo ( output ) ; return skipPointer ; } } 	0	['6', '1', '1', '3', '16', '0', '1', '2', '0', '0.466666667', '178', '1', '1', '0', '0.611111111', '0', '0', '28.16666667', '3', '1.3333', '0']
package org . apache . lucene . queryParser ; import java . util . Vector ; import java . io . * ; import java . text . * ; import java . util . * ; import org . apache . lucene . index . Term ; import org . apache . lucene . analysis . * ; import org . apache . lucene . document . * ; import org . apache . lucene . search . * ; import org . apache . lucene . util . Parameter ; public class QueryParser implements QueryParserConstants { private static final int CONJ_NONE = 0 ; private static final int CONJ_AND = 1 ; private static final int CONJ_OR = 2 ; private static final int MOD_NONE = 0 ; private static final int MOD_NOT = 10 ; private static final int MOD_REQ = 11 ; public static final Operator AND_OPERATOR = Operator . AND ; public static final Operator OR_OPERATOR = Operator . OR ; private Operator operator = OR_OPERATOR ; boolean lowercaseExpandedTerms = true ; boolean useOldRangeQuery = false ; boolean allowLeadingWildcard = false ; Analyzer analyzer ; String field ; int phraseSlop = 0 ; float fuzzyMinSim = FuzzyQuery . defaultMinSimilarity ; int fuzzyPrefixLength = FuzzyQuery . defaultPrefixLength ; Locale locale = Locale . getDefault ( ) ; DateTools . Resolution dateResolution = null ; Map fieldToDateResolution = null ; static public final class Operator extends Parameter { private Operator ( String name ) { super ( name ) ; } static public final Operator OR = new Operator ( "OR" ) ; static public final Operator AND = new Operator ( "AND" ) ; } public QueryParser ( String f , Analyzer a ) { this ( new FastCharStream ( new StringReader ( "" ) ) ) ; analyzer = a ; field = f ; } public Query parse ( String query ) throws ParseException { ReInit ( new FastCharStream ( new StringReader ( query ) ) ) ; try { return TopLevelQuery ( field ) ; } catch ( ParseException tme ) { throw new ParseException ( "Cannot parse '" + query + "': " + tme . getMessage ( ) ) ; } catch ( TokenMgrError tme ) { throw new ParseException ( "Cannot parse '" + query + "': " + tme . getMessage ( ) ) ; } catch ( BooleanQuery . TooManyClauses tmc ) { throw new ParseException ( "Cannot parse '" + query + "': too many boolean clauses" ) ; } } public Analyzer getAnalyzer ( ) { return analyzer ; } public String getField ( ) { return field ; } public float getFuzzyMinSim ( ) { return fuzzyMinSim ; } public void setFuzzyMinSim ( float fuzzyMinSim ) { this . fuzzyMinSim = fuzzyMinSim ; } public int getFuzzyPrefixLength ( ) { return fuzzyPrefixLength ; } public void setFuzzyPrefixLength ( int fuzzyPrefixLength ) { this . fuzzyPrefixLength = fuzzyPrefixLength ; } public void setPhraseSlop ( int phraseSlop ) { this . phraseSlop = phraseSlop ; } public int getPhraseSlop ( ) { return phraseSlop ; } public void setAllowLeadingWildcard ( boolean allowLeadingWildcard ) { this . allowLeadingWildcard = allowLeadingWildcard ; } public boolean getAllowLeadingWildcard ( ) { return allowLeadingWildcard ; } public void setDefaultOperator ( Operator op ) { this . operator = op ; } public Operator getDefaultOperator ( ) { return operator ; } public void setLowercaseExpandedTerms ( boolean lowercaseExpandedTerms ) { this . lowercaseExpandedTerms = lowercaseExpandedTerms ; } public boolean getLowercaseExpandedTerms ( ) { return lowercaseExpandedTerms ; } public void setUseOldRangeQuery ( boolean useOldRangeQuery ) { this . useOldRangeQuery = useOldRangeQuery ; } public boolean getUseOldRangeQuery ( ) { return useOldRangeQuery ; } public void setLocale ( Locale locale ) { this . locale = locale ; } public Locale getLocale ( ) { return locale ; } public void setDateResolution ( DateTools . Resolution dateResolution ) { this . dateResolution = dateResolution ; } public void setDateResolution ( String fieldName , DateTools . Resolution dateResolution ) { if ( fieldName == null ) { throw new IllegalArgumentException ( "Field cannot be null." ) ; } if ( fieldToDateResolution == null ) { fieldToDateResolution = new HashMap ( ) ; } fieldToDateResolution . put ( fieldName , dateResolution ) ; } public DateTools . Resolution getDateResolution ( String fieldName ) { if ( fieldName == null ) { throw new IllegalArgumentException ( "Field cannot be null." ) ; } if ( fieldToDateResolution == null ) { return this . dateResolution ; } DateTools . Resolution resolution = ( DateTools . Resolution ) fieldToDateResolution . get ( fieldName ) ; if ( resolution == null ) { resolution = this . dateResolution ; } return resolution ; } protected void addClause ( Vector clauses , int conj , int mods , Query q ) { boolean required , prohibited ; if ( clauses . size ( ) > 0 && conj == CONJ_AND ) { BooleanClause c = ( BooleanClause ) clauses . elementAt ( clauses . size ( ) - 1 ) ; if ( ! c . isProhibited ( ) ) c . setOccur ( BooleanClause . Occur . MUST ) ; } if ( clauses . size ( ) > 0 && operator == AND_OPERATOR && conj == CONJ_OR ) { BooleanClause c = ( BooleanClause ) clauses . elementAt ( clauses . size ( ) - 1 ) ; if ( ! c . isProhibited ( ) ) c . setOccur ( BooleanClause . Occur . SHOULD ) ; } if ( q == null ) return ; if ( operator == OR_OPERATOR ) { prohibited = ( mods == MOD_NOT ) ; required = ( mods == MOD_REQ ) ; if ( conj == CONJ_AND && ! prohibited ) { required = true ; } } else { prohibited = ( mods == MOD_NOT ) ; required = ( ! prohibited && conj != CONJ_OR ) ; } if ( required && ! prohibited ) clauses . addElement ( new BooleanClause ( q , BooleanClause . Occur . MUST ) ) ; else if ( ! required && ! prohibited ) clauses . addElement ( new BooleanClause ( q , BooleanClause . Occur . SHOULD ) ) ; else if ( ! required && prohibited ) clauses . addElement ( new BooleanClause ( q , BooleanClause . Occur . MUST_NOT ) ) ; else throw new RuntimeException ( "Clause cannot be both required and prohibited" ) ; } protected Query getFieldQuery ( String field , String queryText ) throws ParseException { TokenStream source = analyzer . tokenStream ( field , new StringReader ( queryText ) ) ; Vector v = new Vector ( ) ; org . apache . lucene . analysis . Token t ; int positionCount = 0 ; boolean severalTokensAtSamePosition = false ; while ( true ) { try { t = source . next ( ) ; } catch ( IOException e ) { t = null ; } if ( t == null ) break ; v . addElement ( t ) ; if ( t . getPositionIncrement ( ) != 0 ) positionCount += t . getPositionIncrement ( ) ; else severalTokensAtSamePosition = true ; } try { source . close ( ) ; } catch ( IOException e ) { } if ( v . size ( ) == 0 ) return null ; else if ( v . size ( ) == 1 ) { t = ( org . apache . lucene . analysis . Token ) v . elementAt ( 0 ) ; return new TermQuery ( new Term ( field , t . termText ( ) ) ) ; } else { if ( severalTokensAtSamePosition ) { if ( positionCount == 1 ) { BooleanQuery q = new BooleanQuery ( true ) ; for ( int i = 0 ; i < v . size ( ) ; i ++ ) { t = ( org . apache . lucene . analysis . Token ) v . elementAt ( i ) ; TermQuery currentQuery = new TermQuery ( new Term ( field , t . termText ( ) ) ) ; q . add ( currentQuery , BooleanClause . Occur . SHOULD ) ; } return q ; } else { MultiPhraseQuery mpq = new MultiPhraseQuery ( ) ; mpq . setSlop ( phraseSlop ) ; List multiTerms = new ArrayList ( ) ; for ( int i = 0 ; i < v . size ( ) ; i ++ ) { t = ( org . apache . lucene . analysis . Token ) v . elementAt ( i ) ; if ( t . getPositionIncrement ( ) == 1 && multiTerms . size ( ) > 0 ) { mpq . add ( ( Term [ ] ) multiTerms . toArray ( new Term [ 0 ] ) ) ; multiTerms . clear ( ) ; } multiTerms . add ( new Term ( field , t . termText ( ) ) ) ; } mpq . add ( ( Term [ ] ) multiTerms . toArray ( new Term [ 0 ] ) ) ; return mpq ; } } else { PhraseQuery q = new PhraseQuery ( ) ; q . setSlop ( phraseSlop ) ; for ( int i = 0 ; i < v . size ( ) ; i ++ ) { q . add ( new Term ( field , ( ( org . apache . lucene . analysis . Token ) v . elementAt ( i ) ) . termText ( ) ) ) ; } return q ; } } } protected Query getFieldQuery ( String field , String queryText , int slop ) throws ParseException { Query query = getFieldQuery ( field , queryText ) ; if ( query instanceof PhraseQuery ) { ( ( PhraseQuery ) query ) . setSlop ( slop ) ; } if ( query instanceof MultiPhraseQuery ) { ( ( MultiPhraseQuery ) query ) . setSlop ( slop ) ; } return query ; } protected Query getRangeQuery ( String field , String part1 , String part2 , boolean inclusive ) throws ParseException { if ( lowercaseExpandedTerms ) { part1 = part1 . toLowerCase ( ) ; part2 = part2 . toLowerCase ( ) ; } try { DateFormat df = DateFormat . getDateInstance ( DateFormat . SHORT , locale ) ; df . setLenient ( true ) ; Date d1 = df . parse ( part1 ) ; Date d2 = df . parse ( part2 ) ; if ( inclusive ) { Calendar cal = Calendar . getInstance ( locale ) ; cal . setTime ( d2 ) ; cal . set ( Calendar . HOUR_OF_DAY , 23 ) ; cal . set ( Calendar . MINUTE , 59 ) ; cal . set ( Calendar . SECOND , 59 ) ; cal . set ( Calendar . MILLISECOND , 999 ) ; d2 = cal . getTime ( ) ; } DateTools . Resolution resolution = getDateResolution ( field ) ; if ( resolution == null ) { part1 = DateField . dateToString ( d1 ) ; part2 = DateField . dateToString ( d2 ) ; } else { part1 = DateTools . dateToString ( d1 , resolution ) ; part2 = DateTools . dateToString ( d2 , resolution ) ; } } catch ( Exception e ) { } if ( useOldRangeQuery ) { return new RangeQuery ( new Term ( field , part1 ) , new Term ( field , part2 ) , inclusive ) ; } else { return new ConstantScoreRangeQuery ( field , part1 , part2 , inclusive , inclusive ) ; } } protected Query getBooleanQuery ( Vector clauses ) throws ParseException { return getBooleanQuery ( clauses , false ) ; } protected Query getBooleanQuery ( Vector clauses , boolean disableCoord ) throws ParseException { BooleanQuery query = new BooleanQuery ( disableCoord ) ; for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { query . add ( ( BooleanClause ) clauses . elementAt ( i ) ) ; } return query ; } protected Query getWildcardQuery ( String field , String termStr ) throws ParseException { if ( "*" . equals ( field ) ) { if ( "*" . equals ( termStr ) ) return new MatchAllDocsQuery ( ) ; } if ( ! allowLeadingWildcard && ( termStr . startsWith ( "*" ) || termStr . startsWith ( "?" ) ) ) throw new ParseException ( "'*' or '?' not allowed as first character in WildcardQuery" ) ; if ( lowercaseExpandedTerms ) { termStr = termStr . toLowerCase ( ) ; } Term t = new Term ( field , termStr ) ; return new WildcardQuery ( t ) ; } protected Query getPrefixQuery ( String field , String termStr ) throws ParseException { if ( ! allowLeadingWildcard && termStr . startsWith ( "*" ) ) throw new ParseException ( "'*' not allowed as first character in PrefixQuery" ) ; if ( lowercaseExpandedTerms ) { termStr = termStr . toLowerCase ( ) ; } Term t = new Term ( field , termStr ) ; return new PrefixQuery ( t ) ; } protected Query getFuzzyQuery ( String field , String termStr , float minSimilarity ) throws ParseException { if ( lowercaseExpandedTerms ) { termStr = termStr . toLowerCase ( ) ; } Term t = new Term ( field , termStr ) ; return new FuzzyQuery ( t , minSimilarity , fuzzyPrefixLength ) ; } private String discardEscapeChar ( String input ) throws ParseException { char [ ] output = new char [ input . length ( ) ] ; int length = 0 ; boolean lastCharWasEscapeChar = false ; int codePointMultiplier = 0 ; int codePoint = 0 ; for ( int i = 0 ; i < input . length ( ) ; i ++ ) { char curChar = input . charAt ( i ) ; if ( codePointMultiplier > 0 ) { codePoint += hexToInt ( curChar ) * codePointMultiplier ; codePointMultiplier >>>= 4 ; if ( codePointMultiplier == 0 ) { output [ length ++ ] = ( char ) codePoint ; codePoint = 0 ; } } else if ( lastCharWasEscapeChar ) { if ( curChar == 'u' ) { codePointMultiplier = 16 * 16 * 16 ; } else { output [ length ] = curChar ; length ++ ; } lastCharWasEscapeChar = false ; } else { if ( curChar == '\\' ) { lastCharWasEscapeChar = true ; } else { output [ length ] = curChar ; length ++ ; } } } if ( codePointMultiplier > 0 ) { throw new ParseException ( "Truncated unicode escape sequence." ) ; } if ( lastCharWasEscapeChar ) { throw new ParseException ( "Term can not end with escape character." ) ; } return new String ( output , 0 , length ) ; } private static final int hexToInt ( char c ) throws ParseException { if ( '0' <= c && c <= '9' ) { return c - '0' ; } else if ( 'a' <= c && c <= 'f' ) { return c - 'a' + 10 ; } else if ( 'A' <= c && c <= 'F' ) { return c - 'A' + 10 ; } else { throw new ParseException ( "None-hex character in unicode escape sequence: " + c ) ; } } public static String escape ( String s ) { StringBuffer sb = new StringBuffer ( ) ; for ( int i = 0 ; i < s . length ( ) ; i ++ ) { char c = s . charAt ( i ) ; if ( c == '\\' || c == '+' || c == '-' || c == '!' || c == '(' || c == ')' || c == ':' || c == '^' || c == '[' || c == ']' || c == '\"' || c == '{' || c == '}' || c == '~' || c == '*' || c == '?' || c == '|' || c == '&' ) { sb . append ( '\\' ) ; } sb . append ( c ) ; } return sb . toString ( ) ; } public static void main ( String [ ] args ) throws Exception { if ( args . length == 0 ) { System . out . println ( "Usage: java org.apache.lucene.queryParser.QueryParser <input>" ) ; System . exit ( 0 ) ; } QueryParser qp = new QueryParser ( "field" , new org . apache . lucene . analysis . SimpleAnalyzer ( ) ) ; Query q = qp . parse ( args [ 0 ] ) ; System . out . println ( q . toString ( "field" ) ) ; } final public int Conjunction ( ) throws ParseException { int ret = CONJ_NONE ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case AND : case OR : switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case AND : jj_consume_token ( AND ) ; ret = CONJ_AND ; break ; case OR : jj_consume_token ( OR ) ; ret = CONJ_OR ; break ; default : jj_la1 [ 0 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } break ; default : jj_la1 [ 1 ] = jj_gen ; ; } { if ( true ) return ret ; } throw new Error ( "Missing return statement in function" ) ; } final public int Modifiers ( ) throws ParseException { int ret = MOD_NONE ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case NOT : case PLUS : case MINUS : switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case PLUS : jj_consume_token ( PLUS ) ; ret = MOD_REQ ; break ; case MINUS : jj_consume_token ( MINUS ) ; ret = MOD_NOT ; break ; case NOT : jj_consume_token ( NOT ) ; ret = MOD_NOT ; break ; default : jj_la1 [ 2 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } break ; default : jj_la1 [ 3 ] = jj_gen ; ; } { if ( true ) return ret ; } throw new Error ( "Missing return statement in function" ) ; } final public Query TopLevelQuery ( String field ) throws ParseException { Query q ; q = Query ( field ) ; jj_consume_token ( 0 ) ; { if ( true ) return q ; } throw new Error ( "Missing return statement in function" ) ; } final public Query Query ( String field ) throws ParseException { Vector clauses = new Vector ( ) ; Query q , firstQuery = null ; int conj , mods ; mods = Modifiers ( ) ; q = Clause ( field ) ; addClause ( clauses , CONJ_NONE , mods , q ) ; if ( mods == MOD_NONE ) firstQuery = q ; label_1 : while ( true ) { switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case AND : case OR : case NOT : case PLUS : case MINUS : case LPAREN : case STAR : case QUOTED : case TERM : case PREFIXTERM : case WILDTERM : case RANGEIN_START : case RANGEEX_START : case NUMBER : ; break ; default : jj_la1 [ 4 ] = jj_gen ; break label_1 ; } conj = Conjunction ( ) ; mods = Modifiers ( ) ; q = Clause ( field ) ; addClause ( clauses , conj , mods , q ) ; } if ( clauses . size ( ) == 1 && firstQuery != null ) { if ( true ) return firstQuery ; } else { { if ( true ) return getBooleanQuery ( clauses ) ; } } throw new Error ( "Missing return statement in function" ) ; } final public Query Clause ( String field ) throws ParseException { Query q ; Token fieldToken = null , boost = null ; if ( jj_2_1 ( 2 ) ) { switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case TERM : fieldToken = jj_consume_token ( TERM ) ; jj_consume_token ( COLON ) ; field = discardEscapeChar ( fieldToken . image ) ; break ; case STAR : jj_consume_token ( STAR ) ; jj_consume_token ( COLON ) ; field = "*" ; break ; default : jj_la1 [ 5 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } } else { ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case STAR : case QUOTED : case TERM : case PREFIXTERM : case WILDTERM : case RANGEIN_START : case RANGEEX_START : case NUMBER : q = Term ( field ) ; break ; case LPAREN : jj_consume_token ( LPAREN ) ; q = Query ( field ) ; jj_consume_token ( RPAREN ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case CARAT : jj_consume_token ( CARAT ) ; boost = jj_consume_token ( NUMBER ) ; break ; default : jj_la1 [ 6 ] = jj_gen ; ; } break ; default : jj_la1 [ 7 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } if ( boost != null ) { float f = ( float ) 1.0 ; try { f = Float . valueOf ( boost . image ) . floatValue ( ) ; q . setBoost ( f ) ; } catch ( Exception ignored ) { } } { if ( true ) return q ; } throw new Error ( "Missing return statement in function" ) ; } final public Query Term ( String field ) throws ParseException { Token term , boost = null , fuzzySlop = null , goop1 , goop2 ; boolean prefix = false ; boolean wildcard = false ; boolean fuzzy = false ; boolean rangein = false ; Query q ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case STAR : case TERM : case PREFIXTERM : case WILDTERM : case NUMBER : switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case TERM : term = jj_consume_token ( TERM ) ; break ; case STAR : term = jj_consume_token ( STAR ) ; wildcard = true ; break ; case PREFIXTERM : term = jj_consume_token ( PREFIXTERM ) ; prefix = true ; break ; case WILDTERM : term = jj_consume_token ( WILDTERM ) ; wildcard = true ; break ; case NUMBER : term = jj_consume_token ( NUMBER ) ; break ; default : jj_la1 [ 8 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case FUZZY_SLOP : fuzzySlop = jj_consume_token ( FUZZY_SLOP ) ; fuzzy = true ; break ; default : jj_la1 [ 9 ] = jj_gen ; ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case CARAT : jj_consume_token ( CARAT ) ; boost = jj_consume_token ( NUMBER ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case FUZZY_SLOP : fuzzySlop = jj_consume_token ( FUZZY_SLOP ) ; fuzzy = true ; break ; default : jj_la1 [ 10 ] = jj_gen ; ; } break ; default : jj_la1 [ 11 ] = jj_gen ; ; } String termImage = discardEscapeChar ( term . image ) ; if ( wildcard ) { q = getWildcardQuery ( field , termImage ) ; } else if ( prefix ) { q = getPrefixQuery ( field , discardEscapeChar ( term . image . substring ( 0 , term . image . length ( ) - 1 ) ) ) ; } else if ( fuzzy ) { float fms = fuzzyMinSim ; try { fms = Float . valueOf ( fuzzySlop . image . substring ( 1 ) ) . floatValue ( ) ; } catch ( Exception ignored ) { } if ( fms < 0.0f || fms > 1.0f ) { { if ( true ) throw new ParseException ( "Minimum similarity for a FuzzyQuery has to be between 0.0f and 1.0f !" ) ; } } q = getFuzzyQuery ( field , termImage , fms ) ; } else { q = getFieldQuery ( field , termImage ) ; } break ; case RANGEIN_START : jj_consume_token ( RANGEIN_START ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case RANGEIN_GOOP : goop1 = jj_consume_token ( RANGEIN_GOOP ) ; break ; case RANGEIN_QUOTED : goop1 = jj_consume_token ( RANGEIN_QUOTED ) ; break ; default : jj_la1 [ 12 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case RANGEIN_TO : jj_consume_token ( RANGEIN_TO ) ; break ; default : jj_la1 [ 13 ] = jj_gen ; ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case RANGEIN_GOOP : goop2 = jj_consume_token ( RANGEIN_GOOP ) ; break ; case RANGEIN_QUOTED : goop2 = jj_consume_token ( RANGEIN_QUOTED ) ; break ; default : jj_la1 [ 14 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } jj_consume_token ( RANGEIN_END ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case CARAT : jj_consume_token ( CARAT ) ; boost = jj_consume_token ( NUMBER ) ; break ; default : jj_la1 [ 15 ] = jj_gen ; ; } if ( goop1 . kind == RANGEIN_QUOTED ) { goop1 . image = goop1 . image . substring ( 1 , goop1 . image . length ( ) - 1 ) ; } if ( goop2 . kind == RANGEIN_QUOTED ) { goop2 . image = goop2 . image . substring ( 1 , goop2 . image . length ( ) - 1 ) ; } q = getRangeQuery ( field , discardEscapeChar ( goop1 . image ) , discardEscapeChar ( goop2 . image ) , true ) ; break ; case RANGEEX_START : jj_consume_token ( RANGEEX_START ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case RANGEEX_GOOP : goop1 = jj_consume_token ( RANGEEX_GOOP ) ; break ; case RANGEEX_QUOTED : goop1 = jj_consume_token ( RANGEEX_QUOTED ) ; break ; default : jj_la1 [ 16 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case RANGEEX_TO : jj_consume_token ( RANGEEX_TO ) ; break ; default : jj_la1 [ 17 ] = jj_gen ; ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case RANGEEX_GOOP : goop2 = jj_consume_token ( RANGEEX_GOOP ) ; break ; case RANGEEX_QUOTED : goop2 = jj_consume_token ( RANGEEX_QUOTED ) ; break ; default : jj_la1 [ 18 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } jj_consume_token ( RANGEEX_END ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case CARAT : jj_consume_token ( CARAT ) ; boost = jj_consume_token ( NUMBER ) ; break ; default : jj_la1 [ 19 ] = jj_gen ; ; } if ( goop1 . kind == RANGEEX_QUOTED ) { goop1 . image = goop1 . image . substring ( 1 , goop1 . image . length ( ) - 1 ) ; } if ( goop2 . kind == RANGEEX_QUOTED ) { goop2 . image = goop2 . image . substring ( 1 , goop2 . image . length ( ) - 1 ) ; } q = getRangeQuery ( field , discardEscapeChar ( goop1 . image ) , discardEscapeChar ( goop2 . image ) , false ) ; break ; case QUOTED : term = jj_consume_token ( QUOTED ) ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case FUZZY_SLOP : fuzzySlop = jj_consume_token ( FUZZY_SLOP ) ; break ; default : jj_la1 [ 20 ] = jj_gen ; ; } switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case CARAT : jj_consume_token ( CARAT ) ; boost = jj_consume_token ( NUMBER ) ; break ; default : jj_la1 [ 21 ] = jj_gen ; ; } int s = phraseSlop ; if ( fuzzySlop != null ) { try { s = Float . valueOf ( fuzzySlop . image . substring ( 1 ) ) . intValue ( ) ; } catch ( Exception ignored ) { } } q = getFieldQuery ( field , discardEscapeChar ( term . image . substring ( 1 , term . image . length ( ) - 1 ) ) , s ) ; break ; default : jj_la1 [ 22 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } if ( boost != null ) { float f = ( float ) 1.0 ; try { f = Float . valueOf ( boost . image ) . floatValue ( ) ; } catch ( Exception ignored ) { } if ( q != null ) { q . setBoost ( f ) ; } } { if ( true ) return q ; } throw new Error ( "Missing return statement in function" ) ; } final private boolean jj_2_1 ( int xla ) { jj_la = xla ; jj_lastpos = jj_scanpos = token ; try { return ! jj_3_1 ( ) ; } catch ( LookaheadSuccess ls ) { return true ; } finally { jj_save ( 0 , xla ) ; } } final private boolean jj_3_1 ( ) { Token xsp ; xsp = jj_scanpos ; if ( jj_3R_2 ( ) ) { jj_scanpos = xsp ; if ( jj_3R_3 ( ) ) return true ; } return false ; } final private boolean jj_3R_3 ( ) { if ( jj_scan_token ( STAR ) ) return true ; if ( jj_scan_token ( COLON ) ) return true ; return false ; } final private boolean jj_3R_2 ( ) { if ( jj_scan_token ( TERM ) ) return true ; if ( jj_scan_token ( COLON ) ) return true ; return false ; } public QueryParserTokenManager token_source ; public Token token , jj_nt ; private int jj_ntk ; private Token jj_scanpos , jj_lastpos ; private int jj_la ; public boolean lookingAhead = false ; private boolean jj_semLA ; private int jj_gen ; final private int [ ] jj_la1 = new int [ 23 ] ; static private int [ ] jj_la1_0 ; static private int [ ] jj_la1_1 ; static { jj_la1_0 ( ) ; jj_la1_1 ( ) ; } private static void jj_la1_0 ( ) { jj_la1_0 = new int [ ] { 0x180 , 0x180 , 0xe00 , 0xe00 , 0x1f69f80 , 0x48000 , 0x10000 , 0x1f69000 , 0x1348000 , 0x80000 , 0x80000 , 0x10000 , 0x18000000 , 0x2000000 , 0x18000000 , 0x10000 , 0x80000000 , 0x20000000 , 0x80000000 , 0x10000 , 0x80000 , 0x10000 , 0x1f68000 , } ; } private static void jj_la1_1 ( ) { jj_la1_1 = new int [ ] { 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x1 , 0x0 , 0x1 , 0x0 , 0x0 , 0x0 , 0x0 , } ; } final private JJCalls [ ] jj_2_rtns = new JJCalls [ 1 ] ; private boolean jj_rescan = false ; private int jj_gc = 0 ; public QueryParser ( CharStream stream ) { token_source = new QueryParserTokenManager ( stream ) ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 23 ; i ++ ) jj_la1 [ i ] = - 1 ; for ( int i = 0 ; i < jj_2_rtns . length ; i ++ ) jj_2_rtns [ i ] = new JJCalls ( ) ; } public void ReInit ( CharStream stream ) { token_source . ReInit ( stream ) ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 23 ; i ++ ) jj_la1 [ i ] = - 1 ; for ( int i = 0 ; i < jj_2_rtns . length ; i ++ ) jj_2_rtns [ i ] = new JJCalls ( ) ; } public QueryParser ( QueryParserTokenManager tm ) { token_source = tm ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 23 ; i ++ ) jj_la1 [ i ] = - 1 ; for ( int i = 0 ; i < jj_2_rtns . length ; i ++ ) jj_2_rtns [ i ] = new JJCalls ( ) ; } public void ReInit ( QueryParserTokenManager tm ) { token_source = tm ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 23 ; i ++ ) jj_la1 [ i ] = - 1 ; for ( int i = 0 ; i < jj_2_rtns . length ; i ++ ) jj_2_rtns [ i ] = new JJCalls ( ) ; } final private Token jj_consume_token ( int kind ) throws ParseException { Token oldToken ; if ( ( oldToken = token ) . next != null ) token = token . next ; else token = token . next = token_source . getNextToken ( ) ; jj_ntk = - 1 ; if ( token . kind == kind ) { jj_gen ++ ; if ( ++ jj_gc > 100 ) { jj_gc = 0 ; for ( int i = 0 ; i < jj_2_rtns . length ; i ++ ) { JJCalls c = jj_2_rtns [ i ] ; while ( c != null ) { if ( c . gen < jj_gen ) c . first = null ; c = c . next ; } } } return token ; } token = oldToken ; jj_kind = kind ; throw generateParseException ( ) ; } static private final class LookaheadSuccess extends java . lang . Error { } final private LookaheadSuccess jj_ls = new LookaheadSuccess ( ) ; final private boolean jj_scan_token ( int kind ) { if ( jj_scanpos == jj_lastpos ) { jj_la -- ; if ( jj_scanpos . next == null ) { jj_lastpos = jj_scanpos = jj_scanpos . next = token_source . getNextToken ( ) ; } else { jj_lastpos = jj_scanpos = jj_scanpos . next ; } } else { jj_scanpos = jj_scanpos . next ; } if ( jj_rescan ) { int i = 0 ; Token tok = token ; while ( tok != null && tok != jj_scanpos ) { i ++ ; tok = tok . next ; } if ( tok != null ) jj_add_error_token ( kind , i ) ; } if ( jj_scanpos . kind != kind ) return true ; if ( jj_la == 0 && jj_scanpos == jj_lastpos ) throw jj_ls ; return false ; } final public Token getNextToken ( ) { if ( token . next != null ) token = token . next ; else token = token . next = token_source . getNextToken ( ) ; jj_ntk = - 1 ; jj_gen ++ ; return token ; } final public Token getToken ( int index ) { Token t = lookingAhead ? jj_scanpos : token ; for ( int i = 0 ; i < index ; i ++ ) { if ( t . next != null ) t = t . next ; else t = t . next = token_source . getNextToken ( ) ; } return t ; } final private int jj_ntk ( ) { if ( ( jj_nt = token . next ) == null ) return ( jj_ntk = ( token . next = token_source . getNextToken ( ) ) . kind ) ; else return ( jj_ntk = jj_nt . kind ) ; } private java . util . Vector jj_expentries = new java . util . Vector ( ) ; private int [ ] jj_expentry ; private int jj_kind = - 1 ; private int [ ] jj_lasttokens = new int [ 100 ] ; private int jj_endpos ; private void jj_add_error_token ( int kind , int pos ) { if ( pos >= 100 ) return ; if ( pos == jj_endpos + 1 ) { jj_lasttokens [ jj_endpos ++ ] = kind ; } else if ( jj_endpos != 0 ) { jj_expentry = new int [ jj_endpos ] ; for ( int i = 0 ; i < jj_endpos ; i ++ ) { jj_expentry [ i ] = jj_lasttokens [ i ] ; } boolean exists = false ; for ( java . util . Enumeration e = jj_expentries . elements ( ) ; e . hasMoreElements ( ) ; ) { int [ ] oldentry = ( int [ ] ) ( e . nextElement ( ) ) ; if ( oldentry . length == jj_expentry . length ) { exists = true ; for ( int i = 0 ; i < jj_expentry . length ; i ++ ) { if ( oldentry [ i ] != jj_expentry [ i ] ) { exists = false ; break ; } } if ( exists ) break ; } } if ( ! exists ) jj_expentries . addElement ( jj_expentry ) ; if ( pos != 0 ) jj_lasttokens [ ( jj_endpos = pos ) - 1 ] = kind ; } } public ParseException generateParseException ( ) { jj_expentries . removeAllElements ( ) ; boolean [ ] la1tokens = new boolean [ 33 ] ; for ( int i = 0 ; i < 33 ; i ++ ) { la1tokens [ i ] = false ; } if ( jj_kind >= 0 ) { la1tokens [ jj_kind ] = true ; jj_kind = - 1 ; } for ( int i = 0 ; i < 23 ; i ++ ) { if ( jj_la1 [ i ] == jj_gen ) { for ( int j = 0 ; j < 32 ; j ++ ) { if ( ( jj_la1_0 [ i ] & ( 1 << j ) ) != 0 ) { la1tokens [ j ] = true ; } if ( ( jj_la1_1 [ i ] & ( 1 << j ) ) != 0 ) { la1tokens [ 32 + j ] = true ; } } } } for ( int i = 0 ; i < 33 ; i ++ ) { if ( la1tokens [ i ] ) { jj_expentry = new int [ 1 ] ; jj_expentry [ 0 ] = i ; jj_expentries . addElement ( jj_expentry ) ; } } jj_endpos = 0 ; jj_rescan_token ( ) ; jj_add_error_token ( 0 , 0 ) ; int [ ] [ ] exptokseq = new int [ jj_expentries . size ( ) ] [ ] ; for ( int i = 0 ; i < jj_expentries . size ( ) ; i ++ ) { exptokseq [ i ] = ( int [ ] ) jj_expentries . elementAt ( i ) ; } return new ParseException ( token , exptokseq , tokenImage ) ; } final public void enable_tracing ( ) { } final public void disable_tracing ( ) { } final private void jj_rescan_token ( ) { jj_rescan = true ; for ( int i = 0 ; i < 1 ; i ++ ) { JJCalls p = jj_2_rtns [ i ] ; do { if ( p . gen > jj_gen ) { jj_la = p . arg ; jj_lastpos = jj_scanpos = p . first ; switch ( i ) { case 0 : jj_3_1 ( ) ; break ; } } p = p . next ; } while ( p != null ) ; } jj_rescan = false ; } final private void jj_save ( int index , int xla ) { JJCalls p = jj_2_rtns [ index ] ; while ( p . gen > jj_gen ) { if ( p . next == null ) { p = p . next = new JJCalls ( ) ; break ; } p = p . next ; } p . gen = jj_gen + xla - jj_la ; p . first = token ; p . arg = xla ; } static final class JJCalls { int gen ; Token first ; int arg ; JJCalls next ; } } 	1	['64', '1', '1', '34', '151', '1520', '1', '33', '40', '0.893046107', '3329', '0.595238095', '12', '0', '0.112169312', '0', '0', '50.359375', '23', '2.4844', '7']
package org . apache . lucene . analysis ; import java . io . Reader ; public class LetterTokenizer extends CharTokenizer { public LetterTokenizer ( Reader in ) { super ( in ) ; } protected boolean isTokenChar ( char c ) { return Character . isLetter ( c ) ; } } 	0	['2', '4', '1', '2', '4', '1', '1', '1', '1', '2', '9', '0', '0', '0.875', '0.666666667', '1', '1', '3.5', '1', '0.5', '0']
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; import java . text . Collator ; import java . util . Locale ; class FieldDocSortedHitQueue extends PriorityQueue { volatile SortField [ ] fields ; volatile Collator [ ] collators ; FieldDocSortedHitQueue ( SortField [ ] fields , int size ) { this . fields = fields ; this . collators = hasCollators ( fields ) ; initialize ( size ) ; } synchronized void setFields ( SortField [ ] fields ) { if ( this . fields == null ) { this . fields = fields ; this . collators = hasCollators ( fields ) ; } } SortField [ ] getFields ( ) { return fields ; } private Collator [ ] hasCollators ( final SortField [ ] fields ) { if ( fields == null ) return null ; Collator [ ] ret = new Collator [ fields . length ] ; for ( int i = 0 ; i < fields . length ; ++ i ) { Locale locale = fields [ i ] . getLocale ( ) ; if ( locale != null ) ret [ i ] = Collator . getInstance ( locale ) ; } return ret ; } protected final boolean lessThan ( final Object a , final Object b ) { final FieldDoc docA = ( FieldDoc ) a ; final FieldDoc docB = ( FieldDoc ) b ; final int n = fields . length ; int c = 0 ; for ( int i = 0 ; i < n && c == 0 ; ++ i ) { final int type = fields [ i ] . getType ( ) ; switch ( type ) { case SortField . SCORE : float r1 = ( ( Float ) docA . fields [ i ] ) . floatValue ( ) ; float r2 = ( ( Float ) docB . fields [ i ] ) . floatValue ( ) ; if ( r1 > r2 ) c = - 1 ; if ( r1 < r2 ) c = 1 ; break ; case SortField . DOC : case SortField . INT : int i1 = ( ( Integer ) docA . fields [ i ] ) . intValue ( ) ; int i2 = ( ( Integer ) docB . fields [ i ] ) . intValue ( ) ; if ( i1 < i2 ) c = - 1 ; if ( i1 > i2 ) c = 1 ; break ; case SortField . STRING : String s1 = ( String ) docA . fields [ i ] ; String s2 = ( String ) docB . fields [ i ] ; if ( s1 == null ) c = ( s2 == null ) ? 0 : - 1 ; else if ( s2 == null ) c = 1 ; else if ( fields [ i ] . getLocale ( ) == null ) { c = s1 . compareTo ( s2 ) ; } else { c = collators [ i ] . compare ( s1 , s2 ) ; } break ; case SortField . FLOAT : float f1 = ( ( Float ) docA . fields [ i ] ) . floatValue ( ) ; float f2 = ( ( Float ) docB . fields [ i ] ) . floatValue ( ) ; if ( f1 < f2 ) c = - 1 ; if ( f1 > f2 ) c = 1 ; break ; case SortField . CUSTOM : c = docA . fields [ i ] . compareTo ( docB . fields [ i ] ) ; break ; case SortField . AUTO : throw new RuntimeException ( "FieldDocSortedHitQueue cannot use an AUTO SortField" ) ; default : throw new RuntimeException ( "invalid SortField type: " + type ) ; } if ( fields [ i ] . getReverse ( ) ) { c = - c ; } } if ( c == 0 ) return docA . doc > docB . doc ; return c > 0 ; } } 	1	['5', '2', '0', '6', '21', '0', '3', '3', '0', '0.375', '274', '0', '1', '0.733333333', '0.5', '1', '3', '53.4', '18', '5', '2']
package org . apache . lucene . search ; import org . apache . lucene . util . Parameter ; public class BooleanClause implements java . io . Serializable { public static final class Occur extends Parameter implements java . io . Serializable { private Occur ( String name ) { super ( name ) ; } public String toString ( ) { if ( this == MUST ) return "+" ; if ( this == MUST_NOT ) return "-" ; return "" ; } public static final Occur MUST = new Occur ( "MUST" ) ; public static final Occur SHOULD = new Occur ( "SHOULD" ) ; public static final Occur MUST_NOT = new Occur ( "MUST_NOT" ) ; } private Query query ; private Occur occur ; public BooleanClause ( Query query , Occur occur ) { this . query = query ; this . occur = occur ; } public Occur getOccur ( ) { return occur ; } public void setOccur ( Occur occur ) { this . occur = occur ; } public Query getQuery ( ) { return query ; } public void setQuery ( Query query ) { this . query = query ; } public boolean isProhibited ( ) { return Occur . MUST_NOT . equals ( occur ) ; } public boolean isRequired ( ) { return Occur . MUST . equals ( occur ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof BooleanClause ) ) return false ; BooleanClause other = ( BooleanClause ) o ; return this . query . equals ( other . query ) && this . occur . equals ( other . occur ) ; } public int hashCode ( ) { return query . hashCode ( ) ^ ( Occur . MUST . equals ( occur ) ? 1 : 0 ) ^ ( Occur . MUST_NOT . equals ( occur ) ? 2 : 0 ) ; } public String toString ( ) { return occur . toString ( ) + query . toString ( ) ; } } 	0	['10', '1', '0', '6', '18', '0', '5', '2', '10', '0.333333333', '104', '1', '2', '0', '0.375', '1', '1', '9.2', '4', '1.4', '0']
package org . apache . lucene . search . function ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . FieldCache ; public abstract class FieldCacheSource extends ValueSource { private String field ; private FieldCache cache = FieldCache . DEFAULT ; public FieldCacheSource ( String field ) { this . field = field ; } public final DocValues getValues ( IndexReader reader ) throws IOException { return getCachedFieldValues ( cache , field , reader ) ; } public String description ( ) { return field ; } public abstract DocValues getCachedFieldValues ( FieldCache cache , String field , IndexReader reader ) throws IOException ; public final boolean equals ( Object o ) { if ( ! ( o instanceof FieldCacheSource ) ) { return false ; } FieldCacheSource other = ( FieldCacheSource ) o ; return this . cache == other . cache && this . field . equals ( other . field ) && cachedFieldSourceEquals ( other ) ; } public final int hashCode ( ) { return cache . hashCode ( ) + field . hashCode ( ) + cachedFieldSourceHashCode ( ) ; } public abstract boolean cachedFieldSourceEquals ( FieldCacheSource other ) ; public abstract int cachedFieldSourceHashCode ( ) ; } 	1	['8', '2', '4', '8', '12', '8', '4', '4', '8', '0.5', '68', '1', '1', '0.416666667', '0.3125', '2', '2', '7.25', '5', '1.375', '2']
package org . apache . lucene . util ; public class ToStringUtils { public static String boost ( float boost ) { if ( boost != 1.0f ) { return "^" + Float . toString ( boost ) ; } else return "" ; } } 	0	['2', '1', '0', '17', '7', '1', '17', '0', '2', '2', '21', '0', '0', '0', '0.5', '0', '0', '9.5', '2', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; import org . apache . lucene . search . BooleanClause . Occur ; import java . io . IOException ; import java . util . * ; public class BooleanQuery extends Query { private static int maxClauseCount = 1024 ; public static class TooManyClauses extends RuntimeException { public TooManyClauses ( ) { } public String getMessage ( ) { return "maxClauseCount is set to " + maxClauseCount ; } } public static int getMaxClauseCount ( ) { return maxClauseCount ; } public static void setMaxClauseCount ( int maxClauseCount ) { if ( maxClauseCount < 1 ) throw new IllegalArgumentException ( "maxClauseCount must be >= 1" ) ; BooleanQuery . maxClauseCount = maxClauseCount ; } private ArrayList clauses = new ArrayList ( ) ; private boolean disableCoord ; public BooleanQuery ( ) { } public BooleanQuery ( boolean disableCoord ) { this . disableCoord = disableCoord ; } public boolean isCoordDisabled ( ) { return disableCoord ; } public Similarity getSimilarity ( Searcher searcher ) { Similarity result = super . getSimilarity ( searcher ) ; if ( disableCoord ) { result = new SimilarityDelegator ( result ) { public float coord ( int overlap , int maxOverlap ) { return 1.0f ; } } ; } return result ; } public void setMinimumNumberShouldMatch ( int min ) { this . minNrShouldMatch = min ; } protected int minNrShouldMatch = 0 ; public int getMinimumNumberShouldMatch ( ) { return minNrShouldMatch ; } public void add ( Query query , BooleanClause . Occur occur ) { add ( new BooleanClause ( query , occur ) ) ; } public void add ( BooleanClause clause ) { if ( clauses . size ( ) >= maxClauseCount ) throw new TooManyClauses ( ) ; clauses . add ( clause ) ; } public BooleanClause [ ] getClauses ( ) { return ( BooleanClause [ ] ) clauses . toArray ( new BooleanClause [ clauses . size ( ) ] ) ; } public List clauses ( ) { return clauses ; } private class BooleanWeight implements Weight { protected Similarity similarity ; protected Vector weights = new Vector ( ) ; public BooleanWeight ( Searcher searcher ) throws IOException { this . similarity = getSimilarity ( searcher ) ; for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . get ( i ) ; weights . add ( c . getQuery ( ) . createWeight ( searcher ) ) ; } } public Query getQuery ( ) { return BooleanQuery . this ; } public float getValue ( ) { return getBoost ( ) ; } public float sumOfSquaredWeights ( ) throws IOException { float sum = 0.0f ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . get ( i ) ; Weight w = ( Weight ) weights . elementAt ( i ) ; float s = w . sumOfSquaredWeights ( ) ; if ( ! c . isProhibited ( ) ) sum += s ; } sum *= getBoost ( ) * getBoost ( ) ; return sum ; } public void normalize ( float norm ) { norm *= getBoost ( ) ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . get ( i ) ; Weight w = ( Weight ) weights . elementAt ( i ) ; w . normalize ( norm ) ; } } public Scorer scorer ( IndexReader reader ) throws IOException { BooleanScorer2 result = new BooleanScorer2 ( similarity , minNrShouldMatch , allowDocsOutOfOrder ) ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . get ( i ) ; Weight w = ( Weight ) weights . elementAt ( i ) ; Scorer subScorer = w . scorer ( reader ) ; if ( subScorer != null ) result . add ( subScorer , c . isRequired ( ) , c . isProhibited ( ) ) ; else if ( c . isRequired ( ) ) return null ; } return result ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { final int minShouldMatch = BooleanQuery . this . getMinimumNumberShouldMatch ( ) ; ComplexExplanation sumExpl = new ComplexExplanation ( ) ; sumExpl . setDescription ( "sum of:" ) ; int coord = 0 ; int maxCoord = 0 ; float sum = 0.0f ; boolean fail = false ; int shouldMatchCount = 0 ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . get ( i ) ; Weight w = ( Weight ) weights . elementAt ( i ) ; Explanation e = w . explain ( reader , doc ) ; if ( ! c . isProhibited ( ) ) maxCoord ++ ; if ( e . isMatch ( ) ) { if ( ! c . isProhibited ( ) ) { sumExpl . addDetail ( e ) ; sum += e . getValue ( ) ; coord ++ ; } else { Explanation r = new Explanation ( 0.0f , "match on prohibited clause (" + c . getQuery ( ) . toString ( ) + ")" ) ; r . addDetail ( e ) ; sumExpl . addDetail ( r ) ; fail = true ; } if ( c . getOccur ( ) . equals ( Occur . SHOULD ) ) shouldMatchCount ++ ; } else if ( c . isRequired ( ) ) { Explanation r = new Explanation ( 0.0f , "no match on required clause (" + c . getQuery ( ) . toString ( ) + ")" ) ; r . addDetail ( e ) ; sumExpl . addDetail ( r ) ; fail = true ; } } if ( fail ) { sumExpl . setMatch ( Boolean . FALSE ) ; sumExpl . setValue ( 0.0f ) ; sumExpl . setDescription ( "Failure to meet condition(s) of required/prohibited clause(s)" ) ; return sumExpl ; } else if ( shouldMatchCount < minShouldMatch ) { sumExpl . setMatch ( Boolean . FALSE ) ; sumExpl . setValue ( 0.0f ) ; sumExpl . setDescription ( "Failure to match minimum number " + "of optional clauses: " + minShouldMatch ) ; return sumExpl ; } sumExpl . setMatch ( 0 < coord ? Boolean . TRUE : Boolean . FALSE ) ; sumExpl . setValue ( sum ) ; float coordFactor = similarity . coord ( coord , maxCoord ) ; if ( coordFactor == 1.0f ) return sumExpl ; else { ComplexExplanation result = new ComplexExplanation ( sumExpl . isMatch ( ) , sum * coordFactor , "product of:" ) ; result . addDetail ( sumExpl ) ; result . addDetail ( new Explanation ( coordFactor , "coord(" + coord + "/" + maxCoord + ")" ) ) ; return result ; } } } private static boolean allowDocsOutOfOrder = false ; public static void setAllowDocsOutOfOrder ( boolean allow ) { allowDocsOutOfOrder = allow ; } public static boolean getAllowDocsOutOfOrder ( ) { return allowDocsOutOfOrder ; } public static void setUseScorer14 ( boolean use14 ) { setAllowDocsOutOfOrder ( use14 ) ; } public static boolean getUseScorer14 ( ) { return getAllowDocsOutOfOrder ( ) ; } protected Weight createWeight ( Searcher searcher ) throws IOException { return new BooleanWeight ( searcher ) ; } public Query rewrite ( IndexReader reader ) throws IOException { if ( clauses . size ( ) == 1 ) { BooleanClause c = ( BooleanClause ) clauses . get ( 0 ) ; if ( ! c . isProhibited ( ) ) { Query query = c . getQuery ( ) . rewrite ( reader ) ; if ( getBoost ( ) != 1.0f ) { if ( query == c . getQuery ( ) ) query = ( Query ) query . clone ( ) ; query . setBoost ( getBoost ( ) * query . getBoost ( ) ) ; } return query ; } } BooleanQuery clone = null ; for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . get ( i ) ; Query query = c . getQuery ( ) . rewrite ( reader ) ; if ( query != c . getQuery ( ) ) { if ( clone == null ) clone = ( BooleanQuery ) this . clone ( ) ; clone . clauses . set ( i , new BooleanClause ( query , c . getOccur ( ) ) ) ; } } if ( clone != null ) { return clone ; } else return this ; } public void extractTerms ( Set terms ) { for ( Iterator i = clauses . iterator ( ) ; i . hasNext ( ) ; ) { BooleanClause clause = ( BooleanClause ) i . next ( ) ; clause . getQuery ( ) . extractTerms ( terms ) ; } } public Object clone ( ) { BooleanQuery clone = ( BooleanQuery ) super . clone ( ) ; clone . clauses = ( ArrayList ) this . clauses . clone ( ) ; return clone ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; boolean needParens = ( getBoost ( ) != 1.0 ) || ( getMinimumNumberShouldMatch ( ) > 0 ) ; if ( needParens ) { buffer . append ( "(" ) ; } for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . get ( i ) ; if ( c . isProhibited ( ) ) buffer . append ( "-" ) ; else if ( c . isRequired ( ) ) buffer . append ( "+" ) ; Query subQuery = c . getQuery ( ) ; if ( subQuery instanceof BooleanQuery ) { buffer . append ( "(" ) ; buffer . append ( c . getQuery ( ) . toString ( field ) ) ; buffer . append ( ")" ) ; } else buffer . append ( c . getQuery ( ) . toString ( field ) ) ; if ( i != clauses . size ( ) - 1 ) buffer . append ( " " ) ; } if ( needParens ) { buffer . append ( ")" ) ; } if ( getMinimumNumberShouldMatch ( ) > 0 ) { buffer . append ( '~' ) ; buffer . append ( getMinimumNumberShouldMatch ( ) ) ; } if ( getBoost ( ) != 1.0f ) { buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; } return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof BooleanQuery ) ) return false ; BooleanQuery other = ( BooleanQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && this . clauses . equals ( other . clauses ) && this . getMinimumNumberShouldMatch ( ) == other . getMinimumNumberShouldMatch ( ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ clauses . hashCode ( ) + getMinimumNumberShouldMatch ( ) ; } } 	1	['27', '2', '0', '19', '64', '171', '12', '11', '22', '0.769230769', '443', '1', '0', '0.333333333', '0.102564103', '2', '6', '15.22222222', '12', '1.5926', '3']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import java . io . IOException ; public class WildcardQuery extends MultiTermQuery { private boolean termContainsWildcard ; public WildcardQuery ( Term term ) { super ( term ) ; this . termContainsWildcard = ( term . text ( ) . indexOf ( '*' ) != - 1 ) || ( term . text ( ) . indexOf ( '?' ) != - 1 ) ; } protected FilteredTermEnum getEnum ( IndexReader reader ) throws IOException { return new WildcardTermEnum ( reader , getTerm ( ) ) ; } public boolean equals ( Object o ) { if ( o instanceof WildcardQuery ) return super . equals ( o ) ; return false ; } public Query rewrite ( IndexReader reader ) throws IOException { if ( this . termContainsWildcard ) { return super . rewrite ( reader ) ; } return new TermQuery ( getTerm ( ) ) ; } } 	0	['4', '3', '0', '8', '12', '4', '1', '7', '3', '0.666666667', '55', '1', '0', '0.857142857', '0.5', '1', '1', '12.5', '2', '1', '0']
package org . apache . lucene . index ; public final class Term implements Comparable , java . io . Serializable { String field ; String text ; public Term ( String fld , String txt ) { this ( fld , txt , true ) ; } Term ( String fld , String txt , boolean intern ) { field = intern ? fld . intern ( ) : fld ; text = txt ; } public final String field ( ) { return field ; } public final String text ( ) { return text ; } public Term createTerm ( String text ) { return new Term ( field , text , false ) ; } public final boolean equals ( Object o ) { if ( o == this ) return true ; if ( o == null ) return false ; if ( ! ( o instanceof Term ) ) return false ; Term other = ( Term ) o ; return field == other . field && text . equals ( other . text ) ; } public final int hashCode ( ) { return field . hashCode ( ) + text . hashCode ( ) ; } public int compareTo ( Object other ) { return compareTo ( ( Term ) other ) ; } public final int compareTo ( Term other ) { if ( field == other . field ) return text . compareTo ( other . text ) ; else return field . compareTo ( other . field ) ; } final void set ( String fld , String txt ) { field = fld ; text = txt ; } public final String toString ( ) { return field + ":" + text ; } private void readObject ( java . io . ObjectInputStream in ) throws java . io . IOException , ClassNotFoundException { in . defaultReadObject ( ) ; field = field . intern ( ) ; } } 	1	['12', '1', '0', '71', '21', '0', '71', '0', '9', '0.136363636', '138', '0', '0', '0', '0.291666667', '1', '1', '10.33333333', '6', '1.3333', '1']
package org . apache . lucene . document ; import org . apache . lucene . search . PrefixQuery ; import org . apache . lucene . search . RangeQuery ; import java . util . Date ; public class DateField { private DateField ( ) { } private static int DATE_LEN = Long . toString ( 1000L * 365 * 24 * 60 * 60 * 1000 , Character . MAX_RADIX ) . length ( ) ; public static String MIN_DATE_STRING ( ) { return timeToString ( 0 ) ; } public static String MAX_DATE_STRING ( ) { char [ ] buffer = new char [ DATE_LEN ] ; char c = Character . forDigit ( Character . MAX_RADIX - 1 , Character . MAX_RADIX ) ; for ( int i = 0 ; i < DATE_LEN ; i ++ ) buffer [ i ] = c ; return new String ( buffer ) ; } public static String dateToString ( Date date ) { return timeToString ( date . getTime ( ) ) ; } public static String timeToString ( long time ) { if ( time < 0 ) throw new RuntimeException ( "time '" + time + "' is too early, must be >= 0" ) ; String s = Long . toString ( time , Character . MAX_RADIX ) ; if ( s . length ( ) > DATE_LEN ) throw new RuntimeException ( "time '" + time + "' is too late, length of string " + "representation must be <= " + DATE_LEN ) ; if ( s . length ( ) < DATE_LEN ) { StringBuffer sb = new StringBuffer ( s ) ; while ( sb . length ( ) < DATE_LEN ) sb . insert ( 0 , 0 ) ; s = sb . toString ( ) ; } return s ; } public static long stringToTime ( String s ) { return Long . parseLong ( s , Character . MAX_RADIX ) ; } public static Date stringToDate ( String s ) { return new Date ( stringToTime ( s ) ) ; } } 	0	['8', '1', '0', '1', '25', '22', '1', '0', '6', '0.428571429', '126', '1', '0', '0', '0.178571429', '0', '0', '14.625', '5', '1.375', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . Set ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermDocs ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; public class TermQuery extends Query { private Term term ; private class TermWeight implements Weight { private Similarity similarity ; private float value ; private float idf ; private float queryNorm ; private float queryWeight ; public TermWeight ( Searcher searcher ) throws IOException { this . similarity = getSimilarity ( searcher ) ; idf = similarity . idf ( term , searcher ) ; } public String toString ( ) { return "weight(" + TermQuery . this + ")" ; } public Query getQuery ( ) { return TermQuery . this ; } public float getValue ( ) { return value ; } public float sumOfSquaredWeights ( ) { queryWeight = idf * getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float queryNorm ) { this . queryNorm = queryNorm ; queryWeight *= queryNorm ; value = queryWeight * idf ; } public Scorer scorer ( IndexReader reader ) throws IOException { TermDocs termDocs = reader . termDocs ( term ) ; if ( termDocs == null ) return null ; return new TermScorer ( this , termDocs , similarity , reader . norms ( term . field ( ) ) ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { ComplexExplanation result = new ComplexExplanation ( ) ; result . setDescription ( "weight(" + getQuery ( ) + " in " + doc + "), product of:" ) ; Explanation idfExpl = new Explanation ( idf , "idf(docFreq=" + reader . docFreq ( term ) + ")" ) ; Explanation queryExpl = new Explanation ( ) ; queryExpl . setDescription ( "queryWeight(" + getQuery ( ) + "), product of:" ) ; Explanation boostExpl = new Explanation ( getBoost ( ) , "boost" ) ; if ( getBoost ( ) != 1.0f ) queryExpl . addDetail ( boostExpl ) ; queryExpl . addDetail ( idfExpl ) ; Explanation queryNormExpl = new Explanation ( queryNorm , "queryNorm" ) ; queryExpl . addDetail ( queryNormExpl ) ; queryExpl . setValue ( boostExpl . getValue ( ) * idfExpl . getValue ( ) * queryNormExpl . getValue ( ) ) ; result . addDetail ( queryExpl ) ; String field = term . field ( ) ; ComplexExplanation fieldExpl = new ComplexExplanation ( ) ; fieldExpl . setDescription ( "fieldWeight(" + term + " in " + doc + "), product of:" ) ; Explanation tfExpl = scorer ( reader ) . explain ( doc ) ; fieldExpl . addDetail ( tfExpl ) ; fieldExpl . addDetail ( idfExpl ) ; Explanation fieldNormExpl = new Explanation ( ) ; byte [ ] fieldNorms = reader . norms ( field ) ; float fieldNorm = fieldNorms != null ? Similarity . decodeNorm ( fieldNorms [ doc ] ) : 0.0f ; fieldNormExpl . setValue ( fieldNorm ) ; fieldNormExpl . setDescription ( "fieldNorm(field=" + field + ", doc=" + doc + ")" ) ; fieldExpl . addDetail ( fieldNormExpl ) ; fieldExpl . setMatch ( Boolean . valueOf ( tfExpl . isMatch ( ) ) ) ; fieldExpl . setValue ( tfExpl . getValue ( ) * idfExpl . getValue ( ) * fieldNormExpl . getValue ( ) ) ; result . addDetail ( fieldExpl ) ; result . setMatch ( fieldExpl . getMatch ( ) ) ; result . setValue ( queryExpl . getValue ( ) * fieldExpl . getValue ( ) ) ; if ( queryExpl . getValue ( ) == 1.0f ) return fieldExpl ; return result ; } } public TermQuery ( Term t ) { term = t ; } public Term getTerm ( ) { return term ; } protected Weight createWeight ( Searcher searcher ) throws IOException { return new TermWeight ( searcher ) ; } public void extractTerms ( Set terms ) { terms . add ( getTerm ( ) ) ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! term . field ( ) . equals ( field ) ) { buffer . append ( term . field ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( term . text ( ) ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof TermQuery ) ) return false ; TermQuery other = ( TermQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && this . term . equals ( other . term ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ term . hashCode ( ) ; } } 	1	['8', '2', '0', '15', '22', '0', '10', '6', '6', '0.142857143', '100', '1', '1', '0.631578947', '0.232142857', '2', '2', '11.375', '4', '1.375', '1']
package org . apache . lucene . document ; import java . io . Serializable ; public final class FieldSelectorResult implements Serializable { public transient static final FieldSelectorResult LOAD = new FieldSelectorResult ( 0 ) ; public transient static final FieldSelectorResult LAZY_LOAD = new FieldSelectorResult ( 1 ) ; public transient static final FieldSelectorResult NO_LOAD = new FieldSelectorResult ( 2 ) ; public transient static final FieldSelectorResult LOAD_AND_BREAK = new FieldSelectorResult ( 3 ) ; public transient static final FieldSelectorResult LOAD_FOR_MERGE = new FieldSelectorResult ( 4 ) ; public transient static final FieldSelectorResult SIZE = new FieldSelectorResult ( 5 ) ; public transient static final FieldSelectorResult SIZE_AND_BREAK = new FieldSelectorResult ( 6 ) ; private int id ; private FieldSelectorResult ( int id ) { this . id = id ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( o == null || getClass ( ) != o . getClass ( ) ) return false ; final FieldSelectorResult that = ( FieldSelectorResult ) o ; if ( id != that . id ) return false ; return true ; } public int hashCode ( ) { return id ; } } 	0	['4', '1', '0', '7', '6', '0', '7', '0', '2', '0.875', '83', '0.125', '7', '0', '0.555555556', '1', '1', '17.75', '5', '1.5', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; import java . io . Reader ; public abstract class CharTokenizer extends Tokenizer { public CharTokenizer ( Reader input ) { super ( input ) ; } private int offset = 0 , bufferIndex = 0 , dataLen = 0 ; private static final int MAX_WORD_LEN = 255 ; private static final int IO_BUFFER_SIZE = 1024 ; private final char [ ] buffer = new char [ MAX_WORD_LEN ] ; private final char [ ] ioBuffer = new char [ IO_BUFFER_SIZE ] ; protected abstract boolean isTokenChar ( char c ) ; protected char normalize ( char c ) { return c ; } public final Token next ( ) throws IOException { int length = 0 ; int start = offset ; while ( true ) { final char c ; offset ++ ; if ( bufferIndex >= dataLen ) { dataLen = input . read ( ioBuffer ) ; bufferIndex = 0 ; } ; if ( dataLen == - 1 ) { if ( length > 0 ) break ; else return null ; } else c = ioBuffer [ bufferIndex ++ ] ; if ( isTokenChar ( c ) ) { if ( length == 0 ) start = offset - 1 ; buffer [ length ++ ] = normalize ( c ) ; if ( length == MAX_WORD_LEN ) break ; } else if ( length > 0 ) break ; } return new Token ( new String ( buffer , 0 , length ) , start , start + length ) ; } } 	1	['4', '3', '2', '4', '8', '4', '2', '2', '2', '0.857142857', '122', '1', '0', '0.571428571', '0.583333333', '0', '0', '27.75', '1', '0.75', '2']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; import java . io . Serializable ; public interface SortComparatorSource extends Serializable { ScoreDocComparator newComparator ( IndexReader reader , String fieldname ) throws IOException ; } 	0	['1', '1', '0', '6', '1', '0', '4', '2', '1', '2', '1', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . queryParser ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . search . BooleanClause ; import org . apache . lucene . search . BooleanQuery ; import org . apache . lucene . search . MultiPhraseQuery ; import org . apache . lucene . search . PhraseQuery ; import org . apache . lucene . search . Query ; import java . util . Vector ; import java . util . Map ; public class MultiFieldQueryParser extends QueryParser { protected String [ ] fields ; protected Map boosts ; public MultiFieldQueryParser ( String [ ] fields , Analyzer analyzer , Map boosts ) { this ( fields , analyzer ) ; this . boosts = boosts ; } public MultiFieldQueryParser ( String [ ] fields , Analyzer analyzer ) { super ( null , analyzer ) ; this . fields = fields ; } protected Query getFieldQuery ( String field , String queryText , int slop ) throws ParseException { if ( field == null ) { Vector clauses = new Vector ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { Query q = getFieldQuery ( fields [ i ] , queryText ) ; if ( q != null ) { if ( boosts != null ) { Float boost = ( Float ) boosts . get ( fields [ i ] ) ; if ( boost != null ) { q . setBoost ( boost . floatValue ( ) ) ; } } if ( q instanceof PhraseQuery ) { ( ( PhraseQuery ) q ) . setSlop ( slop ) ; } if ( q instanceof MultiPhraseQuery ) { ( ( MultiPhraseQuery ) q ) . setSlop ( slop ) ; } clauses . add ( new BooleanClause ( q , BooleanClause . Occur . SHOULD ) ) ; } } if ( clauses . size ( ) == 0 ) return null ; return getBooleanQuery ( clauses , true ) ; } return super . getFieldQuery ( field , queryText ) ; } protected Query getFieldQuery ( String field , String queryText ) throws ParseException { return getFieldQuery ( field , queryText , 0 ) ; } protected Query getFuzzyQuery ( String field , String termStr , float minSimilarity ) throws ParseException { if ( field == null ) { Vector clauses = new Vector ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( getFuzzyQuery ( fields [ i ] , termStr , minSimilarity ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getFuzzyQuery ( field , termStr , minSimilarity ) ; } protected Query getPrefixQuery ( String field , String termStr ) throws ParseException { if ( field == null ) { Vector clauses = new Vector ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( getPrefixQuery ( fields [ i ] , termStr ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getPrefixQuery ( field , termStr ) ; } protected Query getWildcardQuery ( String field , String termStr ) throws ParseException { if ( field == null ) { Vector clauses = new Vector ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( getWildcardQuery ( fields [ i ] , termStr ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getWildcardQuery ( field , termStr ) ; } protected Query getRangeQuery ( String field , String part1 , String part2 , boolean inclusive ) throws ParseException { if ( field == null ) { Vector clauses = new Vector ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( getRangeQuery ( fields [ i ] , part1 , part2 , inclusive ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getRangeQuery ( field , part1 , part2 , inclusive ) ; } public static Query parse ( String [ ] queries , String [ ] fields , Analyzer analyzer ) throws ParseException { if ( queries . length != fields . length ) throw new IllegalArgumentException ( "queries.length != fields.length" ) ; BooleanQuery bQuery = new BooleanQuery ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { QueryParser qp = new QueryParser ( fields [ i ] , analyzer ) ; Query q = qp . parse ( queries [ i ] ) ; bQuery . add ( q , BooleanClause . Occur . SHOULD ) ; } return bQuery ; } public static Query parse ( String query , String [ ] fields , BooleanClause . Occur [ ] flags , Analyzer analyzer ) throws ParseException { if ( fields . length != flags . length ) throw new IllegalArgumentException ( "fields.length != flags.length" ) ; BooleanQuery bQuery = new BooleanQuery ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { QueryParser qp = new QueryParser ( fields [ i ] , analyzer ) ; Query q = qp . parse ( query ) ; bQuery . add ( q , flags [ i ] ) ; } return bQuery ; } public static Query parse ( String [ ] queries , String [ ] fields , BooleanClause . Occur [ ] flags , Analyzer analyzer ) throws ParseException { if ( ! ( queries . length == fields . length && queries . length == flags . length ) ) throw new IllegalArgumentException ( "queries, fields, and flags array have have different length" ) ; BooleanQuery bQuery = new BooleanQuery ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { QueryParser qp = new QueryParser ( fields [ i ] , analyzer ) ; Query q = qp . parse ( queries [ i ] ) ; bQuery . add ( q , flags [ i ] ) ; } return bQuery ; } } 	1	['11', '2', '0', '9', '31', '23', '0', '9', '5', '0.55', '410', '1', '0', '0.869565217', '0.313131313', '1', '5', '36.09090909', '1', '0.8182', '2']
package org . apache . lucene . search ; import java . io . IOException ; public class ReqOptSumScorer extends Scorer { private Scorer reqScorer ; private Scorer optScorer ; public ReqOptSumScorer ( Scorer reqScorer , Scorer optScorer ) { super ( null ) ; this . reqScorer = reqScorer ; this . optScorer = optScorer ; } private boolean firstTimeOptScorer = true ; public boolean next ( ) throws IOException { return reqScorer . next ( ) ; } public boolean skipTo ( int target ) throws IOException { return reqScorer . skipTo ( target ) ; } public int doc ( ) { return reqScorer . doc ( ) ; } public float score ( ) throws IOException { int curDoc = reqScorer . doc ( ) ; float reqScore = reqScorer . score ( ) ; if ( firstTimeOptScorer ) { firstTimeOptScorer = false ; if ( ! optScorer . skipTo ( curDoc ) ) { optScorer = null ; return reqScore ; } } else if ( optScorer == null ) { return reqScore ; } else if ( ( optScorer . doc ( ) < curDoc ) && ( ! optScorer . skipTo ( curDoc ) ) ) { optScorer = null ; return reqScore ; } return ( optScorer . doc ( ) == curDoc ) ? reqScore + optScorer . score ( ) : reqScore ; } public Explanation explain ( int doc ) throws IOException { Explanation res = new Explanation ( ) ; res . setDescription ( "required, optional" ) ; res . addDetail ( reqScorer . explain ( doc ) ) ; res . addDetail ( optScorer . explain ( doc ) ) ; return res ; } } 	0	['6', '2', '0', '4', '15', '0', '1', '3', '6', '0.466666667', '113', '1', '2', '0.615384615', '0.5', '1', '3', '17.33333333', '1', '0.8333', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class Directory { protected LockFactory lockFactory ; public abstract String [ ] list ( ) throws IOException ; public abstract boolean fileExists ( String name ) throws IOException ; public abstract long fileModified ( String name ) throws IOException ; public abstract void touchFile ( String name ) throws IOException ; public abstract void deleteFile ( String name ) throws IOException ; public abstract void renameFile ( String from , String to ) throws IOException ; public abstract long fileLength ( String name ) throws IOException ; public abstract IndexOutput createOutput ( String name ) throws IOException ; public abstract IndexInput openInput ( String name ) throws IOException ; public IndexInput openInput ( String name , int bufferSize ) throws IOException { return openInput ( name ) ; } public Lock makeLock ( String name ) { return lockFactory . makeLock ( name ) ; } public void clearLock ( String name ) throws IOException { if ( lockFactory != null ) { lockFactory . clearLock ( name ) ; } } public abstract void close ( ) throws IOException ; public void setLockFactory ( LockFactory lockFactory ) { this . lockFactory = lockFactory ; lockFactory . setLockPrefix ( this . getLockID ( ) ) ; } public LockFactory getLockFactory ( ) { return this . lockFactory ; } public String getLockID ( ) { return this . toString ( ) ; } public static void copy ( Directory src , Directory dest , boolean closeDirSrc ) throws IOException { final String [ ] files = src . list ( ) ; if ( files == null ) throw new IOException ( "cannot read directory " + src + ": list() returned null" ) ; byte [ ] buf = new byte [ BufferedIndexOutput . BUFFER_SIZE ] ; for ( int i = 0 ; i < files . length ; i ++ ) { IndexOutput os = null ; IndexInput is = null ; try { os = dest . createOutput ( files [ i ] ) ; is = src . openInput ( files [ i ] ) ; long len = is . length ( ) ; long readCount = 0 ; while ( readCount < len ) { int toRead = readCount + BufferedIndexOutput . BUFFER_SIZE > len ? ( int ) ( len - readCount ) : BufferedIndexOutput . BUFFER_SIZE ; is . readBytes ( buf , 0 , toRead ) ; os . writeBytes ( buf , toRead ) ; readCount += toRead ; } } finally { try { if ( os != null ) os . close ( ) ; } finally { if ( is != null ) is . close ( ) ; } } } if ( closeDirSrc ) src . close ( ) ; } } 	1	['18', '1', '3', '36', '33', '141', '32', '4', '18', '0.823529412', '177', '1', '1', '0', '0.296296296', '0', '0', '8.777777778', '1', '0.9444', '4']
package org . apache . lucene . document ; public class NumberTools { private static final int RADIX = 36 ; private static final char NEGATIVE_PREFIX = '-' ; private static final char POSITIVE_PREFIX = '0' ; public static final String MIN_STRING_VALUE = NEGATIVE_PREFIX + "0000000000000" ; public static final String MAX_STRING_VALUE = POSITIVE_PREFIX + "1y2p0ij32e8e7" ; public static final int STR_SIZE = MIN_STRING_VALUE . length ( ) ; public static String longToString ( long l ) { if ( l == Long . MIN_VALUE ) { return MIN_STRING_VALUE ; } StringBuffer buf = new StringBuffer ( STR_SIZE ) ; if ( l < 0 ) { buf . append ( NEGATIVE_PREFIX ) ; l = Long . MAX_VALUE + l + 1 ; } else { buf . append ( POSITIVE_PREFIX ) ; } String num = Long . toString ( l , RADIX ) ; int padLen = STR_SIZE - num . length ( ) - buf . length ( ) ; while ( padLen -- > 0 ) { buf . append ( '0' ) ; } buf . append ( num ) ; return buf . toString ( ) ; } public static long stringToLong ( String str ) { if ( str == null ) { throw new NullPointerException ( "string cannot be null" ) ; } if ( str . length ( ) != STR_SIZE ) { throw new NumberFormatException ( "string is the wrong size" ) ; } if ( str . equals ( MIN_STRING_VALUE ) ) { return Long . MIN_VALUE ; } char prefix = str . charAt ( 0 ) ; long l = Long . parseLong ( str . substring ( 1 ) , RADIX ) ; if ( prefix == POSITIVE_PREFIX ) { } else if ( prefix == NEGATIVE_PREFIX ) { l = l - Long . MAX_VALUE - 1 ; } else { throw new NumberFormatException ( "string does not begin with the correct prefix" ) ; } return l ; } } 	0	['4', '1', '0', '0', '18', '0', '0', '0', '3', '1.166666667', '130', '0.5', '0', '0', '0.333333333', '0', '0', '30', '6', '2.5', '0']
package org . apache . lucene . analysis ; import java . io . * ; class PorterStemmer { private char [ ] b ; private int i , j , k , k0 ; private boolean dirty = false ; private static final int INC = 50 ; private static final int EXTRA = 1 ; public PorterStemmer ( ) { b = new char [ INC ] ; i = 0 ; } public void reset ( ) { i = 0 ; dirty = false ; } public void add ( char ch ) { if ( b . length <= i + EXTRA ) { char [ ] new_b = new char [ b . length + INC ] ; for ( int c = 0 ; c < b . length ; c ++ ) new_b [ c ] = b [ c ] ; b = new_b ; } b [ i ++ ] = ch ; } public String toString ( ) { return new String ( b , 0 , i ) ; } public int getResultLength ( ) { return i ; } public char [ ] getResultBuffer ( ) { return b ; } private final boolean cons ( int i ) { switch ( b [ i ] ) { case 'a' : case 'e' : case 'i' : case 'o' : case 'u' : return false ; case 'y' : return ( i == k0 ) ? true : ! cons ( i - 1 ) ; default : return true ; } } private final int m ( ) { int n = 0 ; int i = k0 ; while ( true ) { if ( i > j ) return n ; if ( ! cons ( i ) ) break ; i ++ ; } i ++ ; while ( true ) { while ( true ) { if ( i > j ) return n ; if ( cons ( i ) ) break ; i ++ ; } i ++ ; n ++ ; while ( true ) { if ( i > j ) return n ; if ( ! cons ( i ) ) break ; i ++ ; } i ++ ; } } private final boolean vowelinstem ( ) { int i ; for ( i = k0 ; i <= j ; i ++ ) if ( ! cons ( i ) ) return true ; return false ; } private final boolean doublec ( int j ) { if ( j < k0 + 1 ) return false ; if ( b [ j ] != b [ j - 1 ] ) return false ; return cons ( j ) ; } private final boolean cvc ( int i ) { if ( i < k0 + 2 || ! cons ( i ) || cons ( i - 1 ) || ! cons ( i - 2 ) ) return false ; else { int ch = b [ i ] ; if ( ch == 'w' || ch == 'x' || ch == 'y' ) return false ; } return true ; } private final boolean ends ( String s ) { int l = s . length ( ) ; int o = k - l + 1 ; if ( o < k0 ) return false ; for ( int i = 0 ; i < l ; i ++ ) if ( b [ o + i ] != s . charAt ( i ) ) return false ; j = k - l ; return true ; } void setto ( String s ) { int l = s . length ( ) ; int o = j + 1 ; for ( int i = 0 ; i < l ; i ++ ) b [ o + i ] = s . charAt ( i ) ; k = j + l ; dirty = true ; } void r ( String s ) { if ( m ( ) > 0 ) setto ( s ) ; } private final void step1 ( ) { if ( b [ k ] == 's' ) { if ( ends ( "sses" ) ) k -= 2 ; else if ( ends ( "ies" ) ) setto ( "i" ) ; else if ( b [ k - 1 ] != 's' ) k -- ; } if ( ends ( "eed" ) ) { if ( m ( ) > 0 ) k -- ; } else if ( ( ends ( "ed" ) || ends ( "ing" ) ) && vowelinstem ( ) ) { k = j ; if ( ends ( "at" ) ) setto ( "ate" ) ; else if ( ends ( "bl" ) ) setto ( "ble" ) ; else if ( ends ( "iz" ) ) setto ( "ize" ) ; else if ( doublec ( k ) ) { int ch = b [ k -- ] ; if ( ch == 'l' || ch == 's' || ch == 'z' ) k ++ ; } else if ( m ( ) == 1 && cvc ( k ) ) setto ( "e" ) ; } } private final void step2 ( ) { if ( ends ( "y" ) && vowelinstem ( ) ) { b [ k ] = 'i' ; dirty = true ; } } private final void step3 ( ) { if ( k == k0 ) return ; switch ( b [ k - 1 ] ) { case 'a' : if ( ends ( "ational" ) ) { r ( "ate" ) ; break ; } if ( ends ( "tional" ) ) { r ( "tion" ) ; break ; } break ; case 'c' : if ( ends ( "enci" ) ) { r ( "ence" ) ; break ; } if ( ends ( "anci" ) ) { r ( "ance" ) ; break ; } break ; case 'e' : if ( ends ( "izer" ) ) { r ( "ize" ) ; break ; } break ; case 'l' : if ( ends ( "bli" ) ) { r ( "ble" ) ; break ; } if ( ends ( "alli" ) ) { r ( "al" ) ; break ; } if ( ends ( "entli" ) ) { r ( "ent" ) ; break ; } if ( ends ( "eli" ) ) { r ( "e" ) ; break ; } if ( ends ( "ousli" ) ) { r ( "ous" ) ; break ; } break ; case 'o' : if ( ends ( "ization" ) ) { r ( "ize" ) ; break ; } if ( ends ( "ation" ) ) { r ( "ate" ) ; break ; } if ( ends ( "ator" ) ) { r ( "ate" ) ; break ; } break ; case 's' : if ( ends ( "alism" ) ) { r ( "al" ) ; break ; } if ( ends ( "iveness" ) ) { r ( "ive" ) ; break ; } if ( ends ( "fulness" ) ) { r ( "ful" ) ; break ; } if ( ends ( "ousness" ) ) { r ( "ous" ) ; break ; } break ; case 't' : if ( ends ( "aliti" ) ) { r ( "al" ) ; break ; } if ( ends ( "iviti" ) ) { r ( "ive" ) ; break ; } if ( ends ( "biliti" ) ) { r ( "ble" ) ; break ; } break ; case 'g' : if ( ends ( "logi" ) ) { r ( "log" ) ; break ; } } } private final void step4 ( ) { switch ( b [ k ] ) { case 'e' : if ( ends ( "icate" ) ) { r ( "ic" ) ; break ; } if ( ends ( "ative" ) ) { r ( "" ) ; break ; } if ( ends ( "alize" ) ) { r ( "al" ) ; break ; } break ; case 'i' : if ( ends ( "iciti" ) ) { r ( "ic" ) ; break ; } break ; case 'l' : if ( ends ( "ical" ) ) { r ( "ic" ) ; break ; } if ( ends ( "ful" ) ) { r ( "" ) ; break ; } break ; case 's' : if ( ends ( "ness" ) ) { r ( "" ) ; break ; } break ; } } private final void step5 ( ) { if ( k == k0 ) return ; switch ( b [ k - 1 ] ) { case 'a' : if ( ends ( "al" ) ) break ; return ; case 'c' : if ( ends ( "ance" ) ) break ; if ( ends ( "ence" ) ) break ; return ; case 'e' : if ( ends ( "er" ) ) break ; return ; case 'i' : if ( ends ( "ic" ) ) break ; return ; case 'l' : if ( ends ( "able" ) ) break ; if ( ends ( "ible" ) ) break ; return ; case 'n' : if ( ends ( "ant" ) ) break ; if ( ends ( "ement" ) ) break ; if ( ends ( "ment" ) ) break ; if ( ends ( "ent" ) ) break ; return ; case 'o' : if ( ends ( "ion" ) && j >= 0 && ( b [ j ] == 's' || b [ j ] == 't' ) ) break ; if ( ends ( "ou" ) ) break ; return ; case 's' : if ( ends ( "ism" ) ) break ; return ; case 't' : if ( ends ( "ate" ) ) break ; if ( ends ( "iti" ) ) break ; return ; case 'u' : if ( ends ( "ous" ) ) break ; return ; case 'v' : if ( ends ( "ive" ) ) break ; return ; case 'z' : if ( ends ( "ize" ) ) break ; return ; default : return ; } if ( m ( ) > 1 ) k = j ; } private final void step6 ( ) { j = k ; if ( b [ k ] == 'e' ) { int a = m ( ) ; if ( a > 1 || a == 1 && ! cvc ( k - 1 ) ) k -- ; } if ( b [ k ] == 'l' && doublec ( k ) && m ( ) > 1 ) k -- ; } public String stem ( String s ) { if ( stem ( s . toCharArray ( ) , s . length ( ) ) ) return toString ( ) ; else return s ; } public boolean stem ( char [ ] word ) { return stem ( word , word . length ) ; } public boolean stem ( char [ ] wordBuffer , int offset , int wordLen ) { reset ( ) ; if ( b . length < wordLen ) { char [ ] new_b = new char [ wordLen + EXTRA ] ; b = new_b ; } for ( int j = 0 ; j < wordLen ; j ++ ) b [ j ] = wordBuffer [ offset + j ] ; i = wordLen ; return stem ( 0 ) ; } public boolean stem ( char [ ] word , int wordLen ) { return stem ( word , 0 , wordLen ) ; } public boolean stem ( ) { return stem ( 0 ) ; } public boolean stem ( int i0 ) { k = i - 1 ; k0 = i0 ; if ( k > k0 + 1 ) { step1 ( ) ; step2 ( ) ; step3 ( ) ; step4 ( ) ; step5 ( ) ; step6 ( ) ; } if ( i != k + 1 ) dirty = true ; i = k + 1 ; return dirty ; } public static void main ( String [ ] args ) { PorterStemmer s = new PorterStemmer ( ) ; for ( int i = 0 ; i < args . length ; i ++ ) { try { InputStream in = new FileInputStream ( args [ i ] ) ; byte [ ] buffer = new byte [ 1024 ] ; int bufferLen , offset , ch ; bufferLen = in . read ( buffer ) ; offset = 0 ; s . reset ( ) ; while ( true ) { if ( offset < bufferLen ) ch = buffer [ offset ++ ] ; else { bufferLen = in . read ( buffer ) ; offset = 0 ; if ( bufferLen < 0 ) ch = - 1 ; else ch = buffer [ offset ++ ] ; } if ( Character . isLetter ( ( char ) ch ) ) { s . add ( Character . toLowerCase ( ( char ) ch ) ) ; } else { s . stem ( ) ; System . out . print ( s . toString ( ) ) ; s . reset ( ) ; if ( ch < 0 ) break ; else { System . out . print ( ( char ) ch ) ; } } } in . close ( ) ; } catch ( IOException e ) { System . out . println ( "error reading " + args [ i ] ) ; } } } } 	1	['27', '1', '0', '1', '43', '13', '1', '0', '13', '0.600961538', '1174', '1', '0', '0', '0.25308642', '0', '0', '42.18518519', '26', '5.7407', '1']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import java . io . IOException ; public final class FuzzyTermEnum extends FilteredTermEnum { private static final int TYPICAL_LONGEST_WORD_IN_INDEX = 19 ; private int [ ] [ ] d ; private float similarity ; private boolean endEnum = false ; private Term searchTerm = null ; private final String field ; private final String text ; private final String prefix ; private final float minimumSimilarity ; private final float scale_factor ; private final int [ ] maxDistances = new int [ TYPICAL_LONGEST_WORD_IN_INDEX ] ; public FuzzyTermEnum ( IndexReader reader , Term term ) throws IOException { this ( reader , term , FuzzyQuery . defaultMinSimilarity , FuzzyQuery . defaultPrefixLength ) ; } public FuzzyTermEnum ( IndexReader reader , Term term , float minSimilarity ) throws IOException { this ( reader , term , minSimilarity , FuzzyQuery . defaultPrefixLength ) ; } public FuzzyTermEnum ( IndexReader reader , Term term , final float minSimilarity , final int prefixLength ) throws IOException { super ( ) ; if ( minSimilarity >= 1.0f ) throw new IllegalArgumentException ( "minimumSimilarity cannot be greater than or equal to 1" ) ; else if ( minSimilarity < 0.0f ) throw new IllegalArgumentException ( "minimumSimilarity cannot be less than 0" ) ; if ( prefixLength < 0 ) throw new IllegalArgumentException ( "prefixLength cannot be less than 0" ) ; this . minimumSimilarity = minSimilarity ; this . scale_factor = 1.0f / ( 1.0f - minimumSimilarity ) ; this . searchTerm = term ; this . field = searchTerm . field ( ) ; final int fullSearchTermLength = searchTerm . text ( ) . length ( ) ; final int realPrefixLength = prefixLength > fullSearchTermLength ? fullSearchTermLength : prefixLength ; this . text = searchTerm . text ( ) . substring ( realPrefixLength ) ; this . prefix = searchTerm . text ( ) . substring ( 0 , realPrefixLength ) ; initializeMaxDistances ( ) ; this . d = initDistanceArray ( ) ; setEnum ( reader . terms ( new Term ( searchTerm . field ( ) , prefix ) ) ) ; } protected final boolean termCompare ( Term term ) { if ( field == term . field ( ) && term . text ( ) . startsWith ( prefix ) ) { final String target = term . text ( ) . substring ( prefix . length ( ) ) ; this . similarity = similarity ( target ) ; return ( similarity > minimumSimilarity ) ; } endEnum = true ; return false ; } public final float difference ( ) { return ( float ) ( ( similarity - minimumSimilarity ) * scale_factor ) ; } public final boolean endEnum ( ) { return endEnum ; } private static final int min ( int a , int b , int c ) { final int t = ( a < b ) ? a : b ; return ( t < c ) ? t : c ; } private final int [ ] [ ] initDistanceArray ( ) { return new int [ this . text . length ( ) + 1 ] [ TYPICAL_LONGEST_WORD_IN_INDEX ] ; } private synchronized final float similarity ( final String target ) { final int m = target . length ( ) ; final int n = text . length ( ) ; if ( n == 0 ) { return prefix . length ( ) == 0 ? 0.0f : 1.0f - ( ( float ) m / prefix . length ( ) ) ; } if ( m == 0 ) { return prefix . length ( ) == 0 ? 0.0f : 1.0f - ( ( float ) n / prefix . length ( ) ) ; } final int maxDistance = getMaxDistance ( m ) ; if ( maxDistance < Math . abs ( m - n ) ) { return 0.0f ; } if ( d [ 0 ] . length <= m ) { growDistanceArray ( m ) ; } for ( int i = 0 ; i <= n ; i ++ ) d [ i ] [ 0 ] = i ; for ( int j = 0 ; j <= m ; j ++ ) d [ 0 ] [ j ] = j ; for ( int i = 1 ; i <= n ; i ++ ) { int bestPossibleEditDistance = m ; final char s_i = text . charAt ( i - 1 ) ; for ( int j = 1 ; j <= m ; j ++ ) { if ( s_i != target . charAt ( j - 1 ) ) { d [ i ] [ j ] = min ( d [ i - 1 ] [ j ] , d [ i ] [ j - 1 ] , d [ i - 1 ] [ j - 1 ] ) + 1 ; } else { d [ i ] [ j ] = min ( d [ i - 1 ] [ j ] + 1 , d [ i ] [ j - 1 ] + 1 , d [ i - 1 ] [ j - 1 ] ) ; } bestPossibleEditDistance = Math . min ( bestPossibleEditDistance , d [ i ] [ j ] ) ; } if ( i > maxDistance && bestPossibleEditDistance > maxDistance ) { return 0.0f ; } } return 1.0f - ( ( float ) d [ n ] [ m ] / ( float ) ( prefix . length ( ) + Math . min ( n , m ) ) ) ; } private void growDistanceArray ( int m ) { for ( int i = 0 ; i < d . length ; i ++ ) { d [ i ] = new int [ m + 1 ] ; } } private final int getMaxDistance ( int m ) { return ( m < maxDistances . length ) ? maxDistances [ m ] : calculateMaxDistance ( m ) ; } private void initializeMaxDistances ( ) { for ( int i = 0 ; i < maxDistances . length ; i ++ ) { maxDistances [ i ] = calculateMaxDistance ( i ) ; } } private int calculateMaxDistance ( int m ) { return ( int ) ( ( 1 - minimumSimilarity ) * ( Math . min ( text . length ( ) , m ) + prefix . length ( ) ) ) ; } public void close ( ) throws IOException { super . close ( ) ; } } 	0	['14', '3', '0', '5', '29', '53', '1', '4', '6', '0.692307692', '514', '1', '1', '0.541666667', '0.333333333', '1', '4', '34.92857143', '14', '2.2857', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . CorruptIndexException ; import org . apache . lucene . index . Term ; import org . apache . lucene . document . Document ; public abstract class Searcher implements Searchable { public final Hits search ( Query query ) throws IOException { return search ( query , ( Filter ) null ) ; } public Hits search ( Query query , Filter filter ) throws IOException { return new Hits ( this , query , filter ) ; } public Hits search ( Query query , Sort sort ) throws IOException { return new Hits ( this , query , null , sort ) ; } public Hits search ( Query query , Filter filter , Sort sort ) throws IOException { return new Hits ( this , query , filter , sort ) ; } public TopFieldDocs search ( Query query , Filter filter , int n , Sort sort ) throws IOException { return search ( createWeight ( query ) , filter , n , sort ) ; } public void search ( Query query , HitCollector results ) throws IOException { search ( query , ( Filter ) null , results ) ; } public void search ( Query query , Filter filter , HitCollector results ) throws IOException { search ( createWeight ( query ) , filter , results ) ; } public TopDocs search ( Query query , Filter filter , int n ) throws IOException { return search ( createWeight ( query ) , filter , n ) ; } public Explanation explain ( Query query , int doc ) throws IOException { return explain ( createWeight ( query ) , doc ) ; } private Similarity similarity = Similarity . getDefault ( ) ; public void setSimilarity ( Similarity similarity ) { this . similarity = similarity ; } public Similarity getSimilarity ( ) { return this . similarity ; } protected Weight createWeight ( Query query ) throws IOException { return query . weight ( this ) ; } public int [ ] docFreqs ( Term [ ] terms ) throws IOException { int [ ] result = new int [ terms . length ] ; for ( int i = 0 ; i < terms . length ; i ++ ) { result [ i ] = docFreq ( terms [ i ] ) ; } return result ; } abstract public void search ( Weight weight , Filter filter , HitCollector results ) throws IOException ; abstract public void close ( ) throws IOException ; abstract public int docFreq ( Term term ) throws IOException ; abstract public int maxDoc ( ) throws IOException ; abstract public TopDocs search ( Weight weight , Filter filter , int n ) throws IOException ; abstract public Document doc ( int i ) throws CorruptIndexException , IOException ; abstract public Query rewrite ( Query query ) throws IOException ; abstract public Explanation explain ( Weight weight , int doc ) throws IOException ; abstract public TopFieldDocs search ( Weight weight , Filter filter , int n , Sort sort ) throws IOException ; } 	1	['23', '1', '3', '40', '28', '247', '29', '14', '22', '0.909090909', '131', '1', '1', '0', '0.273913043', '0', '0', '4.652173913', '1', '0.9565', '4']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Query ; import org . apache . lucene . util . ToStringUtils ; public class SpanFirstQuery extends SpanQuery { private SpanQuery match ; private int end ; public SpanFirstQuery ( SpanQuery match , int end ) { this . match = match ; this . end = end ; } public SpanQuery getMatch ( ) { return match ; } public int getEnd ( ) { return end ; } public String getField ( ) { return match . getField ( ) ; } public Collection getTerms ( ) { return match . getTerms ( ) ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "spanFirst(" ) ; buffer . append ( match . toString ( field ) ) ; buffer . append ( ", " ) ; buffer . append ( end ) ; buffer . append ( ")" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public void extractTerms ( Set terms ) { match . extractTerms ( terms ) ; } public Spans getSpans ( final IndexReader reader ) throws IOException { return new Spans ( ) { private Spans spans = match . getSpans ( reader ) ; public boolean next ( ) throws IOException { while ( spans . next ( ) ) { if ( end ( ) <= end ) return true ; } return false ; } public boolean skipTo ( int target ) throws IOException { if ( ! spans . skipTo ( target ) ) return false ; if ( spans . end ( ) <= end ) return true ; return next ( ) ; } public int doc ( ) { return spans . doc ( ) ; } public int start ( ) { return spans . start ( ) ; } public int end ( ) { return spans . end ( ) ; } public String toString ( ) { return "spans(" + SpanFirstQuery . this . toString ( ) + ")" ; } } ; } public Query rewrite ( IndexReader reader ) throws IOException { SpanFirstQuery clone = null ; SpanQuery rewritten = ( SpanQuery ) match . rewrite ( reader ) ; if ( rewritten != match ) { clone = ( SpanFirstQuery ) this . clone ( ) ; clone . match = rewritten ; } if ( clone != null ) { return clone ; } else { return this ; } } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof SpanFirstQuery ) ) return false ; SpanFirstQuery other = ( SpanFirstQuery ) o ; return this . end == other . end && this . match . equals ( other . match ) && this . getBoost ( ) == other . getBoost ( ) ; } public int hashCode ( ) { int h = match . hashCode ( ) ; h ^= ( h << 8 ) | ( h > > > 25 ) ; h ^= Float . floatToRawIntBits ( getBoost ( ) ) ^ end ; return h ; } } 	0	['13', '3', '0', '6', '30', '0', '1', '6', '11', '0.416666667', '176', '1', '1', '0.571428571', '0.192307692', '2', '2', '12.38461538', '6', '1.3077', '0']
package org . apache . lucene . search . payloads ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermPositions ; import org . apache . lucene . search . * ; import org . apache . lucene . search . spans . SpanScorer ; import org . apache . lucene . search . spans . SpanTermQuery ; import org . apache . lucene . search . spans . SpanWeight ; import org . apache . lucene . search . spans . TermSpans ; import java . io . IOException ; public class BoostingTermQuery extends SpanTermQuery { public BoostingTermQuery ( Term term ) { super ( term ) ; } protected Weight createWeight ( Searcher searcher ) throws IOException { return new BoostingTermWeight ( this , searcher ) ; } protected class BoostingTermWeight extends SpanWeight implements Weight { public BoostingTermWeight ( BoostingTermQuery query , Searcher searcher ) throws IOException { super ( query , searcher ) ; } public Scorer scorer ( IndexReader reader ) throws IOException { return new BoostingSpanScorer ( ( TermSpans ) query . getSpans ( reader ) , this , similarity , reader . norms ( query . getField ( ) ) ) ; } class BoostingSpanScorer extends SpanScorer { byte [ ] payload = new byte [ 256 ] ; private TermPositions positions ; protected float payloadScore ; private int payloadsSeen ; public BoostingSpanScorer ( TermSpans spans , Weight weight , Similarity similarity , byte [ ] norms ) throws IOException { super ( spans , weight , similarity , norms ) ; positions = spans . getPositions ( ) ; } protected boolean setFreqCurrentDoc ( ) throws IOException { if ( ! more ) { return false ; } doc = spans . doc ( ) ; freq = 0.0f ; payloadScore = 0 ; payloadsSeen = 0 ; Similarity similarity1 = getSimilarity ( ) ; while ( more && doc == spans . doc ( ) ) { int matchLength = spans . end ( ) - spans . start ( ) ; freq += similarity1 . sloppyFreq ( matchLength ) ; processPayload ( similarity1 ) ; more = spans . next ( ) ; } return more || ( freq != 0 ) ; } protected void processPayload ( Similarity similarity ) throws IOException { if ( positions . isPayloadAvailable ( ) ) { payload = positions . getPayload ( payload , 0 ) ; payloadScore += similarity . scorePayload ( payload , 0 , positions . getPayloadLength ( ) ) ; payloadsSeen ++ ; } else { } } public float score ( ) throws IOException { return super . score ( ) * ( payloadsSeen > 0 ? ( payloadScore / payloadsSeen ) : 1 ) ; } public Explanation explain ( final int doc ) throws IOException { Explanation result = new Explanation ( ) ; Explanation nonPayloadExpl = super . explain ( doc ) ; result . addDetail ( nonPayloadExpl ) ; Explanation payloadBoost = new Explanation ( ) ; result . addDetail ( payloadBoost ) ; float avgPayloadScore = payloadScore / payloadsSeen ; payloadBoost . setValue ( avgPayloadScore ) ; payloadBoost . setDescription ( "scorePayload(...)" ) ; result . setValue ( nonPayloadExpl . getValue ( ) * avgPayloadScore ) ; result . setDescription ( "btq, product of:" ) ; return result ; } } } public boolean equals ( Object o ) { if ( ! ( o instanceof BoostingTermQuery ) ) return false ; BoostingTermQuery other = ( BoostingTermQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && this . term . equals ( other . term ) ; } } 	1	['3', '4', '0', '5', '7', '3', '1', '5', '2', '2', '38', '0', '0', '0.923076923', '0.5', '1', '1', '11.66666667', '4', '1.6667', '4']
package org . apache . lucene ; public final class LucenePackage { private LucenePackage ( ) { } public static Package get ( ) { return LucenePackage . class . getPackage ( ) ; } } 	0	['3', '1', '0', '0', '8', '3', '0', '0', '1', '1', '27', '0', '0', '0', '0.333333333', '0', '0', '7.666666667', '2', '1', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; public abstract class Analyzer { public abstract TokenStream tokenStream ( String fieldName , Reader reader ) ; public int getPositionIncrementGap ( String fieldName ) { return 0 ; } } 	1	['3', '1', '6', '13', '4', '3', '12', '1', '3', '2', '8', '0', '0', '0', '0.666666667', '0', '0', '1.666666667', '1', '0.6667', '1']
package org . apache . lucene . util ; import java . io . ObjectStreamException ; import java . io . Serializable ; import java . io . StreamCorruptedException ; import java . util . HashMap ; import java . util . Map ; public abstract class Parameter implements Serializable { static Map allParameters = new HashMap ( ) ; private String name ; private Parameter ( ) { } protected Parameter ( String name ) { this . name = name ; String key = makeKey ( name ) ; if ( allParameters . containsKey ( key ) ) throw new IllegalArgumentException ( "Parameter name " + key + " already used!" ) ; allParameters . put ( key , this ) ; } private String makeKey ( String name ) { return getClass ( ) + " " + name ; } public String toString ( ) { return name ; } protected Object readResolve ( ) throws ObjectStreamException { Object par = allParameters . get ( makeKey ( name ) ) ; if ( par == null ) throw new StreamCorruptedException ( "Unknown parameter value: " + name ) ; return par ; } } 	0	['6', '1', '5', '5', '18', '5', '5', '0', '1', '0.6', '88', '0.5', '0', '0', '0.7', '0', '0', '13.33333333', '1', '0.5', '0']
package org . apache . lucene . store ; import java . io . File ; import java . io . IOException ; public class SimpleFSLockFactory extends LockFactory { private File lockDir ; SimpleFSLockFactory ( ) throws IOException { this ( ( File ) null ) ; } public SimpleFSLockFactory ( File lockDir ) throws IOException { setLockDir ( lockDir ) ; } public SimpleFSLockFactory ( String lockDirName ) throws IOException { lockDir = new File ( lockDirName ) ; setLockDir ( lockDir ) ; } void setLockDir ( File lockDir ) throws IOException { this . lockDir = lockDir ; } public Lock makeLock ( String lockName ) { if ( lockPrefix != null ) { lockName = lockPrefix + "-" + lockName ; } return new SimpleFSLock ( lockDir , lockName ) ; } public void clearLock ( String lockName ) throws IOException { if ( lockDir . exists ( ) ) { if ( lockPrefix != null ) { lockName = lockPrefix + "-" + lockName ; } File lockFile = new File ( lockDir , lockName ) ; if ( lockFile . exists ( ) && ! lockFile . delete ( ) ) { throw new IOException ( "Cannot delete " + lockFile ) ; } } } } ; class SimpleFSLock extends Lock { File lockFile ; File lockDir ; public SimpleFSLock ( File lockDir , String lockFileName ) { this . lockDir = lockDir ; lockFile = new File ( lockDir , lockFileName ) ; } public boolean obtain ( ) throws IOException { if ( ! lockDir . exists ( ) ) { if ( ! lockDir . mkdirs ( ) ) throw new IOException ( "Cannot create directory: " + lockDir . getAbsolutePath ( ) ) ; } else if ( ! lockDir . isDirectory ( ) ) { throw new IOException ( "Found regular file where directory expected: " + lockDir . getAbsolutePath ( ) ) ; } return lockFile . createNewFile ( ) ; } public void release ( ) { lockFile . delete ( ) ; } public boolean isLocked ( ) { return lockFile . exists ( ) ; } public String toString ( ) { return "SimpleFSLock@" + lockFile ; } } 	1	['6', '2', '0', '4', '17', '3', '1', '3', '4', '0.2', '102', '1', '0', '0.571428571', '0.611111111', '0', '0', '15.83333333', '2', '0.6667', '3']
package org . apache . lucene . search . spans ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . util . ToStringUtils ; import java . io . IOException ; import java . util . ArrayList ; import java . util . Collection ; import java . util . Set ; public class SpanTermQuery extends SpanQuery { protected Term term ; public SpanTermQuery ( Term term ) { this . term = term ; } public Term getTerm ( ) { return term ; } public String getField ( ) { return term . field ( ) ; } public Collection getTerms ( ) { Collection terms = new ArrayList ( ) ; terms . add ( term ) ; return terms ; } public void extractTerms ( Set terms ) { terms . add ( term ) ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( term . field ( ) . equals ( field ) ) buffer . append ( term . text ( ) ) ; else buffer . append ( term . toString ( ) ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof SpanTermQuery ) ) return false ; SpanTermQuery other = ( SpanTermQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && this . term . equals ( other . term ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ term . hashCode ( ) ^ 0xD23FE494 ; } public Spans getSpans ( final IndexReader reader ) throws IOException { return new TermSpans ( reader . termPositions ( term ) , term ) ; } } 	0	['9', '3', '1', '8', '27', '0', '1', '7', '9', '0', '116', '1', '1', '0.666666667', '0.259259259', '2', '2', '11.77777778', '4', '1.3333', '0']
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . FieldCache ; import org . apache . lucene . search . function . DocValues ; import java . io . IOException ; public class IntFieldSource extends FieldCacheSource { private FieldCache . IntParser parser ; public IntFieldSource ( String field ) { this ( field , null ) ; } public IntFieldSource ( String field , FieldCache . IntParser parser ) { super ( field ) ; this . parser = parser ; } public String description ( ) { return "int(" + super . description ( ) + ')' ; } public DocValues getCachedFieldValues ( FieldCache cache , String field , IndexReader reader ) throws IOException { final int [ ] arr = ( parser == null ) ? cache . getInts ( reader , field ) : cache . getInts ( reader , field , parser ) ; return new DocValues ( reader . maxDoc ( ) ) { public float floatVal ( int doc ) { return ( float ) arr [ doc ] ; } public int intVal ( int doc ) { return arr [ doc ] ; } public String toString ( int doc ) { return description ( ) + '=' + intVal ( doc ) ; } Object getInnerArray ( ) { return arr ; } } ; } public boolean cachedFieldSourceEquals ( FieldCacheSource o ) { if ( o . getClass ( ) != IntFieldSource . class ) { return false ; } IntFieldSource other = ( IntFieldSource ) o ; return this . parser == null ? other . parser == null : this . parser . getClass ( ) == other . parser . getClass ( ) ; } public int cachedFieldSourceHashCode ( ) { return parser == null ? Integer . class . hashCode ( ) : parser . getClass ( ) . hashCode ( ) ; } } 	1	['7', '3', '0', '7', '22', '9', '2', '6', '6', '0.777777778', '122', '0.333333333', '1', '0.705882353', '0.333333333', '2', '3', '16', '6', '1.7143', '1']
package org . apache . lucene . util ; public abstract class StringHelper { public static final int stringDifference ( String s1 , String s2 ) { int len1 = s1 . length ( ) ; int len2 = s2 . length ( ) ; int len = len1 < len2 ? len1 : len2 ; for ( int i = 0 ; i < len ; i ++ ) { if ( s1 . charAt ( i ) != s2 . charAt ( i ) ) { return i ; } } return len ; } private StringHelper ( ) { } } 	0	['2', '1', '0', '2', '5', '1', '2', '0', '1', '2', '36', '0', '0', '0', '0.5', '0', '0', '17', '4', '2', '0']
package org . apache . lucene . index ; import java . util . List ; import java . io . IOException ; public interface IndexDeletionPolicy { public void onInit ( List commits ) throws IOException ; public void onCommit ( List commits ) throws IOException ; } 	1	['2', '1', '0', '5', '2', '1', '5', '0', '2', '2', '2', '0', '0', '0', '1', '0', '0', '0', '1', '1', '1']
package org . apache . lucene . index ; public interface TermPositionVector extends TermFreqVector { public int [ ] getTermPositions ( int index ) ; public TermVectorOffsetInfo [ ] getOffsets ( int index ) ; } 	0	['2', '1', '0', '4', '2', '1', '2', '2', '2', '2', '2', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Explanation ; import org . apache . lucene . search . Query ; import org . apache . lucene . search . Scorer ; import org . apache . lucene . search . Searcher ; import org . apache . lucene . search . Similarity ; import org . apache . lucene . search . Weight ; import org . apache . lucene . util . ToStringUtils ; import java . util . Set ; public class MatchAllDocsQuery extends Query { public MatchAllDocsQuery ( ) { } private class MatchAllScorer extends Scorer { final IndexReader reader ; int id ; final int maxId ; final float score ; MatchAllScorer ( IndexReader reader , Similarity similarity , Weight w ) { super ( similarity ) ; this . reader = reader ; id = - 1 ; maxId = reader . maxDoc ( ) - 1 ; score = w . getValue ( ) ; } public Explanation explain ( int doc ) { return null ; } public int doc ( ) { return id ; } public boolean next ( ) { while ( id < maxId ) { id ++ ; if ( ! reader . isDeleted ( id ) ) { return true ; } } return false ; } public float score ( ) { return score ; } public boolean skipTo ( int target ) { id = target - 1 ; return next ( ) ; } } private class MatchAllDocsWeight implements Weight { private Similarity similarity ; private float queryWeight ; private float queryNorm ; public MatchAllDocsWeight ( Searcher searcher ) { this . similarity = searcher . getSimilarity ( ) ; } public String toString ( ) { return "weight(" + MatchAllDocsQuery . this + ")" ; } public Query getQuery ( ) { return MatchAllDocsQuery . this ; } public float getValue ( ) { return queryWeight ; } public float sumOfSquaredWeights ( ) { queryWeight = getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float queryNorm ) { this . queryNorm = queryNorm ; queryWeight *= this . queryNorm ; } public Scorer scorer ( IndexReader reader ) { return new MatchAllScorer ( reader , similarity , this ) ; } public Explanation explain ( IndexReader reader , int doc ) { Explanation queryExpl = new ComplexExplanation ( true , getValue ( ) , "MatchAllDocsQuery, product of:" ) ; if ( getBoost ( ) != 1.0f ) { queryExpl . addDetail ( new Explanation ( getBoost ( ) , "boost" ) ) ; } queryExpl . addDetail ( new Explanation ( queryNorm , "queryNorm" ) ) ; return queryExpl ; } } protected Weight createWeight ( Searcher searcher ) { return new MatchAllDocsWeight ( searcher ) ; } public void extractTerms ( Set terms ) { } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "MatchAllDocsQuery" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof MatchAllDocsQuery ) ) return false ; MatchAllDocsQuery other = ( MatchAllDocsQuery ) o ; return this . getBoost ( ) == other . getBoost ( ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ 0x1AA71190 ; } } 	1	['6', '2', '0', '7', '14', '15', '3', '5', '5', '2', '57', '0', '0', '0.705882353', '0.333333333', '2', '3', '8.5', '3', '1.1667', '1']
package org . apache . lucene . util ; import java . io . IOException ; import org . apache . lucene . search . Scorer ; public class ScorerDocQueue { private final HeapedScorerDoc [ ] heap ; private final int maxSize ; private int size ; private class HeapedScorerDoc { Scorer scorer ; int doc ; HeapedScorerDoc ( Scorer s ) { this ( s , s . doc ( ) ) ; } HeapedScorerDoc ( Scorer scorer , int doc ) { this . scorer = scorer ; this . doc = doc ; } void adjust ( ) { doc = scorer . doc ( ) ; } } private HeapedScorerDoc topHSD ; public ScorerDocQueue ( int maxSize ) { size = 0 ; int heapSize = maxSize + 1 ; heap = new HeapedScorerDoc [ heapSize ] ; this . maxSize = maxSize ; topHSD = heap [ 1 ] ; } public final void put ( Scorer scorer ) { size ++ ; heap [ size ] = new HeapedScorerDoc ( scorer ) ; upHeap ( ) ; } public boolean insert ( Scorer scorer ) { if ( size < maxSize ) { put ( scorer ) ; return true ; } else { int docNr = scorer . doc ( ) ; if ( ( size > 0 ) && ( ! ( docNr < topHSD . doc ) ) ) { heap [ 1 ] = new HeapedScorerDoc ( scorer , docNr ) ; downHeap ( ) ; return true ; } else { return false ; } } } public final Scorer top ( ) { return topHSD . scorer ; } public final int topDoc ( ) { return topHSD . doc ; } public final float topScore ( ) throws IOException { return topHSD . scorer . score ( ) ; } public final boolean topNextAndAdjustElsePop ( ) throws IOException { return checkAdjustElsePop ( topHSD . scorer . next ( ) ) ; } public final boolean topSkipToAndAdjustElsePop ( int target ) throws IOException { return checkAdjustElsePop ( topHSD . scorer . skipTo ( target ) ) ; } private boolean checkAdjustElsePop ( boolean cond ) { if ( cond ) { topHSD . doc = topHSD . scorer . doc ( ) ; } else { heap [ 1 ] = heap [ size ] ; heap [ size ] = null ; size -- ; } downHeap ( ) ; return cond ; } public final Scorer pop ( ) { Scorer result = topHSD . scorer ; popNoResult ( ) ; return result ; } private final void popNoResult ( ) { heap [ 1 ] = heap [ size ] ; heap [ size ] = null ; size -- ; downHeap ( ) ; } public final void adjustTop ( ) { topHSD . adjust ( ) ; downHeap ( ) ; } public final int size ( ) { return size ; } public final void clear ( ) { for ( int i = 0 ; i <= size ; i ++ ) { heap [ i ] = null ; } size = 0 ; } private final void upHeap ( ) { int i = size ; HeapedScorerDoc node = heap [ i ] ; int j = i > > > 1 ; while ( ( j > 0 ) && ( node . doc < heap [ j ] . doc ) ) { heap [ i ] = heap [ j ] ; i = j ; j = j > > > 1 ; } heap [ i ] = node ; topHSD = heap [ 1 ] ; } private final void downHeap ( ) { int i = 1 ; HeapedScorerDoc node = heap [ i ] ; int j = i << 1 ; int k = j + 1 ; if ( ( k <= size ) && ( heap [ k ] . doc < heap [ j ] . doc ) ) { j = k ; } while ( ( j <= size ) && ( heap [ j ] . doc < node . doc ) ) { heap [ i ] = heap [ j ] ; i = j ; j = i << 1 ; k = j + 1 ; if ( k <= size && ( heap [ k ] . doc < heap [ j ] . doc ) ) { j = k ; } } heap [ i ] = node ; topHSD = heap [ 1 ] ; } } 	0	['16', '1', '0', '3', '24', '0', '2', '2', '12', '0.383333333', '361', '1', '2', '0', '0.328125', '0', '0', '21.3125', '7', '1.75', '0']
package org . apache . lucene . index ; public interface IndexCommitPoint { public String getSegmentsFileName ( ) ; public void delete ( ) ; } 	1	['2', '1', '0', '2', '2', '1', '2', '0', '2', '2', '2', '0', '0', '0', '1', '0', '0', '0', '1', '1', '2']
package org . apache . lucene . search ; public class DefaultSimilarity extends Similarity { public float lengthNorm ( String fieldName , int numTerms ) { return ( float ) ( 1.0 / Math . sqrt ( numTerms ) ) ; } public float queryNorm ( float sumOfSquaredWeights ) { return ( float ) ( 1.0 / Math . sqrt ( sumOfSquaredWeights ) ) ; } public float tf ( float freq ) { return ( float ) Math . sqrt ( freq ) ; } public float sloppyFreq ( int distance ) { return 1.0f / ( distance + 1 ) ; } public float idf ( int docFreq , int numDocs ) { return ( float ) ( Math . log ( numDocs / ( double ) ( docFreq + 1 ) ) + 1.0 ) ; } public float coord ( int overlap , int maxOverlap ) { return overlap / ( float ) maxOverlap ; } } 	0	['7', '2', '0', '3', '10', '21', '3', '1', '7', '2', '54', '0', '0', '0.714285714', '0.5', '1', '2', '6.714285714', '1', '0.8571', '0']
package org . apache . lucene . index ; import java . util . * ; class SegmentTermVector implements TermFreqVector { private String field ; private String terms [ ] ; private int termFreqs [ ] ; SegmentTermVector ( String field , String terms [ ] , int termFreqs [ ] ) { this . field = field ; this . terms = terms ; this . termFreqs = termFreqs ; } public String getField ( ) { return field ; } public String toString ( ) { StringBuffer sb = new StringBuffer ( ) ; sb . append ( '{' ) ; sb . append ( field ) . append ( ": " ) ; if ( terms != null ) { for ( int i = 0 ; i < terms . length ; i ++ ) { if ( i > 0 ) sb . append ( ", " ) ; sb . append ( terms [ i ] ) . append ( '/' ) . append ( termFreqs [ i ] ) ; } } sb . append ( '}' ) ; return sb . toString ( ) ; } public int size ( ) { return terms == null ? 0 : terms . length ; } public String [ ] getTerms ( ) { return terms ; } public int [ ] getTermFrequencies ( ) { return termFreqs ; } public int indexOf ( String termText ) { if ( terms == null ) return - 1 ; int res = Arrays . binarySearch ( terms , termText ) ; return res >= 0 ? res : - 1 ; } public int [ ] indexesOf ( String [ ] termNumbers , int start , int len ) { int res [ ] = new int [ len ] ; for ( int i = 0 ; i < len ; i ++ ) { res [ i ] = indexOf ( termNumbers [ start + i ] ) ; } return res ; } } 	1	['8', '1', '1', '3', '15', '0', '2', '1', '7', '0.571428571', '133', '1', '0', '0', '0.35', '0', '0', '15.25', '4', '1.75', '1']
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; final class PhraseQueue extends PriorityQueue { PhraseQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object o1 , Object o2 ) { PhrasePositions pp1 = ( PhrasePositions ) o1 ; PhrasePositions pp2 = ( PhrasePositions ) o2 ; if ( pp1 . doc == pp2 . doc ) if ( pp1 . position == pp2 . position ) return pp1 . offset < pp2 . offset ; else return pp1 . position < pp2 . position ; else return pp1 . doc < pp2 . doc ; } } 	0	['2', '2', '0', '5', '4', '1', '3', '2', '0', '2', '51', '0', '0', '0.916666667', '0.666666667', '1', '3', '24.5', '6', '3', '0']
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . function . DocValues ; import org . apache . lucene . util . ToStringUtils ; import java . io . IOException ; import java . io . Serializable ; public abstract class ValueSource implements Serializable { public abstract DocValues getValues ( IndexReader reader ) throws IOException ; public abstract String description ( ) ; public String toString ( ) { return description ( ) ; } public abstract boolean equals ( Object o ) ; public abstract int hashCode ( ) ; } 	1	['6', '1', '3', '8', '7', '15', '6', '2', '6', '2', '12', '0', '0', '0', '0.444444444', '1', '1', '1', '1', '0.8333', '1']
package org . apache . lucene . store ; public class AlreadyClosedException extends IllegalStateException { public AlreadyClosedException ( String message ) { super ( message ) ; } } 	0	['1', '5', '0', '4', '2', '0', '4', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . search ; public class FieldDoc extends ScoreDoc { public Comparable [ ] fields ; public FieldDoc ( int doc , float score ) { super ( doc , score ) ; } public FieldDoc ( int doc , float score , Comparable [ ] fields ) { super ( doc , score ) ; this . fields = fields ; } } 	1	['2', '2', '0', '4', '3', '1', '3', '1', '2', '1', '16', '0', '0', '0', '0.875', '0', '0', '6.5', '0', '0', '1']
package org . apache . lucene . search . spans ; import java . io . IOException ; public interface Spans { boolean next ( ) throws IOException ; boolean skipTo ( int target ) throws IOException ; int doc ( ) ; int start ( ) ; int end ( ) ; } 	0	['5', '1', '0', '20', '5', '10', '20', '0', '5', '2', '5', '0', '0', '0', '0.6', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . queryParser ; public class TokenMgrError extends Error { static final int LEXICAL_ERROR = 0 ; static final int STATIC_LEXER_ERROR = 1 ; static final int INVALID_LEXICAL_STATE = 2 ; static final int LOOP_DETECTED = 3 ; int errorCode ; protected static final String addEscapes ( String str ) { StringBuffer retval = new StringBuffer ( ) ; char ch ; for ( int i = 0 ; i < str . length ( ) ; i ++ ) { switch ( str . charAt ( i ) ) { case 0 : continue ; case '\b' : retval . append ( "\\b" ) ; continue ; case '\t' : retval . append ( "\\t" ) ; continue ; case '\n' : retval . append ( "\\n" ) ; continue ; case '\f' : retval . append ( "\\f" ) ; continue ; case '\r' : retval . append ( "\\r" ) ; continue ; case '\"' : retval . append ( "\\\"" ) ; continue ; case '\'' : retval . append ( "\\\'" ) ; continue ; case '\\' : retval . append ( "\\\\" ) ; continue ; default : if ( ( ch = str . charAt ( i ) ) < 0x20 || ch > 0x7e ) { String s = "0000" + Integer . toString ( ch , 16 ) ; retval . append ( "\\u" + s . substring ( s . length ( ) - 4 , s . length ( ) ) ) ; } else { retval . append ( ch ) ; } continue ; } } return retval . toString ( ) ; } protected static String LexicalError ( boolean EOFSeen , int lexState , int errorLine , int errorColumn , String errorAfter , char curChar ) { return ( "Lexical error at line " + errorLine + ", column " + errorColumn + ".  Encountered: " + ( EOFSeen ? "<EOF> " : ( "\"" + addEscapes ( String . valueOf ( curChar ) ) + "\"" ) + " (" + ( int ) curChar + "), " ) + "after : \"" + addEscapes ( errorAfter ) + "\"" ) ; } public String getMessage ( ) { return super . getMessage ( ) ; } public TokenMgrError ( ) { } public TokenMgrError ( String message , int reason ) { super ( message ) ; errorCode = reason ; } public TokenMgrError ( boolean EOFSeen , int lexState , int errorLine , int errorColumn , String errorAfter , char curChar , int reason ) { this ( LexicalError ( EOFSeen , lexState , errorLine , errorColumn , errorAfter , curChar ) , reason ) ; } } 	1	['6', '3', '0', '2', '19', '15', '2', '0', '4', '1.12', '184', '0', '0', '0.8125', '0.5', '1', '1', '28.83333333', '14', '2.8333', '3']
package org . apache . lucene . index ; import java . io . IOException ; final class SegmentMergeInfo { Term term ; int base ; TermEnum termEnum ; IndexReader reader ; private TermPositions postings ; private int [ ] docMap ; SegmentMergeInfo ( int b , TermEnum te , IndexReader r ) throws IOException { base = b ; reader = r ; termEnum = te ; term = te . term ( ) ; } int [ ] getDocMap ( ) { if ( docMap == null ) { if ( reader . hasDeletions ( ) ) { int maxDoc = reader . maxDoc ( ) ; docMap = new int [ maxDoc ] ; int j = 0 ; for ( int i = 0 ; i < maxDoc ; i ++ ) { if ( reader . isDeleted ( i ) ) docMap [ i ] = - 1 ; else docMap [ i ] = j ++ ; } } } return docMap ; } TermPositions getPositions ( ) throws IOException { if ( postings == null ) { postings = reader . termPositions ( ) ; } return postings ; } final boolean next ( ) throws IOException { if ( termEnum . next ( ) ) { term = termEnum . term ( ) ; return true ; } else { term = null ; return false ; } } final void close ( ) throws IOException { termEnum . close ( ) ; if ( postings != null ) { postings . close ( ) ; } } } 	0	['5', '1', '0', '7', '14', '0', '3', '4', '0', '0.75', '108', '0.333333333', '4', '0', '0.4', '0', '0', '19.4', '5', '1.6', '0']
package org . apache . lucene . index ; import java . util . Vector ; import java . util . Iterator ; import java . util . Collection ; import java . io . IOException ; import org . apache . lucene . document . FieldSelector ; import org . apache . lucene . document . FieldSelectorResult ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; final class SegmentMerger { static final byte [ ] NORMS_HEADER = new byte [ ] { 'N' , 'R' , 'M' , - 1 } ; private Directory directory ; private String segment ; private int termIndexInterval = IndexWriter . DEFAULT_TERM_INDEX_INTERVAL ; private Vector readers = new Vector ( ) ; private FieldInfos fieldInfos ; private int mergedDocs ; SegmentMerger ( Directory dir , String name ) { directory = dir ; segment = name ; } SegmentMerger ( IndexWriter writer , String name ) { directory = writer . getDirectory ( ) ; segment = name ; termIndexInterval = writer . getTermIndexInterval ( ) ; } final void add ( IndexReader reader ) { readers . addElement ( reader ) ; } final IndexReader segmentReader ( int i ) { return ( IndexReader ) readers . elementAt ( i ) ; } final int merge ( ) throws CorruptIndexException , IOException { int value ; mergedDocs = mergeFields ( ) ; mergeTerms ( ) ; mergeNorms ( ) ; if ( fieldInfos . hasVectors ( ) ) mergeVectors ( ) ; return mergedDocs ; } final void closeReaders ( ) throws IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader reader = ( IndexReader ) readers . elementAt ( i ) ; reader . close ( ) ; } } final Vector createCompoundFile ( String fileName ) throws IOException { CompoundFileWriter cfsWriter = new CompoundFileWriter ( directory , fileName ) ; Vector files = new Vector ( IndexFileNames . COMPOUND_EXTENSIONS . length + 1 ) ; for ( int i = 0 ; i < IndexFileNames . COMPOUND_EXTENSIONS . length ; i ++ ) { files . add ( segment + "." + IndexFileNames . COMPOUND_EXTENSIONS [ i ] ) ; } for ( int i = 0 ; i < fieldInfos . size ( ) ; i ++ ) { FieldInfo fi = fieldInfos . fieldInfo ( i ) ; if ( fi . isIndexed && ! fi . omitNorms ) { files . add ( segment + "." + IndexFileNames . NORMS_EXTENSION ) ; break ; } } if ( fieldInfos . hasVectors ( ) ) { for ( int i = 0 ; i < IndexFileNames . VECTOR_EXTENSIONS . length ; i ++ ) { files . add ( segment + "." + IndexFileNames . VECTOR_EXTENSIONS [ i ] ) ; } } Iterator it = files . iterator ( ) ; while ( it . hasNext ( ) ) { cfsWriter . addFile ( ( String ) it . next ( ) ) ; } cfsWriter . close ( ) ; return files ; } private void addIndexed ( IndexReader reader , FieldInfos fieldInfos , Collection names , boolean storeTermVectors , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector , boolean storePayloads ) throws IOException { Iterator i = names . iterator ( ) ; while ( i . hasNext ( ) ) { String field = ( String ) i . next ( ) ; fieldInfos . add ( field , true , storeTermVectors , storePositionWithTermVector , storeOffsetWithTermVector , ! reader . hasNorms ( field ) , storePayloads ) ; } } private final int mergeFields ( ) throws CorruptIndexException , IOException { fieldInfos = new FieldInfos ( ) ; int docCount = 0 ; for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader reader = ( IndexReader ) readers . elementAt ( i ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . TERMVECTOR_WITH_POSITION_OFFSET ) , true , true , true , false ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . TERMVECTOR_WITH_POSITION ) , true , true , false , false ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . TERMVECTOR_WITH_OFFSET ) , true , false , true , false ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . TERMVECTOR ) , true , false , false , false ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . STORES_PAYLOADS ) , false , false , false , true ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . INDEXED ) , false , false , false , false ) ; fieldInfos . add ( reader . getFieldNames ( IndexReader . FieldOption . UNINDEXED ) , false ) ; } fieldInfos . write ( directory , segment + ".fnm" ) ; FieldsWriter fieldsWriter = new FieldsWriter ( directory , segment , fieldInfos ) ; FieldSelector fieldSelectorMerge = new FieldSelector ( ) { public FieldSelectorResult accept ( String fieldName ) { return FieldSelectorResult . LOAD_FOR_MERGE ; } } ; try { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader reader = ( IndexReader ) readers . elementAt ( i ) ; int maxDoc = reader . maxDoc ( ) ; for ( int j = 0 ; j < maxDoc ; j ++ ) if ( ! reader . isDeleted ( j ) ) { fieldsWriter . addDocument ( reader . document ( j , fieldSelectorMerge ) ) ; docCount ++ ; } } } finally { fieldsWriter . close ( ) ; } return docCount ; } private final void mergeVectors ( ) throws IOException { TermVectorsWriter termVectorsWriter = new TermVectorsWriter ( directory , segment , fieldInfos ) ; try { for ( int r = 0 ; r < readers . size ( ) ; r ++ ) { IndexReader reader = ( IndexReader ) readers . elementAt ( r ) ; int maxDoc = reader . maxDoc ( ) ; for ( int docNum = 0 ; docNum < maxDoc ; docNum ++ ) { if ( reader . isDeleted ( docNum ) ) continue ; termVectorsWriter . addAllDocVectors ( reader . getTermFreqVectors ( docNum ) ) ; } } } finally { termVectorsWriter . close ( ) ; } } private IndexOutput freqOutput = null ; private IndexOutput proxOutput = null ; private TermInfosWriter termInfosWriter = null ; private int skipInterval ; private int maxSkipLevels ; private SegmentMergeQueue queue = null ; private DefaultSkipListWriter skipListWriter = null ; private final void mergeTerms ( ) throws CorruptIndexException , IOException { try { freqOutput = directory . createOutput ( segment + ".frq" ) ; proxOutput = directory . createOutput ( segment + ".prx" ) ; termInfosWriter = new TermInfosWriter ( directory , segment , fieldInfos , termIndexInterval ) ; skipInterval = termInfosWriter . skipInterval ; maxSkipLevels = termInfosWriter . maxSkipLevels ; skipListWriter = new DefaultSkipListWriter ( skipInterval , maxSkipLevels , mergedDocs , freqOutput , proxOutput ) ; queue = new SegmentMergeQueue ( readers . size ( ) ) ; mergeTermInfos ( ) ; } finally { if ( freqOutput != null ) freqOutput . close ( ) ; if ( proxOutput != null ) proxOutput . close ( ) ; if ( termInfosWriter != null ) termInfosWriter . close ( ) ; if ( queue != null ) queue . close ( ) ; } } private final void mergeTermInfos ( ) throws CorruptIndexException , IOException { int base = 0 ; for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader reader = ( IndexReader ) readers . elementAt ( i ) ; TermEnum termEnum = reader . terms ( ) ; SegmentMergeInfo smi = new SegmentMergeInfo ( base , termEnum , reader ) ; base += reader . numDocs ( ) ; if ( smi . next ( ) ) queue . put ( smi ) ; else smi . close ( ) ; } SegmentMergeInfo [ ] match = new SegmentMergeInfo [ readers . size ( ) ] ; while ( queue . size ( ) > 0 ) { int matchSize = 0 ; match [ matchSize ++ ] = ( SegmentMergeInfo ) queue . pop ( ) ; Term term = match [ 0 ] . term ; SegmentMergeInfo top = ( SegmentMergeInfo ) queue . top ( ) ; while ( top != null && term . compareTo ( top . term ) == 0 ) { match [ matchSize ++ ] = ( SegmentMergeInfo ) queue . pop ( ) ; top = ( SegmentMergeInfo ) queue . top ( ) ; } mergeTermInfo ( match , matchSize ) ; while ( matchSize > 0 ) { SegmentMergeInfo smi = match [ -- matchSize ] ; if ( smi . next ( ) ) queue . put ( smi ) ; else smi . close ( ) ; } } } private final TermInfo termInfo = new TermInfo ( ) ; private final void mergeTermInfo ( SegmentMergeInfo [ ] smis , int n ) throws CorruptIndexException , IOException { long freqPointer = freqOutput . getFilePointer ( ) ; long proxPointer = proxOutput . getFilePointer ( ) ; int df = appendPostings ( smis , n ) ; long skipPointer = skipListWriter . writeSkip ( freqOutput ) ; if ( df > 0 ) { termInfo . set ( df , freqPointer , proxPointer , ( int ) ( skipPointer - freqPointer ) ) ; termInfosWriter . add ( smis [ 0 ] . term , termInfo ) ; } } private byte [ ] payloadBuffer = null ; private final int appendPostings ( SegmentMergeInfo [ ] smis , int n ) throws CorruptIndexException , IOException { int lastDoc = 0 ; int df = 0 ; skipListWriter . resetSkip ( ) ; boolean storePayloads = fieldInfos . fieldInfo ( smis [ 0 ] . term . field ) . storePayloads ; int lastPayloadLength = - 1 ; for ( int i = 0 ; i < n ; i ++ ) { SegmentMergeInfo smi = smis [ i ] ; TermPositions postings = smi . getPositions ( ) ; int base = smi . base ; int [ ] docMap = smi . getDocMap ( ) ; postings . seek ( smi . termEnum ) ; while ( postings . next ( ) ) { int doc = postings . doc ( ) ; if ( docMap != null ) doc = docMap [ doc ] ; doc += base ; if ( doc < 0 || ( df > 0 && doc <= lastDoc ) ) throw new CorruptIndexException ( "docs out of order (" + doc + " <= " + lastDoc + " )" ) ; df ++ ; if ( ( df % skipInterval ) == 0 ) { skipListWriter . setSkipData ( lastDoc , storePayloads , lastPayloadLength ) ; skipListWriter . bufferSkip ( df ) ; } int docCode = ( doc - lastDoc ) << 1 ; lastDoc = doc ; int freq = postings . freq ( ) ; if ( freq == 1 ) { freqOutput . writeVInt ( docCode | 1 ) ; } else { freqOutput . writeVInt ( docCode ) ; freqOutput . writeVInt ( freq ) ; } int lastPosition = 0 ; for ( int j = 0 ; j < freq ; j ++ ) { int position = postings . nextPosition ( ) ; int delta = position - lastPosition ; if ( storePayloads ) { int payloadLength = postings . getPayloadLength ( ) ; if ( payloadLength == lastPayloadLength ) { proxOutput . writeVInt ( delta * 2 ) ; } else { proxOutput . writeVInt ( delta * 2 + 1 ) ; proxOutput . writeVInt ( payloadLength ) ; lastPayloadLength = payloadLength ; } if ( payloadLength > 0 ) { if ( payloadBuffer == null || payloadBuffer . length < payloadLength ) { payloadBuffer = new byte [ payloadLength ] ; } postings . getPayload ( payloadBuffer , 0 ) ; proxOutput . writeBytes ( payloadBuffer , 0 , payloadLength ) ; } } else { proxOutput . writeVInt ( delta ) ; } lastPosition = position ; } } } return df ; } private void mergeNorms ( ) throws IOException { byte [ ] normBuffer = null ; IndexOutput output = null ; try { for ( int i = 0 ; i < fieldInfos . size ( ) ; i ++ ) { FieldInfo fi = fieldInfos . fieldInfo ( i ) ; if ( fi . isIndexed && ! fi . omitNorms ) { if ( output == null ) { output = directory . createOutput ( segment + "." + IndexFileNames . NORMS_EXTENSION ) ; output . writeBytes ( NORMS_HEADER , NORMS_HEADER . length ) ; } for ( int j = 0 ; j < readers . size ( ) ; j ++ ) { IndexReader reader = ( IndexReader ) readers . elementAt ( j ) ; int maxDoc = reader . maxDoc ( ) ; if ( normBuffer == null || normBuffer . length < maxDoc ) { normBuffer = new byte [ maxDoc ] ; } reader . norms ( fi . name , normBuffer , 0 ) ; if ( ! reader . hasDeletions ( ) ) { output . writeBytes ( normBuffer , maxDoc ) ; } else { for ( int k = 0 ; k < maxDoc ; k ++ ) { if ( ! reader . isDeleted ( k ) ) { output . writeByte ( normBuffer [ k ] ) ; } } } } } } } finally { if ( output != null ) { output . close ( ) ; } } } } 	1	['16', '1', '0', '25', '99', '0', '3', '24', '0', '0.691666667', '1172', '0.9375', '8', '0', '0.2', '0', '0', '71.25', '1', '0.8125', '8']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . util . BitVector ; import org . apache . lucene . store . IndexInput ; class SegmentTermDocs implements TermDocs { protected SegmentReader parent ; protected IndexInput freqStream ; protected int count ; protected int df ; protected BitVector deletedDocs ; int doc = 0 ; int freq ; private int skipInterval ; private int maxSkipLevels ; private DefaultSkipListReader skipListReader ; private long freqBasePointer ; private long proxBasePointer ; private long skipPointer ; private boolean haveSkipped ; protected boolean currentFieldStoresPayloads ; protected SegmentTermDocs ( SegmentReader parent ) { this . parent = parent ; this . freqStream = ( IndexInput ) parent . freqStream . clone ( ) ; this . deletedDocs = parent . deletedDocs ; this . skipInterval = parent . tis . getSkipInterval ( ) ; this . maxSkipLevels = parent . tis . getMaxSkipLevels ( ) ; } public void seek ( Term term ) throws IOException { TermInfo ti = parent . tis . get ( term ) ; seek ( ti , term ) ; } public void seek ( TermEnum termEnum ) throws IOException { TermInfo ti ; Term term ; if ( termEnum instanceof SegmentTermEnum && ( ( SegmentTermEnum ) termEnum ) . fieldInfos == parent . fieldInfos ) { SegmentTermEnum segmentTermEnum = ( ( SegmentTermEnum ) termEnum ) ; term = segmentTermEnum . term ( ) ; ti = segmentTermEnum . termInfo ( ) ; } else { term = termEnum . term ( ) ; ti = parent . tis . get ( term ) ; } seek ( ti , term ) ; } void seek ( TermInfo ti , Term term ) throws IOException { count = 0 ; FieldInfo fi = parent . fieldInfos . fieldInfo ( term . field ) ; currentFieldStoresPayloads = ( fi != null ) ? fi . storePayloads : false ; if ( ti == null ) { df = 0 ; } else { df = ti . docFreq ; doc = 0 ; freqBasePointer = ti . freqPointer ; proxBasePointer = ti . proxPointer ; skipPointer = freqBasePointer + ti . skipOffset ; freqStream . seek ( freqBasePointer ) ; haveSkipped = false ; } } public void close ( ) throws IOException { freqStream . close ( ) ; if ( skipListReader != null ) skipListReader . close ( ) ; } public final int doc ( ) { return doc ; } public final int freq ( ) { return freq ; } protected void skippingDoc ( ) throws IOException { } public boolean next ( ) throws IOException { while ( true ) { if ( count == df ) return false ; int docCode = freqStream . readVInt ( ) ; doc += docCode > > > 1 ; if ( ( docCode & 1 ) != 0 ) freq = 1 ; else freq = freqStream . readVInt ( ) ; count ++ ; if ( deletedDocs == null || ! deletedDocs . get ( doc ) ) break ; skippingDoc ( ) ; } return true ; } public int read ( final int [ ] docs , final int [ ] freqs ) throws IOException { final int length = docs . length ; int i = 0 ; while ( i < length && count < df ) { final int docCode = freqStream . readVInt ( ) ; doc += docCode > > > 1 ; if ( ( docCode & 1 ) != 0 ) freq = 1 ; else freq = freqStream . readVInt ( ) ; count ++ ; if ( deletedDocs == null || ! deletedDocs . get ( doc ) ) { docs [ i ] = doc ; freqs [ i ] = freq ; ++ i ; } } return i ; } protected void skipProx ( long proxPointer , int payloadLength ) throws IOException { } public boolean skipTo ( int target ) throws IOException { if ( df >= skipInterval ) { if ( skipListReader == null ) skipListReader = new DefaultSkipListReader ( ( IndexInput ) freqStream . clone ( ) , maxSkipLevels , skipInterval ) ; if ( ! haveSkipped ) { skipListReader . init ( skipPointer , freqBasePointer , proxBasePointer , df , currentFieldStoresPayloads ) ; haveSkipped = true ; } int newCount = skipListReader . skipTo ( target ) ; if ( newCount > count ) { freqStream . seek ( skipListReader . getFreqPointer ( ) ) ; skipProx ( skipListReader . getProxPointer ( ) , skipListReader . getPayloadLength ( ) ) ; doc = skipListReader . getDoc ( ) ; count = newCount ; } } do { if ( ! next ( ) ) return false ; } while ( target > doc ) ; return true ; } } 	0	['12', '1', '1', '13', '33', '12', '2', '12', '8', '0.690909091', '377', '0.866666667', '4', '0', '0.21875', '0', '0', '29.16666667', '1', '0.9167', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . ArrayList ; class DisjunctionMaxScorer extends Scorer { private ArrayList subScorers = new ArrayList ( ) ; private float tieBreakerMultiplier ; private boolean more = false ; private boolean firstTime = true ; public DisjunctionMaxScorer ( float tieBreakerMultiplier , Similarity similarity ) { super ( similarity ) ; this . tieBreakerMultiplier = tieBreakerMultiplier ; } public void add ( Scorer scorer ) throws IOException { if ( scorer . next ( ) ) { subScorers . add ( scorer ) ; more = true ; } } public boolean next ( ) throws IOException { if ( ! more ) return false ; if ( firstTime ) { heapify ( ) ; firstTime = false ; return true ; } int lastdoc = ( ( Scorer ) subScorers . get ( 0 ) ) . doc ( ) ; do { if ( ( ( Scorer ) subScorers . get ( 0 ) ) . next ( ) ) heapAdjust ( 0 ) ; else { heapRemoveRoot ( ) ; if ( subScorers . isEmpty ( ) ) return ( more = false ) ; } } while ( ( ( Scorer ) subScorers . get ( 0 ) ) . doc ( ) == lastdoc ) ; return true ; } public int doc ( ) { return ( ( Scorer ) subScorers . get ( 0 ) ) . doc ( ) ; } public float score ( ) throws IOException { int doc = ( ( Scorer ) subScorers . get ( 0 ) ) . doc ( ) ; float [ ] sum = { ( ( Scorer ) subScorers . get ( 0 ) ) . score ( ) } , max = { sum [ 0 ] } ; int size = subScorers . size ( ) ; scoreAll ( 1 , size , doc , sum , max ) ; scoreAll ( 2 , size , doc , sum , max ) ; return max [ 0 ] + ( sum [ 0 ] - max [ 0 ] ) * tieBreakerMultiplier ; } private void scoreAll ( int root , int size , int doc , float [ ] sum , float [ ] max ) throws IOException { if ( root < size && ( ( Scorer ) subScorers . get ( root ) ) . doc ( ) == doc ) { float sub = ( ( Scorer ) subScorers . get ( root ) ) . score ( ) ; sum [ 0 ] += sub ; max [ 0 ] = Math . max ( max [ 0 ] , sub ) ; scoreAll ( ( root << 1 ) + 1 , size , doc , sum , max ) ; scoreAll ( ( root << 1 ) + 2 , size , doc , sum , max ) ; } } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { if ( ! more ) return false ; heapify ( ) ; firstTime = false ; } while ( subScorers . size ( ) > 0 && ( ( Scorer ) subScorers . get ( 0 ) ) . doc ( ) < target ) { if ( ( ( Scorer ) subScorers . get ( 0 ) ) . skipTo ( target ) ) heapAdjust ( 0 ) ; else heapRemoveRoot ( ) ; } if ( ( subScorers . size ( ) == 0 ) ) return ( more = false ) ; return true ; } public Explanation explain ( int doc ) throws IOException { throw new UnsupportedOperationException ( ) ; } private void heapify ( ) { int size = subScorers . size ( ) ; for ( int i = ( size > > 1 ) - 1 ; i >= 0 ; i -- ) heapAdjust ( i ) ; } private void heapAdjust ( int root ) { Scorer scorer = ( Scorer ) subScorers . get ( root ) ; int doc = scorer . doc ( ) ; int i = root , size = subScorers . size ( ) ; while ( i <= ( size > > 1 ) - 1 ) { int lchild = ( i << 1 ) + 1 ; Scorer lscorer = ( Scorer ) subScorers . get ( lchild ) ; int ldoc = lscorer . doc ( ) ; int rdoc = Integer . MAX_VALUE , rchild = ( i << 1 ) + 2 ; Scorer rscorer = null ; if ( rchild < size ) { rscorer = ( Scorer ) subScorers . get ( rchild ) ; rdoc = rscorer . doc ( ) ; } if ( ldoc < doc ) { if ( rdoc < ldoc ) { subScorers . set ( i , rscorer ) ; subScorers . set ( rchild , scorer ) ; i = rchild ; } else { subScorers . set ( i , lscorer ) ; subScorers . set ( lchild , scorer ) ; i = lchild ; } } else if ( rdoc < doc ) { subScorers . set ( i , rscorer ) ; subScorers . set ( rchild , scorer ) ; i = rchild ; } else return ; } } private void heapRemoveRoot ( ) { int size = subScorers . size ( ) ; if ( size == 1 ) subScorers . remove ( 0 ) ; else { subScorers . set ( 0 , subScorers . get ( size - 1 ) ) ; subScorers . remove ( size - 1 ) ; heapAdjust ( 0 ) ; } } } 	1	['11', '2', '0', '4', '25', '0', '1', '3', '7', '0.625', '447', '1', '0', '0.444444444', '0.287878788', '1', '3', '39.27272727', '6', '1.5455', '1']
package org . apache . lucene . analysis ; import java . io . Reader ; public final class WhitespaceAnalyzer extends Analyzer { public TokenStream tokenStream ( String fieldName , Reader reader ) { return new WhitespaceTokenizer ( reader ) ; } } 	0	['2', '2', '0', '3', '4', '1', '0', '3', '2', '2', '10', '0', '0', '0.666666667', '0.666666667', '0', '0', '4', '1', '0.5', '0']
package org . apache . lucene . search ; public class SimilarityDelegator extends Similarity { private Similarity delegee ; public SimilarityDelegator ( Similarity delegee ) { this . delegee = delegee ; } public float lengthNorm ( String fieldName , int numTerms ) { return delegee . lengthNorm ( fieldName , numTerms ) ; } public float queryNorm ( float sumOfSquaredWeights ) { return delegee . queryNorm ( sumOfSquaredWeights ) ; } public float tf ( float freq ) { return delegee . tf ( freq ) ; } public float sloppyFreq ( int distance ) { return delegee . sloppyFreq ( distance ) ; } public float idf ( int docFreq , int numDocs ) { return delegee . idf ( docFreq , numDocs ) ; } public float coord ( int overlap , int maxOverlap ) { return delegee . coord ( overlap , maxOverlap ) ; } } 	1	['7', '2', '1', '2', '14', '0', '1', '1', '7', '0', '47', '1', '1', '0.714285714', '0.428571429', '1', '2', '5.571428571', '1', '0.8571', '1']
package org . apache . lucene . analysis . standard ; public class TokenMgrError extends Error { static final int LEXICAL_ERROR = 0 ; static final int STATIC_LEXER_ERROR = 1 ; static final int INVALID_LEXICAL_STATE = 2 ; static final int LOOP_DETECTED = 3 ; int errorCode ; protected static final String addEscapes ( String str ) { StringBuffer retval = new StringBuffer ( ) ; char ch ; for ( int i = 0 ; i < str . length ( ) ; i ++ ) { switch ( str . charAt ( i ) ) { case 0 : continue ; case '\b' : retval . append ( "\\b" ) ; continue ; case '\t' : retval . append ( "\\t" ) ; continue ; case '\n' : retval . append ( "\\n" ) ; continue ; case '\f' : retval . append ( "\\f" ) ; continue ; case '\r' : retval . append ( "\\r" ) ; continue ; case '\"' : retval . append ( "\\\"" ) ; continue ; case '\'' : retval . append ( "\\\'" ) ; continue ; case '\\' : retval . append ( "\\\\" ) ; continue ; default : if ( ( ch = str . charAt ( i ) ) < 0x20 || ch > 0x7e ) { String s = "0000" + Integer . toString ( ch , 16 ) ; retval . append ( "\\u" + s . substring ( s . length ( ) - 4 , s . length ( ) ) ) ; } else { retval . append ( ch ) ; } continue ; } } return retval . toString ( ) ; } protected static String LexicalError ( boolean EOFSeen , int lexState , int errorLine , int errorColumn , String errorAfter , char curChar ) { return ( "Lexical error at line " + errorLine + ", column " + errorColumn + ".  Encountered: " + ( EOFSeen ? "<EOF> " : ( "\"" + addEscapes ( String . valueOf ( curChar ) ) + "\"" ) + " (" + ( int ) curChar + "), " ) + "after : \"" + addEscapes ( errorAfter ) + "\"" ) ; } public String getMessage ( ) { return super . getMessage ( ) ; } public TokenMgrError ( ) { } public TokenMgrError ( String message , int reason ) { super ( message ) ; errorCode = reason ; } public TokenMgrError ( boolean EOFSeen , int lexState , int errorLine , int errorColumn , String errorAfter , char curChar , int reason ) { this ( LexicalError ( EOFSeen , lexState , errorLine , errorColumn , errorAfter , curChar ) , reason ) ; } } 	0	['6', '3', '0', '1', '19', '15', '1', '0', '4', '1.12', '184', '0', '0', '0.8125', '0.5', '1', '1', '28.83333333', '14', '2.8333', '0']
package org . apache . lucene . store ; import java . io . IOException ; import java . util . HashSet ; import java . util . Enumeration ; public class SingleInstanceLockFactory extends LockFactory { private HashSet locks = new HashSet ( ) ; public Lock makeLock ( String lockName ) { return new SingleInstanceLock ( locks , lockName ) ; } public void clearLock ( String lockName ) throws IOException { synchronized ( locks ) { if ( locks . contains ( lockName ) ) { locks . remove ( lockName ) ; } } } } ; class SingleInstanceLock extends Lock { String lockName ; private HashSet locks ; public SingleInstanceLock ( HashSet locks , String lockName ) { this . locks = locks ; this . lockName = lockName ; } public boolean obtain ( ) throws IOException { synchronized ( locks ) { return locks . add ( lockName ) ; } } public void release ( ) { synchronized ( locks ) { locks . remove ( lockName ) ; } } public boolean isLocked ( ) { synchronized ( locks ) { return locks . contains ( lockName ) ; } } public String toString ( ) { return "SingleInstanceLock: " + lockName ; } } 	1	['3', '2', '0', '4', '8', '0', '1', '3', '3', '0', '43', '1', '0', '0.666666667', '0.833333333', '0', '0', '13', '1', '0.6667', '1']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . * ; final class ExactPhraseScorer extends PhraseScorer { ExactPhraseScorer ( Weight weight , TermPositions [ ] tps , int [ ] offsets , Similarity similarity , byte [ ] norms ) { super ( weight , tps , offsets , similarity , norms ) ; } protected final float phraseFreq ( ) throws IOException { pq . clear ( ) ; for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) { pp . firstPosition ( ) ; pq . put ( pp ) ; } pqToList ( ) ; int freq = 0 ; do { while ( first . position < last . position ) { do { if ( ! first . nextPosition ( ) ) return ( float ) freq ; } while ( first . position < last . position ) ; firstToLast ( ) ; } freq ++ ; } while ( last . nextPosition ( ) ) ; return ( float ) freq ; } } 	0	['2', '3', '0', '8', '9', '1', '2', '6', '0', '2', '64', '0', '0', '0.952380952', '0.583333333', '1', '1', '31', '1', '0.5', '0']
package org . apache . lucene . search ; import java . io . Serializable ; import java . util . Locale ; public class SortField implements Serializable { public static final int SCORE = 0 ; public static final int DOC = 1 ; public static final int AUTO = 2 ; public static final int STRING = 3 ; public static final int INT = 4 ; public static final int FLOAT = 5 ; public static final int CUSTOM = 9 ; public static final SortField FIELD_SCORE = new SortField ( null , SCORE ) ; public static final SortField FIELD_DOC = new SortField ( null , DOC ) ; private String field ; private int type = AUTO ; private Locale locale ; boolean reverse = false ; private SortComparatorSource factory ; public SortField ( String field ) { this . field = field . intern ( ) ; } public SortField ( String field , boolean reverse ) { this . field = field . intern ( ) ; this . reverse = reverse ; } public SortField ( String field , int type ) { this . field = ( field != null ) ? field . intern ( ) : field ; this . type = type ; } public SortField ( String field , int type , boolean reverse ) { this . field = ( field != null ) ? field . intern ( ) : field ; this . type = type ; this . reverse = reverse ; } public SortField ( String field , Locale locale ) { this . field = field . intern ( ) ; this . type = STRING ; this . locale = locale ; } public SortField ( String field , Locale locale , boolean reverse ) { this . field = field . intern ( ) ; this . type = STRING ; this . locale = locale ; this . reverse = reverse ; } public SortField ( String field , SortComparatorSource comparator ) { this . field = ( field != null ) ? field . intern ( ) : field ; this . type = CUSTOM ; this . factory = comparator ; } public SortField ( String field , SortComparatorSource comparator , boolean reverse ) { this . field = ( field != null ) ? field . intern ( ) : field ; this . type = CUSTOM ; this . reverse = reverse ; this . factory = comparator ; } public String getField ( ) { return field ; } public int getType ( ) { return type ; } public Locale getLocale ( ) { return locale ; } public boolean getReverse ( ) { return reverse ; } public SortComparatorSource getFactory ( ) { return factory ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; switch ( type ) { case SCORE : buffer . append ( "<score>" ) ; break ; case DOC : buffer . append ( "<doc>" ) ; break ; case CUSTOM : buffer . append ( "<custom:\"" + field + "\": " + factory + ">" ) ; break ; default : buffer . append ( "\"" + field + "\"" ) ; break ; } if ( locale != null ) buffer . append ( "(" + locale + ")" ) ; if ( reverse ) buffer . append ( '!' ) ; return buffer . toString ( ) ; } } 	1	['15', '1', '0', '9', '22', '0', '8', '1', '14', '0.852040816', '297', '0.285714286', '3', '0', '0.380952381', '0', '0', '17.86666667', '7', '0.8', '3']
package org . apache . lucene . search ; import java . io . IOException ; public class ReqExclScorer extends Scorer { private Scorer reqScorer , exclScorer ; public ReqExclScorer ( Scorer reqScorer , Scorer exclScorer ) { super ( null ) ; this . reqScorer = reqScorer ; this . exclScorer = exclScorer ; } private boolean firstTime = true ; public boolean next ( ) throws IOException { if ( firstTime ) { if ( ! exclScorer . next ( ) ) { exclScorer = null ; } firstTime = false ; } if ( reqScorer == null ) { return false ; } if ( ! reqScorer . next ( ) ) { reqScorer = null ; return false ; } if ( exclScorer == null ) { return true ; } return toNonExcluded ( ) ; } private boolean toNonExcluded ( ) throws IOException { int exclDoc = exclScorer . doc ( ) ; do { int reqDoc = reqScorer . doc ( ) ; if ( reqDoc < exclDoc ) { return true ; } else if ( reqDoc > exclDoc ) { if ( ! exclScorer . skipTo ( reqDoc ) ) { exclScorer = null ; return true ; } exclDoc = exclScorer . doc ( ) ; if ( exclDoc > reqDoc ) { return true ; } } } while ( reqScorer . next ( ) ) ; reqScorer = null ; return false ; } public int doc ( ) { return reqScorer . doc ( ) ; } public float score ( ) throws IOException { return reqScorer . score ( ) ; } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { firstTime = false ; if ( ! exclScorer . skipTo ( target ) ) { exclScorer = null ; } } if ( reqScorer == null ) { return false ; } if ( exclScorer == null ) { return reqScorer . skipTo ( target ) ; } if ( ! reqScorer . skipTo ( target ) ) { reqScorer = null ; return false ; } return toNonExcluded ( ) ; } public Explanation explain ( int doc ) throws IOException { Explanation res = new Explanation ( ) ; if ( exclScorer . skipTo ( doc ) && ( exclScorer . doc ( ) == doc ) ) { res . setDescription ( "excluded" ) ; } else { res . setDescription ( "not excluded" ) ; res . addDetail ( reqScorer . explain ( doc ) ) ; } return res ; } } 	0	['7', '2', '0', '4', '16', '0', '1', '3', '6', '0.333333333', '179', '1', '2', '0.571428571', '0.476190476', '1', '3', '24.14285714', '1', '0.8571', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . IndexWriter ; import org . apache . lucene . index . Term ; import org . apache . lucene . util . SmallFloat ; import java . io . IOException ; import java . io . Serializable ; import java . util . Collection ; import java . util . Iterator ; public abstract class Similarity implements Serializable { private static Similarity defaultImpl = new DefaultSimilarity ( ) ; public static void setDefault ( Similarity similarity ) { Similarity . defaultImpl = similarity ; } public static Similarity getDefault ( ) { return Similarity . defaultImpl ; } private static final float [ ] NORM_TABLE = new float [ 256 ] ; static { for ( int i = 0 ; i < 256 ; i ++ ) NORM_TABLE [ i ] = SmallFloat . byte315ToFloat ( ( byte ) i ) ; } public static float decodeNorm ( byte b ) { return NORM_TABLE [ b & 0xFF ] ; } public static float [ ] getNormDecoder ( ) { return NORM_TABLE ; } public abstract float lengthNorm ( String fieldName , int numTokens ) ; public abstract float queryNorm ( float sumOfSquaredWeights ) ; public static byte encodeNorm ( float f ) { return SmallFloat . floatToByte315 ( f ) ; } public float tf ( int freq ) { return tf ( ( float ) freq ) ; } public abstract float sloppyFreq ( int distance ) ; public abstract float tf ( float freq ) ; public float idf ( Term term , Searcher searcher ) throws IOException { return idf ( searcher . docFreq ( term ) , searcher . maxDoc ( ) ) ; } public float idf ( Collection terms , Searcher searcher ) throws IOException { float idf = 0.0f ; Iterator i = terms . iterator ( ) ; while ( i . hasNext ( ) ) { idf += idf ( ( Term ) i . next ( ) , searcher ) ; } return idf ; } public abstract float idf ( int docFreq , int numDocs ) ; public abstract float coord ( int overlap , int maxOverlap ) ; public float scorePayload ( byte [ ] payload , int offset , int length ) { return 1 ; } } 	1	['17', '1', '2', '49', '26', '124', '47', '4', '16', '0.875', '94', '1', '1', '0', '0.175', '0', '0', '4.411764706', '1', '0.8824', '3']
package org . apache . lucene . analysis . standard ; public class ParseException extends java . io . IOException { public ParseException ( Token currentTokenVal , int [ ] [ ] expectedTokenSequencesVal , String [ ] tokenImageVal ) { super ( "" ) ; specialConstructor = true ; currentToken = currentTokenVal ; expectedTokenSequences = expectedTokenSequencesVal ; tokenImage = tokenImageVal ; } public ParseException ( ) { super ( ) ; specialConstructor = false ; } public ParseException ( String message ) { super ( message ) ; specialConstructor = false ; } protected boolean specialConstructor ; public Token currentToken ; public int [ ] [ ] expectedTokenSequences ; public String [ ] tokenImage ; public String getMessage ( ) { if ( ! specialConstructor ) { return super . getMessage ( ) ; } String expected = "" ; int maxSize = 0 ; for ( int i = 0 ; i < expectedTokenSequences . length ; i ++ ) { if ( maxSize < expectedTokenSequences [ i ] . length ) { maxSize = expectedTokenSequences [ i ] . length ; } for ( int j = 0 ; j < expectedTokenSequences [ i ] . length ; j ++ ) { expected += tokenImage [ expectedTokenSequences [ i ] [ j ] ] + " " ; } if ( expectedTokenSequences [ i ] [ expectedTokenSequences [ i ] . length - 1 ] != 0 ) { expected += "..." ; } expected += eol + "    " ; } String retval = "Encountered \"" ; Token tok = currentToken . next ; for ( int i = 0 ; i < maxSize ; i ++ ) { if ( i != 0 ) retval += " " ; if ( tok . kind == 0 ) { retval += tokenImage [ 0 ] ; break ; } retval += add_escapes ( tok . image ) ; tok = tok . next ; } retval += "\" at line " + currentToken . next . beginLine + ", column " + currentToken . next . beginColumn + "." + eol ; if ( expectedTokenSequences . length == 1 ) { retval += "Was expecting:" + eol + "    " ; } else { retval += "Was expecting one of:" + eol + "    " ; } retval += expected ; return retval ; } protected String eol = System . getProperty ( "line.separator" , "\n" ) ; protected String add_escapes ( String str ) { StringBuffer retval = new StringBuffer ( ) ; char ch ; for ( int i = 0 ; i < str . length ( ) ; i ++ ) { switch ( str . charAt ( i ) ) { case 0 : continue ; case '\b' : retval . append ( "\\b" ) ; continue ; case '\t' : retval . append ( "\\t" ) ; continue ; case '\n' : retval . append ( "\\n" ) ; continue ; case '\f' : retval . append ( "\\f" ) ; continue ; case '\r' : retval . append ( "\\r" ) ; continue ; case '\"' : retval . append ( "\\\"" ) ; continue ; case '\'' : retval . append ( "\\\'" ) ; continue ; case '\\' : retval . append ( "\\\\" ) ; continue ; default : if ( ( ch = str . charAt ( i ) ) < 0x20 || ch > 0x7e ) { String s = "0000" + Integer . toString ( ch , 16 ) ; retval . append ( "\\u" + s . substring ( s . length ( ) - 4 , s . length ( ) ) ) ; } else { retval . append ( ch ) ; } continue ; } } return retval . toString ( ) ; } } 	0	['5', '4', '0', '2', '18', '0', '1', '1', '4', '0.55', '380', '0.4', '1', '0.866666667', '0.4', '1', '1', '74', '14', '4.8', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class Lock { public static long LOCK_POLL_INTERVAL = 1000 ; public abstract boolean obtain ( ) throws IOException ; protected Throwable failureReason ; public boolean obtain ( long lockWaitTimeout ) throws LockObtainFailedException , IOException { failureReason = null ; boolean locked = obtain ( ) ; int maxSleepCount = ( int ) ( lockWaitTimeout / LOCK_POLL_INTERVAL ) ; int sleepCount = 0 ; while ( ! locked ) { if ( sleepCount ++ == maxSleepCount ) { String reason = "Lock obtain timed out: " + this . toString ( ) ; if ( failureReason != null ) { reason += ": " + failureReason ; } LockObtainFailedException e = new LockObtainFailedException ( reason ) ; if ( failureReason != null ) { e . initCause ( failureReason ) ; } throw e ; } try { Thread . sleep ( LOCK_POLL_INTERVAL ) ; } catch ( InterruptedException e ) { throw new IOException ( e . toString ( ) ) ; } locked = obtain ( ) ; } return locked ; } public abstract void release ( ) ; public abstract boolean isLocked ( ) ; public abstract static class With { private Lock lock ; private long lockWaitTimeout ; public With ( Lock lock , long lockWaitTimeout ) { this . lock = lock ; this . lockWaitTimeout = lockWaitTimeout ; } protected abstract Object doBody ( ) throws IOException ; public Object run ( ) throws LockObtainFailedException , IOException { boolean locked = false ; try { locked = lock . obtain ( lockWaitTimeout ) ; return doBody ( ) ; } finally { if ( locked ) lock . release ( ) ; } } } } 	1	['6', '1', '4', '15', '17', '13', '14', '1', '5', '0.9', '89', '0.5', '0', '0', '0.6', '0', '0', '13.5', '1', '0.6667', '2']
package org . apache . lucene . index ; class SegmentTermPositionVector extends SegmentTermVector implements TermPositionVector { protected int [ ] [ ] positions ; protected TermVectorOffsetInfo [ ] [ ] offsets ; public static final int [ ] EMPTY_TERM_POS = new int [ 0 ] ; public SegmentTermPositionVector ( String field , String terms [ ] , int termFreqs [ ] , int [ ] [ ] positions , TermVectorOffsetInfo [ ] [ ] offsets ) { super ( field , terms , termFreqs ) ; this . offsets = offsets ; this . positions = positions ; } public TermVectorOffsetInfo [ ] getOffsets ( int index ) { TermVectorOffsetInfo [ ] result = TermVectorOffsetInfo . EMPTY_OFFSET_INFO ; if ( offsets == null ) return null ; if ( index >= 0 && index < offsets . length ) { result = offsets [ index ] ; } return result ; } public int [ ] getTermPositions ( int index ) { int [ ] result = EMPTY_TERM_POS ; if ( positions == null ) return null ; if ( index >= 0 && index < positions . length ) { result = positions [ index ] ; } return result ; } } 	0	['4', '2', '0', '4', '5', '0', '1', '3', '3', '0.666666667', '65', '0.666666667', '1', '0.777777778', '0.476190476', '0', '0', '14.5', '4', '2', '0']
package org . apache . lucene . search ; public class TopDocs implements java . io . Serializable { public int totalHits ; public ScoreDoc [ ] scoreDocs ; private float maxScore ; public float getMaxScore ( ) { return maxScore ; } public void setMaxScore ( float maxScore ) { this . maxScore = maxScore ; } TopDocs ( int totalHits , ScoreDoc [ ] scoreDocs , float maxScore ) { this . totalHits = totalHits ; this . scoreDocs = scoreDocs ; this . maxScore = maxScore ; } } 	1	['3', '1', '1', '14', '4', '0', '13', '1', '2', '0.666666667', '25', '0.333333333', '1', '0', '0.583333333', '0', '0', '6.333333333', '1', '0.6667', '1']
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; final class HitQueue extends PriorityQueue { HitQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object a , Object b ) { ScoreDoc hitA = ( ScoreDoc ) a ; ScoreDoc hitB = ( ScoreDoc ) b ; if ( hitA . score == hitB . score ) return hitA . doc > hitB . doc ; else return hitA . score < hitB . score ; } } 	0	['2', '2', '0', '6', '4', '1', '4', '2', '0', '2', '39', '0', '0', '0.916666667', '0.666666667', '1', '3', '18.5', '4', '2', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; public class TopFieldDocCollector extends TopDocCollector { public TopFieldDocCollector ( IndexReader reader , Sort sort , int numHits ) throws IOException { super ( numHits , new FieldSortedHitQueue ( reader , sort . fields , numHits ) ) ; } public void collect ( int doc , float score ) { if ( score > 0.0f ) { totalHits ++ ; hq . insert ( new FieldDoc ( doc , score ) ) ; } } public TopDocs topDocs ( ) { FieldSortedHitQueue fshq = ( FieldSortedHitQueue ) hq ; ScoreDoc [ ] scoreDocs = new ScoreDoc [ fshq . size ( ) ] ; for ( int i = fshq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = fshq . fillFields ( ( FieldDoc ) fshq . pop ( ) ) ; return new TopFieldDocs ( totalHits , scoreDocs , fshq . getFields ( ) , fshq . getMaxScore ( ) ) ; } } 	1	['3', '3', '0', '11', '13', '1', '1', '10', '3', '2', '70', '0', '0', '0.666666667', '0.533333333', '1', '3', '22.33333333', '2', '1.3333', '2']
package org . apache . lucene . search ; import java . io . IOException ; class NonMatchingScorer extends Scorer { public NonMatchingScorer ( ) { super ( null ) ; } public int doc ( ) { throw new UnsupportedOperationException ( ) ; } public boolean next ( ) throws IOException { return false ; } public float score ( ) { throw new UnsupportedOperationException ( ) ; } public boolean skipTo ( int target ) { return false ; } public Explanation explain ( int doc ) { Explanation e = new Explanation ( ) ; e . setDescription ( "No document matches." ) ; return e ; } } 	0	['6', '2', '0', '4', '10', '15', '1', '3', '6', '2', '31', '0', '0', '0.615384615', '0.666666667', '1', '3', '4.166666667', '1', '0.8333', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; import java . io . IOException ; public abstract class Tokenizer extends TokenStream { protected Reader input ; protected Tokenizer ( ) { } protected Tokenizer ( Reader input ) { this . input = input ; } public void close ( ) throws IOException { input . close ( ) ; } } 	1	['3', '2', '3', '4', '5', '1', '3', '1', '1', '0.5', '17', '1', '0', '0.75', '0.666666667', '0', '0', '4.333333333', '1', '0.3333', '3']
package org . apache . lucene . index ; import java . io . IOException ; public class StaleReaderException extends IOException { public StaleReaderException ( String message ) { super ( message ) ; } } 	0	['1', '4', '0', '2', '2', '0', '2', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . search ; import java . util . ArrayList ; public class ComplexExplanation extends Explanation { private Boolean match ; public ComplexExplanation ( ) { super ( ) ; } public ComplexExplanation ( boolean match , float value , String description ) { super ( value , description ) ; this . match = Boolean . valueOf ( match ) ; } public Boolean getMatch ( ) { return match ; } public void setMatch ( Boolean match ) { this . match = match ; } public boolean isMatch ( ) { Boolean m = getMatch ( ) ; return ( null != m ? m . booleanValue ( ) : super . isMatch ( ) ) ; } protected String getSummary ( ) { if ( null == getMatch ( ) ) return super . getSummary ( ) ; return getValue ( ) + " = " + ( isMatch ( ) ? "(MATCH) " : "(NON-MATCH) " ) + getDescription ( ) ; } } 	1	['6', '2', '0', '10', '18', '9', '9', '1', '5', '0.2', '65', '1', '0', '0.733333333', '0.333333333', '1', '1', '9.666666667', '3', '1.1667', '1']
package org . apache . lucene . search . spans ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . search . * ; import java . io . IOException ; import java . util . HashSet ; import java . util . Iterator ; import java . util . Set ; public class SpanWeight implements Weight { protected Similarity similarity ; protected float value ; protected float idf ; protected float queryNorm ; protected float queryWeight ; protected Set terms ; protected SpanQuery query ; public SpanWeight ( SpanQuery query , Searcher searcher ) throws IOException { this . similarity = query . getSimilarity ( searcher ) ; this . query = query ; terms = new HashSet ( ) ; query . extractTerms ( terms ) ; idf = this . query . getSimilarity ( searcher ) . idf ( terms , searcher ) ; } public Query getQuery ( ) { return query ; } public float getValue ( ) { return value ; } public float sumOfSquaredWeights ( ) throws IOException { queryWeight = idf * query . getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float queryNorm ) { this . queryNorm = queryNorm ; queryWeight *= queryNorm ; value = queryWeight * idf ; } public Scorer scorer ( IndexReader reader ) throws IOException { return new SpanScorer ( query . getSpans ( reader ) , this , similarity , reader . norms ( query . getField ( ) ) ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { ComplexExplanation result = new ComplexExplanation ( ) ; result . setDescription ( "weight(" + getQuery ( ) + " in " + doc + "), product of:" ) ; String field = ( ( SpanQuery ) getQuery ( ) ) . getField ( ) ; StringBuffer docFreqs = new StringBuffer ( ) ; Iterator i = terms . iterator ( ) ; while ( i . hasNext ( ) ) { Term term = ( Term ) i . next ( ) ; docFreqs . append ( term . text ( ) ) ; docFreqs . append ( "=" ) ; docFreqs . append ( reader . docFreq ( term ) ) ; if ( i . hasNext ( ) ) { docFreqs . append ( " " ) ; } } Explanation idfExpl = new Explanation ( idf , "idf(" + field + ": " + docFreqs + ")" ) ; Explanation queryExpl = new Explanation ( ) ; queryExpl . setDescription ( "queryWeight(" + getQuery ( ) + "), product of:" ) ; Explanation boostExpl = new Explanation ( getQuery ( ) . getBoost ( ) , "boost" ) ; if ( getQuery ( ) . getBoost ( ) != 1.0f ) queryExpl . addDetail ( boostExpl ) ; queryExpl . addDetail ( idfExpl ) ; Explanation queryNormExpl = new Explanation ( queryNorm , "queryNorm" ) ; queryExpl . addDetail ( queryNormExpl ) ; queryExpl . setValue ( boostExpl . getValue ( ) * idfExpl . getValue ( ) * queryNormExpl . getValue ( ) ) ; result . addDetail ( queryExpl ) ; ComplexExplanation fieldExpl = new ComplexExplanation ( ) ; fieldExpl . setDescription ( "fieldWeight(" + field + ":" + query . toString ( field ) + " in " + doc + "), product of:" ) ; Explanation tfExpl = scorer ( reader ) . explain ( doc ) ; fieldExpl . addDetail ( tfExpl ) ; fieldExpl . addDetail ( idfExpl ) ; Explanation fieldNormExpl = new Explanation ( ) ; byte [ ] fieldNorms = reader . norms ( field ) ; float fieldNorm = fieldNorms != null ? Similarity . decodeNorm ( fieldNorms [ doc ] ) : 0.0f ; fieldNormExpl . setValue ( fieldNorm ) ; fieldNormExpl . setDescription ( "fieldNorm(field=" + field + ", doc=" + doc + ")" ) ; fieldExpl . addDetail ( fieldNormExpl ) ; fieldExpl . setMatch ( Boolean . valueOf ( tfExpl . isMatch ( ) ) ) ; fieldExpl . setValue ( tfExpl . getValue ( ) * idfExpl . getValue ( ) * fieldNormExpl . getValue ( ) ) ; result . addDetail ( fieldExpl ) ; result . setMatch ( fieldExpl . getMatch ( ) ) ; result . setValue ( queryExpl . getValue ( ) * fieldExpl . getValue ( ) ) ; if ( queryExpl . getValue ( ) == 1.0f ) return fieldExpl ; return result ; } } 	0	['7', '1', '1', '13', '46', '0', '2', '12', '7', '0.69047619', '357', '1', '2', '0', '0.30952381', '0', '0', '49', '1', '0.8571', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . document . Document ; import org . apache . lucene . index . CorruptIndexException ; public class Hit implements java . io . Serializable { private Document doc = null ; private boolean resolved = false ; private Hits hits = null ; private int hitNumber ; Hit ( Hits hits , int hitNumber ) { this . hits = hits ; this . hitNumber = hitNumber ; } public Document getDocument ( ) throws CorruptIndexException , IOException { if ( ! resolved ) fetchTheHit ( ) ; return doc ; } public float getScore ( ) throws IOException { return hits . score ( hitNumber ) ; } public int getId ( ) throws IOException { return hits . id ( hitNumber ) ; } private void fetchTheHit ( ) throws CorruptIndexException , IOException { doc = hits . doc ( hitNumber ) ; resolved = true ; } public float getBoost ( ) throws CorruptIndexException , IOException { return getDocument ( ) . getBoost ( ) ; } public String get ( String name ) throws CorruptIndexException , IOException { return getDocument ( ) . get ( name ) ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "Hit<" ) ; buffer . append ( hits . toString ( ) ) ; buffer . append ( " [" ) ; buffer . append ( hitNumber ) ; buffer . append ( "] " ) ; if ( resolved ) { buffer . append ( "resolved" ) ; } else { buffer . append ( "unresolved" ) ; } buffer . append ( ">" ) ; return buffer . toString ( ) ; } } 	1	['8', '1', '0', '4', '19', '2', '1', '3', '6', '0.178571429', '116', '1', '2', '0', '0.34375', '0', '0', '13', '2', '1', '2']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import org . apache . lucene . store . IndexOutput ; class DefaultSkipListWriter extends MultiLevelSkipListWriter { private int [ ] lastSkipDoc ; private int [ ] lastSkipPayloadLength ; private long [ ] lastSkipFreqPointer ; private long [ ] lastSkipProxPointer ; private IndexOutput freqOutput ; private IndexOutput proxOutput ; private int curDoc ; private boolean curStorePayloads ; private int curPayloadLength ; private long curFreqPointer ; private long curProxPointer ; DefaultSkipListWriter ( int skipInterval , int numberOfSkipLevels , int docCount , IndexOutput freqOutput , IndexOutput proxOutput ) { super ( skipInterval , numberOfSkipLevels , docCount ) ; this . freqOutput = freqOutput ; this . proxOutput = proxOutput ; lastSkipDoc = new int [ numberOfSkipLevels ] ; lastSkipPayloadLength = new int [ numberOfSkipLevels ] ; lastSkipFreqPointer = new long [ numberOfSkipLevels ] ; lastSkipProxPointer = new long [ numberOfSkipLevels ] ; } void setSkipData ( int doc , boolean storePayloads , int payloadLength ) { this . curDoc = doc ; this . curStorePayloads = storePayloads ; this . curPayloadLength = payloadLength ; this . curFreqPointer = freqOutput . getFilePointer ( ) ; this . curProxPointer = proxOutput . getFilePointer ( ) ; } protected void resetSkip ( ) { super . resetSkip ( ) ; Arrays . fill ( lastSkipDoc , 0 ) ; Arrays . fill ( lastSkipPayloadLength , - 1 ) ; Arrays . fill ( lastSkipFreqPointer , freqOutput . getFilePointer ( ) ) ; Arrays . fill ( lastSkipProxPointer , proxOutput . getFilePointer ( ) ) ; } protected void writeSkipData ( int level , IndexOutput skipBuffer ) throws IOException { if ( curStorePayloads ) { int delta = curDoc - lastSkipDoc [ level ] ; if ( curPayloadLength == lastSkipPayloadLength [ level ] ) { skipBuffer . writeVInt ( delta * 2 ) ; } else { skipBuffer . writeVInt ( delta * 2 + 1 ) ; skipBuffer . writeVInt ( curPayloadLength ) ; lastSkipPayloadLength [ level ] = curPayloadLength ; } } else { skipBuffer . writeVInt ( curDoc - lastSkipDoc [ level ] ) ; } skipBuffer . writeVInt ( ( int ) ( curFreqPointer - lastSkipFreqPointer [ level ] ) ) ; skipBuffer . writeVInt ( ( int ) ( curProxPointer - lastSkipProxPointer [ level ] ) ) ; lastSkipDoc [ level ] = curDoc ; lastSkipFreqPointer [ level ] = curFreqPointer ; lastSkipProxPointer [ level ] = curProxPointer ; } } 	0	['4', '2', '0', '3', '10', '0', '1', '2', '0', '0.484848485', '176', '1', '2', '0.625', '0.625', '1', '1', '40.25', '1', '0.75', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . List ; import java . util . ArrayList ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . PriorityQueue ; class NearSpansUnordered implements Spans { private SpanNearQuery query ; private List ordered = new ArrayList ( ) ; private int slop ; private SpansCell first ; private SpansCell last ; private int totalLength ; private CellQueue queue ; private SpansCell max ; private boolean more = true ; private boolean firstTime = true ; private class CellQueue extends PriorityQueue { public CellQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object o1 , Object o2 ) { SpansCell spans1 = ( SpansCell ) o1 ; SpansCell spans2 = ( SpansCell ) o2 ; if ( spans1 . doc ( ) == spans2 . doc ( ) ) { return NearSpansOrdered . docSpansOrdered ( spans1 , spans2 ) ; } else { return spans1 . doc ( ) < spans2 . doc ( ) ; } } } private class SpansCell implements Spans { private Spans spans ; private SpansCell next ; private int length = - 1 ; private int index ; public SpansCell ( Spans spans , int index ) { this . spans = spans ; this . index = index ; } public boolean next ( ) throws IOException { return adjust ( spans . next ( ) ) ; } public boolean skipTo ( int target ) throws IOException { return adjust ( spans . skipTo ( target ) ) ; } private boolean adjust ( boolean condition ) { if ( length != - 1 ) { totalLength -= length ; } if ( condition ) { length = end ( ) - start ( ) ; totalLength += length ; if ( max == null || doc ( ) > max . doc ( ) || ( doc ( ) == max . doc ( ) ) && ( end ( ) > max . end ( ) ) ) { max = this ; } } more = condition ; return condition ; } public int doc ( ) { return spans . doc ( ) ; } public int start ( ) { return spans . start ( ) ; } public int end ( ) { return spans . end ( ) ; } public String toString ( ) { return spans . toString ( ) + "#" + index ; } } public NearSpansUnordered ( SpanNearQuery query , IndexReader reader ) throws IOException { this . query = query ; this . slop = query . getSlop ( ) ; SpanQuery [ ] clauses = query . getClauses ( ) ; queue = new CellQueue ( clauses . length ) ; for ( int i = 0 ; i < clauses . length ; i ++ ) { SpansCell cell = new SpansCell ( clauses [ i ] . getSpans ( reader ) , i ) ; ordered . add ( cell ) ; } } public boolean next ( ) throws IOException { if ( firstTime ) { initList ( true ) ; listToQueue ( ) ; firstTime = false ; } else if ( more ) { if ( min ( ) . next ( ) ) { queue . adjustTop ( ) ; } else { more = false ; } } while ( more ) { boolean queueStale = false ; if ( min ( ) . doc ( ) != max . doc ( ) ) { queueToList ( ) ; queueStale = true ; } while ( more && first . doc ( ) < last . doc ( ) ) { more = first . skipTo ( last . doc ( ) ) ; firstToLast ( ) ; queueStale = true ; } if ( ! more ) return false ; if ( queueStale ) { listToQueue ( ) ; queueStale = false ; } if ( atMatch ( ) ) { return true ; } more = min ( ) . next ( ) ; if ( more ) { queue . adjustTop ( ) ; } } return false ; } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { initList ( false ) ; for ( SpansCell cell = first ; more && cell != null ; cell = cell . next ) { more = cell . skipTo ( target ) ; } if ( more ) { listToQueue ( ) ; } firstTime = false ; } else { while ( more && min ( ) . doc ( ) < target ) { if ( min ( ) . skipTo ( target ) ) { queue . adjustTop ( ) ; } else { more = false ; } } } return more && ( atMatch ( ) || next ( ) ) ; } private SpansCell min ( ) { return ( SpansCell ) queue . top ( ) ; } public int doc ( ) { return min ( ) . doc ( ) ; } public int start ( ) { return min ( ) . start ( ) ; } public int end ( ) { return max . end ( ) ; } public String toString ( ) { return getClass ( ) . getName ( ) + "(" + query . toString ( ) + ")@" + ( firstTime ? "START" : ( more ? ( doc ( ) + ":" + start ( ) + "-" + end ( ) ) : "END" ) ) ; } private void initList ( boolean next ) throws IOException { for ( int i = 0 ; more && i < ordered . size ( ) ; i ++ ) { SpansCell cell = ( SpansCell ) ordered . get ( i ) ; if ( next ) more = cell . next ( ) ; if ( more ) { addToList ( cell ) ; } } } private void addToList ( SpansCell cell ) { if ( last != null ) { last . next = cell ; } else first = cell ; last = cell ; cell . next = null ; } private void firstToLast ( ) { last . next = first ; last = first ; first = first . next ; last . next = null ; } private void queueToList ( ) { last = first = null ; while ( queue . top ( ) != null ) { addToList ( ( SpansCell ) queue . pop ( ) ) ; } } private void listToQueue ( ) { queue . clear ( ) ; for ( SpansCell cell = first ; cell != null ; cell = cell . next ) { queue . put ( cell ) ; } } private boolean atMatch ( ) { return ( min ( ) . doc ( ) == max . doc ( ) ) && ( ( max . end ( ) - min ( ) . start ( ) - totalLength ) <= slop ) ; } } 	1	['19', '1', '0', '6', '48', '71', '3', '6', '7', '0.75', '468', '1', '5', '0', '0.210526316', '0', '0', '23.10526316', '3', '1.3158', '1']
package org . apache . lucene . analysis . standard ; public interface StandardTokenizerConstants { int EOF = 0 ; int ALPHANUM = 1 ; int APOSTROPHE = 2 ; int ACRONYM = 3 ; int COMPANY = 4 ; int EMAIL = 5 ; int HOST = 6 ; int NUM = 7 ; int P = 8 ; int HAS_DIGIT = 9 ; int ALPHA = 10 ; int LETTER = 11 ; int CJ = 12 ; int KOREAN = 13 ; int DIGIT = 14 ; int NOISE = 15 ; int DEFAULT = 0 ; String [ ] tokenImage = { "<EOF>" , "<ALPHANUM>" , "<APOSTROPHE>" , "<ACRONYM>" , "<COMPANY>" , "<EMAIL>" , "<HOST>" , "<NUM>" , "<P>" , "<HAS_DIGIT>" , "<ALPHA>" , "<LETTER>" , "<CJ>" , "<KOREAN>" , "<DIGIT>" , "<NOISE>" , } ; } 	0	['1', '1', '0', '3', '1', '0', '3', '0', '0', '2', '87', '0', '0', '0', '0', '0', '0', '68', '0', '0', '0']
package org . apache . lucene . document ; import java . io . Reader ; import java . io . Serializable ; import org . apache . lucene . analysis . TokenStream ; public interface Fieldable extends Serializable { void setBoost ( float boost ) ; float getBoost ( ) ; String name ( ) ; public String stringValue ( ) ; public Reader readerValue ( ) ; public byte [ ] binaryValue ( ) ; public TokenStream tokenStreamValue ( ) ; boolean isStored ( ) ; boolean isIndexed ( ) ; boolean isTokenized ( ) ; boolean isCompressed ( ) ; boolean isTermVectorStored ( ) ; boolean isStoreOffsetWithTermVector ( ) ; boolean isStorePositionWithTermVector ( ) ; boolean isBinary ( ) ; boolean getOmitNorms ( ) ; void setOmitNorms ( boolean omitNorms ) ; boolean isLazy ( ) ; } 	1	['18', '1', '0', '10', '18', '153', '9', '1', '18', '2', '18', '0', '0', '0', '0.37037037', '0', '0', '0', '1', '1', '2']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; public abstract class SortComparator implements SortComparatorSource { public ScoreDocComparator newComparator ( final IndexReader reader , final String fieldname ) throws IOException { final String field = fieldname . intern ( ) ; final Comparable [ ] cachedValues = FieldCache . DEFAULT . getCustom ( reader , field , SortComparator . this ) ; return new ScoreDocComparator ( ) { public int compare ( ScoreDoc i , ScoreDoc j ) { return cachedValues [ i . doc ] . compareTo ( cachedValues [ j . doc ] ) ; } public Comparable sortValue ( ScoreDoc i ) { return cachedValues [ i . doc ] ; } public int sortType ( ) { return SortField . CUSTOM ; } } ; } protected abstract Comparable getComparable ( String termtext ) ; } 	0	['3', '1', '0', '7', '7', '3', '4', '5', '2', '2', '21', '0', '0', '0', '0.666666667', '0', '0', '6', '1', '0.6667', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . Vector ; import java . util . Iterator ; import org . apache . lucene . document . Document ; import org . apache . lucene . index . CorruptIndexException ; public final class Hits { private Weight weight ; private Searcher searcher ; private Filter filter = null ; private Sort sort = null ; private int length ; private Vector hitDocs = new Vector ( ) ; private HitDoc first ; private HitDoc last ; private int numDocs = 0 ; private int maxDocs = 200 ; Hits ( Searcher s , Query q , Filter f ) throws IOException { weight = q . weight ( s ) ; searcher = s ; filter = f ; getMoreDocs ( 50 ) ; } Hits ( Searcher s , Query q , Filter f , Sort o ) throws IOException { weight = q . weight ( s ) ; searcher = s ; filter = f ; sort = o ; getMoreDocs ( 50 ) ; } private final void getMoreDocs ( int min ) throws IOException { if ( hitDocs . size ( ) > min ) { min = hitDocs . size ( ) ; } int n = min * 2 ; TopDocs topDocs = ( sort == null ) ? searcher . search ( weight , filter , n ) : searcher . search ( weight , filter , n , sort ) ; length = topDocs . totalHits ; ScoreDoc [ ] scoreDocs = topDocs . scoreDocs ; float scoreNorm = 1.0f ; if ( length > 0 && topDocs . getMaxScore ( ) > 1.0f ) { scoreNorm = 1.0f / topDocs . getMaxScore ( ) ; } int end = scoreDocs . length < length ? scoreDocs . length : length ; for ( int i = hitDocs . size ( ) ; i < end ; i ++ ) { hitDocs . addElement ( new HitDoc ( scoreDocs [ i ] . score * scoreNorm , scoreDocs [ i ] . doc ) ) ; } } public final int length ( ) { return length ; } public final Document doc ( int n ) throws CorruptIndexException , IOException { HitDoc hitDoc = hitDoc ( n ) ; remove ( hitDoc ) ; addToFront ( hitDoc ) ; if ( numDocs > maxDocs ) { HitDoc oldLast = last ; remove ( last ) ; oldLast . doc = null ; } if ( hitDoc . doc == null ) { hitDoc . doc = searcher . doc ( hitDoc . id ) ; } return hitDoc . doc ; } public final float score ( int n ) throws IOException { return hitDoc ( n ) . score ; } public final int id ( int n ) throws IOException { return hitDoc ( n ) . id ; } public Iterator iterator ( ) { return new HitIterator ( this ) ; } private final HitDoc hitDoc ( int n ) throws IOException { if ( n >= length ) { throw new IndexOutOfBoundsException ( "Not a valid hit number: " + n ) ; } if ( n >= hitDocs . size ( ) ) { getMoreDocs ( n ) ; } return ( HitDoc ) hitDocs . elementAt ( n ) ; } private final void addToFront ( HitDoc hitDoc ) { if ( first == null ) { last = hitDoc ; } else { first . prev = hitDoc ; } hitDoc . next = first ; first = hitDoc ; hitDoc . prev = null ; numDocs ++ ; } private final void remove ( HitDoc hitDoc ) { if ( hitDoc . doc == null ) { return ; } if ( hitDoc . next == null ) { last = hitDoc . prev ; } else { hitDoc . next . prev = hitDoc . prev ; } if ( hitDoc . prev == null ) { first = hitDoc . next ; } else { hitDoc . prev . next = hitDoc . next ; } numDocs -- ; } } final class HitDoc { float score ; int id ; Document doc = null ; HitDoc next ; HitDoc prev ; HitDoc ( float s , int i ) { score = s ; id = i ; } } 	1	['11', '1', '0', '13', '28', '19', '3', '12', '5', '0.64', '338', '1', '6', '0', '0.324675325', '0', '0', '28.81818182', '4', '1.1818', '2']
package org . apache . lucene . index ; import java . io . IOException ; public abstract class TermEnum { public abstract boolean next ( ) throws IOException ; public abstract Term term ( ) ; public abstract int docFreq ( ) ; public abstract void close ( ) throws IOException ; public boolean skipTo ( Term target ) throws IOException { do { if ( ! next ( ) ) return false ; } while ( target . compareTo ( term ( ) ) > 0 ) ; return true ; } } 	0	['6', '1', '5', '33', '8', '15', '32', '1', '6', '2', '21', '0', '0', '0', '0.583333333', '0', '0', '2.5', '1', '0.8333', '0']
package org . apache . lucene . queryParser ; public class Token { public int kind ; public int beginLine , beginColumn , endLine , endColumn ; public String image ; public Token next ; public Token specialToken ; public String toString ( ) { return image ; } public static final Token newToken ( int ofKind ) { switch ( ofKind ) { default : return new Token ( ) ; } } } 	1	['3', '1', '0', '4', '4', '3', '4', '0', '3', '1.4375', '23', '0', '2', '0', '0.5', '0', '0', '4', '2', '1', '1']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . TermDocs ; final class TermScorer extends Scorer { private Weight weight ; private TermDocs termDocs ; private byte [ ] norms ; private float weightValue ; private int doc ; private final int [ ] docs = new int [ 32 ] ; private final int [ ] freqs = new int [ 32 ] ; private int pointer ; private int pointerMax ; private static final int SCORE_CACHE_SIZE = 32 ; private float [ ] scoreCache = new float [ SCORE_CACHE_SIZE ] ; TermScorer ( Weight weight , TermDocs td , Similarity similarity , byte [ ] norms ) { super ( similarity ) ; this . weight = weight ; this . termDocs = td ; this . norms = norms ; this . weightValue = weight . getValue ( ) ; for ( int i = 0 ; i < SCORE_CACHE_SIZE ; i ++ ) scoreCache [ i ] = getSimilarity ( ) . tf ( i ) * weightValue ; } public void score ( HitCollector hc ) throws IOException { next ( ) ; score ( hc , Integer . MAX_VALUE ) ; } protected boolean score ( HitCollector c , int end ) throws IOException { Similarity similarity = getSimilarity ( ) ; float [ ] normDecoder = Similarity . getNormDecoder ( ) ; while ( doc < end ) { int f = freqs [ pointer ] ; float score = f < SCORE_CACHE_SIZE ? scoreCache [ f ] : similarity . tf ( f ) * weightValue ; score *= normDecoder [ norms [ doc ] & 0xFF ] ; c . collect ( doc , score ) ; if ( ++ pointer >= pointerMax ) { pointerMax = termDocs . read ( docs , freqs ) ; if ( pointerMax != 0 ) { pointer = 0 ; } else { termDocs . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } } doc = docs [ pointer ] ; } return true ; } public int doc ( ) { return doc ; } public boolean next ( ) throws IOException { pointer ++ ; if ( pointer >= pointerMax ) { pointerMax = termDocs . read ( docs , freqs ) ; if ( pointerMax != 0 ) { pointer = 0 ; } else { termDocs . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } } doc = docs [ pointer ] ; return true ; } public float score ( ) { int f = freqs [ pointer ] ; float raw = f < SCORE_CACHE_SIZE ? scoreCache [ f ] : getSimilarity ( ) . tf ( f ) * weightValue ; return raw * Similarity . decodeNorm ( norms [ doc ] ) ; } public boolean skipTo ( int target ) throws IOException { for ( pointer ++ ; pointer < pointerMax ; pointer ++ ) { if ( docs [ pointer ] >= target ) { doc = docs [ pointer ] ; return true ; } } boolean result = termDocs . skipTo ( target ) ; if ( result ) { pointerMax = 1 ; pointer = 0 ; docs [ pointer ] = doc = termDocs . doc ( ) ; freqs [ pointer ] = termDocs . freq ( ) ; } else { doc = Integer . MAX_VALUE ; } return result ; } public Explanation explain ( int doc ) throws IOException { TermQuery query = ( TermQuery ) weight . getQuery ( ) ; Explanation tfExplanation = new Explanation ( ) ; int tf = 0 ; while ( pointer < pointerMax ) { if ( docs [ pointer ] == doc ) tf = freqs [ pointer ] ; pointer ++ ; } if ( tf == 0 ) { if ( termDocs . skipTo ( doc ) ) { if ( termDocs . doc ( ) == doc ) { tf = termDocs . freq ( ) ; } } } termDocs . close ( ) ; tfExplanation . setValue ( getSimilarity ( ) . tf ( tf ) ) ; tfExplanation . setDescription ( "tf(termFreq(" + query . getTerm ( ) + ")=" + tf + ")" ) ; return tfExplanation ; } public String toString ( ) { return "scorer(" + weight + ")" ; } } 	0	['9', '2', '0', '10', '31', '0', '1', '9', '7', '0.545454545', '409', '1', '2', '0.5', '0.285714286', '1', '3', '43.22222222', '2', '1', '0']
package org . apache . lucene . store ; import java . io . IOException ; class RAMInputStream extends IndexInput implements Cloneable { static final int BUFFER_SIZE = RAMOutputStream . BUFFER_SIZE ; private RAMFile file ; private long length ; private byte [ ] currentBuffer ; private int currentBufferIndex ; private int bufferPosition ; private long bufferStart ; private int bufferLength ; public RAMInputStream ( RAMFile f ) { file = f ; length = file . length ; currentBufferIndex = - 1 ; currentBuffer = null ; } public void close ( ) { } public long length ( ) { return length ; } public byte readByte ( ) throws IOException { if ( bufferPosition >= bufferLength ) { currentBufferIndex ++ ; switchCurrentBuffer ( ) ; } return currentBuffer [ bufferPosition ++ ] ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { while ( len > 0 ) { if ( bufferPosition >= bufferLength ) { currentBufferIndex ++ ; switchCurrentBuffer ( ) ; } int remainInBuffer = bufferLength - bufferPosition ; int bytesToCopy = len < remainInBuffer ? len : remainInBuffer ; System . arraycopy ( currentBuffer , bufferPosition , b , offset , bytesToCopy ) ; offset += bytesToCopy ; len -= bytesToCopy ; bufferPosition += bytesToCopy ; } } private final void switchCurrentBuffer ( ) throws IOException { if ( currentBufferIndex >= file . buffers . size ( ) ) { throw new IOException ( "Read past EOF" ) ; } else { currentBuffer = ( byte [ ] ) file . buffers . get ( currentBufferIndex ) ; bufferPosition = 0 ; bufferStart = BUFFER_SIZE * currentBufferIndex ; bufferLength = ( int ) ( length - bufferStart ) ; if ( bufferLength > BUFFER_SIZE ) { bufferLength = BUFFER_SIZE ; } } } public long getFilePointer ( ) { return currentBufferIndex < 0 ? 0 : bufferStart + bufferPosition ; } public void seek ( long pos ) throws IOException { long bufferStart = currentBufferIndex * BUFFER_SIZE ; if ( pos < bufferStart || pos >= bufferStart + BUFFER_SIZE ) { currentBufferIndex = ( int ) ( pos / BUFFER_SIZE ) ; switchCurrentBuffer ( ) ; } bufferPosition = ( int ) ( pos % BUFFER_SIZE ) ; } } 	1	['8', '2', '0', '3', '13', '0', '1', '2', '7', '0.5', '204', '0.875', '1', '0.666666667', '0.3', '1', '4', '23.5', '2', '1', '3']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermEnum ; public abstract class FilteredTermEnum extends TermEnum { private Term currentTerm = null ; private TermEnum actualEnum = null ; public FilteredTermEnum ( ) { } protected abstract boolean termCompare ( Term term ) ; public abstract float difference ( ) ; protected abstract boolean endEnum ( ) ; protected void setEnum ( TermEnum actualEnum ) throws IOException { this . actualEnum = actualEnum ; Term term = actualEnum . term ( ) ; if ( term != null && termCompare ( term ) ) currentTerm = term ; else next ( ) ; } public int docFreq ( ) { if ( actualEnum == null ) return - 1 ; return actualEnum . docFreq ( ) ; } public boolean next ( ) throws IOException { if ( actualEnum == null ) return false ; currentTerm = null ; while ( currentTerm == null ) { if ( endEnum ( ) ) return false ; if ( actualEnum . next ( ) ) { Term term = actualEnum . term ( ) ; if ( termCompare ( term ) ) { currentTerm = term ; return true ; } } else return false ; } currentTerm = null ; return false ; } public Term term ( ) { return currentTerm ; } public void close ( ) throws IOException { actualEnum . close ( ) ; currentTerm = null ; actualEnum = null ; } } 	0	['9', '2', '2', '7', '14', '8', '5', '2', '6', '0.5', '103', '1', '2', '0.384615385', '0.407407407', '1', '2', '10.22222222', '2', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . util . BitSet ; import java . util . WeakHashMap ; import java . util . Map ; import java . io . IOException ; public class CachingWrapperFilter extends Filter { protected Filter filter ; protected transient Map cache ; public CachingWrapperFilter ( Filter filter ) { this . filter = filter ; } public BitSet bits ( IndexReader reader ) throws IOException { if ( cache == null ) { cache = new WeakHashMap ( ) ; } synchronized ( cache ) { BitSet cached = ( BitSet ) cache . get ( reader ) ; if ( cached != null ) { return cached ; } } final BitSet bits = filter . bits ( reader ) ; synchronized ( cache ) { cache . put ( reader , bits ) ; } return bits ; } public String toString ( ) { return "CachingWrapperFilter(" + filter + ")" ; } public boolean equals ( Object o ) { if ( ! ( o instanceof CachingWrapperFilter ) ) return false ; return this . filter . equals ( ( ( CachingWrapperFilter ) o ) . filter ) ; } public int hashCode ( ) { return filter . hashCode ( ) ^ 0x1117BF25 ; } } 	1	['5', '2', '1', '3', '16', '0', '1', '2', '5', '0.5', '102', '1', '1', '0.2', '0.4', '1', '1', '19', '2', '1', '1']
package org . apache . lucene . index ; import java . io . IOException ; public interface TermDocs { void seek ( Term term ) throws IOException ; void seek ( TermEnum termEnum ) throws IOException ; int doc ( ) ; int freq ( ) ; boolean next ( ) throws IOException ; int read ( int [ ] docs , int [ ] freqs ) throws IOException ; boolean skipTo ( int target ) throws IOException ; void close ( ) throws IOException ; } 	0	['8', '1', '0', '27', '8', '28', '25', '2', '8', '2', '8', '0', '0', '0', '0.3', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . store ; import java . util . ArrayList ; import java . io . Serializable ; class RAMFile implements Serializable { private static final long serialVersionUID = 1l ; ArrayList buffers = new ArrayList ( ) ; long length ; RAMDirectory directory ; long sizeInBytes ; private long lastModified = System . currentTimeMillis ( ) ; RAMFile ( ) { } RAMFile ( RAMDirectory directory ) { this . directory = directory ; } synchronized long getLength ( ) { return length ; } synchronized void setLength ( long length ) { this . length = length ; } synchronized long getLastModified ( ) { return lastModified ; } synchronized void setLastModified ( long lastModified ) { this . lastModified = lastModified ; } final byte [ ] addBuffer ( int size ) { byte [ ] buffer = new byte [ size ] ; if ( directory != null ) synchronized ( directory ) { buffers . add ( buffer ) ; directory . sizeInBytes += size ; sizeInBytes += size ; } else buffers . add ( buffer ) ; return buffer ; } long getSizeInBytes ( ) { synchronized ( directory ) { return sizeInBytes ; } } } 	1	['8', '1', '0', '3', '12', '6', '3', '1', '0', '0.80952381', '115', '0.333333333', '1', '0', '0.375', '0', '0', '12.625', '2', '0.875', '2']
package org . apache . lucene . analysis ; import java . io . Reader ; import java . util . Map ; import java . util . HashMap ; public class PerFieldAnalyzerWrapper extends Analyzer { private Analyzer defaultAnalyzer ; private Map analyzerMap = new HashMap ( ) ; public PerFieldAnalyzerWrapper ( Analyzer defaultAnalyzer ) { this . defaultAnalyzer = defaultAnalyzer ; } public void addAnalyzer ( String fieldName , Analyzer analyzer ) { analyzerMap . put ( fieldName , analyzer ) ; } public TokenStream tokenStream ( String fieldName , Reader reader ) { Analyzer analyzer = ( Analyzer ) analyzerMap . get ( fieldName ) ; if ( analyzer == null ) { analyzer = defaultAnalyzer ; } return analyzer . tokenStream ( fieldName , reader ) ; } public int getPositionIncrementGap ( String fieldName ) { Analyzer analyzer = ( Analyzer ) analyzerMap . get ( fieldName ) ; if ( analyzer == null ) analyzer = defaultAnalyzer ; return analyzer . getPositionIncrementGap ( fieldName ) ; } public String toString ( ) { return "PerFieldAnalyzerWrapper(" + analyzerMap + ", default=" + defaultAnalyzer + ")" ; } } 	0	['5', '2', '0', '2', '15', '0', '0', '2', '5', '0.125', '73', '1', '1', '0.333333333', '0.55', '0', '0', '13.2', '2', '1.2', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; import java . util . HashSet ; import java . util . Set ; public final class StopFilter extends TokenFilter { private final Set stopWords ; private final boolean ignoreCase ; public StopFilter ( TokenStream input , String [ ] stopWords ) { this ( input , stopWords , false ) ; } public StopFilter ( TokenStream in , String [ ] stopWords , boolean ignoreCase ) { super ( in ) ; this . ignoreCase = ignoreCase ; this . stopWords = makeStopSet ( stopWords , ignoreCase ) ; } public StopFilter ( TokenStream input , Set stopWords , boolean ignoreCase ) { super ( input ) ; this . ignoreCase = ignoreCase ; this . stopWords = stopWords ; } public StopFilter ( TokenStream in , Set stopWords ) { this ( in , stopWords , false ) ; } public static final Set makeStopSet ( String [ ] stopWords ) { return makeStopSet ( stopWords , false ) ; } public static final Set makeStopSet ( String [ ] stopWords , boolean ignoreCase ) { HashSet stopTable = new HashSet ( stopWords . length ) ; for ( int i = 0 ; i < stopWords . length ; i ++ ) stopTable . add ( ignoreCase ? stopWords [ i ] . toLowerCase ( ) : stopWords [ i ] ) ; return stopTable ; } public final Token next ( ) throws IOException { for ( Token token = input . next ( ) ; token != null ; token = input . next ( ) ) { String termText = ignoreCase ? token . termText . toLowerCase ( ) : token . termText ; if ( ! stopWords . contains ( termText ) ) return token ; } return null ; } } 	1	['7', '3', '0', '5', '13', '15', '2', '3', '7', '0.333333333', '106', '1', '0', '0.571428571', '0.514285714', '0', '0', '13.85714286', '3', '0.7143', '4']
package org . apache . lucene . search ; public abstract class HitCollector { public abstract void collect ( int doc , float score ) ; } 	0	['2', '1', '6', '22', '3', '1', '22', '0', '2', '2', '5', '0', '0', '0', '0.666666667', '0', '0', '1.5', '1', '0.5', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; public class ConstantScoreRangeQuery extends Query { private final String fieldName ; private final String lowerVal ; private final String upperVal ; private final boolean includeLower ; private final boolean includeUpper ; public ConstantScoreRangeQuery ( String fieldName , String lowerVal , String upperVal , boolean includeLower , boolean includeUpper ) { if ( lowerVal == null ) { includeLower = true ; } else if ( includeLower && lowerVal . equals ( "" ) ) { lowerVal = null ; } if ( upperVal == null ) { includeUpper = true ; } this . fieldName = fieldName . intern ( ) ; this . lowerVal = lowerVal ; this . upperVal = upperVal ; this . includeLower = includeLower ; this . includeUpper = includeUpper ; } public String getField ( ) { return fieldName ; } public String getLowerVal ( ) { return lowerVal ; } public String getUpperVal ( ) { return upperVal ; } public boolean includesLower ( ) { return includeLower ; } public boolean includesUpper ( ) { return includeUpper ; } public Query rewrite ( IndexReader reader ) throws IOException { RangeFilter rangeFilt = new RangeFilter ( fieldName , lowerVal != null ? lowerVal : "" , upperVal , lowerVal == "" ? false : includeLower , upperVal == null ? false : includeUpper ) ; Query q = new ConstantScoreQuery ( rangeFilt ) ; q . setBoost ( getBoost ( ) ) ; return q ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! getField ( ) . equals ( field ) ) { buffer . append ( getField ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( includeLower ? '[' : '{' ) ; buffer . append ( lowerVal != null ? lowerVal : "*" ) ; buffer . append ( " TO " ) ; buffer . append ( upperVal != null ? upperVal : "*" ) ; buffer . append ( includeUpper ? ']' : '}' ) ; if ( getBoost ( ) != 1.0f ) { buffer . append ( "^" ) ; buffer . append ( Float . toString ( getBoost ( ) ) ) ; } return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof ConstantScoreRangeQuery ) ) return false ; ConstantScoreRangeQuery other = ( ConstantScoreRangeQuery ) o ; if ( this . fieldName != other . fieldName || this . includeLower != other . includeLower || this . includeUpper != other . includeUpper ) { return false ; } if ( this . lowerVal != null ? ! this . lowerVal . equals ( other . lowerVal ) : other . lowerVal != null ) return false ; if ( this . upperVal != null ? ! this . upperVal . equals ( other . upperVal ) : other . upperVal != null ) return false ; return this . getBoost ( ) == other . getBoost ( ) ; } public int hashCode ( ) { int h = Float . floatToIntBits ( getBoost ( ) ) ^ fieldName . hashCode ( ) ; h ^= lowerVal != null ? lowerVal . hashCode ( ) : 0x965a965a ; h ^= ( h << 17 ) | ( h > > > 16 ) ; h ^= ( upperVal != null ? ( upperVal . hashCode ( ) ) : 0x5a695a69 ) ; h ^= ( includeLower ? 0x665599aa : 0 ) ^ ( includeUpper ? 0x99aa5566 : 0 ) ; return h ; } } 	1	['10', '2', '0', '6', '24', '0', '1', '5', '10', '0.444444444', '313', '1', '0', '0.571428571', '0.3', '2', '3', '29.8', '13', '3.1', '1']
package org . apache . lucene . analysis ; import java . io . Reader ; public class WhitespaceTokenizer extends CharTokenizer { public WhitespaceTokenizer ( Reader in ) { super ( in ) ; } protected boolean isTokenChar ( char c ) { return ! Character . isWhitespace ( c ) ; } } 	0	['2', '4', '0', '2', '4', '1', '1', '1', '1', '2', '13', '0', '0', '0.875', '0.666666667', '1', '1', '5.5', '2', '1', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; public abstract class TokenFilter extends TokenStream { protected TokenStream input ; protected TokenFilter ( TokenStream input ) { this . input = input ; } public void close ( ) throws IOException { input . close ( ) ; } } 	1	['2', '2', '7', '8', '4', '0', '7', '1', '1', '0', '13', '1', '1', '0.75', '0.75', '0', '0', '5', '1', '0.5', '2']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Query ; import org . apache . lucene . util . ToStringUtils ; public class SpanNotQuery extends SpanQuery { private SpanQuery include ; private SpanQuery exclude ; public SpanNotQuery ( SpanQuery include , SpanQuery exclude ) { this . include = include ; this . exclude = exclude ; if ( ! include . getField ( ) . equals ( exclude . getField ( ) ) ) throw new IllegalArgumentException ( "Clauses must have same field." ) ; } public SpanQuery getInclude ( ) { return include ; } public SpanQuery getExclude ( ) { return exclude ; } public String getField ( ) { return include . getField ( ) ; } public Collection getTerms ( ) { return include . getTerms ( ) ; } public void extractTerms ( Set terms ) { include . extractTerms ( terms ) ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "spanNot(" ) ; buffer . append ( include . toString ( field ) ) ; buffer . append ( ", " ) ; buffer . append ( exclude . toString ( field ) ) ; buffer . append ( ")" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public Spans getSpans ( final IndexReader reader ) throws IOException { return new Spans ( ) { private Spans includeSpans = include . getSpans ( reader ) ; private boolean moreInclude = true ; private Spans excludeSpans = exclude . getSpans ( reader ) ; private boolean moreExclude = excludeSpans . next ( ) ; public boolean next ( ) throws IOException { if ( moreInclude ) moreInclude = includeSpans . next ( ) ; while ( moreInclude && moreExclude ) { if ( includeSpans . doc ( ) > excludeSpans . doc ( ) ) moreExclude = excludeSpans . skipTo ( includeSpans . doc ( ) ) ; while ( moreExclude && includeSpans . doc ( ) == excludeSpans . doc ( ) && excludeSpans . end ( ) <= includeSpans . start ( ) ) { moreExclude = excludeSpans . next ( ) ; } if ( ! moreExclude || includeSpans . doc ( ) != excludeSpans . doc ( ) || includeSpans . end ( ) <= excludeSpans . start ( ) ) break ; moreInclude = includeSpans . next ( ) ; } return moreInclude ; } public boolean skipTo ( int target ) throws IOException { if ( moreInclude ) moreInclude = includeSpans . skipTo ( target ) ; if ( ! moreInclude ) return false ; if ( moreExclude && includeSpans . doc ( ) > excludeSpans . doc ( ) ) moreExclude = excludeSpans . skipTo ( includeSpans . doc ( ) ) ; while ( moreExclude && includeSpans . doc ( ) == excludeSpans . doc ( ) && excludeSpans . end ( ) <= includeSpans . start ( ) ) { moreExclude = excludeSpans . next ( ) ; } if ( ! moreExclude || includeSpans . doc ( ) != excludeSpans . doc ( ) || includeSpans . end ( ) <= excludeSpans . start ( ) ) return true ; return next ( ) ; } public int doc ( ) { return includeSpans . doc ( ) ; } public int start ( ) { return includeSpans . start ( ) ; } public int end ( ) { return includeSpans . end ( ) ; } public String toString ( ) { return "spans(" + SpanNotQuery . this . toString ( ) + ")" ; } } ; } public Query rewrite ( IndexReader reader ) throws IOException { SpanNotQuery clone = null ; SpanQuery rewrittenInclude = ( SpanQuery ) include . rewrite ( reader ) ; if ( rewrittenInclude != include ) { clone = ( SpanNotQuery ) this . clone ( ) ; clone . include = rewrittenInclude ; } SpanQuery rewrittenExclude = ( SpanQuery ) exclude . rewrite ( reader ) ; if ( rewrittenExclude != exclude ) { if ( clone == null ) clone = ( SpanNotQuery ) this . clone ( ) ; clone . exclude = rewrittenExclude ; } if ( clone != null ) { return clone ; } else { return this ; } } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof SpanNotQuery ) ) return false ; SpanNotQuery other = ( SpanNotQuery ) o ; return this . include . equals ( other . include ) && this . exclude . equals ( other . exclude ) && this . getBoost ( ) == other . getBoost ( ) ; } public int hashCode ( ) { int h = include . hashCode ( ) ; h = ( h << 1 ) | ( h > > > 31 ) ; h ^= exclude . hashCode ( ) ; h = ( h << 1 ) | ( h > > > 31 ) ; h ^= Float . floatToRawIntBits ( getBoost ( ) ) ; return h ; } } 	0	['13', '3', '0', '6', '31', '0', '1', '6', '11', '0.375', '218', '1', '2', '0.571428571', '0.208791209', '2', '2', '15.61538462', '6', '1.3077', '0']
package org . apache . lucene . index ; import java . util . List ; public final class KeepOnlyLastCommitDeletionPolicy implements IndexDeletionPolicy { public void onInit ( List commits ) { onCommit ( commits ) ; } public void onCommit ( List commits ) { int size = commits . size ( ) ; for ( int i = 0 ; i < size - 1 ; i ++ ) { ( ( IndexCommitPoint ) commits . get ( i ) ) . delete ( ) ; } } } 	1	['3', '1', '0', '4', '7', '3', '2', '2', '3', '2', '28', '0', '0', '0', '0.833333333', '0', '0', '8.333333333', '2', '1', '1']
package org . apache . lucene . search ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . FieldSelector ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . CorruptIndexException ; import java . io . IOException ; import java . rmi . Naming ; import java . rmi . RMISecurityManager ; import java . rmi . RemoteException ; import java . rmi . server . UnicastRemoteObject ; public class RemoteSearchable extends UnicastRemoteObject implements Searchable { private Searchable local ; public RemoteSearchable ( Searchable local ) throws RemoteException { super ( ) ; this . local = local ; } public void search ( Weight weight , Filter filter , HitCollector results ) throws IOException { local . search ( weight , filter , results ) ; } public void close ( ) throws IOException { local . close ( ) ; } public int docFreq ( Term term ) throws IOException { return local . docFreq ( term ) ; } public int [ ] docFreqs ( Term [ ] terms ) throws IOException { return local . docFreqs ( terms ) ; } public int maxDoc ( ) throws IOException { return local . maxDoc ( ) ; } public TopDocs search ( Weight weight , Filter filter , int n ) throws IOException { return local . search ( weight , filter , n ) ; } public TopFieldDocs search ( Weight weight , Filter filter , int n , Sort sort ) throws IOException { return local . search ( weight , filter , n , sort ) ; } public Document doc ( int i ) throws CorruptIndexException , IOException { return local . doc ( i ) ; } public Document doc ( int i , FieldSelector fieldSelector ) throws CorruptIndexException , IOException { return local . doc ( i , fieldSelector ) ; } public Query rewrite ( Query original ) throws IOException { return local . rewrite ( original ) ; } public Explanation explain ( Weight weight , int doc ) throws IOException { return local . explain ( weight , doc ) ; } public static void main ( String args [ ] ) throws Exception { String indexName = null ; if ( args != null && args . length == 1 ) indexName = args [ 0 ] ; if ( indexName == null ) { System . out . println ( "Usage: org.apache.lucene.search.RemoteSearchable <index>" ) ; return ; } if ( System . getSecurityManager ( ) == null ) { System . setSecurityManager ( new RMISecurityManager ( ) ) ; } Searchable local = new IndexSearcher ( indexName ) ; RemoteSearchable impl = new RemoteSearchable ( local ) ; Naming . rebind ( "//localhost/Searchable" , impl ) ; } } 	0	['13', '4', '0', '14', '31', '0', '0', '14', '13', '0', '120', '1', '1', '0.6', '0.205128205', '0', '0', '8.153846154', '1', '0.9231', '0']
package org . apache . lucene . analysis . standard ; import org . apache . lucene . analysis . * ; import java . io . File ; import java . io . IOException ; import java . io . Reader ; import java . util . Set ; public class StandardAnalyzer extends Analyzer { private Set stopSet ; public static final String [ ] STOP_WORDS = StopAnalyzer . ENGLISH_STOP_WORDS ; public StandardAnalyzer ( ) { this ( STOP_WORDS ) ; } public StandardAnalyzer ( Set stopWords ) { stopSet = stopWords ; } public StandardAnalyzer ( String [ ] stopWords ) { stopSet = StopFilter . makeStopSet ( stopWords ) ; } public StandardAnalyzer ( File stopwords ) throws IOException { stopSet = WordlistLoader . getWordSet ( stopwords ) ; } public StandardAnalyzer ( Reader stopwords ) throws IOException { stopSet = WordlistLoader . getWordSet ( stopwords ) ; } public TokenStream tokenStream ( String fieldName , Reader reader ) { TokenStream result = new StandardTokenizer ( reader ) ; result = new StandardFilter ( result ) ; result = new LowerCaseFilter ( result ) ; result = new StopFilter ( result , stopSet ) ; return result ; } } 	1	['7', '2', '0', '8', '15', '0', '0', '8', '6', '0.5', '67', '0.5', '0', '0.666666667', '0.333333333', '0', '0', '8.285714286', '1', '0.1429', '6']
package org . apache . lucene . index ; public interface TermFreqVector { public String getField ( ) ; public int size ( ) ; public String [ ] getTerms ( ) ; public int [ ] getTermFrequencies ( ) ; public int indexOf ( String term ) ; public int [ ] indexesOf ( String [ ] terms , int start , int len ) ; } 	0	['6', '1', '0', '11', '6', '15', '11', '0', '6', '2', '6', '0', '0', '0', '0.375', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . store ; import java . io . IOException ; public class RAMOutputStream extends IndexOutput { static final int BUFFER_SIZE = 1024 ; private RAMFile file ; private byte [ ] currentBuffer ; private int currentBufferIndex ; private int bufferPosition ; private long bufferStart ; private int bufferLength ; public RAMOutputStream ( ) { this ( new RAMFile ( ) ) ; } RAMOutputStream ( RAMFile f ) { file = f ; currentBufferIndex = - 1 ; currentBuffer = null ; } public void writeTo ( IndexOutput out ) throws IOException { flush ( ) ; final long end = file . length ; long pos = 0 ; int buffer = 0 ; while ( pos < end ) { int length = BUFFER_SIZE ; long nextPos = pos + length ; if ( nextPos > end ) { length = ( int ) ( end - pos ) ; } out . writeBytes ( ( byte [ ] ) file . buffers . get ( buffer ++ ) , length ) ; pos = nextPos ; } } public void reset ( ) { try { seek ( 0 ) ; } catch ( IOException e ) { throw new RuntimeException ( e . toString ( ) ) ; } file . setLength ( 0 ) ; } public void close ( ) throws IOException { flush ( ) ; } public void seek ( long pos ) throws IOException { setFileLength ( ) ; if ( pos < bufferStart || pos >= bufferStart + bufferLength ) { currentBufferIndex = ( int ) ( pos / BUFFER_SIZE ) ; switchCurrentBuffer ( ) ; } bufferPosition = ( int ) ( pos % BUFFER_SIZE ) ; } public long length ( ) { return file . length ; } public void writeByte ( byte b ) throws IOException { if ( bufferPosition == bufferLength ) { currentBufferIndex ++ ; switchCurrentBuffer ( ) ; } currentBuffer [ bufferPosition ++ ] = b ; } public void writeBytes ( byte [ ] b , int offset , int len ) throws IOException { while ( len > 0 ) { if ( bufferPosition == bufferLength ) { currentBufferIndex ++ ; switchCurrentBuffer ( ) ; } int remainInBuffer = currentBuffer . length - bufferPosition ; int bytesToCopy = len < remainInBuffer ? len : remainInBuffer ; System . arraycopy ( b , offset , currentBuffer , bufferPosition , bytesToCopy ) ; offset += bytesToCopy ; len -= bytesToCopy ; bufferPosition += bytesToCopy ; } } private final void switchCurrentBuffer ( ) throws IOException { if ( currentBufferIndex == file . buffers . size ( ) ) { currentBuffer = file . addBuffer ( BUFFER_SIZE ) ; } else { currentBuffer = ( byte [ ] ) file . buffers . get ( currentBufferIndex ) ; } bufferPosition = 0 ; bufferStart = BUFFER_SIZE * currentBufferIndex ; bufferLength = currentBuffer . length ; } private void setFileLength ( ) { long pointer = bufferStart + bufferPosition ; if ( pointer > file . length ) { file . setLength ( pointer ) ; } } public void flush ( ) throws IOException { file . setLastModified ( System . currentTimeMillis ( ) ) ; setFileLength ( ) ; } public long getFilePointer ( ) { return currentBufferIndex < 0 ? 0 : bufferStart + bufferPosition ; } } 	1	['13', '2', '0', '4', '25', '0', '2', '2', '10', '0.523809524', '292', '0.857142857', '1', '0.56', '0.208791209', '1', '3', '20.92307692', '2', '1', '4']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . List ; import java . util . Collection ; import java . util . ArrayList ; import java . util . Iterator ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . PriorityQueue ; import org . apache . lucene . util . ToStringUtils ; import org . apache . lucene . search . Query ; public class SpanOrQuery extends SpanQuery { private List clauses ; private String field ; public SpanOrQuery ( SpanQuery [ ] clauses ) { this . clauses = new ArrayList ( clauses . length ) ; for ( int i = 0 ; i < clauses . length ; i ++ ) { SpanQuery clause = clauses [ i ] ; if ( i == 0 ) { field = clause . getField ( ) ; } else if ( ! clause . getField ( ) . equals ( field ) ) { throw new IllegalArgumentException ( "Clauses must have same field." ) ; } this . clauses . add ( clause ) ; } } public SpanQuery [ ] getClauses ( ) { return ( SpanQuery [ ] ) clauses . toArray ( new SpanQuery [ clauses . size ( ) ] ) ; } public String getField ( ) { return field ; } public Collection getTerms ( ) { Collection terms = new ArrayList ( ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; terms . addAll ( clause . getTerms ( ) ) ; } return terms ; } public void extractTerms ( Set terms ) { Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; clause . extractTerms ( terms ) ; } } public Query rewrite ( IndexReader reader ) throws IOException { SpanOrQuery clone = null ; for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { SpanQuery c = ( SpanQuery ) clauses . get ( i ) ; SpanQuery query = ( SpanQuery ) c . rewrite ( reader ) ; if ( query != c ) { if ( clone == null ) clone = ( SpanOrQuery ) this . clone ( ) ; clone . clauses . set ( i , query ) ; } } if ( clone != null ) { return clone ; } else { return this ; } } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "spanOr([" ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; buffer . append ( clause . toString ( field ) ) ; if ( i . hasNext ( ) ) { buffer . append ( ", " ) ; } } buffer . append ( "])" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( o == null || getClass ( ) != o . getClass ( ) ) return false ; final SpanOrQuery that = ( SpanOrQuery ) o ; if ( ! clauses . equals ( that . clauses ) ) return false ; if ( ! field . equals ( that . field ) ) return false ; return getBoost ( ) == that . getBoost ( ) ; } public int hashCode ( ) { int h = clauses . hashCode ( ) ; h ^= ( h << 10 ) | ( h > > > 23 ) ; h ^= Float . floatToRawIntBits ( getBoost ( ) ) ; return h ; } private class SpanQueue extends PriorityQueue { public SpanQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object o1 , Object o2 ) { Spans spans1 = ( Spans ) o1 ; Spans spans2 = ( Spans ) o2 ; if ( spans1 . doc ( ) == spans2 . doc ( ) ) { if ( spans1 . start ( ) == spans2 . start ( ) ) { return spans1 . end ( ) < spans2 . end ( ) ; } else { return spans1 . start ( ) < spans2 . start ( ) ; } } else { return spans1 . doc ( ) < spans2 . doc ( ) ; } } } public Spans getSpans ( final IndexReader reader ) throws IOException { if ( clauses . size ( ) == 1 ) return ( ( SpanQuery ) clauses . get ( 0 ) ) . getSpans ( reader ) ; return new Spans ( ) { private SpanQueue queue = null ; private boolean initSpanQueue ( int target ) throws IOException { queue = new SpanQueue ( clauses . size ( ) ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { Spans spans = ( ( SpanQuery ) i . next ( ) ) . getSpans ( reader ) ; if ( ( ( target == - 1 ) && spans . next ( ) ) || ( ( target != - 1 ) && spans . skipTo ( target ) ) ) { queue . put ( spans ) ; } } return queue . size ( ) != 0 ; } public boolean next ( ) throws IOException { if ( queue == null ) { return initSpanQueue ( - 1 ) ; } if ( queue . size ( ) == 0 ) { return false ; } if ( top ( ) . next ( ) ) { queue . adjustTop ( ) ; return true ; } queue . pop ( ) ; return queue . size ( ) != 0 ; } private Spans top ( ) { return ( Spans ) queue . top ( ) ; } public boolean skipTo ( int target ) throws IOException { if ( queue == null ) { return initSpanQueue ( target ) ; } while ( queue . size ( ) != 0 && top ( ) . doc ( ) < target ) { if ( top ( ) . skipTo ( target ) ) { queue . adjustTop ( ) ; } else { queue . pop ( ) ; } } return queue . size ( ) != 0 ; } public int doc ( ) { return top ( ) . doc ( ) ; } public int start ( ) { return top ( ) . start ( ) ; } public int end ( ) { return top ( ) . end ( ) ; } public String toString ( ) { return "spans(" + SpanOrQuery . this + ")@" + ( ( queue == null ) ? "START" : ( queue . size ( ) > 0 ? ( doc ( ) + ":" + start ( ) + "-" + end ( ) ) : "END" ) ) ; } } ; } } 	0	['11', '3', '0', '8', '42', '0', '3', '6', '10', '0.45', '286', '1', '0', '0.615384615', '0.220779221', '2', '2', '24.81818182', '7', '1.7273', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . BufferedIndexInput ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . Lock ; import java . util . HashMap ; import java . io . IOException ; class CompoundFileReader extends Directory { private int readBufferSize ; private static final class FileEntry { long offset ; long length ; } private Directory directory ; private String fileName ; private IndexInput stream ; private HashMap entries = new HashMap ( ) ; public CompoundFileReader ( Directory dir , String name ) throws IOException { this ( dir , name , BufferedIndexInput . BUFFER_SIZE ) ; } public CompoundFileReader ( Directory dir , String name , int readBufferSize ) throws IOException { directory = dir ; fileName = name ; this . readBufferSize = readBufferSize ; boolean success = false ; try { stream = dir . openInput ( name , readBufferSize ) ; int count = stream . readVInt ( ) ; FileEntry entry = null ; for ( int i = 0 ; i < count ; i ++ ) { long offset = stream . readLong ( ) ; String id = stream . readString ( ) ; if ( entry != null ) { entry . length = offset - entry . offset ; } entry = new FileEntry ( ) ; entry . offset = offset ; entries . put ( id , entry ) ; } if ( entry != null ) { entry . length = stream . length ( ) - entry . offset ; } success = true ; } finally { if ( ! success && ( stream != null ) ) { try { stream . close ( ) ; } catch ( IOException e ) { } } } } public Directory getDirectory ( ) { return directory ; } public String getName ( ) { return fileName ; } public synchronized void close ( ) throws IOException { if ( stream == null ) throw new IOException ( "Already closed" ) ; entries . clear ( ) ; stream . close ( ) ; stream = null ; } public synchronized IndexInput openInput ( String id ) throws IOException { return openInput ( id , readBufferSize ) ; } public synchronized IndexInput openInput ( String id , int readBufferSize ) throws IOException { if ( stream == null ) throw new IOException ( "Stream closed" ) ; FileEntry entry = ( FileEntry ) entries . get ( id ) ; if ( entry == null ) throw new IOException ( "No sub-file with id " + id + " found" ) ; return new CSIndexInput ( stream , entry . offset , entry . length , readBufferSize ) ; } public String [ ] list ( ) { String res [ ] = new String [ entries . size ( ) ] ; return ( String [ ] ) entries . keySet ( ) . toArray ( res ) ; } public boolean fileExists ( String name ) { return entries . containsKey ( name ) ; } public long fileModified ( String name ) throws IOException { return directory . fileModified ( fileName ) ; } public void touchFile ( String name ) throws IOException { directory . touchFile ( fileName ) ; } public void deleteFile ( String name ) { throw new UnsupportedOperationException ( ) ; } public void renameFile ( String from , String to ) { throw new UnsupportedOperationException ( ) ; } public long fileLength ( String name ) throws IOException { FileEntry e = ( FileEntry ) entries . get ( name ) ; if ( e == null ) throw new IOException ( "File " + name + " does not exist" ) ; return e . length ; } public IndexOutput createOutput ( String name ) { throw new UnsupportedOperationException ( ) ; } public Lock makeLock ( String name ) { throw new UnsupportedOperationException ( ) ; } static final class CSIndexInput extends BufferedIndexInput { IndexInput base ; long fileOffset ; long length ; CSIndexInput ( final IndexInput base , final long fileOffset , final long length ) { this ( base , fileOffset , length , BufferedIndexInput . BUFFER_SIZE ) ; } CSIndexInput ( final IndexInput base , final long fileOffset , final long length , int readBufferSize ) { super ( readBufferSize ) ; this . base = base ; this . fileOffset = fileOffset ; this . length = length ; } protected void readInternal ( byte [ ] b , int offset , int len ) throws IOException { synchronized ( base ) { long start = getFilePointer ( ) ; if ( start + len > length ) throw new IOException ( "read past EOF" ) ; base . seek ( fileOffset + start ) ; base . readBytes ( b , offset , len ) ; } } protected void seekInternal ( long pos ) { } public void close ( ) { } public long length ( ) { return length ; } } } 	1	['16', '2', '0', '9', '40', '70', '2', '7', '16', '0.72', '273', '1', '2', '0.548387097', '0.5', '1', '5', '15.75', '1', '0.875', '2']
package org . apache . lucene . store ; import java . io . IOException ; public class NoLockFactory extends LockFactory { private static NoLock singletonLock = new NoLock ( ) ; private static NoLockFactory singleton = new NoLockFactory ( ) ; public static NoLockFactory getNoLockFactory ( ) { return singleton ; } public Lock makeLock ( String lockName ) { return singletonLock ; } public void clearLock ( String lockName ) { } ; } ; class NoLock extends Lock { public boolean obtain ( ) throws IOException { return true ; } public void release ( ) { } public boolean isLocked ( ) { return false ; } public String toString ( ) { return "NoLock" ; } } 	0	['5', '2', '0', '4', '7', '6', '1', '3', '4', '0.75', '24', '1', '2', '0.571428571', '0.625', '0', '0', '3.4', '1', '0.6', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . Set ; import java . util . Vector ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermPositions ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; public class PhraseQuery extends Query { private String field ; private Vector terms = new Vector ( ) ; private Vector positions = new Vector ( ) ; private int slop = 0 ; public PhraseQuery ( ) { } public void setSlop ( int s ) { slop = s ; } public int getSlop ( ) { return slop ; } public void add ( Term term ) { int position = 0 ; if ( positions . size ( ) > 0 ) position = ( ( Integer ) positions . lastElement ( ) ) . intValue ( ) + 1 ; add ( term , position ) ; } public void add ( Term term , int position ) { if ( terms . size ( ) == 0 ) field = term . field ( ) ; else if ( term . field ( ) != field ) throw new IllegalArgumentException ( "All phrase terms must be in the same field: " + term ) ; terms . addElement ( term ) ; positions . addElement ( new Integer ( position ) ) ; } public Term [ ] getTerms ( ) { return ( Term [ ] ) terms . toArray ( new Term [ 0 ] ) ; } public int [ ] getPositions ( ) { int [ ] result = new int [ positions . size ( ) ] ; for ( int i = 0 ; i < positions . size ( ) ; i ++ ) result [ i ] = ( ( Integer ) positions . elementAt ( i ) ) . intValue ( ) ; return result ; } private class PhraseWeight implements Weight { private Similarity similarity ; private float value ; private float idf ; private float queryNorm ; private float queryWeight ; public PhraseWeight ( Searcher searcher ) throws IOException { this . similarity = getSimilarity ( searcher ) ; idf = similarity . idf ( terms , searcher ) ; } public String toString ( ) { return "weight(" + PhraseQuery . this + ")" ; } public Query getQuery ( ) { return PhraseQuery . this ; } public float getValue ( ) { return value ; } public float sumOfSquaredWeights ( ) { queryWeight = idf * getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float queryNorm ) { this . queryNorm = queryNorm ; queryWeight *= queryNorm ; value = queryWeight * idf ; } public Scorer scorer ( IndexReader reader ) throws IOException { if ( terms . size ( ) == 0 ) return null ; TermPositions [ ] tps = new TermPositions [ terms . size ( ) ] ; for ( int i = 0 ; i < terms . size ( ) ; i ++ ) { TermPositions p = reader . termPositions ( ( Term ) terms . elementAt ( i ) ) ; if ( p == null ) return null ; tps [ i ] = p ; } if ( slop == 0 ) return new ExactPhraseScorer ( this , tps , getPositions ( ) , similarity , reader . norms ( field ) ) ; else return new SloppyPhraseScorer ( this , tps , getPositions ( ) , similarity , slop , reader . norms ( field ) ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { Explanation result = new Explanation ( ) ; result . setDescription ( "weight(" + getQuery ( ) + " in " + doc + "), product of:" ) ; StringBuffer docFreqs = new StringBuffer ( ) ; StringBuffer query = new StringBuffer ( ) ; query . append ( '\"' ) ; for ( int i = 0 ; i < terms . size ( ) ; i ++ ) { if ( i != 0 ) { docFreqs . append ( " " ) ; query . append ( " " ) ; } Term term = ( Term ) terms . elementAt ( i ) ; docFreqs . append ( term . text ( ) ) ; docFreqs . append ( "=" ) ; docFreqs . append ( reader . docFreq ( term ) ) ; query . append ( term . text ( ) ) ; } query . append ( '\"' ) ; Explanation idfExpl = new Explanation ( idf , "idf(" + field + ": " + docFreqs + ")" ) ; Explanation queryExpl = new Explanation ( ) ; queryExpl . setDescription ( "queryWeight(" + getQuery ( ) + "), product of:" ) ; Explanation boostExpl = new Explanation ( getBoost ( ) , "boost" ) ; if ( getBoost ( ) != 1.0f ) queryExpl . addDetail ( boostExpl ) ; queryExpl . addDetail ( idfExpl ) ; Explanation queryNormExpl = new Explanation ( queryNorm , "queryNorm" ) ; queryExpl . addDetail ( queryNormExpl ) ; queryExpl . setValue ( boostExpl . getValue ( ) * idfExpl . getValue ( ) * queryNormExpl . getValue ( ) ) ; result . addDetail ( queryExpl ) ; Explanation fieldExpl = new Explanation ( ) ; fieldExpl . setDescription ( "fieldWeight(" + field + ":" + query + " in " + doc + "), product of:" ) ; Explanation tfExpl = scorer ( reader ) . explain ( doc ) ; fieldExpl . addDetail ( tfExpl ) ; fieldExpl . addDetail ( idfExpl ) ; Explanation fieldNormExpl = new Explanation ( ) ; byte [ ] fieldNorms = reader . norms ( field ) ; float fieldNorm = fieldNorms != null ? Similarity . decodeNorm ( fieldNorms [ doc ] ) : 0.0f ; fieldNormExpl . setValue ( fieldNorm ) ; fieldNormExpl . setDescription ( "fieldNorm(field=" + field + ", doc=" + doc + ")" ) ; fieldExpl . addDetail ( fieldNormExpl ) ; fieldExpl . setValue ( tfExpl . getValue ( ) * idfExpl . getValue ( ) * fieldNormExpl . getValue ( ) ) ; result . addDetail ( fieldExpl ) ; result . setValue ( queryExpl . getValue ( ) * fieldExpl . getValue ( ) ) ; if ( queryExpl . getValue ( ) == 1.0f ) return fieldExpl ; return result ; } } protected Weight createWeight ( Searcher searcher ) throws IOException { if ( terms . size ( ) == 1 ) { Term term = ( Term ) terms . elementAt ( 0 ) ; Query termQuery = new TermQuery ( term ) ; termQuery . setBoost ( getBoost ( ) ) ; return termQuery . createWeight ( searcher ) ; } return new PhraseWeight ( searcher ) ; } public void extractTerms ( Set queryTerms ) { queryTerms . addAll ( terms ) ; } public String toString ( String f ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! field . equals ( f ) ) { buffer . append ( field ) ; buffer . append ( ":" ) ; } buffer . append ( "\"" ) ; for ( int i = 0 ; i < terms . size ( ) ; i ++ ) { buffer . append ( ( ( Term ) terms . elementAt ( i ) ) . text ( ) ) ; if ( i != terms . size ( ) - 1 ) buffer . append ( " " ) ; } buffer . append ( "\"" ) ; if ( slop != 0 ) { buffer . append ( "~" ) ; buffer . append ( slop ) ; } buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof PhraseQuery ) ) return false ; PhraseQuery other = ( PhraseQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && ( this . slop == other . slop ) && this . terms . equals ( other . terms ) && this . positions . equals ( other . positions ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ slop ^ terms . hashCode ( ) ^ positions . hashCode ( ) ; } } 	1	['15', '2', '0', '9', '43', '0', '3', '7', '11', '0.589285714', '303', '1', '0', '0.461538462', '0.191666667', '2', '3', '18.93333333', '6', '1.8', '1']
package org . apache . lucene . search ; public class ScoreDoc implements java . io . Serializable { public float score ; public int doc ; public ScoreDoc ( int doc , float score ) { this . doc = doc ; this . score = score ; } } 	0	['1', '1', '1', '19', '2', '0', '19', '0', '1', '2', '12', '0', '0', '0', '1', '0', '0', '9', '0', '0', '0']
package org . apache . lucene . search ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . analysis . TokenStream ; import org . apache . lucene . index . TermFreqVector ; import java . io . IOException ; import java . io . StringReader ; import java . util . * ; public class QueryTermVector implements TermFreqVector { private String [ ] terms = new String [ 0 ] ; private int [ ] termFreqs = new int [ 0 ] ; public String getField ( ) { return null ; } public QueryTermVector ( String [ ] queryTerms ) { processTerms ( queryTerms ) ; } public QueryTermVector ( String queryString , Analyzer analyzer ) { if ( analyzer != null ) { TokenStream stream = analyzer . tokenStream ( "" , new StringReader ( queryString ) ) ; if ( stream != null ) { Token next = null ; List terms = new ArrayList ( ) ; try { while ( ( next = stream . next ( ) ) != null ) { terms . add ( next . termText ( ) ) ; } processTerms ( ( String [ ] ) terms . toArray ( new String [ terms . size ( ) ] ) ) ; } catch ( IOException e ) { } } } } private void processTerms ( String [ ] queryTerms ) { if ( queryTerms != null ) { Arrays . sort ( queryTerms ) ; Map tmpSet = new HashMap ( queryTerms . length ) ; List tmpList = new ArrayList ( queryTerms . length ) ; List tmpFreqs = new ArrayList ( queryTerms . length ) ; int j = 0 ; for ( int i = 0 ; i < queryTerms . length ; i ++ ) { String term = queryTerms [ i ] ; Integer position = ( Integer ) tmpSet . get ( term ) ; if ( position == null ) { tmpSet . put ( term , new Integer ( j ++ ) ) ; tmpList . add ( term ) ; tmpFreqs . add ( new Integer ( 1 ) ) ; } else { Integer integer = ( Integer ) tmpFreqs . get ( position . intValue ( ) ) ; tmpFreqs . set ( position . intValue ( ) , new Integer ( integer . intValue ( ) + 1 ) ) ; } } terms = ( String [ ] ) tmpList . toArray ( terms ) ; termFreqs = new int [ tmpFreqs . size ( ) ] ; int i = 0 ; for ( Iterator iter = tmpFreqs . iterator ( ) ; iter . hasNext ( ) ; ) { Integer integer = ( Integer ) iter . next ( ) ; termFreqs [ i ++ ] = integer . intValue ( ) ; } } } public final String toString ( ) { StringBuffer sb = new StringBuffer ( ) ; sb . append ( '{' ) ; for ( int i = 0 ; i < terms . length ; i ++ ) { if ( i > 0 ) sb . append ( ", " ) ; sb . append ( terms [ i ] ) . append ( '/' ) . append ( termFreqs [ i ] ) ; } sb . append ( '}' ) ; return sb . toString ( ) ; } public int size ( ) { return terms . length ; } public String [ ] getTerms ( ) { return terms ; } public int [ ] getTermFrequencies ( ) { return termFreqs ; } public int indexOf ( String term ) { int res = Arrays . binarySearch ( terms , term ) ; return res >= 0 ? res : - 1 ; } public int [ ] indexesOf ( String [ ] terms , int start , int len ) { int res [ ] = new int [ len ] ; for ( int i = 0 ; i < len ; i ++ ) { res [ i ] = indexOf ( terms [ i ] ) ; } return res ; } } 	1	['10', '1', '0', '4', '37', '0', '0', '4', '9', '0.388888889', '280', '1', '0', '0', '0.34', '0', '0', '26.8', '5', '1.6', '1']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Arrays ; import java . util . Comparator ; import org . apache . lucene . index . IndexReader ; class NearSpansOrdered implements Spans { private final int allowedSlop ; private boolean firstTime = true ; private boolean more = false ; private final Spans [ ] subSpans ; private boolean inSameDoc = false ; private int matchDoc = - 1 ; private int matchStart = - 1 ; private int matchEnd = - 1 ; private final Spans [ ] subSpansByDoc ; private final Comparator spanDocComparator = new Comparator ( ) { public int compare ( Object o1 , Object o2 ) { return ( ( Spans ) o1 ) . doc ( ) - ( ( Spans ) o2 ) . doc ( ) ; } } ; private SpanNearQuery query ; public NearSpansOrdered ( SpanNearQuery spanNearQuery , IndexReader reader ) throws IOException { if ( spanNearQuery . getClauses ( ) . length < 2 ) { throw new IllegalArgumentException ( "Less than 2 clauses: " + spanNearQuery ) ; } allowedSlop = spanNearQuery . getSlop ( ) ; SpanQuery [ ] clauses = spanNearQuery . getClauses ( ) ; subSpans = new Spans [ clauses . length ] ; subSpansByDoc = new Spans [ clauses . length ] ; for ( int i = 0 ; i < clauses . length ; i ++ ) { subSpans [ i ] = clauses [ i ] . getSpans ( reader ) ; subSpansByDoc [ i ] = subSpans [ i ] ; } query = spanNearQuery ; } public int doc ( ) { return matchDoc ; } public int start ( ) { return matchStart ; } public int end ( ) { return matchEnd ; } public boolean next ( ) throws IOException { if ( firstTime ) { firstTime = false ; for ( int i = 0 ; i < subSpans . length ; i ++ ) { if ( ! subSpans [ i ] . next ( ) ) { more = false ; return false ; } } more = true ; } return advanceAfterOrdered ( ) ; } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { firstTime = false ; for ( int i = 0 ; i < subSpans . length ; i ++ ) { if ( ! subSpans [ i ] . skipTo ( target ) ) { more = false ; return false ; } } more = true ; } else if ( more && ( subSpans [ 0 ] . doc ( ) < target ) ) { if ( subSpans [ 0 ] . skipTo ( target ) ) { inSameDoc = false ; } else { more = false ; return false ; } } return advanceAfterOrdered ( ) ; } private boolean advanceAfterOrdered ( ) throws IOException { while ( more && ( inSameDoc || toSameDoc ( ) ) ) { if ( stretchToOrder ( ) && shrinkToAfterShortestMatch ( ) ) { return true ; } } return false ; } private boolean toSameDoc ( ) throws IOException { Arrays . sort ( subSpansByDoc , spanDocComparator ) ; int firstIndex = 0 ; int maxDoc = subSpansByDoc [ subSpansByDoc . length - 1 ] . doc ( ) ; while ( subSpansByDoc [ firstIndex ] . doc ( ) != maxDoc ) { if ( ! subSpansByDoc [ firstIndex ] . skipTo ( maxDoc ) ) { more = false ; inSameDoc = false ; return false ; } maxDoc = subSpansByDoc [ firstIndex ] . doc ( ) ; if ( ++ firstIndex == subSpansByDoc . length ) { firstIndex = 0 ; } } for ( int i = 0 ; i < subSpansByDoc . length ; i ++ ) { assert ( subSpansByDoc [ i ] . doc ( ) == maxDoc ) : " NearSpansOrdered.toSameDoc() spans " + subSpansByDoc [ 0 ] + "\n at doc " + subSpansByDoc [ i ] . doc ( ) + ", but should be at " + maxDoc ; } inSameDoc = true ; return true ; } static final boolean docSpansOrdered ( Spans spans1 , Spans spans2 ) { assert spans1 . doc ( ) == spans2 . doc ( ) : "doc1 " + spans1 . doc ( ) + " != doc2 " + spans2 . doc ( ) ; int start1 = spans1 . start ( ) ; int start2 = spans2 . start ( ) ; return ( start1 == start2 ) ? ( spans1 . end ( ) < spans2 . end ( ) ) : ( start1 < start2 ) ; } private static final boolean docSpansOrdered ( int start1 , int end1 , int start2 , int end2 ) { return ( start1 == start2 ) ? ( end1 < end2 ) : ( start1 < start2 ) ; } private boolean stretchToOrder ( ) throws IOException { matchDoc = subSpans [ 0 ] . doc ( ) ; for ( int i = 1 ; inSameDoc && ( i < subSpans . length ) ; i ++ ) { while ( ! docSpansOrdered ( subSpans [ i - 1 ] , subSpans [ i ] ) ) { if ( ! subSpans [ i ] . next ( ) ) { inSameDoc = false ; more = false ; break ; } else if ( matchDoc != subSpans [ i ] . doc ( ) ) { inSameDoc = false ; break ; } } } return inSameDoc ; } private boolean shrinkToAfterShortestMatch ( ) throws IOException { matchStart = subSpans [ subSpans . length - 1 ] . start ( ) ; matchEnd = subSpans [ subSpans . length - 1 ] . end ( ) ; int matchSlop = 0 ; int lastStart = matchStart ; int lastEnd = matchEnd ; for ( int i = subSpans . length - 2 ; i >= 0 ; i -- ) { Spans prevSpans = subSpans [ i ] ; int prevStart = prevSpans . start ( ) ; int prevEnd = prevSpans . end ( ) ; while ( true ) { if ( ! prevSpans . next ( ) ) { inSameDoc = false ; more = false ; break ; } else if ( matchDoc != prevSpans . doc ( ) ) { inSameDoc = false ; break ; } else { int ppStart = prevSpans . start ( ) ; int ppEnd = prevSpans . end ( ) ; if ( ! docSpansOrdered ( ppStart , ppEnd , lastStart , lastEnd ) ) { break ; } else { prevStart = ppStart ; prevEnd = ppEnd ; } } } assert prevStart <= matchStart ; if ( matchStart > prevEnd ) { matchSlop += ( matchStart - prevEnd ) ; } matchStart = prevStart ; lastStart = prevStart ; lastEnd = prevEnd ; } return matchSlop <= allowedSlop ; } public String toString ( ) { return getClass ( ) . getName ( ) + "(" + query . toString ( ) + ")@" + ( firstTime ? "START" : ( more ? ( doc ( ) + ":" + start ( ) + "-" + end ( ) ) : "END" ) ) ; } } 	0	['15', '1', '0', '6', '41', '25', '3', '5', '7', '0.747252747', '661', '0.846153846', '3', '0', '0.202380952', '0', '0', '42.2', '6', '1.5333', '0']
package org . apache . lucene . index ; import java . io . ByteArrayOutputStream ; import java . io . IOException ; import java . util . Iterator ; import java . util . zip . Deflater ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; final class FieldsWriter { static final byte FIELD_IS_TOKENIZED = 0x1 ; static final byte FIELD_IS_BINARY = 0x2 ; static final byte FIELD_IS_COMPRESSED = 0x4 ; private FieldInfos fieldInfos ; private IndexOutput fieldsStream ; private IndexOutput indexStream ; FieldsWriter ( Directory d , String segment , FieldInfos fn ) throws IOException { fieldInfos = fn ; fieldsStream = d . createOutput ( segment + ".fdt" ) ; indexStream = d . createOutput ( segment + ".fdx" ) ; } final void close ( ) throws IOException { fieldsStream . close ( ) ; indexStream . close ( ) ; } final void addDocument ( Document doc ) throws IOException { indexStream . writeLong ( fieldsStream . getFilePointer ( ) ) ; int storedCount = 0 ; Iterator fieldIterator = doc . getFields ( ) . iterator ( ) ; while ( fieldIterator . hasNext ( ) ) { Fieldable field = ( Fieldable ) fieldIterator . next ( ) ; if ( field . isStored ( ) ) storedCount ++ ; } fieldsStream . writeVInt ( storedCount ) ; fieldIterator = doc . getFields ( ) . iterator ( ) ; while ( fieldIterator . hasNext ( ) ) { Fieldable field = ( Fieldable ) fieldIterator . next ( ) ; boolean disableCompression = ( field instanceof FieldsReader . FieldForMerge ) ; if ( field . isStored ( ) ) { fieldsStream . writeVInt ( fieldInfos . fieldNumber ( field . name ( ) ) ) ; byte bits = 0 ; if ( field . isTokenized ( ) ) bits |= FieldsWriter . FIELD_IS_TOKENIZED ; if ( field . isBinary ( ) ) bits |= FieldsWriter . FIELD_IS_BINARY ; if ( field . isCompressed ( ) ) bits |= FieldsWriter . FIELD_IS_COMPRESSED ; fieldsStream . writeByte ( bits ) ; if ( field . isCompressed ( ) ) { byte [ ] data = null ; if ( disableCompression ) { data = field . binaryValue ( ) ; } else { if ( field . isBinary ( ) ) { data = compress ( field . binaryValue ( ) ) ; } else { data = compress ( field . stringValue ( ) . getBytes ( "UTF-8" ) ) ; } } final int len = data . length ; fieldsStream . writeVInt ( len ) ; fieldsStream . writeBytes ( data , len ) ; } else { if ( field . isBinary ( ) ) { byte [ ] data = field . binaryValue ( ) ; final int len = data . length ; fieldsStream . writeVInt ( len ) ; fieldsStream . writeBytes ( data , len ) ; } else { fieldsStream . writeString ( field . stringValue ( ) ) ; } } } } } private final byte [ ] compress ( byte [ ] input ) { Deflater compressor = new Deflater ( ) ; compressor . setLevel ( Deflater . BEST_COMPRESSION ) ; compressor . setInput ( input ) ; compressor . finish ( ) ; ByteArrayOutputStream bos = new ByteArrayOutputStream ( input . length ) ; byte [ ] buf = new byte [ 1024 ] ; while ( ! compressor . finished ( ) ) { int count = compressor . deflate ( buf ) ; bos . write ( buf , 0 , count ) ; } compressor . end ( ) ; return bos . toByteArray ( ) ; } } 	1	['4', '1', '0', '8', '39', '0', '2', '6', '0', '0.888888889', '235', '0.5', '3', '0', '0.375', '0', '0', '56.25', '2', '1', '7']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; public interface Weight extends java . io . Serializable { Query getQuery ( ) ; float getValue ( ) ; float sumOfSquaredWeights ( ) throws IOException ; void normalize ( float norm ) ; Scorer scorer ( IndexReader reader ) throws IOException ; Explanation explain ( IndexReader reader , int doc ) throws IOException ; } 	0	['6', '1', '0', '47', '6', '15', '44', '4', '6', '2', '6', '0', '0', '0', '0.416666667', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . IndexInput ; import java . io . IOException ; final class SegmentTermPositions extends SegmentTermDocs implements TermPositions { private IndexInput proxStream ; private int proxCount ; private int position ; private int payloadLength ; private boolean needToLoadPayload ; private long lazySkipPointer = 0 ; private int lazySkipProxCount = 0 ; SegmentTermPositions ( SegmentReader p ) { super ( p ) ; this . proxStream = null ; } final void seek ( TermInfo ti , Term term ) throws IOException { super . seek ( ti , term ) ; if ( ti != null ) lazySkipPointer = ti . proxPointer ; lazySkipProxCount = 0 ; proxCount = 0 ; payloadLength = 0 ; needToLoadPayload = false ; } public final void close ( ) throws IOException { super . close ( ) ; if ( proxStream != null ) proxStream . close ( ) ; } public final int nextPosition ( ) throws IOException { lazySkip ( ) ; proxCount -- ; return position += readDeltaPosition ( ) ; } private final int readDeltaPosition ( ) throws IOException { int delta = proxStream . readVInt ( ) ; if ( currentFieldStoresPayloads ) { if ( ( delta & 1 ) != 0 ) { payloadLength = proxStream . readVInt ( ) ; } delta >>>= 1 ; needToLoadPayload = true ; } else { payloadLength = 0 ; needToLoadPayload = false ; } return delta ; } protected final void skippingDoc ( ) throws IOException { lazySkipProxCount += freq ; } public final boolean next ( ) throws IOException { lazySkipProxCount += proxCount ; if ( super . next ( ) ) { proxCount = freq ; position = 0 ; return true ; } return false ; } public final int read ( final int [ ] docs , final int [ ] freqs ) { throw new UnsupportedOperationException ( "TermPositions does not support processing multiple documents in one call. Use TermDocs instead." ) ; } protected void skipProx ( long proxPointer , int payloadLength ) throws IOException { lazySkipPointer = proxPointer ; lazySkipProxCount = 0 ; proxCount = 0 ; this . payloadLength = payloadLength ; needToLoadPayload = false ; } private void skipPositions ( int n ) throws IOException { for ( int f = n ; f > 0 ; f -- ) { readDeltaPosition ( ) ; skipPayload ( ) ; } } private void skipPayload ( ) throws IOException { if ( needToLoadPayload && payloadLength > 0 ) { proxStream . seek ( proxStream . getFilePointer ( ) + payloadLength ) ; } needToLoadPayload = false ; } private void lazySkip ( ) throws IOException { if ( proxStream == null ) { proxStream = ( IndexInput ) parent . proxStream . clone ( ) ; } skipPayload ( ) ; if ( lazySkipPointer != 0 ) { proxStream . seek ( lazySkipPointer ) ; lazySkipPointer = 0 ; } if ( lazySkipProxCount != 0 ) { skipPositions ( lazySkipProxCount ) ; lazySkipProxCount = 0 ; } } public int getPayloadLength ( ) { return payloadLength ; } public byte [ ] getPayload ( byte [ ] data , int offset ) throws IOException { if ( ! needToLoadPayload ) { throw new IOException ( "Payload cannot be loaded more than once for the same term position." ) ; } byte [ ] retArray ; int retOffset ; if ( data == null || data . length - offset < payloadLength ) { retArray = new byte [ payloadLength ] ; retOffset = 0 ; } else { retArray = data ; retOffset = offset ; } proxStream . readBytes ( retArray , retOffset , payloadLength ) ; needToLoadPayload = false ; return retArray ; } public boolean isPayloadAvailable ( ) { return needToLoadPayload && payloadLength > 0 ; } } 	1	['15', '2', '0', '6', '27', '7', '1', '6', '7', '0.612244898', '287', '1', '1', '0.44', '0.2', '1', '3', '17.66666667', '3', '1.0667', '1']
package org . apache . lucene . analysis ; import java . io . File ; import java . io . IOException ; import java . io . Reader ; import java . util . Set ; public final class StopAnalyzer extends Analyzer { private Set stopWords ; public static final String [ ] ENGLISH_STOP_WORDS = { "a" , "an" , "and" , "are" , "as" , "at" , "be" , "but" , "by" , "for" , "if" , "in" , "into" , "is" , "it" , "no" , "not" , "of" , "on" , "or" , "such" , "that" , "the" , "their" , "then" , "there" , "these" , "they" , "this" , "to" , "was" , "will" , "with" } ; public StopAnalyzer ( ) { stopWords = StopFilter . makeStopSet ( ENGLISH_STOP_WORDS ) ; } public StopAnalyzer ( Set stopWords ) { this . stopWords = stopWords ; } public StopAnalyzer ( String [ ] stopWords ) { this . stopWords = StopFilter . makeStopSet ( stopWords ) ; } public StopAnalyzer ( File stopwordsFile ) throws IOException { stopWords = WordlistLoader . getWordSet ( stopwordsFile ) ; } public StopAnalyzer ( Reader stopwords ) throws IOException { stopWords = WordlistLoader . getWordSet ( stopwords ) ; } public TokenStream tokenStream ( String fieldName , Reader reader ) { return new StopFilter ( new LowerCaseTokenizer ( reader ) , stopWords ) ; } } 	0	['7', '2', '0', '6', '13', '0', '1', '5', '6', '0.5', '189', '0.5', '0', '0.666666667', '0.333333333', '0', '0', '25.71428571', '1', '0.1429', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class IndexInput implements Cloneable { private char [ ] chars ; public abstract byte readByte ( ) throws IOException ; public abstract void readBytes ( byte [ ] b , int offset , int len ) throws IOException ; public int readInt ( ) throws IOException { return ( ( readByte ( ) & 0xFF ) << 24 ) | ( ( readByte ( ) & 0xFF ) << 16 ) | ( ( readByte ( ) & 0xFF ) << 8 ) | ( readByte ( ) & 0xFF ) ; } public int readVInt ( ) throws IOException { byte b = readByte ( ) ; int i = b & 0x7F ; for ( int shift = 7 ; ( b & 0x80 ) != 0 ; shift += 7 ) { b = readByte ( ) ; i |= ( b & 0x7F ) << shift ; } return i ; } public long readLong ( ) throws IOException { return ( ( ( long ) readInt ( ) ) << 32 ) | ( readInt ( ) & 0xFFFFFFFFL ) ; } public long readVLong ( ) throws IOException { byte b = readByte ( ) ; long i = b & 0x7F ; for ( int shift = 7 ; ( b & 0x80 ) != 0 ; shift += 7 ) { b = readByte ( ) ; i |= ( b & 0x7FL ) << shift ; } return i ; } public String readString ( ) throws IOException { int length = readVInt ( ) ; if ( chars == null || length > chars . length ) chars = new char [ length ] ; readChars ( chars , 0 , length ) ; return new String ( chars , 0 , length ) ; } public void readChars ( char [ ] buffer , int start , int length ) throws IOException { final int end = start + length ; for ( int i = start ; i < end ; i ++ ) { byte b = readByte ( ) ; if ( ( b & 0x80 ) == 0 ) buffer [ i ] = ( char ) ( b & 0x7F ) ; else if ( ( b & 0xE0 ) != 0xE0 ) { buffer [ i ] = ( char ) ( ( ( b & 0x1F ) << 6 ) | ( readByte ( ) & 0x3F ) ) ; } else buffer [ i ] = ( char ) ( ( ( b & 0x0F ) << 12 ) | ( ( readByte ( ) & 0x3F ) << 6 ) | ( readByte ( ) & 0x3F ) ) ; } } public void skipChars ( int length ) throws IOException { for ( int i = 0 ; i < length ; i ++ ) { byte b = readByte ( ) ; if ( ( b & 0x80 ) == 0 ) { } else if ( ( b & 0xE0 ) != 0xE0 ) { readByte ( ) ; } else { readByte ( ) ; readByte ( ) ; } } } public abstract void close ( ) throws IOException ; public abstract long getFilePointer ( ) ; public abstract void seek ( long pos ) throws IOException ; public abstract long length ( ) ; public Object clone ( ) { IndexInput clone = null ; try { clone = ( IndexInput ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . chars = null ; return clone ; } } 	1	['15', '1', '5', '31', '18', '103', '31', '0', '15', '0.928571429', '256', '1', '0', '0', '0.28', '0', '0', '16', '1', '0.9333', '1']
package org . apache . lucene . analysis . standard ; import java . io . * ; public class StandardTokenizerTokenManager implements StandardTokenizerConstants { public java . io . PrintStream debugStream = System . out ; public void setDebugStream ( java . io . PrintStream ds ) { debugStream = ds ; } private final int jjMoveStringLiteralDfa0_0 ( ) { return jjMoveNfa_0 ( 0 , 0 ) ; } private final void jjCheckNAdd ( int state ) { if ( jjrounds [ state ] != jjround ) { jjstateSet [ jjnewStateCnt ++ ] = state ; jjrounds [ state ] = jjround ; } } private final void jjAddStates ( int start , int end ) { do { jjstateSet [ jjnewStateCnt ++ ] = jjnextStates [ start ] ; } while ( start ++ != end ) ; } private final void jjCheckNAddTwoStates ( int state1 , int state2 ) { jjCheckNAdd ( state1 ) ; jjCheckNAdd ( state2 ) ; } private final void jjCheckNAddStates ( int start , int end ) { do { jjCheckNAdd ( jjnextStates [ start ] ) ; } while ( start ++ != end ) ; } private final void jjCheckNAddStates ( int start ) { jjCheckNAdd ( jjnextStates [ start ] ) ; jjCheckNAdd ( jjnextStates [ start + 1 ] ) ; } static final long [ ] jjbitVec0 = { 0xfff0000000000000L , 0xffffffffffffdfffL , 0xffffffffL , 0x600000000000000L } ; static final long [ ] jjbitVec2 = { 0x0L , 0xffffffffffffffffL , 0xffffffffffffffffL , 0xffffffffffffffffL } ; static final long [ ] jjbitVec3 = { 0xffffffffffffffffL , 0xffffffffffffffffL , 0xffffL , 0xffff000000000000L } ; static final long [ ] jjbitVec4 = { 0xffffffffffffffffL , 0xffffffffffffffffL , 0x0L , 0x0L } ; static final long [ ] jjbitVec5 = { 0xffffffffffffffffL , 0xffffffffffffffffL , 0xffffffffffffffffL , 0x0L } ; static final long [ ] jjbitVec6 = { 0x0L , 0xffffffe000000000L , 0xffffffffL , 0x0L } ; static final long [ ] jjbitVec7 = { 0x20000L , 0x0L , 0xfffff00000000000L , 0x7fffffL } ; static final long [ ] jjbitVec8 = { 0xffffffffffffffffL , 0xffffffffffffffffL , 0xffffffffffffL , 0x0L } ; static final long [ ] jjbitVec9 = { 0xfffffffeL , 0x0L , 0x0L , 0x0L } ; static final long [ ] jjbitVec10 = { 0x0L , 0x0L , 0x0L , 0xff7fffffff7fffffL } ; static final long [ ] jjbitVec11 = { 0x0L , 0x0L , 0xffffffff00000000L , 0x1fffffffL } ; static final long [ ] jjbitVec12 = { 0x1600L , 0x0L , 0x0L , 0x0L } ; static final long [ ] jjbitVec13 = { 0x0L , 0xffc000000000L , 0x0L , 0xffc000000000L } ; static final long [ ] jjbitVec14 = { 0x0L , 0x3ff00000000L , 0x0L , 0x3ff000000000000L } ; static final long [ ] jjbitVec15 = { 0x0L , 0xffc000000000L , 0x0L , 0xff8000000000L } ; static final long [ ] jjbitVec16 = { 0x0L , 0xffc000000000L , 0x0L , 0x0L } ; static final long [ ] jjbitVec17 = { 0x0L , 0x3ff0000L , 0x0L , 0x3ff0000L } ; static final long [ ] jjbitVec18 = { 0x0L , 0x3ffL , 0x0L , 0x0L } ; static final long [ ] jjbitVec19 = { 0xfffffffeL , 0x0L , 0xfffff00000000000L , 0x7fffffL } ; private final int jjMoveNfa_0 ( int startState , int curPos ) { int [ ] nextStates ; int startsAt = 0 ; jjnewStateCnt = 75 ; int i = 1 ; jjstateSet [ 0 ] = startState ; int j , kind = 0x7fffffff ; for ( ; ; ) { if ( ++ jjround == 0x7fffffff ) ReInitRounds ( ) ; if ( curChar < 64 ) { long l = 1L << curChar ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( ( 0x3ff000000000000L & l ) != 0L ) { if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; } if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 12 , 17 ) ; if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 18 , 23 ) ; break ; case 2 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 18 , 23 ) ; break ; case 3 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 3 , 4 ) ; break ; case 4 : case 5 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 5 , 6 ) ; break ; case 6 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 7 ) ; break ; case 7 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAdd ( 7 ) ; break ; case 8 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 8 , 9 ) ; break ; case 9 : case 10 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 10 , 11 ) ; break ; case 11 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 12 ) ; break ; case 12 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 12 , 13 ) ; break ; case 13 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 14 , 15 ) ; break ; case 14 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 14 , 15 ) ; break ; case 15 : case 16 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 11 , 16 ) ; break ; case 17 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 17 , 18 ) ; break ; case 18 : case 19 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 19 , 20 ) ; break ; case 20 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 21 ) ; break ; case 21 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 21 , 22 ) ; break ; case 22 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 23 , 24 ) ; break ; case 23 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 23 , 24 ) ; break ; case 24 : case 25 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 26 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 27 ) ; break ; case 27 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 22 , 27 ) ; break ; case 28 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 12 , 17 ) ; break ; case 29 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; break ; case 30 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAdd ( 30 ) ; break ; case 31 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 24 , 26 ) ; break ; case 32 : if ( ( 0x600000000000L & l ) != 0L ) jjCheckNAdd ( 33 ) ; break ; case 33 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddStates ( 27 , 29 ) ; break ; case 35 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 35 , 36 ) ; break ; case 36 : if ( ( 0x600000000000L & l ) != 0L ) jjCheckNAdd ( 37 ) ; break ; case 37 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 5 ) kind = 5 ; jjCheckNAddTwoStates ( 36 , 37 ) ; break ; case 38 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 38 , 39 ) ; break ; case 39 : if ( curChar == 46 ) jjCheckNAdd ( 40 ) ; break ; case 40 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 6 ) kind = 6 ; jjCheckNAddTwoStates ( 39 , 40 ) ; break ; case 41 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 41 , 42 ) ; break ; case 42 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 43 , 44 ) ; break ; case 43 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 43 , 44 ) ; break ; case 44 : case 45 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAdd ( 45 ) ; break ; case 46 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 46 , 47 ) ; break ; case 47 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 48 , 49 ) ; break ; case 48 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 48 , 49 ) ; break ; case 49 : case 50 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 50 , 51 ) ; break ; case 51 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 52 ) ; break ; case 52 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 47 , 52 ) ; break ; case 53 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 53 , 54 ) ; break ; case 54 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 55 , 56 ) ; break ; case 55 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 55 , 56 ) ; break ; case 56 : case 57 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 57 , 58 ) ; break ; case 58 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAdd ( 59 ) ; break ; case 59 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 59 , 60 ) ; break ; case 60 : if ( ( 0xf00000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 61 , 62 ) ; break ; case 61 : if ( ( 0x3ff000000000000L & l ) != 0L ) jjCheckNAddTwoStates ( 61 , 62 ) ; break ; case 62 : case 63 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 58 , 63 ) ; break ; case 66 : if ( curChar == 39 ) jjstateSet [ jjnewStateCnt ++ ] = 67 ; break ; case 69 : if ( curChar == 46 ) jjCheckNAdd ( 70 ) ; break ; case 71 : if ( curChar != 46 ) break ; if ( kind > 3 ) kind = 3 ; jjCheckNAdd ( 70 ) ; break ; case 73 : if ( curChar == 38 ) jjstateSet [ jjnewStateCnt ++ ] = 74 ; break ; default : break ; } } while ( i != startsAt ) ; } else if ( curChar < 128 ) { long l = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 30 , 35 ) ; if ( ( 0x7fffffe07fffffeL & l ) != 0L ) { if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; } if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 18 , 23 ) ; break ; case 2 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 18 , 23 ) ; break ; case 3 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 3 , 4 ) ; break ; case 5 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjAddStates ( 36 , 37 ) ; break ; case 6 : if ( curChar == 95 ) jjCheckNAdd ( 7 ) ; break ; case 7 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAdd ( 7 ) ; break ; case 8 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 8 , 9 ) ; break ; case 10 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 10 , 11 ) ; break ; case 11 : if ( curChar == 95 ) jjCheckNAdd ( 12 ) ; break ; case 12 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 12 , 13 ) ; break ; case 13 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 14 , 15 ) ; break ; case 14 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 14 , 15 ) ; break ; case 16 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 11 , 16 ) ; break ; case 17 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 17 , 18 ) ; break ; case 19 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjAddStates ( 38 , 39 ) ; break ; case 20 : if ( curChar == 95 ) jjCheckNAdd ( 21 ) ; break ; case 21 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 21 , 22 ) ; break ; case 22 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 23 , 24 ) ; break ; case 23 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 23 , 24 ) ; break ; case 25 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjAddStates ( 40 , 41 ) ; break ; case 26 : if ( curChar == 95 ) jjCheckNAdd ( 27 ) ; break ; case 27 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 22 , 27 ) ; break ; case 29 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; break ; case 30 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAdd ( 30 ) ; break ; case 31 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 24 , 26 ) ; break ; case 32 : if ( curChar == 95 ) jjCheckNAdd ( 33 ) ; break ; case 33 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 27 , 29 ) ; break ; case 34 : if ( curChar == 64 ) jjCheckNAdd ( 35 ) ; break ; case 35 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 35 , 36 ) ; break ; case 37 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 5 ) kind = 5 ; jjCheckNAddTwoStates ( 36 , 37 ) ; break ; case 38 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 38 , 39 ) ; break ; case 40 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 6 ) kind = 6 ; jjCheckNAddTwoStates ( 39 , 40 ) ; break ; case 41 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 41 , 42 ) ; break ; case 42 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 43 , 44 ) ; break ; case 43 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 43 , 44 ) ; break ; case 45 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjstateSet [ jjnewStateCnt ++ ] = 45 ; break ; case 46 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 46 , 47 ) ; break ; case 47 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 48 , 49 ) ; break ; case 48 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 48 , 49 ) ; break ; case 50 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjAddStates ( 42 , 43 ) ; break ; case 51 : if ( curChar == 95 ) jjCheckNAdd ( 52 ) ; break ; case 52 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 47 , 52 ) ; break ; case 53 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 53 , 54 ) ; break ; case 54 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 55 , 56 ) ; break ; case 55 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 55 , 56 ) ; break ; case 57 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 57 , 58 ) ; break ; case 58 : if ( curChar == 95 ) jjCheckNAdd ( 59 ) ; break ; case 59 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 59 , 60 ) ; break ; case 60 : if ( curChar == 95 ) jjCheckNAddTwoStates ( 61 , 62 ) ; break ; case 61 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 61 , 62 ) ; break ; case 63 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 58 , 63 ) ; break ; case 64 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddStates ( 30 , 35 ) ; break ; case 65 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 65 , 66 ) ; break ; case 67 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 2 ) kind = 2 ; jjCheckNAddTwoStates ( 66 , 67 ) ; break ; case 68 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 68 , 69 ) ; break ; case 70 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjAddStates ( 44 , 45 ) ; break ; case 72 : if ( ( 0x7fffffe07fffffeL & l ) != 0L ) jjCheckNAddTwoStates ( 72 , 73 ) ; break ; case 73 : if ( curChar == 64 ) jjCheckNAdd ( 74 ) ; break ; case 74 : if ( ( 0x7fffffe07fffffeL & l ) == 0L ) break ; if ( kind > 4 ) kind = 4 ; jjCheckNAdd ( 74 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else { int hiByte = ( int ) ( curChar > > 8 ) ; int i1 = hiByte > > 6 ; long l1 = 1L << ( hiByte & 077 ) ; int i2 = ( curChar & 0xff ) > > 6 ; long l2 = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) { if ( kind > 12 ) kind = 12 ; } if ( jjCanMove_1 ( hiByte , i1 , i2 , l1 , l2 ) ) { if ( kind > 13 ) kind = 13 ; } if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 18 , 23 ) ; if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 12 , 17 ) ; if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) { if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; } if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 30 , 35 ) ; break ; case 1 : if ( jjCanMove_1 ( hiByte , i1 , i2 , l1 , l2 ) && kind > 13 ) kind = 13 ; break ; case 2 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 18 , 23 ) ; break ; case 3 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 3 , 4 ) ; break ; case 4 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 5 , 6 ) ; break ; case 5 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 5 , 6 ) ; break ; case 7 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjstateSet [ jjnewStateCnt ++ ] = 7 ; break ; case 8 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 8 , 9 ) ; break ; case 9 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 10 , 11 ) ; break ; case 10 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 10 , 11 ) ; break ; case 12 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 46 , 47 ) ; break ; case 14 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 48 , 49 ) ; break ; case 15 : if ( ! jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 11 , 16 ) ; break ; case 16 : if ( ! jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 11 , 16 ) ; break ; case 17 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 17 , 18 ) ; break ; case 18 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 19 , 20 ) ; break ; case 19 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 19 , 20 ) ; break ; case 21 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 21 , 22 ) ; break ; case 23 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 50 , 51 ) ; break ; case 24 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 25 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 27 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 22 , 27 ) ; break ; case 28 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 12 , 17 ) ; break ; case 29 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAddStates ( 0 , 11 ) ; break ; case 30 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 1 ) kind = 1 ; jjCheckNAdd ( 30 ) ; break ; case 31 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 24 , 26 ) ; break ; case 33 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 27 , 29 ) ; break ; case 35 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 35 , 36 ) ; break ; case 37 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 5 ) kind = 5 ; jjCheckNAddTwoStates ( 36 , 37 ) ; break ; case 38 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 38 , 39 ) ; break ; case 40 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 6 ) kind = 6 ; jjCheckNAddTwoStates ( 39 , 40 ) ; break ; case 41 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 41 , 42 ) ; break ; case 43 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 52 , 53 ) ; break ; case 44 : if ( ! jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAdd ( 45 ) ; break ; case 45 : if ( ! jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAdd ( 45 ) ; break ; case 46 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 46 , 47 ) ; break ; case 48 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 54 , 55 ) ; break ; case 49 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 50 , 51 ) ; break ; case 50 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 50 , 51 ) ; break ; case 52 : if ( ! jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 47 , 52 ) ; break ; case 53 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 53 , 54 ) ; break ; case 55 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 56 , 57 ) ; break ; case 56 : if ( jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 57 , 58 ) ; break ; case 57 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 57 , 58 ) ; break ; case 59 : if ( jjCanMove_4 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 58 , 59 ) ; break ; case 61 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 60 , 61 ) ; break ; case 62 : if ( ! jjCanMove_3 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 58 , 63 ) ; break ; case 63 : if ( ! jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 7 ) kind = 7 ; jjCheckNAddTwoStates ( 58 , 63 ) ; break ; case 64 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 30 , 35 ) ; break ; case 65 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 65 , 66 ) ; break ; case 67 : if ( ! jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 2 ) kind = 2 ; jjCheckNAddTwoStates ( 66 , 67 ) ; break ; case 68 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 68 , 69 ) ; break ; case 70 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 44 , 45 ) ; break ; case 72 : if ( jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddTwoStates ( 72 , 73 ) ; break ; case 74 : if ( ! jjCanMove_2 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 4 ) kind = 4 ; jjstateSet [ jjnewStateCnt ++ ] = 74 ; break ; default : break ; } } while ( i != startsAt ) ; } if ( kind != 0x7fffffff ) { jjmatchedKind = kind ; jjmatchedPos = curPos ; kind = 0x7fffffff ; } ++ curPos ; if ( ( i = jjnewStateCnt ) == ( startsAt = 75 - ( jjnewStateCnt = startsAt ) ) ) return curPos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return curPos ; } } } static final int [ ] jjnextStates = { 30 , 31 , 32 , 34 , 38 , 39 , 41 , 42 , 46 , 47 , 53 , 54 , 5 , 6 , 10 , 11 , 19 , 20 , 3 , 4 , 8 , 9 , 17 , 18 , 31 , 32 , 34 , 32 , 33 , 34 , 65 , 66 , 68 , 69 , 72 , 73 , 5 , 6 , 19 , 20 , 25 , 26 , 50 , 51 , 70 , 71 , 12 , 13 , 14 , 15 , 23 , 24 , 43 , 44 , 48 , 49 , 55 , 56 , 59 , 60 , 61 , 62 , } ; private static final boolean jjCanMove_0 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 48 : return ( ( jjbitVec2 [ i2 ] & l2 ) != 0L ) ; case 49 : return ( ( jjbitVec3 [ i2 ] & l2 ) != 0L ) ; case 51 : return ( ( jjbitVec4 [ i2 ] & l2 ) != 0L ) ; case 77 : return ( ( jjbitVec5 [ i2 ] & l2 ) != 0L ) ; case 255 : return ( ( jjbitVec6 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec0 [ i1 ] & l1 ) != 0L ) return true ; return false ; } } private static final boolean jjCanMove_1 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 215 : return ( ( jjbitVec8 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec7 [ i1 ] & l1 ) != 0L ) return true ; return false ; } } private static final boolean jjCanMove_2 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 0 : return ( ( jjbitVec10 [ i2 ] & l2 ) != 0L ) ; case 255 : return ( ( jjbitVec11 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec9 [ i1 ] & l1 ) != 0L ) return true ; return false ; } } private static final boolean jjCanMove_3 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 6 : return ( ( jjbitVec14 [ i2 ] & l2 ) != 0L ) ; case 11 : return ( ( jjbitVec15 [ i2 ] & l2 ) != 0L ) ; case 13 : return ( ( jjbitVec16 [ i2 ] & l2 ) != 0L ) ; case 14 : return ( ( jjbitVec17 [ i2 ] & l2 ) != 0L ) ; case 16 : return ( ( jjbitVec18 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec12 [ i1 ] & l1 ) != 0L ) if ( ( jjbitVec13 [ i2 ] & l2 ) == 0L ) return false ; else return true ; return false ; } } private static final boolean jjCanMove_4 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 0 : return ( ( jjbitVec10 [ i2 ] & l2 ) != 0L ) ; case 215 : return ( ( jjbitVec8 [ i2 ] & l2 ) != 0L ) ; case 255 : return ( ( jjbitVec11 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec19 [ i1 ] & l1 ) != 0L ) return true ; return false ; } } public static final String [ ] jjstrLiteralImages = { "" , null , null , null , null , null , null , null , null , null , null , null , null , null , null , null , } ; public static final String [ ] lexStateNames = { "DEFAULT" , } ; static final long [ ] jjtoToken = { 0x30ffL , } ; static final long [ ] jjtoSkip = { 0x8000L , } ; protected CharStream input_stream ; private final int [ ] jjrounds = new int [ 75 ] ; private final int [ ] jjstateSet = new int [ 150 ] ; protected char curChar ; public StandardTokenizerTokenManager ( CharStream stream ) { input_stream = stream ; } public StandardTokenizerTokenManager ( CharStream stream , int lexState ) { this ( stream ) ; SwitchTo ( lexState ) ; } public void ReInit ( CharStream stream ) { jjmatchedPos = jjnewStateCnt = 0 ; curLexState = defaultLexState ; input_stream = stream ; ReInitRounds ( ) ; } private final void ReInitRounds ( ) { int i ; jjround = 0x80000001 ; for ( i = 75 ; i -- > 0 ; ) jjrounds [ i ] = 0x80000000 ; } public void ReInit ( CharStream stream , int lexState ) { ReInit ( stream ) ; SwitchTo ( lexState ) ; } public void SwitchTo ( int lexState ) { if ( lexState >= 1 || lexState < 0 ) throw new TokenMgrError ( "Error: Ignoring invalid lexical state : " + lexState + ". State unchanged." , TokenMgrError . INVALID_LEXICAL_STATE ) ; else curLexState = lexState ; } protected Token jjFillToken ( ) { Token t = Token . newToken ( jjmatchedKind ) ; t . kind = jjmatchedKind ; String im = jjstrLiteralImages [ jjmatchedKind ] ; t . image = ( im == null ) ? input_stream . GetImage ( ) : im ; t . beginLine = input_stream . getBeginLine ( ) ; t . beginColumn = input_stream . getBeginColumn ( ) ; t . endLine = input_stream . getEndLine ( ) ; t . endColumn = input_stream . getEndColumn ( ) ; return t ; } int curLexState = 0 ; int defaultLexState = 0 ; int jjnewStateCnt ; int jjround ; int jjmatchedPos ; int jjmatchedKind ; public Token getNextToken ( ) { int kind ; Token specialToken = null ; Token matchedToken ; int curPos = 0 ; EOFLoop : for ( ; ; ) { try { curChar = input_stream . BeginToken ( ) ; } catch ( java . io . IOException e ) { jjmatchedKind = 0 ; matchedToken = jjFillToken ( ) ; return matchedToken ; } jjmatchedKind = 0x7fffffff ; jjmatchedPos = 0 ; curPos = jjMoveStringLiteralDfa0_0 ( ) ; if ( jjmatchedPos == 0 && jjmatchedKind > 15 ) { jjmatchedKind = 15 ; } if ( jjmatchedKind != 0x7fffffff ) { if ( jjmatchedPos + 1 < curPos ) input_stream . backup ( curPos - jjmatchedPos - 1 ) ; if ( ( jjtoToken [ jjmatchedKind > > 6 ] & ( 1L << ( jjmatchedKind & 077 ) ) ) != 0L ) { matchedToken = jjFillToken ( ) ; return matchedToken ; } else { continue EOFLoop ; } } int error_line = input_stream . getEndLine ( ) ; int error_column = input_stream . getEndColumn ( ) ; String error_after = null ; boolean EOFSeen = false ; try { input_stream . readChar ( ) ; input_stream . backup ( 1 ) ; } catch ( java . io . IOException e1 ) { EOFSeen = true ; error_after = curPos <= 1 ? "" : input_stream . GetImage ( ) ; if ( curChar == '\n' || curChar == '\r' ) { error_line ++ ; error_column = 0 ; } else error_column ++ ; } if ( ! EOFSeen ) { input_stream . backup ( 1 ) ; error_after = curPos <= 1 ? "" : input_stream . GetImage ( ) ; } throw new TokenMgrError ( EOFSeen , curLexState , error_line , error_column , error_after , curChar , TokenMgrError . LEXICAL_ERROR ) ; } } } 	0	['22', '1', '0', '5', '38', '153', '1', '4', '7', '0.850340136', '3785', '0.114285714', '1', '0', '0.380952381', '0', '0', '169.4545455', '236', '14.0455', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; public abstract class TokenStream { public abstract Token next ( ) throws IOException ; public void reset ( ) throws IOException { } public void close ( ) throws IOException { } } 	1	['4', '1', '2', '24', '5', '6', '23', '1', '4', '2', '9', '0', '0', '0', '1', '0', '0', '1.25', '1', '0.75', '4']
package org . apache . lucene . analysis . standard ; public class Token { public int kind ; public int beginLine , beginColumn , endLine , endColumn ; public String image ; public Token next ; public Token specialToken ; public String toString ( ) { return image ; } public static final Token newToken ( int ofKind ) { switch ( ofKind ) { default : return new Token ( ) ; } } } 	0	['3', '1', '0', '3', '4', '3', '3', '0', '3', '1.4375', '23', '0', '2', '0', '0.5', '0', '0', '4', '2', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermDocs ; import org . apache . lucene . index . TermEnum ; import java . io . IOException ; import java . util . BitSet ; public class RangeFilter extends Filter { private String fieldName ; private String lowerTerm ; private String upperTerm ; private boolean includeLower ; private boolean includeUpper ; public RangeFilter ( String fieldName , String lowerTerm , String upperTerm , boolean includeLower , boolean includeUpper ) { this . fieldName = fieldName ; this . lowerTerm = lowerTerm ; this . upperTerm = upperTerm ; this . includeLower = includeLower ; this . includeUpper = includeUpper ; if ( null == lowerTerm && null == upperTerm ) { throw new IllegalArgumentException ( "At least one value must be non-null" ) ; } if ( includeLower && null == lowerTerm ) { throw new IllegalArgumentException ( "The lower bound must be non-null to be inclusive" ) ; } if ( includeUpper && null == upperTerm ) { throw new IllegalArgumentException ( "The upper bound must be non-null to be inclusive" ) ; } } public static RangeFilter Less ( String fieldName , String upperTerm ) { return new RangeFilter ( fieldName , null , upperTerm , false , true ) ; } public static RangeFilter More ( String fieldName , String lowerTerm ) { return new RangeFilter ( fieldName , lowerTerm , null , true , false ) ; } public BitSet bits ( IndexReader reader ) throws IOException { BitSet bits = new BitSet ( reader . maxDoc ( ) ) ; TermEnum enumerator = ( null != lowerTerm ? reader . terms ( new Term ( fieldName , lowerTerm ) ) : reader . terms ( new Term ( fieldName , "" ) ) ) ; try { if ( enumerator . term ( ) == null ) { return bits ; } boolean checkLower = false ; if ( ! includeLower ) checkLower = true ; TermDocs termDocs = reader . termDocs ( ) ; try { do { Term term = enumerator . term ( ) ; if ( term != null && term . field ( ) . equals ( fieldName ) ) { if ( ! checkLower || null == lowerTerm || term . text ( ) . compareTo ( lowerTerm ) > 0 ) { checkLower = false ; if ( upperTerm != null ) { int compare = upperTerm . compareTo ( term . text ( ) ) ; if ( ( compare < 0 ) || ( ! includeUpper && compare == 0 ) ) { break ; } } termDocs . seek ( enumerator . term ( ) ) ; while ( termDocs . next ( ) ) { bits . set ( termDocs . doc ( ) ) ; } } } else { break ; } } while ( enumerator . next ( ) ) ; } finally { termDocs . close ( ) ; } } finally { enumerator . close ( ) ; } return bits ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( fieldName ) ; buffer . append ( ":" ) ; buffer . append ( includeLower ? "[" : "{" ) ; if ( null != lowerTerm ) { buffer . append ( lowerTerm ) ; } buffer . append ( "-" ) ; if ( null != upperTerm ) { buffer . append ( upperTerm ) ; } buffer . append ( includeUpper ? "]" : "}" ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof RangeFilter ) ) return false ; RangeFilter other = ( RangeFilter ) o ; if ( ! this . fieldName . equals ( other . fieldName ) || this . includeLower != other . includeLower || this . includeUpper != other . includeUpper ) { return false ; } if ( this . lowerTerm != null ? ! this . lowerTerm . equals ( other . lowerTerm ) : other . lowerTerm != null ) return false ; if ( this . upperTerm != null ? ! this . upperTerm . equals ( other . upperTerm ) : other . upperTerm != null ) return false ; return true ; } public int hashCode ( ) { int h = fieldName . hashCode ( ) ; h ^= lowerTerm != null ? lowerTerm . hashCode ( ) : 0xB6ECE882 ; h = ( h << 1 ) | ( h > > > 31 ) ; h ^= ( upperTerm != null ? ( upperTerm . hashCode ( ) ) : 0x91BEC2C2 ) ; h ^= ( includeLower ? 0xD484B933 : 0 ) ^ ( includeUpper ? 0x6AE423AC : 0 ) ; return h ; } } 	1	['7', '2', '0', '6', '30', '1', '1', '5', '7', '0', '373', '1', '0', '0.142857143', '0.314285714', '1', '1', '51.57142857', '12', '3.5714', '3']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermEnum ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; public class PrefixQuery extends Query { private Term prefix ; public PrefixQuery ( Term prefix ) { this . prefix = prefix ; } public Term getPrefix ( ) { return prefix ; } public Query rewrite ( IndexReader reader ) throws IOException { BooleanQuery query = new BooleanQuery ( true ) ; TermEnum enumerator = reader . terms ( prefix ) ; try { String prefixText = prefix . text ( ) ; String prefixField = prefix . field ( ) ; do { Term term = enumerator . term ( ) ; if ( term != null && term . text ( ) . startsWith ( prefixText ) && term . field ( ) == prefixField ) { TermQuery tq = new TermQuery ( term ) ; tq . setBoost ( getBoost ( ) ) ; query . add ( tq , BooleanClause . Occur . SHOULD ) ; } else { break ; } } while ( enumerator . next ( ) ) ; } finally { enumerator . close ( ) ; } return query ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! prefix . field ( ) . equals ( field ) ) { buffer . append ( prefix . field ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( prefix . text ( ) ) ; buffer . append ( '*' ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof PrefixQuery ) ) return false ; PrefixQuery other = ( PrefixQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && this . prefix . equals ( other . prefix ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ prefix . hashCode ( ) ^ 0x6634D93C ; } } 	0	['6', '2', '0', '9', '28', '0', '1', '8', '6', '0', '147', '1', '1', '0.705882353', '0.333333333', '2', '3', '23.33333333', '4', '1.5', '0']
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . FieldCache ; import java . io . IOException ; public class ReverseOrdFieldSource extends ValueSource { public String field ; public ReverseOrdFieldSource ( String field ) { this . field = field ; } public String description ( ) { return "rord(" + field + ')' ; } public DocValues getValues ( IndexReader reader ) throws IOException { final FieldCache . StringIndex sindex = FieldCache . DEFAULT . getStringIndex ( reader , field ) ; final int arr [ ] = sindex . order ; final int end = sindex . lookup . length ; return new DocValues ( arr . length ) { public float floatVal ( int doc ) { return ( float ) ( end - arr [ doc ] ) ; } public int intVal ( int doc ) { return end - arr [ doc ] ; } public String strVal ( int doc ) { return Integer . toString ( intVal ( doc ) ) ; } public String toString ( int doc ) { return description ( ) + '=' + strVal ( doc ) ; } Object getInnerArray ( ) { return arr ; } } ; } public boolean equals ( Object o ) { if ( o . getClass ( ) != ReverseOrdFieldSource . class ) return false ; ReverseOrdFieldSource other = ( ReverseOrdFieldSource ) o ; return this . field . equals ( other . field ) ; } private static final int hcode = ReverseOrdFieldSource . class . hashCode ( ) ; public int hashCode ( ) { return hcode + field . hashCode ( ) ; } } 	1	['7', '2', '0', '6', '21', '0', '1', '6', '5', '0.666666667', '99', '0.333333333', '0', '0.5', '0.375', '2', '2', '12.71428571', '3', '1', '2']
package org . apache . lucene . analysis ; import java . io . Reader ; public final class LowerCaseTokenizer extends LetterTokenizer { public LowerCaseTokenizer ( Reader in ) { super ( in ) ; } protected char normalize ( char c ) { return Character . toLowerCase ( c ) ; } } 	0	['2', '5', '0', '3', '4', '1', '2', '1', '1', '2', '9', '0', '0', '0.888888889', '0.666666667', '1', '1', '3.5', '1', '0.5', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; public final class PorterStemFilter extends TokenFilter { private PorterStemmer stemmer ; public PorterStemFilter ( TokenStream in ) { super ( in ) ; stemmer = new PorterStemmer ( ) ; } public final Token next ( ) throws IOException { Token token = input . next ( ) ; if ( token == null ) return null ; else { String s = stemmer . stem ( token . termText ) ; if ( s != token . termText ) token . termText = s ; return token ; } } } 	1	['2', '3', '0', '4', '6', '0', '0', '4', '2', '0', '35', '1', '1', '0.8', '0.75', '0', '0', '16', '1', '0.5', '1']
package org . apache . lucene . util ; import java . io . IOException ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . IndexOutput ; public final class BitVector { private byte [ ] bits ; private int size ; private int count = - 1 ; public BitVector ( int n ) { size = n ; bits = new byte [ ( size > > 3 ) + 1 ] ; } public final void set ( int bit ) { if ( bit >= size ) { throw new ArrayIndexOutOfBoundsException ( bit ) ; } bits [ bit > > 3 ] |= 1 << ( bit & 7 ) ; count = - 1 ; } public final void clear ( int bit ) { if ( bit >= size ) { throw new ArrayIndexOutOfBoundsException ( bit ) ; } bits [ bit > > 3 ] &= ~ ( 1 << ( bit & 7 ) ) ; count = - 1 ; } public final boolean get ( int bit ) { if ( bit >= size ) { throw new ArrayIndexOutOfBoundsException ( bit ) ; } return ( bits [ bit > > 3 ] & ( 1 << ( bit & 7 ) ) ) != 0 ; } public final int size ( ) { return size ; } public final int count ( ) { if ( count == - 1 ) { int c = 0 ; int end = bits . length ; for ( int i = 0 ; i < end ; i ++ ) c += BYTE_COUNTS [ bits [ i ] & 0xFF ] ; count = c ; } return count ; } private static final byte [ ] BYTE_COUNTS = { 0 , 1 , 1 , 2 , 1 , 2 , 2 , 3 , 1 , 2 , 2 , 3 , 2 , 3 , 3 , 4 , 1 , 2 , 2 , 3 , 2 , 3 , 3 , 4 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 1 , 2 , 2 , 3 , 2 , 3 , 3 , 4 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 1 , 2 , 2 , 3 , 2 , 3 , 3 , 4 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 4 , 5 , 5 , 6 , 5 , 6 , 6 , 7 , 1 , 2 , 2 , 3 , 2 , 3 , 3 , 4 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 4 , 5 , 5 , 6 , 5 , 6 , 6 , 7 , 2 , 3 , 3 , 4 , 3 , 4 , 4 , 5 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 4 , 5 , 5 , 6 , 5 , 6 , 6 , 7 , 3 , 4 , 4 , 5 , 4 , 5 , 5 , 6 , 4 , 5 , 5 , 6 , 5 , 6 , 6 , 7 , 4 , 5 , 5 , 6 , 5 , 6 , 6 , 7 , 5 , 6 , 6 , 7 , 6 , 7 , 7 , 8 } ; public final void write ( Directory d , String name ) throws IOException { IndexOutput output = d . createOutput ( name ) ; try { if ( isSparse ( ) ) { writeDgaps ( output ) ; } else { writeBits ( output ) ; } } finally { output . close ( ) ; } } private void writeBits ( IndexOutput output ) throws IOException { output . writeInt ( size ( ) ) ; output . writeInt ( count ( ) ) ; output . writeBytes ( bits , bits . length ) ; } private void writeDgaps ( IndexOutput output ) throws IOException { output . writeInt ( - 1 ) ; output . writeInt ( size ( ) ) ; output . writeInt ( count ( ) ) ; int last = 0 ; int n = count ( ) ; int m = bits . length ; for ( int i = 0 ; i < m && n > 0 ; i ++ ) { if ( bits [ i ] != 0 ) { output . writeVInt ( i - last ) ; output . writeByte ( bits [ i ] ) ; last = i ; n -= BYTE_COUNTS [ bits [ i ] & 0xFF ] ; } } } private boolean isSparse ( ) { int factor = 10 ; if ( bits . length < ( 1 << 7 ) ) return factor * ( 4 + ( 8 + 8 ) * count ( ) ) < size ( ) ; if ( bits . length < ( 1 << 14 ) ) return factor * ( 4 + ( 8 + 16 ) * count ( ) ) < size ( ) ; if ( bits . length < ( 1 << 21 ) ) return factor * ( 4 + ( 8 + 24 ) * count ( ) ) < size ( ) ; if ( bits . length < ( 1 << 28 ) ) return factor * ( 4 + ( 8 + 32 ) * count ( ) ) < size ( ) ; return factor * ( 4 + ( 8 + 40 ) * count ( ) ) < size ( ) ; } public BitVector ( Directory d , String name ) throws IOException { IndexInput input = d . openInput ( name ) ; try { size = input . readInt ( ) ; if ( size == - 1 ) { readDgaps ( input ) ; } else { readBits ( input ) ; } } finally { input . close ( ) ; } } private void readBits ( IndexInput input ) throws IOException { count = input . readInt ( ) ; bits = new byte [ ( size > > 3 ) + 1 ] ; input . readBytes ( bits , 0 , bits . length ) ; } private void readDgaps ( IndexInput input ) throws IOException { size = input . readInt ( ) ; count = input . readInt ( ) ; bits = new byte [ ( size > > 3 ) + 1 ] ; int last = 0 ; int n = count ( ) ; while ( n > 0 ) { last += input . readVInt ( ) ; bits [ last ] = input . readByte ( ) ; n -= BYTE_COUNTS [ bits [ last ] & 0xFF ] ; } } } 	0	['14', '1', '0', '5', '28', '0', '2', '3', '8', '0.288461538', '1483', '1', '0', '0', '0.320512821', '0', '0', '104.6428571', '10', '1.8571', '0']
package org . apache . lucene . index ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . document . Document ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . FSDirectory ; import org . apache . lucene . store . LockObtainFailedException ; import java . io . File ; import java . io . IOException ; import java . io . PrintStream ; public class IndexModifier { protected IndexWriter indexWriter = null ; protected IndexReader indexReader = null ; protected Directory directory = null ; protected Analyzer analyzer = null ; protected boolean open = false ; protected PrintStream infoStream = null ; protected boolean useCompoundFile = true ; protected int maxBufferedDocs = IndexWriter . DEFAULT_MAX_BUFFERED_DOCS ; protected int maxFieldLength = IndexWriter . DEFAULT_MAX_FIELD_LENGTH ; protected int mergeFactor = IndexWriter . DEFAULT_MERGE_FACTOR ; public IndexModifier ( Directory directory , Analyzer analyzer , boolean create ) throws CorruptIndexException , LockObtainFailedException , IOException { init ( directory , analyzer , create ) ; } public IndexModifier ( String dirName , Analyzer analyzer , boolean create ) throws CorruptIndexException , LockObtainFailedException , IOException { Directory dir = FSDirectory . getDirectory ( dirName ) ; init ( dir , analyzer , create ) ; } public IndexModifier ( File file , Analyzer analyzer , boolean create ) throws CorruptIndexException , LockObtainFailedException , IOException { Directory dir = FSDirectory . getDirectory ( file ) ; init ( dir , analyzer , create ) ; } protected void init ( Directory directory , Analyzer analyzer , boolean create ) throws CorruptIndexException , LockObtainFailedException , IOException { this . directory = directory ; synchronized ( this . directory ) { this . analyzer = analyzer ; indexWriter = new IndexWriter ( directory , analyzer , create ) ; open = true ; } } protected void assureOpen ( ) { if ( ! open ) { throw new IllegalStateException ( "Index is closed" ) ; } } protected void createIndexWriter ( ) throws CorruptIndexException , LockObtainFailedException , IOException { if ( indexWriter == null ) { if ( indexReader != null ) { indexReader . close ( ) ; indexReader = null ; } indexWriter = new IndexWriter ( directory , analyzer , false ) ; indexWriter . setInfoStream ( infoStream ) ; indexWriter . setUseCompoundFile ( useCompoundFile ) ; indexWriter . setMaxBufferedDocs ( maxBufferedDocs ) ; indexWriter . setMaxFieldLength ( maxFieldLength ) ; indexWriter . setMergeFactor ( mergeFactor ) ; } } protected void createIndexReader ( ) throws CorruptIndexException , IOException { if ( indexReader == null ) { if ( indexWriter != null ) { indexWriter . close ( ) ; indexWriter = null ; } indexReader = IndexReader . open ( directory ) ; } } public void flush ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . close ( ) ; indexWriter = null ; createIndexWriter ( ) ; } else { indexReader . close ( ) ; indexReader = null ; createIndexReader ( ) ; } } } public void addDocument ( Document doc , Analyzer docAnalyzer ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; if ( docAnalyzer != null ) indexWriter . addDocument ( doc , docAnalyzer ) ; else indexWriter . addDocument ( doc ) ; } } public void addDocument ( Document doc ) throws CorruptIndexException , LockObtainFailedException , IOException { addDocument ( doc , null ) ; } public int deleteDocuments ( Term term ) throws StaleReaderException , CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexReader ( ) ; return indexReader . deleteDocuments ( term ) ; } } public void deleteDocument ( int docNum ) throws StaleReaderException , CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexReader ( ) ; indexReader . deleteDocument ( docNum ) ; } } public int docCount ( ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { return indexWriter . docCount ( ) ; } else { return indexReader . numDocs ( ) ; } } } public void optimize ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; indexWriter . optimize ( ) ; } } public void setInfoStream ( PrintStream infoStream ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . setInfoStream ( infoStream ) ; } this . infoStream = infoStream ; } } public PrintStream getInfoStream ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; return indexWriter . getInfoStream ( ) ; } } public void setUseCompoundFile ( boolean useCompoundFile ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . setUseCompoundFile ( useCompoundFile ) ; } this . useCompoundFile = useCompoundFile ; } } public boolean getUseCompoundFile ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; return indexWriter . getUseCompoundFile ( ) ; } } public void setMaxFieldLength ( int maxFieldLength ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . setMaxFieldLength ( maxFieldLength ) ; } this . maxFieldLength = maxFieldLength ; } } public int getMaxFieldLength ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; return indexWriter . getMaxFieldLength ( ) ; } } public void setMaxBufferedDocs ( int maxBufferedDocs ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . setMaxBufferedDocs ( maxBufferedDocs ) ; } this . maxBufferedDocs = maxBufferedDocs ; } } public int getMaxBufferedDocs ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; return indexWriter . getMaxBufferedDocs ( ) ; } } public void setMergeFactor ( int mergeFactor ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . setMergeFactor ( mergeFactor ) ; } this . mergeFactor = mergeFactor ; } } public int getMergeFactor ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; return indexWriter . getMergeFactor ( ) ; } } public void close ( ) throws CorruptIndexException , IOException { synchronized ( directory ) { if ( ! open ) throw new IllegalStateException ( "Index is closed already" ) ; if ( indexWriter != null ) { indexWriter . close ( ) ; indexWriter = null ; } else { indexReader . close ( ) ; indexReader = null ; } open = false ; } } public String toString ( ) { return "Index@" + directory ; } } 	1	['26', '1', '0', '10', '55', '0', '0', '10', '22', '0.344', '705', '1', '4', '0', '0.184615385', '0', '0', '25.73076923', '2', '1.1538', '5']
package org . apache . lucene . index ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . analysis . TokenStream ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . search . Similarity ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; import java . io . IOException ; import java . io . PrintStream ; import java . io . Reader ; import java . io . StringReader ; import java . util . Arrays ; import java . util . BitSet ; import java . util . Enumeration ; import java . util . Hashtable ; import java . util . Iterator ; import java . util . LinkedList ; import java . util . List ; final class DocumentWriter { private Analyzer analyzer ; private Directory directory ; private Similarity similarity ; private FieldInfos fieldInfos ; private int maxFieldLength ; private int termIndexInterval = IndexWriter . DEFAULT_TERM_INDEX_INTERVAL ; private PrintStream infoStream ; DocumentWriter ( Directory directory , Analyzer analyzer , Similarity similarity , int maxFieldLength ) { this . directory = directory ; this . analyzer = analyzer ; this . similarity = similarity ; this . maxFieldLength = maxFieldLength ; } DocumentWriter ( Directory directory , Analyzer analyzer , IndexWriter writer ) { this . directory = directory ; this . analyzer = analyzer ; this . similarity = writer . getSimilarity ( ) ; this . maxFieldLength = writer . getMaxFieldLength ( ) ; this . termIndexInterval = writer . getTermIndexInterval ( ) ; } final void addDocument ( String segment , Document doc ) throws CorruptIndexException , IOException { fieldInfos = new FieldInfos ( ) ; fieldInfos . add ( doc ) ; postingTable . clear ( ) ; fieldLengths = new int [ fieldInfos . size ( ) ] ; fieldPositions = new int [ fieldInfos . size ( ) ] ; fieldOffsets = new int [ fieldInfos . size ( ) ] ; fieldStoresPayloads = new BitSet ( fieldInfos . size ( ) ) ; fieldBoosts = new float [ fieldInfos . size ( ) ] ; Arrays . fill ( fieldBoosts , doc . getBoost ( ) ) ; try { invertDocument ( doc ) ; Posting [ ] postings = sortPostingTable ( ) ; fieldInfos . write ( directory , segment + ".fnm" ) ; FieldsWriter fieldsWriter = new FieldsWriter ( directory , segment , fieldInfos ) ; try { fieldsWriter . addDocument ( doc ) ; } finally { fieldsWriter . close ( ) ; } writePostings ( postings , segment ) ; writeNorms ( segment ) ; } finally { IOException ex = null ; Iterator it = openTokenStreams . iterator ( ) ; while ( it . hasNext ( ) ) { try { ( ( TokenStream ) it . next ( ) ) . close ( ) ; } catch ( IOException e ) { if ( ex != null ) { ex = e ; } } } openTokenStreams . clear ( ) ; if ( ex != null ) { throw ex ; } } } private final Hashtable postingTable = new Hashtable ( ) ; private int [ ] fieldLengths ; private int [ ] fieldPositions ; private int [ ] fieldOffsets ; private float [ ] fieldBoosts ; private BitSet fieldStoresPayloads ; private List openTokenStreams = new LinkedList ( ) ; private final void invertDocument ( Document doc ) throws IOException { Iterator fieldIterator = doc . getFields ( ) . iterator ( ) ; while ( fieldIterator . hasNext ( ) ) { Fieldable field = ( Fieldable ) fieldIterator . next ( ) ; String fieldName = field . name ( ) ; int fieldNumber = fieldInfos . fieldNumber ( fieldName ) ; int length = fieldLengths [ fieldNumber ] ; int position = fieldPositions [ fieldNumber ] ; if ( length > 0 ) position += analyzer . getPositionIncrementGap ( fieldName ) ; int offset = fieldOffsets [ fieldNumber ] ; if ( field . isIndexed ( ) ) { if ( ! field . isTokenized ( ) ) { String stringValue = field . stringValue ( ) ; if ( field . isStoreOffsetWithTermVector ( ) ) addPosition ( fieldName , stringValue , position ++ , null , new TermVectorOffsetInfo ( offset , offset + stringValue . length ( ) ) ) ; else addPosition ( fieldName , stringValue , position ++ , null , null ) ; offset += stringValue . length ( ) ; length ++ ; } else { TokenStream stream = field . tokenStreamValue ( ) ; if ( stream == null ) { Reader reader ; if ( field . readerValue ( ) != null ) reader = field . readerValue ( ) ; else if ( field . stringValue ( ) != null ) reader = new StringReader ( field . stringValue ( ) ) ; else throw new IllegalArgumentException ( "field must have either String or Reader value" ) ; stream = analyzer . tokenStream ( fieldName , reader ) ; } openTokenStreams . add ( stream ) ; stream . reset ( ) ; Token lastToken = null ; for ( Token t = stream . next ( ) ; t != null ; t = stream . next ( ) ) { position += ( t . getPositionIncrement ( ) - 1 ) ; Payload payload = t . getPayload ( ) ; if ( payload != null ) { fieldStoresPayloads . set ( fieldNumber ) ; } TermVectorOffsetInfo termVectorOffsetInfo ; if ( field . isStoreOffsetWithTermVector ( ) ) { termVectorOffsetInfo = new TermVectorOffsetInfo ( offset + t . startOffset ( ) , offset + t . endOffset ( ) ) ; } else { termVectorOffsetInfo = null ; } addPosition ( fieldName , t . termText ( ) , position ++ , payload , termVectorOffsetInfo ) ; lastToken = t ; if ( ++ length >= maxFieldLength ) { if ( infoStream != null ) infoStream . println ( "maxFieldLength " + maxFieldLength + " reached, ignoring following tokens" ) ; break ; } } if ( lastToken != null ) offset += lastToken . endOffset ( ) + 1 ; } fieldLengths [ fieldNumber ] = length ; fieldPositions [ fieldNumber ] = position ; fieldBoosts [ fieldNumber ] *= field . getBoost ( ) ; fieldOffsets [ fieldNumber ] = offset ; } } for ( int i = fieldStoresPayloads . nextSetBit ( 0 ) ; i >= 0 ; i = fieldStoresPayloads . nextSetBit ( i + 1 ) ) { fieldInfos . fieldInfo ( i ) . storePayloads = true ; } } private final Term termBuffer = new Term ( "" , "" ) ; private final void addPosition ( String field , String text , int position , Payload payload , TermVectorOffsetInfo offset ) { termBuffer . set ( field , text ) ; Posting ti = ( Posting ) postingTable . get ( termBuffer ) ; if ( ti != null ) { int freq = ti . freq ; if ( ti . positions . length == freq ) { int [ ] newPositions = new int [ freq * 2 ] ; int [ ] positions = ti . positions ; System . arraycopy ( positions , 0 , newPositions , 0 , freq ) ; ti . positions = newPositions ; if ( ti . payloads != null ) { Payload [ ] newPayloads = new Payload [ freq * 2 ] ; Payload [ ] payloads = ti . payloads ; System . arraycopy ( payloads , 0 , newPayloads , 0 , payloads . length ) ; ti . payloads = newPayloads ; } } ti . positions [ freq ] = position ; if ( payload != null ) { if ( ti . payloads == null ) { ti . payloads = new Payload [ ti . positions . length ] ; } ti . payloads [ freq ] = payload ; } if ( offset != null ) { if ( ti . offsets . length == freq ) { TermVectorOffsetInfo [ ] newOffsets = new TermVectorOffsetInfo [ freq * 2 ] ; TermVectorOffsetInfo [ ] offsets = ti . offsets ; System . arraycopy ( offsets , 0 , newOffsets , 0 , freq ) ; ti . offsets = newOffsets ; } ti . offsets [ freq ] = offset ; } ti . freq = freq + 1 ; } else { Term term = new Term ( field , text , false ) ; postingTable . put ( term , new Posting ( term , position , payload , offset ) ) ; } } private final Posting [ ] sortPostingTable ( ) { Posting [ ] array = new Posting [ postingTable . size ( ) ] ; Enumeration postings = postingTable . elements ( ) ; for ( int i = 0 ; postings . hasMoreElements ( ) ; i ++ ) array [ i ] = ( Posting ) postings . nextElement ( ) ; quickSort ( array , 0 , array . length - 1 ) ; return array ; } private static final void quickSort ( Posting [ ] postings , int lo , int hi ) { if ( lo >= hi ) return ; int mid = ( lo + hi ) / 2 ; if ( postings [ lo ] . term . compareTo ( postings [ mid ] . term ) > 0 ) { Posting tmp = postings [ lo ] ; postings [ lo ] = postings [ mid ] ; postings [ mid ] = tmp ; } if ( postings [ mid ] . term . compareTo ( postings [ hi ] . term ) > 0 ) { Posting tmp = postings [ mid ] ; postings [ mid ] = postings [ hi ] ; postings [ hi ] = tmp ; if ( postings [ lo ] . term . compareTo ( postings [ mid ] . term ) > 0 ) { Posting tmp2 = postings [ lo ] ; postings [ lo ] = postings [ mid ] ; postings [ mid ] = tmp2 ; } } int left = lo + 1 ; int right = hi - 1 ; if ( left >= right ) return ; Term partition = postings [ mid ] . term ; for ( ; ; ) { while ( postings [ right ] . term . compareTo ( partition ) > 0 ) -- right ; while ( left < right && postings [ left ] . term . compareTo ( partition ) <= 0 ) ++ left ; if ( left < right ) { Posting tmp = postings [ left ] ; postings [ left ] = postings [ right ] ; postings [ right ] = tmp ; -- right ; } else { break ; } } quickSort ( postings , lo , left ) ; quickSort ( postings , left + 1 , hi ) ; } private final void writePostings ( Posting [ ] postings , String segment ) throws CorruptIndexException , IOException { IndexOutput freq = null , prox = null ; TermInfosWriter tis = null ; TermVectorsWriter termVectorWriter = null ; try { freq = directory . createOutput ( segment + ".frq" ) ; prox = directory . createOutput ( segment + ".prx" ) ; tis = new TermInfosWriter ( directory , segment , fieldInfos , termIndexInterval ) ; TermInfo ti = new TermInfo ( ) ; String currentField = null ; boolean currentFieldHasPayloads = false ; for ( int i = 0 ; i < postings . length ; i ++ ) { Posting posting = postings [ i ] ; String termField = posting . term . field ( ) ; if ( currentField != termField ) { currentField = termField ; FieldInfo fi = fieldInfos . fieldInfo ( currentField ) ; currentFieldHasPayloads = fi . storePayloads ; if ( fi . storeTermVector ) { if ( termVectorWriter == null ) { termVectorWriter = new TermVectorsWriter ( directory , segment , fieldInfos ) ; termVectorWriter . openDocument ( ) ; } termVectorWriter . openField ( currentField ) ; } else if ( termVectorWriter != null ) { termVectorWriter . closeField ( ) ; } } ti . set ( 1 , freq . getFilePointer ( ) , prox . getFilePointer ( ) , - 1 ) ; tis . add ( posting . term , ti ) ; int postingFreq = posting . freq ; if ( postingFreq == 1 ) freq . writeVInt ( 1 ) ; else { freq . writeVInt ( 0 ) ; freq . writeVInt ( postingFreq ) ; } int lastPosition = 0 ; int [ ] positions = posting . positions ; Payload [ ] payloads = posting . payloads ; int lastPayloadLength = - 1 ; for ( int j = 0 ; j < postingFreq ; j ++ ) { int position = positions [ j ] ; int delta = position - lastPosition ; if ( currentFieldHasPayloads ) { int payloadLength = 0 ; Payload payload = null ; if ( payloads != null ) { payload = payloads [ j ] ; if ( payload != null ) { payloadLength = payload . length ; } } if ( payloadLength == lastPayloadLength ) { prox . writeVInt ( delta * 2 ) ; } else { prox . writeVInt ( delta * 2 + 1 ) ; prox . writeVInt ( payloadLength ) ; lastPayloadLength = payloadLength ; } if ( payloadLength > 0 ) { prox . writeBytes ( payload . data , payload . offset , payload . length ) ; } } else { prox . writeVInt ( delta ) ; } lastPosition = position ; } if ( termVectorWriter != null && termVectorWriter . isFieldOpen ( ) ) { termVectorWriter . addTerm ( posting . term . text ( ) , postingFreq , posting . positions , posting . offsets ) ; } } if ( termVectorWriter != null ) termVectorWriter . closeDocument ( ) ; } finally { IOException keep = null ; if ( freq != null ) try { freq . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( prox != null ) try { prox . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tis != null ) try { tis . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( termVectorWriter != null ) try { termVectorWriter . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( keep != null ) throw ( IOException ) keep . fillInStackTrace ( ) ; } } private final void writeNorms ( String segment ) throws IOException { for ( int n = 0 ; n < fieldInfos . size ( ) ; n ++ ) { FieldInfo fi = fieldInfos . fieldInfo ( n ) ; if ( fi . isIndexed && ! fi . omitNorms ) { float norm = fieldBoosts [ n ] * similarity . lengthNorm ( fi . name , fieldLengths [ n ] ) ; IndexOutput norms = directory . createOutput ( segment + ".f" + n ) ; try { norms . writeByte ( Similarity . encodeNorm ( norm ) ) ; } finally { norms . close ( ) ; } } } } void setInfoStream ( PrintStream infoStream ) { this . infoStream = infoStream ; } int getNumFields ( ) { return fieldInfos . size ( ) ; } } final class Posting { Term term ; int freq ; int [ ] positions ; Payload [ ] payloads ; TermVectorOffsetInfo [ ] offsets ; Posting ( Term t , int position , Payload payload , TermVectorOffsetInfo offset ) { term = t ; freq = 1 ; positions = new int [ 1 ] ; positions [ 0 ] = position ; if ( payload != null ) { payloads = new Payload [ 1 ] ; payloads [ 0 ] = payload ; } else payloads = null ; if ( offset != null ) { offsets = new TermVectorOffsetInfo [ 1 ] ; offsets [ 0 ] = offset ; } else offsets = null ; } } 	0	['11', '1', '0', '20', '102', '1', '1', '20', '0', '0.733333333', '1192', '1', '5', '0', '0.227272727', '0', '0', '106', '10', '2.3636', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexInput ; final class SegmentTermEnum extends TermEnum implements Cloneable { private IndexInput input ; FieldInfos fieldInfos ; long size ; long position = - 1 ; private TermBuffer termBuffer = new TermBuffer ( ) ; private TermBuffer prevBuffer = new TermBuffer ( ) ; private TermBuffer scratch ; private TermInfo termInfo = new TermInfo ( ) ; private int format ; private boolean isIndex = false ; long indexPointer = 0 ; int indexInterval ; int skipInterval ; int maxSkipLevels ; private int formatM1SkipInterval ; SegmentTermEnum ( IndexInput i , FieldInfos fis , boolean isi ) throws CorruptIndexException , IOException { input = i ; fieldInfos = fis ; isIndex = isi ; maxSkipLevels = 1 ; int firstInt = input . readInt ( ) ; if ( firstInt >= 0 ) { format = 0 ; size = firstInt ; indexInterval = 128 ; skipInterval = Integer . MAX_VALUE ; } else { format = firstInt ; if ( format < TermInfosWriter . FORMAT ) throw new CorruptIndexException ( "Unknown format version:" + format ) ; size = input . readLong ( ) ; if ( format == - 1 ) { if ( ! isIndex ) { indexInterval = input . readInt ( ) ; formatM1SkipInterval = input . readInt ( ) ; } skipInterval = Integer . MAX_VALUE ; } else { indexInterval = input . readInt ( ) ; skipInterval = input . readInt ( ) ; if ( format == - 3 ) { maxSkipLevels = input . readInt ( ) ; } } } } protected Object clone ( ) { SegmentTermEnum clone = null ; try { clone = ( SegmentTermEnum ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . input = ( IndexInput ) input . clone ( ) ; clone . termInfo = new TermInfo ( termInfo ) ; clone . termBuffer = ( TermBuffer ) termBuffer . clone ( ) ; clone . prevBuffer = ( TermBuffer ) prevBuffer . clone ( ) ; clone . scratch = null ; return clone ; } final void seek ( long pointer , int p , Term t , TermInfo ti ) throws IOException { input . seek ( pointer ) ; position = p ; termBuffer . set ( t ) ; prevBuffer . reset ( ) ; termInfo . set ( ti ) ; } public final boolean next ( ) throws IOException { if ( position ++ >= size - 1 ) { termBuffer . reset ( ) ; return false ; } prevBuffer . set ( termBuffer ) ; termBuffer . read ( input , fieldInfos ) ; termInfo . docFreq = input . readVInt ( ) ; termInfo . freqPointer += input . readVLong ( ) ; termInfo . proxPointer += input . readVLong ( ) ; if ( format == - 1 ) { if ( ! isIndex ) { if ( termInfo . docFreq > formatM1SkipInterval ) { termInfo . skipOffset = input . readVInt ( ) ; } } } else { if ( termInfo . docFreq >= skipInterval ) termInfo . skipOffset = input . readVInt ( ) ; } if ( isIndex ) indexPointer += input . readVLong ( ) ; return true ; } final void scanTo ( Term term ) throws IOException { if ( scratch == null ) scratch = new TermBuffer ( ) ; scratch . set ( term ) ; while ( scratch . compareTo ( termBuffer ) > 0 && next ( ) ) { } } public final Term term ( ) { return termBuffer . toTerm ( ) ; } final Term prev ( ) { return prevBuffer . toTerm ( ) ; } final TermInfo termInfo ( ) { return new TermInfo ( termInfo ) ; } final void termInfo ( TermInfo ti ) { ti . set ( termInfo ) ; } public final int docFreq ( ) { return termInfo . docFreq ; } final long freqPointer ( ) { return termInfo . freqPointer ; } final long proxPointer ( ) { return termInfo . proxPointer ; } public final void close ( ) throws IOException { input . close ( ) ; } } 	1	['13', '2', '0', '10', '38', '0', '3', '7', '4', '0.766666667', '367', '0.533333333', '6', '0.294117647', '0.211538462', '1', '2', '26.07692308', '1', '0.9231', '3']
package org . apache . lucene . util ; public final class Constants { private Constants ( ) { } public static final String JAVA_VERSION = System . getProperty ( "java.version" ) ; public static final boolean JAVA_1_1 = JAVA_VERSION . startsWith ( "1.1." ) ; public static final boolean JAVA_1_2 = JAVA_VERSION . startsWith ( "1.2." ) ; public static final boolean JAVA_1_3 = JAVA_VERSION . startsWith ( "1.3." ) ; public static final String OS_NAME = System . getProperty ( "os.name" ) ; public static final boolean LINUX = OS_NAME . startsWith ( "Linux" ) ; public static final boolean WINDOWS = OS_NAME . startsWith ( "Windows" ) ; public static final boolean SUN_OS = OS_NAME . startsWith ( "SunOS" ) ; } 	0	['2', '1', '0', '0', '5', '1', '0', '0', '0', '1', '44', '0', '0', '0', '1', '0', '0', '17', '0', '0', '0']
package org . apache . lucene . search . function ; import java . io . IOException ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . ComplexExplanation ; import org . apache . lucene . search . Explanation ; import org . apache . lucene . search . Query ; import org . apache . lucene . search . Scorer ; import org . apache . lucene . search . Searcher ; import org . apache . lucene . search . Similarity ; import org . apache . lucene . search . Weight ; import org . apache . lucene . util . ToStringUtils ; public class CustomScoreQuery extends Query { private Query subQuery ; private ValueSourceQuery valSrcQuery ; private boolean strict = false ; public CustomScoreQuery ( Query subQuery ) { this ( subQuery , null ) ; } public CustomScoreQuery ( Query subQuery , ValueSourceQuery valSrcQuery ) { super ( ) ; this . subQuery = subQuery ; this . valSrcQuery = valSrcQuery ; if ( subQuery == null ) throw new IllegalArgumentException ( "<subqyery> must not be null!" ) ; } public Query rewrite ( IndexReader reader ) throws IOException { subQuery = subQuery . rewrite ( reader ) ; if ( valSrcQuery != null ) { valSrcQuery = ( ValueSourceQuery ) valSrcQuery . rewrite ( reader ) ; } return this ; } public void extractTerms ( Set terms ) { subQuery . extractTerms ( terms ) ; if ( valSrcQuery != null ) { valSrcQuery . extractTerms ( terms ) ; } } public Object clone ( ) { CustomScoreQuery clone = ( CustomScoreQuery ) super . clone ( ) ; clone . subQuery = ( Query ) subQuery . clone ( ) ; if ( valSrcQuery != null ) { clone . valSrcQuery = ( ValueSourceQuery ) valSrcQuery . clone ( ) ; } return clone ; } public String toString ( String field ) { StringBuffer sb = new StringBuffer ( name ( ) ) . append ( "(" ) ; sb . append ( subQuery . toString ( field ) ) ; if ( valSrcQuery != null ) { sb . append ( ", " ) . append ( valSrcQuery . toString ( field ) ) ; } sb . append ( ")" ) ; sb . append ( strict ? " STRICT" : "" ) ; return sb . toString ( ) + ToStringUtils . boost ( getBoost ( ) ) ; } public boolean equals ( Object o ) { if ( getClass ( ) != o . getClass ( ) ) { return false ; } CustomScoreQuery other = ( CustomScoreQuery ) o ; return this . getBoost ( ) == other . getBoost ( ) && this . subQuery . equals ( other . subQuery ) && ( this . valSrcQuery == null ? other . valSrcQuery == null : this . valSrcQuery . equals ( other . valSrcQuery ) ) ; } public int hashCode ( ) { int valSrcHash = valSrcQuery == null ? 0 : valSrcQuery . hashCode ( ) ; return ( getClass ( ) . hashCode ( ) + subQuery . hashCode ( ) + valSrcHash ) ^ Float . floatToIntBits ( getBoost ( ) ) ; } public float customScore ( int doc , float subQueryScore , float valSrcScore ) { return valSrcScore * subQueryScore ; } public Explanation customExplain ( int doc , Explanation subQueryExpl , Explanation valSrcExpl ) { float valSrcScore = valSrcExpl == null ? 1 : valSrcExpl . getValue ( ) ; Explanation exp = new Explanation ( valSrcScore * subQueryExpl . getValue ( ) , "custom score: product of:" ) ; exp . addDetail ( subQueryExpl ) ; if ( valSrcExpl != null ) { exp . addDetail ( valSrcExpl ) ; } return exp ; } private class CustomWeight implements Weight { Searcher searcher ; Weight subQueryWeight ; Weight valSrcWeight ; boolean qStrict ; public CustomWeight ( Searcher searcher ) throws IOException { this . searcher = searcher ; this . subQueryWeight = subQuery . weight ( searcher ) ; if ( valSrcQuery != null ) { this . valSrcWeight = valSrcQuery . createWeight ( searcher ) ; } this . qStrict = strict ; } public Query getQuery ( ) { return CustomScoreQuery . this ; } public float getValue ( ) { return getBoost ( ) ; } public float sumOfSquaredWeights ( ) throws IOException { float sum = subQueryWeight . sumOfSquaredWeights ( ) ; if ( valSrcWeight != null ) { if ( qStrict ) { valSrcWeight . sumOfSquaredWeights ( ) ; } else { sum += valSrcWeight . sumOfSquaredWeights ( ) ; } } sum *= getBoost ( ) * getBoost ( ) ; return sum ; } public void normalize ( float norm ) { norm *= getBoost ( ) ; subQueryWeight . normalize ( norm ) ; if ( valSrcWeight != null ) { if ( qStrict ) { valSrcWeight . normalize ( 1 ) ; } else { valSrcWeight . normalize ( norm ) ; } } } public Scorer scorer ( IndexReader reader ) throws IOException { Scorer subQueryScorer = subQueryWeight . scorer ( reader ) ; Scorer valSrcScorer = ( valSrcWeight == null ? null : valSrcWeight . scorer ( reader ) ) ; return new CustomScorer ( getSimilarity ( searcher ) , reader , this , subQueryScorer , valSrcScorer ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { return scorer ( reader ) . explain ( doc ) ; } } private class CustomScorer extends Scorer { private final CustomWeight weight ; private final float qWeight ; private Scorer subQueryScorer ; private Scorer valSrcScorer ; private IndexReader reader ; private CustomScorer ( Similarity similarity , IndexReader reader , CustomWeight w , Scorer subQueryScorer , Scorer valSrcScorer ) throws IOException { super ( similarity ) ; this . weight = w ; this . qWeight = w . getValue ( ) ; this . subQueryScorer = subQueryScorer ; this . valSrcScorer = valSrcScorer ; this . reader = reader ; } public boolean next ( ) throws IOException { boolean hasNext = subQueryScorer . next ( ) ; if ( valSrcScorer != null && hasNext ) { valSrcScorer . skipTo ( subQueryScorer . doc ( ) ) ; } return hasNext ; } public int doc ( ) { return subQueryScorer . doc ( ) ; } public float score ( ) throws IOException { float valSrcScore = ( valSrcScorer == null ? 1 : valSrcScorer . score ( ) ) ; return qWeight * customScore ( subQueryScorer . doc ( ) , subQueryScorer . score ( ) , valSrcScore ) ; } public boolean skipTo ( int target ) throws IOException { boolean hasNext = subQueryScorer . skipTo ( target ) ; if ( valSrcScorer != null && hasNext ) { valSrcScorer . skipTo ( subQueryScorer . doc ( ) ) ; } return hasNext ; } public Explanation explain ( int doc ) throws IOException { Explanation subQueryExpl = weight . subQueryWeight . explain ( reader , doc ) ; if ( ! subQueryExpl . isMatch ( ) ) { return subQueryExpl ; } Explanation valSrcExpl = valSrcScorer == null ? null : valSrcScorer . explain ( doc ) ; Explanation customExp = customExplain ( doc , subQueryExpl , valSrcExpl ) ; float sc = qWeight * customExp . getValue ( ) ; Explanation res = new ComplexExplanation ( true , sc , CustomScoreQuery . this . toString ( ) + ", product of:" ) ; res . addDetail ( customExp ) ; res . addDetail ( new Explanation ( qWeight , "queryBoost" ) ) ; return res ; } } protected Weight createWeight ( Searcher searcher ) throws IOException { return new CustomWeight ( searcher ) ; } public boolean isStrict ( ) { return strict ; } public void setStrict ( boolean strict ) { this . strict = strict ; } public String name ( ) { return "custom" ; } } 	1	['17', '2', '0', '9', '43', '48', '2', '8', '13', '0.5625', '263', '1', '2', '0.444444444', '0.135746606', '2', '4', '14.29411765', '7', '1.6471', '1']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . util . ToStringUtils ; public abstract class MultiTermQuery extends Query { private Term term ; public MultiTermQuery ( Term term ) { this . term = term ; } public Term getTerm ( ) { return term ; } protected abstract FilteredTermEnum getEnum ( IndexReader reader ) throws IOException ; public Query rewrite ( IndexReader reader ) throws IOException { FilteredTermEnum enumerator = getEnum ( reader ) ; BooleanQuery query = new BooleanQuery ( true ) ; try { do { Term t = enumerator . term ( ) ; if ( t != null ) { TermQuery tq = new TermQuery ( t ) ; tq . setBoost ( getBoost ( ) * enumerator . difference ( ) ) ; query . add ( tq , BooleanClause . Occur . SHOULD ) ; } } while ( enumerator . next ( ) ) ; } finally { enumerator . close ( ) ; } return query ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! term . field ( ) . equals ( field ) ) { buffer . append ( term . field ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( term . text ( ) ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof MultiTermQuery ) ) return false ; final MultiTermQuery multiTermQuery = ( MultiTermQuery ) o ; if ( ! term . equals ( multiTermQuery . term ) ) return false ; return getBoost ( ) == multiTermQuery . getBoost ( ) ; } public int hashCode ( ) { return term . hashCode ( ) + Float . floatToRawIntBits ( getBoost ( ) ) ; } } 	0	['7', '2', '2', '10', '27', '1', '2', '8', '6', '0.333333333', '134', '1', '1', '0.666666667', '0.342857143', '2', '3', '18', '5', '1.5714', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class LockFactory { protected String lockPrefix = "" ; public void setLockPrefix ( String lockPrefix ) { this . lockPrefix = lockPrefix ; } public String getLockPrefix ( ) { return this . lockPrefix ; } public abstract Lock makeLock ( String lockName ) ; abstract public void clearLock ( String lockName ) throws IOException ; } 	1	['5', '1', '4', '8', '6', '4', '7', '1', '5', '0.5', '19', '1', '0', '0', '0.8', '0', '0', '2.6', '1', '0.8', '1']
package org . apache . lucene . search ; import java . io . IOException ; final class BooleanScorer extends Scorer { private SubScorer scorers = null ; private BucketTable bucketTable = new BucketTable ( ) ; private int maxCoord = 1 ; private float [ ] coordFactors = null ; private int requiredMask = 0 ; private int prohibitedMask = 0 ; private int nextMask = 1 ; private final int minNrShouldMatch ; BooleanScorer ( Similarity similarity ) { this ( similarity , 1 ) ; } BooleanScorer ( Similarity similarity , int minNrShouldMatch ) { super ( similarity ) ; this . minNrShouldMatch = minNrShouldMatch ; } static final class SubScorer { public Scorer scorer ; public boolean done ; public boolean required = false ; public boolean prohibited = false ; public HitCollector collector ; public SubScorer next ; public SubScorer ( Scorer scorer , boolean required , boolean prohibited , HitCollector collector , SubScorer next ) throws IOException { this . scorer = scorer ; this . done = ! scorer . next ( ) ; this . required = required ; this . prohibited = prohibited ; this . collector = collector ; this . next = next ; } } final void add ( Scorer scorer , boolean required , boolean prohibited ) throws IOException { int mask = 0 ; if ( required || prohibited ) { if ( nextMask == 0 ) throw new IndexOutOfBoundsException ( "More than 32 required/prohibited clauses in query." ) ; mask = nextMask ; nextMask = nextMask << 1 ; } else mask = 0 ; if ( ! prohibited ) maxCoord ++ ; if ( prohibited ) prohibitedMask |= mask ; else if ( required ) requiredMask |= mask ; scorers = new SubScorer ( scorer , required , prohibited , bucketTable . newCollector ( mask ) , scorers ) ; } private final void computeCoordFactors ( ) { coordFactors = new float [ maxCoord ] ; for ( int i = 0 ; i < maxCoord ; i ++ ) coordFactors [ i ] = getSimilarity ( ) . coord ( i , maxCoord - 1 ) ; } private int end ; private Bucket current ; public void score ( HitCollector hc ) throws IOException { next ( ) ; score ( hc , Integer . MAX_VALUE ) ; } protected boolean score ( HitCollector hc , int max ) throws IOException { if ( coordFactors == null ) computeCoordFactors ( ) ; boolean more ; Bucket tmp ; do { bucketTable . first = null ; while ( current != null ) { if ( ( current . bits & prohibitedMask ) == 0 && ( current . bits & requiredMask ) == requiredMask ) { if ( current . doc >= max ) { tmp = current ; current = current . next ; tmp . next = bucketTable . first ; bucketTable . first = tmp ; continue ; } if ( current . coord >= minNrShouldMatch ) { hc . collect ( current . doc , current . score * coordFactors [ current . coord ] ) ; } } current = current . next ; } if ( bucketTable . first != null ) { current = bucketTable . first ; bucketTable . first = current . next ; return true ; } more = false ; end += BucketTable . SIZE ; for ( SubScorer sub = scorers ; sub != null ; sub = sub . next ) { if ( ! sub . done ) { sub . done = ! sub . scorer . score ( sub . collector , end ) ; if ( ! sub . done ) more = true ; } } current = bucketTable . first ; } while ( current != null || more ) ; return false ; } public int doc ( ) { return current . doc ; } public boolean next ( ) throws IOException { boolean more ; do { while ( bucketTable . first != null ) { current = bucketTable . first ; bucketTable . first = current . next ; if ( ( current . bits & prohibitedMask ) == 0 && ( current . bits & requiredMask ) == requiredMask && current . coord >= minNrShouldMatch ) { return true ; } } more = false ; end += BucketTable . SIZE ; for ( SubScorer sub = scorers ; sub != null ; sub = sub . next ) { Scorer scorer = sub . scorer ; while ( ! sub . done && scorer . doc ( ) < end ) { sub . collector . collect ( scorer . doc ( ) , scorer . score ( ) ) ; sub . done = ! scorer . next ( ) ; } if ( ! sub . done ) { more = true ; } } } while ( bucketTable . first != null || more ) ; return false ; } public float score ( ) { if ( coordFactors == null ) computeCoordFactors ( ) ; return current . score * coordFactors [ current . coord ] ; } static final class Bucket { int doc = - 1 ; float score ; int bits ; int coord ; Bucket next ; } static final class BucketTable { public static final int SIZE = 1 << 11 ; public static final int MASK = SIZE - 1 ; final Bucket [ ] buckets = new Bucket [ SIZE ] ; Bucket first = null ; public BucketTable ( ) { } public final int size ( ) { return SIZE ; } public HitCollector newCollector ( int mask ) { return new Collector ( mask , this ) ; } } static final class Collector extends HitCollector { private BucketTable bucketTable ; private int mask ; public Collector ( int mask , BucketTable bucketTable ) { this . mask = mask ; this . bucketTable = bucketTable ; } public final void collect ( final int doc , final float score ) { final BucketTable table = bucketTable ; final int i = doc & BucketTable . MASK ; Bucket bucket = table . buckets [ i ] ; if ( bucket == null ) table . buckets [ i ] = bucket = new Bucket ( ) ; if ( bucket . doc != doc ) { bucket . doc = doc ; bucket . score = score ; bucket . bits = mask ; bucket . coord = 1 ; bucket . next = table . first ; table . first = bucket ; } else { bucket . score += score ; bucket . bits |= mask ; bucket . coord ++ ; } } } public boolean skipTo ( int target ) { throw new UnsupportedOperationException ( ) ; } public Explanation explain ( int doc ) { throw new UnsupportedOperationException ( ) ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "boolean(" ) ; for ( SubScorer sub = scorers ; sub != null ; sub = sub . next ) { buffer . append ( sub . scorer . toString ( ) ) ; buffer . append ( " " ) ; } buffer . append ( ")" ) ; return buffer . toString ( ) ; } } 	0	['12', '2', '0', '8', '29', '26', '1', '7', '7', '0.609090909', '461', '1', '3', '0.444444444', '0.305555556', '1', '3', '36.58333333', '2', '1.0833', '0']
package org . apache . lucene . search ; public class TopFieldDocs extends TopDocs { public SortField [ ] fields ; TopFieldDocs ( int totalHits , ScoreDoc [ ] scoreDocs , SortField [ ] fields , float maxScore ) { super ( totalHits , scoreDocs , maxScore ) ; this . fields = fields ; } } 	1	['1', '2', '0', '14', '2', '0', '11', '3', '0', '2', '11', '0', '1', '1', '1', '0', '0', '9', '0', '0', '1']
package org . apache . lucene . search ; public class QueryFilter extends CachingWrapperFilter { public QueryFilter ( Query query ) { super ( new QueryWrapperFilter ( query ) ) ; } public boolean equals ( Object o ) { return super . equals ( ( QueryFilter ) o ) ; } public int hashCode ( ) { return super . hashCode ( ) ^ 0x923F64B9 ; } } 	0	['3', '3', '0', '4', '7', '3', '0', '4', '3', '2', '20', '0', '0', '0.714285714', '0.555555556', '2', '3', '5.666666667', '1', '0.6667', '0']
package org . apache . lucene . queryParser ; import java . util . Vector ; import java . io . * ; import java . text . * ; import java . util . * ; import org . apache . lucene . index . Term ; import org . apache . lucene . analysis . * ; import org . apache . lucene . document . * ; import org . apache . lucene . search . * ; import org . apache . lucene . util . Parameter ; public class QueryParserTokenManager implements QueryParserConstants { public java . io . PrintStream debugStream = System . out ; public void setDebugStream ( java . io . PrintStream ds ) { debugStream = ds ; } private final int jjStopStringLiteralDfa_3 ( int pos , long active0 ) { switch ( pos ) { default : return - 1 ; } } private final int jjStartNfa_3 ( int pos , long active0 ) { return jjMoveNfa_3 ( jjStopStringLiteralDfa_3 ( pos , active0 ) , pos + 1 ) ; } private final int jjStopAtPos ( int pos , int kind ) { jjmatchedKind = kind ; jjmatchedPos = pos ; return pos + 1 ; } private final int jjStartNfaWithStates_3 ( int pos , int kind , int state ) { jjmatchedKind = kind ; jjmatchedPos = pos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return pos + 1 ; } return jjMoveNfa_3 ( state , pos + 1 ) ; } private final int jjMoveStringLiteralDfa0_3 ( ) { switch ( curChar ) { case 40 : return jjStopAtPos ( 0 , 12 ) ; case 41 : return jjStopAtPos ( 0 , 13 ) ; case 42 : return jjStartNfaWithStates_3 ( 0 , 15 , 36 ) ; case 43 : return jjStopAtPos ( 0 , 10 ) ; case 45 : return jjStopAtPos ( 0 , 11 ) ; case 58 : return jjStopAtPos ( 0 , 14 ) ; case 91 : return jjStopAtPos ( 0 , 22 ) ; case 94 : return jjStopAtPos ( 0 , 16 ) ; case 123 : return jjStopAtPos ( 0 , 23 ) ; default : return jjMoveNfa_3 ( 0 , 0 ) ; } } private final void jjCheckNAdd ( int state ) { if ( jjrounds [ state ] != jjround ) { jjstateSet [ jjnewStateCnt ++ ] = state ; jjrounds [ state ] = jjround ; } } private final void jjAddStates ( int start , int end ) { do { jjstateSet [ jjnewStateCnt ++ ] = jjnextStates [ start ] ; } while ( start ++ != end ) ; } private final void jjCheckNAddTwoStates ( int state1 , int state2 ) { jjCheckNAdd ( state1 ) ; jjCheckNAdd ( state2 ) ; } private final void jjCheckNAddStates ( int start , int end ) { do { jjCheckNAdd ( jjnextStates [ start ] ) ; } while ( start ++ != end ) ; } private final void jjCheckNAddStates ( int start ) { jjCheckNAdd ( jjnextStates [ start ] ) ; jjCheckNAdd ( jjnextStates [ start + 1 ] ) ; } static final long [ ] jjbitVec0 = { 0xfffffffffffffffeL , 0xffffffffffffffffL , 0xffffffffffffffffL , 0xffffffffffffffffL } ; static final long [ ] jjbitVec2 = { 0x0L , 0x0L , 0xffffffffffffffffL , 0xffffffffffffffffL } ; private final int jjMoveNfa_3 ( int startState , int curPos ) { int [ ] nextStates ; int startsAt = 0 ; jjnewStateCnt = 36 ; int i = 1 ; jjstateSet [ 0 ] = startState ; int j , kind = 0x7fffffff ; for ( ; ; ) { if ( ++ jjround == 0x7fffffff ) ReInitRounds ( ) ; if ( curChar < 64 ) { long l = 1L << curChar ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 36 : case 25 : if ( ( 0xfbfffcf8ffffd9ffL & l ) == 0L ) break ; if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 0 : if ( ( 0xfbffd4f8ffffd9ffL & l ) != 0L ) { if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; } else if ( ( 0x100002600L & l ) != 0L ) { if ( kind > 6 ) kind = 6 ; } else if ( curChar == 34 ) jjCheckNAddTwoStates ( 15 , 17 ) ; else if ( curChar == 33 ) { if ( kind > 9 ) kind = 9 ; } if ( ( 0x7bffd0f8ffffd9ffL & l ) != 0L ) { if ( kind > 18 ) kind = 18 ; jjCheckNAddStates ( 0 , 4 ) ; } else if ( curChar == 42 ) { if ( kind > 20 ) kind = 20 ; } if ( curChar == 38 ) jjstateSet [ jjnewStateCnt ++ ] = 4 ; break ; case 4 : if ( curChar == 38 && kind > 7 ) kind = 7 ; break ; case 5 : if ( curChar == 38 ) jjstateSet [ jjnewStateCnt ++ ] = 4 ; break ; case 13 : if ( curChar == 33 && kind > 9 ) kind = 9 ; break ; case 14 : if ( curChar == 34 ) jjCheckNAddTwoStates ( 15 , 17 ) ; break ; case 15 : if ( ( 0xfffffffbffffffffL & l ) != 0L ) jjCheckNAddStates ( 5 , 7 ) ; break ; case 16 : if ( curChar == 34 ) jjCheckNAddStates ( 5 , 7 ) ; break ; case 18 : if ( curChar == 34 && kind > 17 ) kind = 17 ; break ; case 20 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 19 ) kind = 19 ; jjAddStates ( 8 , 9 ) ; break ; case 21 : if ( curChar == 46 ) jjCheckNAdd ( 22 ) ; break ; case 22 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 19 ) kind = 19 ; jjCheckNAdd ( 22 ) ; break ; case 23 : if ( curChar == 42 && kind > 20 ) kind = 20 ; break ; case 24 : if ( ( 0xfbffd4f8ffffd9ffL & l ) == 0L ) break ; if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 27 : if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 28 : if ( ( 0x7bffd0f8ffffd9ffL & l ) == 0L ) break ; if ( kind > 18 ) kind = 18 ; jjCheckNAddStates ( 0 , 4 ) ; break ; case 29 : if ( ( 0x7bfff8f8ffffd9ffL & l ) == 0L ) break ; if ( kind > 18 ) kind = 18 ; jjCheckNAddTwoStates ( 29 , 30 ) ; break ; case 31 : if ( kind > 18 ) kind = 18 ; jjCheckNAddTwoStates ( 29 , 30 ) ; break ; case 32 : if ( ( 0x7bfff8f8ffffd9ffL & l ) != 0L ) jjCheckNAddStates ( 10 , 12 ) ; break ; case 34 : jjCheckNAddStates ( 10 , 12 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else if ( curChar < 128 ) { long l = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 36 : if ( ( 0x97ffffff87ffffffL & l ) != 0L ) { if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; } else if ( curChar == 92 ) jjCheckNAddTwoStates ( 27 , 27 ) ; break ; case 0 : if ( ( 0x97ffffff87ffffffL & l ) != 0L ) { if ( kind > 18 ) kind = 18 ; jjCheckNAddStates ( 0 , 4 ) ; } else if ( curChar == 92 ) jjCheckNAddStates ( 13 , 15 ) ; else if ( curChar == 126 ) { if ( kind > 19 ) kind = 19 ; jjstateSet [ jjnewStateCnt ++ ] = 20 ; } if ( ( 0x97ffffff87ffffffL & l ) != 0L ) { if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; } if ( curChar == 78 ) jjstateSet [ jjnewStateCnt ++ ] = 11 ; else if ( curChar == 124 ) jjstateSet [ jjnewStateCnt ++ ] = 8 ; else if ( curChar == 79 ) jjstateSet [ jjnewStateCnt ++ ] = 6 ; else if ( curChar == 65 ) jjstateSet [ jjnewStateCnt ++ ] = 2 ; break ; case 1 : if ( curChar == 68 && kind > 7 ) kind = 7 ; break ; case 2 : if ( curChar == 78 ) jjstateSet [ jjnewStateCnt ++ ] = 1 ; break ; case 3 : if ( curChar == 65 ) jjstateSet [ jjnewStateCnt ++ ] = 2 ; break ; case 6 : if ( curChar == 82 && kind > 8 ) kind = 8 ; break ; case 7 : if ( curChar == 79 ) jjstateSet [ jjnewStateCnt ++ ] = 6 ; break ; case 8 : if ( curChar == 124 && kind > 8 ) kind = 8 ; break ; case 9 : if ( curChar == 124 ) jjstateSet [ jjnewStateCnt ++ ] = 8 ; break ; case 10 : if ( curChar == 84 && kind > 9 ) kind = 9 ; break ; case 11 : if ( curChar == 79 ) jjstateSet [ jjnewStateCnt ++ ] = 10 ; break ; case 12 : if ( curChar == 78 ) jjstateSet [ jjnewStateCnt ++ ] = 11 ; break ; case 15 : jjAddStates ( 5 , 7 ) ; break ; case 17 : if ( curChar == 92 ) jjstateSet [ jjnewStateCnt ++ ] = 16 ; break ; case 19 : if ( curChar != 126 ) break ; if ( kind > 19 ) kind = 19 ; jjstateSet [ jjnewStateCnt ++ ] = 20 ; break ; case 24 : if ( ( 0x97ffffff87ffffffL & l ) == 0L ) break ; if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 25 : if ( ( 0x97ffffff87ffffffL & l ) == 0L ) break ; if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 26 : if ( curChar == 92 ) jjCheckNAddTwoStates ( 27 , 27 ) ; break ; case 27 : if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 28 : if ( ( 0x97ffffff87ffffffL & l ) == 0L ) break ; if ( kind > 18 ) kind = 18 ; jjCheckNAddStates ( 0 , 4 ) ; break ; case 29 : if ( ( 0x97ffffff87ffffffL & l ) == 0L ) break ; if ( kind > 18 ) kind = 18 ; jjCheckNAddTwoStates ( 29 , 30 ) ; break ; case 30 : if ( curChar == 92 ) jjCheckNAddTwoStates ( 31 , 31 ) ; break ; case 31 : if ( kind > 18 ) kind = 18 ; jjCheckNAddTwoStates ( 29 , 30 ) ; break ; case 32 : if ( ( 0x97ffffff87ffffffL & l ) != 0L ) jjCheckNAddStates ( 10 , 12 ) ; break ; case 33 : if ( curChar == 92 ) jjCheckNAddTwoStates ( 34 , 34 ) ; break ; case 34 : jjCheckNAddStates ( 10 , 12 ) ; break ; case 35 : if ( curChar == 92 ) jjCheckNAddStates ( 13 , 15 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else { int hiByte = ( int ) ( curChar > > 8 ) ; int i1 = hiByte > > 6 ; long l1 = 1L << ( hiByte & 077 ) ; int i2 = ( curChar & 0xff ) > > 6 ; long l2 = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 36 : case 25 : case 27 : if ( ! jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 0 : if ( jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) { if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; } if ( jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) { if ( kind > 18 ) kind = 18 ; jjCheckNAddStates ( 0 , 4 ) ; } break ; case 15 : if ( jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 5 , 7 ) ; break ; case 24 : if ( ! jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 21 ) kind = 21 ; jjCheckNAddTwoStates ( 25 , 26 ) ; break ; case 28 : if ( ! jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 18 ) kind = 18 ; jjCheckNAddStates ( 0 , 4 ) ; break ; case 29 : case 31 : if ( ! jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 18 ) kind = 18 ; jjCheckNAddTwoStates ( 29 , 30 ) ; break ; case 32 : case 34 : if ( jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) jjCheckNAddStates ( 10 , 12 ) ; break ; default : break ; } } while ( i != startsAt ) ; } if ( kind != 0x7fffffff ) { jjmatchedKind = kind ; jjmatchedPos = curPos ; kind = 0x7fffffff ; } ++ curPos ; if ( ( i = jjnewStateCnt ) == ( startsAt = 36 - ( jjnewStateCnt = startsAt ) ) ) return curPos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return curPos ; } } } private final int jjStopStringLiteralDfa_1 ( int pos , long active0 ) { switch ( pos ) { case 0 : if ( ( active0 & 0x20000000L ) != 0L ) { jjmatchedKind = 32 ; return 6 ; } return - 1 ; default : return - 1 ; } } private final int jjStartNfa_1 ( int pos , long active0 ) { return jjMoveNfa_1 ( jjStopStringLiteralDfa_1 ( pos , active0 ) , pos + 1 ) ; } private final int jjStartNfaWithStates_1 ( int pos , int kind , int state ) { jjmatchedKind = kind ; jjmatchedPos = pos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return pos + 1 ; } return jjMoveNfa_1 ( state , pos + 1 ) ; } private final int jjMoveStringLiteralDfa0_1 ( ) { switch ( curChar ) { case 84 : return jjMoveStringLiteralDfa1_1 ( 0x20000000L ) ; case 125 : return jjStopAtPos ( 0 , 30 ) ; default : return jjMoveNfa_1 ( 0 , 0 ) ; } } private final int jjMoveStringLiteralDfa1_1 ( long active0 ) { try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { jjStopStringLiteralDfa_1 ( 0 , active0 ) ; return 1 ; } switch ( curChar ) { case 79 : if ( ( active0 & 0x20000000L ) != 0L ) return jjStartNfaWithStates_1 ( 1 , 29 , 6 ) ; break ; default : break ; } return jjStartNfa_1 ( 0 , active0 ) ; } private final int jjMoveNfa_1 ( int startState , int curPos ) { int [ ] nextStates ; int startsAt = 0 ; jjnewStateCnt = 7 ; int i = 1 ; jjstateSet [ 0 ] = startState ; int j , kind = 0x7fffffff ; for ( ; ; ) { if ( ++ jjround == 0x7fffffff ) ReInitRounds ( ) ; if ( curChar < 64 ) { long l = 1L << curChar ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( ( 0xfffffffeffffffffL & l ) != 0L ) { if ( kind > 32 ) kind = 32 ; jjCheckNAdd ( 6 ) ; } if ( ( 0x100002600L & l ) != 0L ) { if ( kind > 6 ) kind = 6 ; } else if ( curChar == 34 ) jjCheckNAddTwoStates ( 2 , 4 ) ; break ; case 1 : if ( curChar == 34 ) jjCheckNAddTwoStates ( 2 , 4 ) ; break ; case 2 : if ( ( 0xfffffffbffffffffL & l ) != 0L ) jjCheckNAddStates ( 16 , 18 ) ; break ; case 3 : if ( curChar == 34 ) jjCheckNAddStates ( 16 , 18 ) ; break ; case 5 : if ( curChar == 34 && kind > 31 ) kind = 31 ; break ; case 6 : if ( ( 0xfffffffeffffffffL & l ) == 0L ) break ; if ( kind > 32 ) kind = 32 ; jjCheckNAdd ( 6 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else if ( curChar < 128 ) { long l = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : case 6 : if ( ( 0xdfffffffffffffffL & l ) == 0L ) break ; if ( kind > 32 ) kind = 32 ; jjCheckNAdd ( 6 ) ; break ; case 2 : jjAddStates ( 16 , 18 ) ; break ; case 4 : if ( curChar == 92 ) jjstateSet [ jjnewStateCnt ++ ] = 3 ; break ; default : break ; } } while ( i != startsAt ) ; } else { int hiByte = ( int ) ( curChar > > 8 ) ; int i1 = hiByte > > 6 ; long l1 = 1L << ( hiByte & 077 ) ; int i2 = ( curChar & 0xff ) > > 6 ; long l2 = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : case 6 : if ( ! jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 32 ) kind = 32 ; jjCheckNAdd ( 6 ) ; break ; case 2 : if ( jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 16 , 18 ) ; break ; default : break ; } } while ( i != startsAt ) ; } if ( kind != 0x7fffffff ) { jjmatchedKind = kind ; jjmatchedPos = curPos ; kind = 0x7fffffff ; } ++ curPos ; if ( ( i = jjnewStateCnt ) == ( startsAt = 7 - ( jjnewStateCnt = startsAt ) ) ) return curPos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return curPos ; } } } private final int jjMoveStringLiteralDfa0_0 ( ) { return jjMoveNfa_0 ( 0 , 0 ) ; } private final int jjMoveNfa_0 ( int startState , int curPos ) { int [ ] nextStates ; int startsAt = 0 ; jjnewStateCnt = 3 ; int i = 1 ; jjstateSet [ 0 ] = startState ; int j , kind = 0x7fffffff ; for ( ; ; ) { if ( ++ jjround == 0x7fffffff ) ReInitRounds ( ) ; if ( curChar < 64 ) { long l = 1L << curChar ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 24 ) kind = 24 ; jjAddStates ( 19 , 20 ) ; break ; case 1 : if ( curChar == 46 ) jjCheckNAdd ( 2 ) ; break ; case 2 : if ( ( 0x3ff000000000000L & l ) == 0L ) break ; if ( kind > 24 ) kind = 24 ; jjCheckNAdd ( 2 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else if ( curChar < 128 ) { long l = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { default : break ; } } while ( i != startsAt ) ; } else { int hiByte = ( int ) ( curChar > > 8 ) ; int i1 = hiByte > > 6 ; long l1 = 1L << ( hiByte & 077 ) ; int i2 = ( curChar & 0xff ) > > 6 ; long l2 = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { default : break ; } } while ( i != startsAt ) ; } if ( kind != 0x7fffffff ) { jjmatchedKind = kind ; jjmatchedPos = curPos ; kind = 0x7fffffff ; } ++ curPos ; if ( ( i = jjnewStateCnt ) == ( startsAt = 3 - ( jjnewStateCnt = startsAt ) ) ) return curPos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return curPos ; } } } private final int jjStopStringLiteralDfa_2 ( int pos , long active0 ) { switch ( pos ) { case 0 : if ( ( active0 & 0x2000000L ) != 0L ) { jjmatchedKind = 28 ; return 6 ; } return - 1 ; default : return - 1 ; } } private final int jjStartNfa_2 ( int pos , long active0 ) { return jjMoveNfa_2 ( jjStopStringLiteralDfa_2 ( pos , active0 ) , pos + 1 ) ; } private final int jjStartNfaWithStates_2 ( int pos , int kind , int state ) { jjmatchedKind = kind ; jjmatchedPos = pos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return pos + 1 ; } return jjMoveNfa_2 ( state , pos + 1 ) ; } private final int jjMoveStringLiteralDfa0_2 ( ) { switch ( curChar ) { case 84 : return jjMoveStringLiteralDfa1_2 ( 0x2000000L ) ; case 93 : return jjStopAtPos ( 0 , 26 ) ; default : return jjMoveNfa_2 ( 0 , 0 ) ; } } private final int jjMoveStringLiteralDfa1_2 ( long active0 ) { try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { jjStopStringLiteralDfa_2 ( 0 , active0 ) ; return 1 ; } switch ( curChar ) { case 79 : if ( ( active0 & 0x2000000L ) != 0L ) return jjStartNfaWithStates_2 ( 1 , 25 , 6 ) ; break ; default : break ; } return jjStartNfa_2 ( 0 , active0 ) ; } private final int jjMoveNfa_2 ( int startState , int curPos ) { int [ ] nextStates ; int startsAt = 0 ; jjnewStateCnt = 7 ; int i = 1 ; jjstateSet [ 0 ] = startState ; int j , kind = 0x7fffffff ; for ( ; ; ) { if ( ++ jjround == 0x7fffffff ) ReInitRounds ( ) ; if ( curChar < 64 ) { long l = 1L << curChar ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : if ( ( 0xfffffffeffffffffL & l ) != 0L ) { if ( kind > 28 ) kind = 28 ; jjCheckNAdd ( 6 ) ; } if ( ( 0x100002600L & l ) != 0L ) { if ( kind > 6 ) kind = 6 ; } else if ( curChar == 34 ) jjCheckNAddTwoStates ( 2 , 4 ) ; break ; case 1 : if ( curChar == 34 ) jjCheckNAddTwoStates ( 2 , 4 ) ; break ; case 2 : if ( ( 0xfffffffbffffffffL & l ) != 0L ) jjCheckNAddStates ( 16 , 18 ) ; break ; case 3 : if ( curChar == 34 ) jjCheckNAddStates ( 16 , 18 ) ; break ; case 5 : if ( curChar == 34 && kind > 27 ) kind = 27 ; break ; case 6 : if ( ( 0xfffffffeffffffffL & l ) == 0L ) break ; if ( kind > 28 ) kind = 28 ; jjCheckNAdd ( 6 ) ; break ; default : break ; } } while ( i != startsAt ) ; } else if ( curChar < 128 ) { long l = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : case 6 : if ( ( 0xffffffffdfffffffL & l ) == 0L ) break ; if ( kind > 28 ) kind = 28 ; jjCheckNAdd ( 6 ) ; break ; case 2 : jjAddStates ( 16 , 18 ) ; break ; case 4 : if ( curChar == 92 ) jjstateSet [ jjnewStateCnt ++ ] = 3 ; break ; default : break ; } } while ( i != startsAt ) ; } else { int hiByte = ( int ) ( curChar > > 8 ) ; int i1 = hiByte > > 6 ; long l1 = 1L << ( hiByte & 077 ) ; int i2 = ( curChar & 0xff ) > > 6 ; long l2 = 1L << ( curChar & 077 ) ; MatchLoop : do { switch ( jjstateSet [ -- i ] ) { case 0 : case 6 : if ( ! jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) break ; if ( kind > 28 ) kind = 28 ; jjCheckNAdd ( 6 ) ; break ; case 2 : if ( jjCanMove_0 ( hiByte , i1 , i2 , l1 , l2 ) ) jjAddStates ( 16 , 18 ) ; break ; default : break ; } } while ( i != startsAt ) ; } if ( kind != 0x7fffffff ) { jjmatchedKind = kind ; jjmatchedPos = curPos ; kind = 0x7fffffff ; } ++ curPos ; if ( ( i = jjnewStateCnt ) == ( startsAt = 7 - ( jjnewStateCnt = startsAt ) ) ) return curPos ; try { curChar = input_stream . readChar ( ) ; } catch ( java . io . IOException e ) { return curPos ; } } } static final int [ ] jjnextStates = { 29 , 32 , 23 , 33 , 30 , 15 , 17 , 18 , 20 , 21 , 32 , 23 , 33 , 31 , 34 , 27 , 2 , 4 , 5 , 0 , 1 , } ; private static final boolean jjCanMove_0 ( int hiByte , int i1 , int i2 , long l1 , long l2 ) { switch ( hiByte ) { case 0 : return ( ( jjbitVec2 [ i2 ] & l2 ) != 0L ) ; default : if ( ( jjbitVec0 [ i1 ] & l1 ) != 0L ) return true ; return false ; } } public static final String [ ] jjstrLiteralImages = { "" , null , null , null , null , null , null , null , null , null , "\53" , "\55" , "\50" , "\51" , "\72" , "\52" , "\136" , null , null , null , null , null , "\133" , "\173" , null , "\124\117" , "\135" , null , null , "\124\117" , "\175" , null , null , } ; public static final String [ ] lexStateNames = { "Boost" , "RangeEx" , "RangeIn" , "DEFAULT" , } ; public static final int [ ] jjnewLexState = { - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , - 1 , 0 , - 1 , - 1 , - 1 , - 1 , - 1 , 2 , 1 , 3 , - 1 , 3 , - 1 , - 1 , - 1 , 3 , - 1 , - 1 , } ; static final long [ ] jjtoToken = { 0x1ffffff81L , } ; static final long [ ] jjtoSkip = { 0x40L , } ; protected CharStream input_stream ; private final int [ ] jjrounds = new int [ 36 ] ; private final int [ ] jjstateSet = new int [ 72 ] ; protected char curChar ; public QueryParserTokenManager ( CharStream stream ) { input_stream = stream ; } public QueryParserTokenManager ( CharStream stream , int lexState ) { this ( stream ) ; SwitchTo ( lexState ) ; } public void ReInit ( CharStream stream ) { jjmatchedPos = jjnewStateCnt = 0 ; curLexState = defaultLexState ; input_stream = stream ; ReInitRounds ( ) ; } private final void ReInitRounds ( ) { int i ; jjround = 0x80000001 ; for ( i = 36 ; i -- > 0 ; ) jjrounds [ i ] = 0x80000000 ; } public void ReInit ( CharStream stream , int lexState ) { ReInit ( stream ) ; SwitchTo ( lexState ) ; } public void SwitchTo ( int lexState ) { if ( lexState >= 4 || lexState < 0 ) throw new TokenMgrError ( "Error: Ignoring invalid lexical state : " + lexState + ". State unchanged." , TokenMgrError . INVALID_LEXICAL_STATE ) ; else curLexState = lexState ; } protected Token jjFillToken ( ) { Token t = Token . newToken ( jjmatchedKind ) ; t . kind = jjmatchedKind ; String im = jjstrLiteralImages [ jjmatchedKind ] ; t . image = ( im == null ) ? input_stream . GetImage ( ) : im ; t . beginLine = input_stream . getBeginLine ( ) ; t . beginColumn = input_stream . getBeginColumn ( ) ; t . endLine = input_stream . getEndLine ( ) ; t . endColumn = input_stream . getEndColumn ( ) ; return t ; } int curLexState = 3 ; int defaultLexState = 3 ; int jjnewStateCnt ; int jjround ; int jjmatchedPos ; int jjmatchedKind ; public Token getNextToken ( ) { int kind ; Token specialToken = null ; Token matchedToken ; int curPos = 0 ; EOFLoop : for ( ; ; ) { try { curChar = input_stream . BeginToken ( ) ; } catch ( java . io . IOException e ) { jjmatchedKind = 0 ; matchedToken = jjFillToken ( ) ; return matchedToken ; } switch ( curLexState ) { case 0 : jjmatchedKind = 0x7fffffff ; jjmatchedPos = 0 ; curPos = jjMoveStringLiteralDfa0_0 ( ) ; break ; case 1 : jjmatchedKind = 0x7fffffff ; jjmatchedPos = 0 ; curPos = jjMoveStringLiteralDfa0_1 ( ) ; break ; case 2 : jjmatchedKind = 0x7fffffff ; jjmatchedPos = 0 ; curPos = jjMoveStringLiteralDfa0_2 ( ) ; break ; case 3 : jjmatchedKind = 0x7fffffff ; jjmatchedPos = 0 ; curPos = jjMoveStringLiteralDfa0_3 ( ) ; break ; } if ( jjmatchedKind != 0x7fffffff ) { if ( jjmatchedPos + 1 < curPos ) input_stream . backup ( curPos - jjmatchedPos - 1 ) ; if ( ( jjtoToken [ jjmatchedKind > > 6 ] & ( 1L << ( jjmatchedKind & 077 ) ) ) != 0L ) { matchedToken = jjFillToken ( ) ; if ( jjnewLexState [ jjmatchedKind ] != - 1 ) curLexState = jjnewLexState [ jjmatchedKind ] ; return matchedToken ; } else { if ( jjnewLexState [ jjmatchedKind ] != - 1 ) curLexState = jjnewLexState [ jjmatchedKind ] ; continue EOFLoop ; } } int error_line = input_stream . getEndLine ( ) ; int error_column = input_stream . getEndColumn ( ) ; String error_after = null ; boolean EOFSeen = false ; try { input_stream . readChar ( ) ; input_stream . backup ( 1 ) ; } catch ( java . io . IOException e1 ) { EOFSeen = true ; error_after = curPos <= 1 ? "" : input_stream . GetImage ( ) ; if ( curChar == '\n' || curChar == '\r' ) { error_line ++ ; error_column = 0 ; } else error_column ++ ; } if ( ! EOFSeen ) { input_stream . backup ( 1 ) ; error_after = curPos <= 1 ? "" : input_stream . GetImage ( ) ; } throw new TokenMgrError ( EOFSeen , curLexState , error_line , error_column , error_after , curChar , TokenMgrError . LEXICAL_ERROR ) ; } } } 	1	['36', '1', '0', '5', '52', '282', '1', '4', '7', '0.720300752', '3060', '0.210526316', '1', '0', '0.405714286', '0', '0', '83.47222222', '112', '7.6389', '4']
package org . apache . lucene . queryParser ; import java . io . * ; public final class FastCharStream implements CharStream { char [ ] buffer = null ; int bufferLength = 0 ; int bufferPosition = 0 ; int tokenStart = 0 ; int bufferStart = 0 ; Reader input ; public FastCharStream ( Reader r ) { input = r ; } public final char readChar ( ) throws IOException { if ( bufferPosition >= bufferLength ) refill ( ) ; return buffer [ bufferPosition ++ ] ; } private final void refill ( ) throws IOException { int newPosition = bufferLength - tokenStart ; if ( tokenStart == 0 ) { if ( buffer == null ) { buffer = new char [ 2048 ] ; } else if ( bufferLength == buffer . length ) { char [ ] newBuffer = new char [ buffer . length * 2 ] ; System . arraycopy ( buffer , 0 , newBuffer , 0 , bufferLength ) ; buffer = newBuffer ; } } else { System . arraycopy ( buffer , tokenStart , buffer , 0 , newPosition ) ; } bufferLength = newPosition ; bufferPosition = newPosition ; bufferStart += tokenStart ; tokenStart = 0 ; int charsRead = input . read ( buffer , newPosition , buffer . length - newPosition ) ; if ( charsRead == - 1 ) throw new IOException ( "read past eof" ) ; else bufferLength += charsRead ; } public final char BeginToken ( ) throws IOException { tokenStart = bufferPosition ; return readChar ( ) ; } public final void backup ( int amount ) { bufferPosition -= amount ; } public final String GetImage ( ) { return new String ( buffer , tokenStart , bufferPosition - tokenStart ) ; } public final char [ ] GetSuffix ( int len ) { char [ ] value = new char [ len ] ; System . arraycopy ( buffer , bufferPosition - len , value , 0 , len ) ; return value ; } public final void Done ( ) { try { input . close ( ) ; } catch ( IOException e ) { System . err . println ( "Caught: " + e + "; ignoring." ) ; } } public final int getColumn ( ) { return bufferStart + bufferPosition ; } public final int getLine ( ) { return 1 ; } public final int getEndColumn ( ) { return bufferStart + bufferPosition ; } public final int getEndLine ( ) { return 1 ; } public final int getBeginColumn ( ) { return bufferStart + tokenStart ; } public final int getBeginLine ( ) { return 1 ; } } 	0	['14', '1', '0', '2', '25', '3', '1', '1', '13', '0.602564103', '237', '0', '0', '0', '0.404761905', '0', '0', '15.5', '1', '0.9286', '0']
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . * ; import org . apache . lucene . util . ToStringUtils ; import java . io . IOException ; import java . util . Set ; public class ValueSourceQuery extends Query { ValueSource valSrc ; public ValueSourceQuery ( ValueSource valSrc ) { this . valSrc = valSrc ; } public Query rewrite ( IndexReader reader ) throws IOException { return this ; } public void extractTerms ( Set terms ) { } private class ValueSourceWeight implements Weight { Searcher searcher ; float queryNorm ; float queryWeight ; public ValueSourceWeight ( Searcher searcher ) { this . searcher = searcher ; } public Query getQuery ( ) { return ValueSourceQuery . this ; } public float getValue ( ) { return queryWeight ; } public float sumOfSquaredWeights ( ) throws IOException { queryWeight = getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float norm ) { this . queryNorm = norm ; queryWeight *= this . queryNorm ; } public Scorer scorer ( IndexReader reader ) throws IOException { return new ValueSourceScorer ( getSimilarity ( searcher ) , reader , this ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { return scorer ( reader ) . explain ( doc ) ; } } private class ValueSourceScorer extends Scorer { private final IndexReader reader ; private final ValueSourceWeight weight ; private final int maxDoc ; private final float qWeight ; private int doc = - 1 ; private final DocValues vals ; private ValueSourceScorer ( Similarity similarity , IndexReader reader , ValueSourceWeight w ) throws IOException { super ( similarity ) ; this . weight = w ; this . qWeight = w . getValue ( ) ; this . reader = reader ; this . maxDoc = reader . maxDoc ( ) ; vals = valSrc . getValues ( reader ) ; } public boolean next ( ) throws IOException { for ( ; ; ) { ++ doc ; if ( doc >= maxDoc ) { return false ; } if ( reader . isDeleted ( doc ) ) { continue ; } return true ; } } public int doc ( ) { return doc ; } public float score ( ) throws IOException { return qWeight * vals . floatVal ( doc ) ; } public boolean skipTo ( int target ) throws IOException { doc = target - 1 ; return next ( ) ; } public Explanation explain ( int doc ) throws IOException { float sc = qWeight * vals . floatVal ( doc ) ; Explanation result = new ComplexExplanation ( true , sc , ValueSourceQuery . this . toString ( ) + ", product of:" ) ; result . addDetail ( vals . explain ( doc ) ) ; result . addDetail ( new Explanation ( getBoost ( ) , "boost" ) ) ; result . addDetail ( new Explanation ( weight . queryNorm , "queryNorm" ) ) ; return result ; } } protected Weight createWeight ( Searcher searcher ) { return new ValueSourceQuery . ValueSourceWeight ( searcher ) ; } public String toString ( String field ) { return valSrc . toString ( ) + ToStringUtils . boost ( getBoost ( ) ) ; } public boolean equals ( Object o ) { if ( getClass ( ) != o . getClass ( ) ) { return false ; } ValueSourceQuery other = ( ValueSourceQuery ) o ; return this . getBoost ( ) == other . getBoost ( ) && this . valSrc . equals ( other . valSrc ) ; } public int hashCode ( ) { return ( getClass ( ) . hashCode ( ) + valSrc . hashCode ( ) ) ^ Float . floatToIntBits ( getBoost ( ) ) ; } } 	1	['7', '2', '1', '11', '20', '9', '5', '7', '6', '0.5', '74', '0', '1', '0.666666667', '0.265306122', '2', '3', '9.428571429', '4', '1.2857', '1']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; public class WildcardTermEnum extends FilteredTermEnum { Term searchTerm ; String field = "" ; String text = "" ; String pre = "" ; int preLen = 0 ; boolean endEnum = false ; public WildcardTermEnum ( IndexReader reader , Term term ) throws IOException { super ( ) ; searchTerm = term ; field = searchTerm . field ( ) ; text = searchTerm . text ( ) ; int sidx = text . indexOf ( WILDCARD_STRING ) ; int cidx = text . indexOf ( WILDCARD_CHAR ) ; int idx = sidx ; if ( idx == - 1 ) { idx = cidx ; } else if ( cidx >= 0 ) { idx = Math . min ( idx , cidx ) ; } pre = searchTerm . text ( ) . substring ( 0 , idx ) ; preLen = pre . length ( ) ; text = text . substring ( preLen ) ; setEnum ( reader . terms ( new Term ( searchTerm . field ( ) , pre ) ) ) ; } protected final boolean termCompare ( Term term ) { if ( field == term . field ( ) ) { String searchText = term . text ( ) ; if ( searchText . startsWith ( pre ) ) { return wildcardEquals ( text , 0 , searchText , preLen ) ; } } endEnum = true ; return false ; } public final float difference ( ) { return 1.0f ; } public final boolean endEnum ( ) { return endEnum ; } public static final char WILDCARD_STRING = '*' ; public static final char WILDCARD_CHAR = '?' ; public static final boolean wildcardEquals ( String pattern , int patternIdx , String string , int stringIdx ) { int p = patternIdx ; for ( int s = stringIdx ; ; ++ p , ++ s ) { boolean sEnd = ( s >= string . length ( ) ) ; boolean pEnd = ( p >= pattern . length ( ) ) ; if ( sEnd ) { boolean justWildcardsLeft = true ; int wildcardSearchPos = p ; while ( wildcardSearchPos < pattern . length ( ) && justWildcardsLeft ) { char wildchar = pattern . charAt ( wildcardSearchPos ) ; if ( wildchar != WILDCARD_CHAR && wildchar != WILDCARD_STRING ) { justWildcardsLeft = false ; } else { if ( wildchar == WILDCARD_CHAR ) { return false ; } wildcardSearchPos ++ ; } } if ( justWildcardsLeft ) { return true ; } } if ( sEnd || pEnd ) { break ; } if ( pattern . charAt ( p ) == WILDCARD_CHAR ) { continue ; } if ( pattern . charAt ( p ) == WILDCARD_STRING ) { ++ p ; for ( int i = string . length ( ) ; i >= s ; -- i ) { if ( wildcardEquals ( pattern , p , string , i ) ) { return true ; } } break ; } if ( pattern . charAt ( p ) != string . charAt ( s ) ) { break ; } } return false ; } public void close ( ) throws IOException { super . close ( ) ; searchTerm = null ; field = null ; text = null ; } } 	0	['6', '3', '0', '5', '20', '5', '1', '4', '5', '0.825', '247', '0', '1', '0.722222222', '0.333333333', '1', '4', '38.83333333', '16', '3.6667', '0']
package org . apache . lucene . index ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . FieldSelector ; import org . apache . lucene . document . FieldSelectorResult ; import org . apache . lucene . document . Fieldable ; import java . io . IOException ; import java . util . * ; public class ParallelReader extends IndexReader { private List readers = new ArrayList ( ) ; private SortedMap fieldToReader = new TreeMap ( ) ; private Map readerToFields = new HashMap ( ) ; private List storedFieldReaders = new ArrayList ( ) ; private int maxDoc ; private int numDocs ; private boolean hasDeletions ; public ParallelReader ( ) throws IOException { super ( null ) ; } public void add ( IndexReader reader ) throws IOException { ensureOpen ( ) ; add ( reader , false ) ; } public void add ( IndexReader reader , boolean ignoreStoredFields ) throws IOException { ensureOpen ( ) ; if ( readers . size ( ) == 0 ) { this . maxDoc = reader . maxDoc ( ) ; this . numDocs = reader . numDocs ( ) ; this . hasDeletions = reader . hasDeletions ( ) ; } if ( reader . maxDoc ( ) != maxDoc ) throw new IllegalArgumentException ( "All readers must have same maxDoc: " + maxDoc + "!=" + reader . maxDoc ( ) ) ; if ( reader . numDocs ( ) != numDocs ) throw new IllegalArgumentException ( "All readers must have same numDocs: " + numDocs + "!=" + reader . numDocs ( ) ) ; Collection fields = reader . getFieldNames ( IndexReader . FieldOption . ALL ) ; readerToFields . put ( reader , fields ) ; Iterator i = fields . iterator ( ) ; while ( i . hasNext ( ) ) { String field = ( String ) i . next ( ) ; if ( fieldToReader . get ( field ) == null ) fieldToReader . put ( field , reader ) ; } if ( ! ignoreStoredFields ) storedFieldReaders . add ( reader ) ; readers . add ( reader ) ; } public int numDocs ( ) { return numDocs ; } public int maxDoc ( ) { return maxDoc ; } public boolean hasDeletions ( ) { return hasDeletions ; } public boolean isDeleted ( int n ) { if ( readers . size ( ) > 0 ) return ( ( IndexReader ) readers . get ( 0 ) ) . isDeleted ( n ) ; return false ; } protected void doDelete ( int n ) throws CorruptIndexException , IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { ( ( IndexReader ) readers . get ( i ) ) . deleteDocument ( n ) ; } hasDeletions = true ; } protected void doUndeleteAll ( ) throws CorruptIndexException , IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { ( ( IndexReader ) readers . get ( i ) ) . undeleteAll ( ) ; } hasDeletions = false ; } public Document document ( int n , FieldSelector fieldSelector ) throws CorruptIndexException , IOException { ensureOpen ( ) ; Document result = new Document ( ) ; for ( int i = 0 ; i < storedFieldReaders . size ( ) ; i ++ ) { IndexReader reader = ( IndexReader ) storedFieldReaders . get ( i ) ; boolean include = ( fieldSelector == null ) ; if ( ! include ) { Iterator it = ( ( Collection ) readerToFields . get ( reader ) ) . iterator ( ) ; while ( it . hasNext ( ) ) if ( fieldSelector . accept ( ( String ) it . next ( ) ) != FieldSelectorResult . NO_LOAD ) { include = true ; break ; } } if ( include ) { Iterator fieldIterator = reader . document ( n , fieldSelector ) . getFields ( ) . iterator ( ) ; while ( fieldIterator . hasNext ( ) ) { result . add ( ( Fieldable ) fieldIterator . next ( ) ) ; } } } return result ; } public TermFreqVector [ ] getTermFreqVectors ( int n ) throws IOException { ensureOpen ( ) ; ArrayList results = new ArrayList ( ) ; Iterator i = fieldToReader . entrySet ( ) . iterator ( ) ; while ( i . hasNext ( ) ) { Map . Entry e = ( Map . Entry ) i . next ( ) ; String field = ( String ) e . getKey ( ) ; IndexReader reader = ( IndexReader ) e . getValue ( ) ; TermFreqVector vector = reader . getTermFreqVector ( n , field ) ; if ( vector != null ) results . add ( vector ) ; } return ( TermFreqVector [ ] ) results . toArray ( new TermFreqVector [ results . size ( ) ] ) ; } public TermFreqVector getTermFreqVector ( int n , String field ) throws IOException { ensureOpen ( ) ; IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; return reader == null ? null : reader . getTermFreqVector ( n , field ) ; } public boolean hasNorms ( String field ) throws IOException { ensureOpen ( ) ; IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; return reader == null ? false : reader . hasNorms ( field ) ; } public byte [ ] norms ( String field ) throws IOException { ensureOpen ( ) ; IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; return reader == null ? null : reader . norms ( field ) ; } public void norms ( String field , byte [ ] result , int offset ) throws IOException { ensureOpen ( ) ; IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; if ( reader != null ) reader . norms ( field , result , offset ) ; } protected void doSetNorm ( int n , String field , byte value ) throws CorruptIndexException , IOException { IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; if ( reader != null ) reader . doSetNorm ( n , field , value ) ; } public TermEnum terms ( ) throws IOException { ensureOpen ( ) ; return new ParallelTermEnum ( ) ; } public TermEnum terms ( Term term ) throws IOException { ensureOpen ( ) ; return new ParallelTermEnum ( term ) ; } public int docFreq ( Term term ) throws IOException { ensureOpen ( ) ; IndexReader reader = ( ( IndexReader ) fieldToReader . get ( term . field ( ) ) ) ; return reader == null ? 0 : reader . docFreq ( term ) ; } public TermDocs termDocs ( Term term ) throws IOException { ensureOpen ( ) ; return new ParallelTermDocs ( term ) ; } public TermDocs termDocs ( ) throws IOException { ensureOpen ( ) ; return new ParallelTermDocs ( ) ; } public TermPositions termPositions ( Term term ) throws IOException { ensureOpen ( ) ; return new ParallelTermPositions ( term ) ; } public TermPositions termPositions ( ) throws IOException { ensureOpen ( ) ; return new ParallelTermPositions ( ) ; } protected void doCommit ( ) throws IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) ( ( IndexReader ) readers . get ( i ) ) . commit ( ) ; } protected synchronized void doClose ( ) throws IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) ( ( IndexReader ) readers . get ( i ) ) . close ( ) ; } public Collection getFieldNames ( IndexReader . FieldOption fieldNames ) { ensureOpen ( ) ; Set fieldSet = new HashSet ( ) ; for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader reader = ( ( IndexReader ) readers . get ( i ) ) ; Collection names = reader . getFieldNames ( fieldNames ) ; fieldSet . addAll ( names ) ; } return fieldSet ; } private class ParallelTermEnum extends TermEnum { private String field ; private Iterator fieldIterator ; private TermEnum termEnum ; public ParallelTermEnum ( ) throws IOException { field = ( String ) fieldToReader . firstKey ( ) ; if ( field != null ) termEnum = ( ( IndexReader ) fieldToReader . get ( field ) ) . terms ( ) ; } public ParallelTermEnum ( Term term ) throws IOException { field = term . field ( ) ; IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; if ( reader != null ) termEnum = reader . terms ( term ) ; } public boolean next ( ) throws IOException { if ( termEnum == null ) return false ; if ( termEnum . next ( ) && termEnum . term ( ) . field ( ) == field ) return true ; termEnum . close ( ) ; if ( fieldIterator == null ) { fieldIterator = fieldToReader . tailMap ( field ) . keySet ( ) . iterator ( ) ; fieldIterator . next ( ) ; } while ( fieldIterator . hasNext ( ) ) { field = ( String ) fieldIterator . next ( ) ; termEnum = ( ( IndexReader ) fieldToReader . get ( field ) ) . terms ( new Term ( field , "" ) ) ; Term term = termEnum . term ( ) ; if ( term != null && term . field ( ) == field ) return true ; else termEnum . close ( ) ; } return false ; } public Term term ( ) { if ( termEnum == null ) return null ; return termEnum . term ( ) ; } public int docFreq ( ) { if ( termEnum == null ) return 0 ; return termEnum . docFreq ( ) ; } public void close ( ) throws IOException { if ( termEnum != null ) termEnum . close ( ) ; } } private class ParallelTermDocs implements TermDocs { protected TermDocs termDocs ; public ParallelTermDocs ( ) { } public ParallelTermDocs ( Term term ) throws IOException { seek ( term ) ; } public int doc ( ) { return termDocs . doc ( ) ; } public int freq ( ) { return termDocs . freq ( ) ; } public void seek ( Term term ) throws IOException { IndexReader reader = ( ( IndexReader ) fieldToReader . get ( term . field ( ) ) ) ; termDocs = reader != null ? reader . termDocs ( term ) : null ; } public void seek ( TermEnum termEnum ) throws IOException { seek ( termEnum . term ( ) ) ; } public boolean next ( ) throws IOException { if ( termDocs == null ) return false ; return termDocs . next ( ) ; } public int read ( final int [ ] docs , final int [ ] freqs ) throws IOException { if ( termDocs == null ) return 0 ; return termDocs . read ( docs , freqs ) ; } public boolean skipTo ( int target ) throws IOException { if ( termDocs == null ) return false ; return termDocs . skipTo ( target ) ; } public void close ( ) throws IOException { if ( termDocs != null ) termDocs . close ( ) ; } } private class ParallelTermPositions extends ParallelTermDocs implements TermPositions { public ParallelTermPositions ( ) { } public ParallelTermPositions ( Term term ) throws IOException { seek ( term ) ; } public void seek ( Term term ) throws IOException { IndexReader reader = ( ( IndexReader ) fieldToReader . get ( term . field ( ) ) ) ; termDocs = reader != null ? reader . termPositions ( term ) : null ; } public int nextPosition ( ) throws IOException { return ( ( TermPositions ) termDocs ) . nextPosition ( ) ; } public int getPayloadLength ( ) { return ( ( TermPositions ) termDocs ) . getPayloadLength ( ) ; } public byte [ ] getPayload ( byte [ ] data , int offset ) throws IOException { return ( ( TermPositions ) termDocs ) . getPayload ( data , offset ) ; } public boolean isPayloadAvailable ( ) { return ( ( TermPositions ) termDocs ) . isPayloadAvailable ( ) ; } } } 	1	['27', '2', '0', '16', '84', '193', '3', '16', '21', '0.824175824', '558', '1', '0', '0.697674419', '0.168350168', '1', '9', '19.40740741', '2', '1.037', '5']
package org . apache . lucene . store ; import java . io . IOException ; public class LockObtainFailedException extends IOException { public LockObtainFailedException ( String message ) { super ( message ) ; } } 	0	['1', '4', '0', '5', '2', '0', '5', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . FieldCache ; import org . apache . lucene . search . function . DocValues ; import java . io . IOException ; public class ShortFieldSource extends FieldCacheSource { private FieldCache . ShortParser parser ; public ShortFieldSource ( String field ) { this ( field , null ) ; } public ShortFieldSource ( String field , FieldCache . ShortParser parser ) { super ( field ) ; this . parser = parser ; } public String description ( ) { return "short(" + super . description ( ) + ')' ; } public DocValues getCachedFieldValues ( FieldCache cache , String field , IndexReader reader ) throws IOException { final short [ ] arr = ( parser == null ) ? cache . getShorts ( reader , field ) : cache . getShorts ( reader , field , parser ) ; return new DocValues ( reader . maxDoc ( ) ) { public float floatVal ( int doc ) { return ( float ) arr [ doc ] ; } public int intVal ( int doc ) { return arr [ doc ] ; } public String toString ( int doc ) { return description ( ) + '=' + intVal ( doc ) ; } Object getInnerArray ( ) { return arr ; } } ; } public boolean cachedFieldSourceEquals ( FieldCacheSource o ) { if ( o . getClass ( ) != ShortFieldSource . class ) { return false ; } ShortFieldSource other = ( ShortFieldSource ) o ; return this . parser == null ? other . parser == null : this . parser . getClass ( ) == other . parser . getClass ( ) ; } public int cachedFieldSourceHashCode ( ) { return parser == null ? Short . class . hashCode ( ) : parser . getClass ( ) . hashCode ( ) ; } } 	1	['7', '3', '0', '7', '22', '9', '2', '6', '6', '0.777777778', '122', '0.333333333', '1', '0.705882353', '0.333333333', '2', '3', '16', '6', '1.7143', '1']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . * ; abstract class PhraseScorer extends Scorer { private Weight weight ; protected byte [ ] norms ; protected float value ; private boolean firstTime = true ; private boolean more = true ; protected PhraseQueue pq ; protected PhrasePositions first , last ; private float freq ; PhraseScorer ( Weight weight , TermPositions [ ] tps , int [ ] offsets , Similarity similarity , byte [ ] norms ) { super ( similarity ) ; this . norms = norms ; this . weight = weight ; this . value = weight . getValue ( ) ; for ( int i = 0 ; i < tps . length ; i ++ ) { PhrasePositions pp = new PhrasePositions ( tps [ i ] , offsets [ i ] ) ; if ( last != null ) { last . next = pp ; } else first = pp ; last = pp ; } pq = new PhraseQueue ( tps . length ) ; } public int doc ( ) { return first . doc ; } public boolean next ( ) throws IOException { if ( firstTime ) { init ( ) ; firstTime = false ; } else if ( more ) { more = last . next ( ) ; } return doNext ( ) ; } private boolean doNext ( ) throws IOException { while ( more ) { while ( more && first . doc < last . doc ) { more = first . skipTo ( last . doc ) ; firstToLast ( ) ; } if ( more ) { freq = phraseFreq ( ) ; if ( freq == 0.0f ) more = last . next ( ) ; else return true ; } } return false ; } public float score ( ) throws IOException { float raw = getSimilarity ( ) . tf ( freq ) * value ; return raw * Similarity . decodeNorm ( norms [ first . doc ] ) ; } public boolean skipTo ( int target ) throws IOException { firstTime = false ; for ( PhrasePositions pp = first ; more && pp != null ; pp = pp . next ) { more = pp . skipTo ( target ) ; } if ( more ) sort ( ) ; return doNext ( ) ; } protected abstract float phraseFreq ( ) throws IOException ; private void init ( ) throws IOException { for ( PhrasePositions pp = first ; more && pp != null ; pp = pp . next ) more = pp . next ( ) ; if ( more ) sort ( ) ; } private void sort ( ) { pq . clear ( ) ; for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) pq . put ( pp ) ; pqToList ( ) ; } protected final void pqToList ( ) { last = first = null ; while ( pq . top ( ) != null ) { PhrasePositions pp = ( PhrasePositions ) pq . pop ( ) ; if ( last != null ) { last . next = pp ; } else first = pp ; last = pp ; pp . next = null ; } } protected final void firstToLast ( ) { last . next = first ; last = first ; first = first . next ; last . next = null ; } public Explanation explain ( final int doc ) throws IOException { Explanation tfExplanation = new Explanation ( ) ; while ( next ( ) && doc ( ) < doc ) { } float phraseFreq = ( doc ( ) == doc ) ? freq : 0.0f ; tfExplanation . setValue ( getSimilarity ( ) . tf ( phraseFreq ) ) ; tfExplanation . setDescription ( "tf(phraseFreq=" + phraseFreq + ")" ) ; return tfExplanation ; } public String toString ( ) { return "scorer(" + weight + ")" ; } } 	0	['13', '2', '2', '9', '34', '0', '2', '7', '6', '0.666666667', '345', '1', '4', '0.4', '0.21978022', '1', '3', '24.84615385', '3', '1.1538', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; import java . io . IOException ; import java . util . BitSet ; import java . util . Set ; public class FilteredQuery extends Query { Query query ; Filter filter ; public FilteredQuery ( Query query , Filter filter ) { this . query = query ; this . filter = filter ; } protected Weight createWeight ( final Searcher searcher ) throws IOException { final Weight weight = query . createWeight ( searcher ) ; final Similarity similarity = query . getSimilarity ( searcher ) ; return new Weight ( ) { private float value ; public float getValue ( ) { return value ; } public float sumOfSquaredWeights ( ) throws IOException { return weight . sumOfSquaredWeights ( ) * getBoost ( ) * getBoost ( ) ; } public void normalize ( float v ) { weight . normalize ( v ) ; value = weight . getValue ( ) * getBoost ( ) ; } public Explanation explain ( IndexReader ir , int i ) throws IOException { Explanation inner = weight . explain ( ir , i ) ; if ( getBoost ( ) != 1 ) { Explanation preBoost = inner ; inner = new Explanation ( inner . getValue ( ) * getBoost ( ) , "product of:" ) ; inner . addDetail ( new Explanation ( getBoost ( ) , "boost" ) ) ; inner . addDetail ( preBoost ) ; } Filter f = FilteredQuery . this . filter ; BitSet matches = f . bits ( ir ) ; if ( matches . get ( i ) ) return inner ; Explanation result = new Explanation ( 0.0f , "failure to match filter: " + f . toString ( ) ) ; result . addDetail ( inner ) ; return result ; } public Query getQuery ( ) { return FilteredQuery . this ; } public Scorer scorer ( IndexReader indexReader ) throws IOException { final Scorer scorer = weight . scorer ( indexReader ) ; final BitSet bitset = filter . bits ( indexReader ) ; return new Scorer ( similarity ) { public boolean next ( ) throws IOException { do { if ( ! scorer . next ( ) ) { return false ; } } while ( ! bitset . get ( scorer . doc ( ) ) ) ; return true ; } public int doc ( ) { return scorer . doc ( ) ; } public boolean skipTo ( int i ) throws IOException { if ( ! scorer . skipTo ( i ) ) { return false ; } while ( ! bitset . get ( scorer . doc ( ) ) ) { int nextFiltered = bitset . nextSetBit ( scorer . doc ( ) + 1 ) ; if ( nextFiltered == - 1 ) { return false ; } else if ( ! scorer . skipTo ( nextFiltered ) ) { return false ; } } return true ; } public float score ( ) throws IOException { return getBoost ( ) * scorer . score ( ) ; } public Explanation explain ( int i ) throws IOException { Explanation exp = scorer . explain ( i ) ; exp . setValue ( getBoost ( ) * exp . getValue ( ) ) ; if ( bitset . get ( i ) ) exp . setDescription ( "allowed by filter: " + exp . getDescription ( ) ) ; else exp . setDescription ( "removed by filter: " + exp . getDescription ( ) ) ; return exp ; } } ; } } ; } public Query rewrite ( IndexReader reader ) throws IOException { Query rewritten = query . rewrite ( reader ) ; if ( rewritten != query ) { FilteredQuery clone = ( FilteredQuery ) this . clone ( ) ; clone . query = rewritten ; return clone ; } else { return this ; } } public Query getQuery ( ) { return query ; } public Filter getFilter ( ) { return filter ; } public void extractTerms ( Set terms ) { getQuery ( ) . extractTerms ( terms ) ; } public String toString ( String s ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "filtered(" ) ; buffer . append ( query . toString ( s ) ) ; buffer . append ( ")->" ) ; buffer . append ( filter ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( o instanceof FilteredQuery ) { FilteredQuery fq = ( FilteredQuery ) o ; return ( query . equals ( fq . query ) && filter . equals ( fq . filter ) && getBoost ( ) == fq . getBoost ( ) ) ; } return false ; } public int hashCode ( ) { return query . hashCode ( ) ^ filter . hashCode ( ) + Float . floatToRawIntBits ( getBoost ( ) ) ; } } 	1	['9', '2', '0', '9', '26', '0', '2', '8', '8', '0.3125', '143', '0', '2', '0.6', '0.222222222', '2', '4', '14.66666667', '5', '1.3333', '2']
package org . apache . lucene . document ; import java . util . Set ; public class SetBasedFieldSelector implements FieldSelector { private Set fieldsToLoad ; private Set lazyFieldsToLoad ; public SetBasedFieldSelector ( Set fieldsToLoad , Set lazyFieldsToLoad ) { this . fieldsToLoad = fieldsToLoad ; this . lazyFieldsToLoad = lazyFieldsToLoad ; } public FieldSelectorResult accept ( String fieldName ) { FieldSelectorResult result = FieldSelectorResult . NO_LOAD ; if ( fieldsToLoad . contains ( fieldName ) == true ) { result = FieldSelectorResult . LOAD ; } if ( lazyFieldsToLoad . contains ( fieldName ) == true ) { result = FieldSelectorResult . LAZY_LOAD ; } return result ; } } 	0	['2', '1', '0', '2', '4', '0', '0', '2', '2', '0', '33', '1', '0', '0', '0.666666667', '0', '0', '14.5', '3', '1.5', '0']
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . FieldCache ; import org . apache . lucene . search . function . DocValues ; import java . io . IOException ; public class FloatFieldSource extends FieldCacheSource { private FieldCache . FloatParser parser ; public FloatFieldSource ( String field ) { this ( field , null ) ; } public FloatFieldSource ( String field , FieldCache . FloatParser parser ) { super ( field ) ; this . parser = parser ; } public String description ( ) { return "float(" + super . description ( ) + ')' ; } public DocValues getCachedFieldValues ( FieldCache cache , String field , IndexReader reader ) throws IOException { final float [ ] arr = ( parser == null ) ? cache . getFloats ( reader , field ) : cache . getFloats ( reader , field , parser ) ; return new DocValues ( reader . maxDoc ( ) ) { public float floatVal ( int doc ) { return arr [ doc ] ; } public String toString ( int doc ) { return description ( ) + '=' + arr [ doc ] ; } Object getInnerArray ( ) { return arr ; } } ; } public boolean cachedFieldSourceEquals ( FieldCacheSource o ) { if ( o . getClass ( ) != FloatFieldSource . class ) { return false ; } FloatFieldSource other = ( FloatFieldSource ) o ; return this . parser == null ? other . parser == null : this . parser . getClass ( ) == other . parser . getClass ( ) ; } public int cachedFieldSourceHashCode ( ) { return parser == null ? Float . class . hashCode ( ) : parser . getClass ( ) . hashCode ( ) ; } } 	1	['7', '3', '0', '7', '22', '9', '2', '6', '6', '0.777777778', '122', '0.333333333', '1', '0.705882353', '0.333333333', '2', '3', '16', '6', '1.7143', '2']
package org . apache . lucene . search ; import java . util . ArrayList ; public class Explanation implements java . io . Serializable { private float value ; private String description ; private ArrayList details ; public Explanation ( ) { } public Explanation ( float value , String description ) { this . value = value ; this . description = description ; } public boolean isMatch ( ) { return ( 0.0f < getValue ( ) ) ; } public float getValue ( ) { return value ; } public void setValue ( float value ) { this . value = value ; } public String getDescription ( ) { return description ; } public void setDescription ( String description ) { this . description = description ; } protected String getSummary ( ) { return getValue ( ) + " = " + getDescription ( ) ; } public Explanation [ ] getDetails ( ) { if ( details == null ) return null ; return ( Explanation [ ] ) details . toArray ( new Explanation [ 0 ] ) ; } public void addDetail ( Explanation detail ) { if ( details == null ) details = new ArrayList ( ) ; details . add ( detail ) ; } public String toString ( ) { return toString ( 0 ) ; } protected String toString ( int depth ) { StringBuffer buffer = new StringBuffer ( ) ; for ( int i = 0 ; i < depth ; i ++ ) { buffer . append ( "  " ) ; } buffer . append ( getSummary ( ) ) ; buffer . append ( "\n" ) ; Explanation [ ] details = getDetails ( ) ; if ( details != null ) { for ( int i = 0 ; i < details . length ; i ++ ) { buffer . append ( details [ i ] . toString ( depth + 1 ) ) ; } } return buffer . toString ( ) ; } public String toHtml ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "<ul>\n" ) ; buffer . append ( "<li>" ) ; buffer . append ( getSummary ( ) ) ; buffer . append ( "<br />\n" ) ; Explanation [ ] details = getDetails ( ) ; if ( details != null ) { for ( int i = 0 ; i < details . length ; i ++ ) { buffer . append ( details [ i ] . toHtml ( ) ) ; } } buffer . append ( "</li>\n" ) ; buffer . append ( "</ul>\n" ) ; return buffer . toString ( ) ; } } 	0	['13', '1', '1', '41', '21', '64', '41', '0', '11', '0.611111111', '197', '1', '0', '0', '0.292307692', '0', '0', '13.92307692', '4', '1.4615', '0']
package org . apache . lucene . search ; import java . util . Comparator ; import java . util . Date ; import java . util . HashMap ; import java . util . Iterator ; import java . util . Map ; import java . util . TreeSet ; public class FilterManager { protected static FilterManager manager ; protected static final int DEFAULT_CACHE_CLEAN_SIZE = 100 ; protected static final long DEFAULT_CACHE_SLEEP_TIME = 1000 * 60 * 10 ; protected Map cache ; protected int cacheCleanSize ; protected long cleanSleepTime ; protected FilterCleaner filterCleaner ; public synchronized static FilterManager getInstance ( ) { if ( manager == null ) { manager = new FilterManager ( ) ; } return manager ; } protected FilterManager ( ) { cache = new HashMap ( ) ; cacheCleanSize = DEFAULT_CACHE_CLEAN_SIZE ; cleanSleepTime = DEFAULT_CACHE_SLEEP_TIME ; filterCleaner = new FilterCleaner ( ) ; Thread fcThread = new Thread ( filterCleaner ) ; fcThread . setDaemon ( true ) ; fcThread . start ( ) ; } public void setCacheSize ( int cacheCleanSize ) { this . cacheCleanSize = cacheCleanSize ; } public void setCleanThreadSleepTime ( long cleanSleepTime ) { this . cleanSleepTime = cleanSleepTime ; } public Filter getFilter ( Filter filter ) { synchronized ( cache ) { FilterItem fi = null ; fi = ( FilterItem ) cache . get ( new Integer ( filter . hashCode ( ) ) ) ; if ( fi != null ) { fi . timestamp = new Date ( ) . getTime ( ) ; return fi . filter ; } cache . put ( new Integer ( filter . hashCode ( ) ) , new FilterItem ( filter ) ) ; return filter ; } } protected class FilterItem { public Filter filter ; public long timestamp ; public FilterItem ( Filter filter ) { this . filter = filter ; this . timestamp = new Date ( ) . getTime ( ) ; } } protected class FilterCleaner implements Runnable { private boolean running = true ; private TreeSet sortedFilterItems ; public FilterCleaner ( ) { sortedFilterItems = new TreeSet ( new Comparator ( ) { public int compare ( Object a , Object b ) { if ( a instanceof Map . Entry && b instanceof Map . Entry ) { FilterItem fia = ( FilterItem ) ( ( Map . Entry ) a ) . getValue ( ) ; FilterItem fib = ( FilterItem ) ( ( Map . Entry ) b ) . getValue ( ) ; if ( fia . timestamp == fib . timestamp ) { return 0 ; } if ( fia . timestamp < fib . timestamp ) { return - 1 ; } return 1 ; } else { throw new ClassCastException ( "Objects are not Map.Entry" ) ; } } } ) ; } public void run ( ) { while ( running ) { if ( cache . size ( ) > cacheCleanSize ) { sortedFilterItems . clear ( ) ; synchronized ( cache ) { sortedFilterItems . addAll ( cache . entrySet ( ) ) ; Iterator it = sortedFilterItems . iterator ( ) ; int numToDelete = ( int ) ( ( cache . size ( ) - cacheCleanSize ) * 1.5 ) ; int counter = 0 ; while ( it . hasNext ( ) && counter ++ < numToDelete ) { Map . Entry entry = ( Map . Entry ) it . next ( ) ; cache . remove ( entry . getKey ( ) ) ; } } sortedFilterItems . clear ( ) ; } try { Thread . sleep ( cleanSleepTime ) ; } catch ( InterruptedException e ) { } } } } } 	1	['5', '1', '0', '5', '18', '4', '4', '3', '4', '0.821428571', '112', '1', '2', '0', '0.35', '0', '0', '20', '2', '1.2', '1']
package org . apache . lucene . index ; public class FieldReaderException extends RuntimeException { public FieldReaderException ( ) { } public FieldReaderException ( Throwable cause ) { super ( cause ) ; } public FieldReaderException ( String message ) { super ( message ) ; } public FieldReaderException ( String message , Throwable cause ) { super ( message , cause ) ; } } 	0	['4', '4', '0', '1', '8', '6', '1', '0', '4', '2', '20', '0', '0', '1', '0.666666667', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . index ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . FieldSelector ; import org . apache . lucene . search . DefaultSimilarity ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . BufferedIndexInput ; import org . apache . lucene . util . BitVector ; import java . io . IOException ; import java . util . * ; class SegmentReader extends IndexReader { private String segment ; private SegmentInfo si ; FieldInfos fieldInfos ; private FieldsReader fieldsReader ; TermInfosReader tis ; TermVectorsReader termVectorsReaderOrig = null ; ThreadLocal termVectorsLocal = new ThreadLocal ( ) ; BitVector deletedDocs = null ; private boolean deletedDocsDirty = false ; private boolean normsDirty = false ; private boolean undeleteAll = false ; private boolean rollbackDeletedDocsDirty = false ; private boolean rollbackNormsDirty = false ; private boolean rollbackUndeleteAll = false ; IndexInput freqStream ; IndexInput proxStream ; private IndexInput singleNormStream ; CompoundFileReader cfsReader = null ; private class Norm { public Norm ( IndexInput in , int number , long normSeek ) { this . in = in ; this . number = number ; this . normSeek = normSeek ; } private IndexInput in ; private byte [ ] bytes ; private boolean dirty ; private int number ; private long normSeek ; private boolean rollbackDirty ; private void reWrite ( SegmentInfo si ) throws IOException { si . advanceNormGen ( this . number ) ; IndexOutput out = directory ( ) . createOutput ( si . getNormFileName ( this . number ) ) ; try { out . writeBytes ( bytes , maxDoc ( ) ) ; } finally { out . close ( ) ; } this . dirty = false ; } public void close ( ) throws IOException { if ( in != null && in != singleNormStream ) { in . close ( ) ; } in = null ; } } private Hashtable norms = new Hashtable ( ) ; private static Class IMPL ; static { try { String name = System . getProperty ( "org.apache.lucene.SegmentReader.class" , SegmentReader . class . getName ( ) ) ; IMPL = Class . forName ( name ) ; } catch ( ClassNotFoundException e ) { throw new RuntimeException ( "cannot load SegmentReader class: " + e , e ) ; } catch ( SecurityException se ) { try { IMPL = Class . forName ( SegmentReader . class . getName ( ) ) ; } catch ( ClassNotFoundException e ) { throw new RuntimeException ( "cannot load default SegmentReader class: " + e , e ) ; } } } protected SegmentReader ( ) { super ( null ) ; } public static SegmentReader get ( SegmentInfo si ) throws CorruptIndexException , IOException { return get ( si . dir , si , null , false , false , BufferedIndexInput . BUFFER_SIZE ) ; } public static SegmentReader get ( SegmentInfo si , int readBufferSize ) throws CorruptIndexException , IOException { return get ( si . dir , si , null , false , false , readBufferSize ) ; } public static SegmentReader get ( SegmentInfos sis , SegmentInfo si , boolean closeDir ) throws CorruptIndexException , IOException { return get ( si . dir , si , sis , closeDir , true , BufferedIndexInput . BUFFER_SIZE ) ; } public static SegmentReader get ( Directory dir , SegmentInfo si , SegmentInfos sis , boolean closeDir , boolean ownDir , int readBufferSize ) throws CorruptIndexException , IOException { SegmentReader instance ; try { instance = ( SegmentReader ) IMPL . newInstance ( ) ; } catch ( Exception e ) { throw new RuntimeException ( "cannot load SegmentReader class: " + e , e ) ; } instance . init ( dir , sis , closeDir , ownDir ) ; instance . initialize ( si , readBufferSize ) ; return instance ; } private void initialize ( SegmentInfo si , int readBufferSize ) throws CorruptIndexException , IOException { segment = si . name ; this . si = si ; boolean success = false ; try { Directory cfsDir = directory ( ) ; if ( si . getUseCompoundFile ( ) ) { cfsReader = new CompoundFileReader ( directory ( ) , segment + ".cfs" , readBufferSize ) ; cfsDir = cfsReader ; } fieldInfos = new FieldInfos ( cfsDir , segment + ".fnm" ) ; fieldsReader = new FieldsReader ( cfsDir , segment , fieldInfos , readBufferSize ) ; if ( fieldsReader . size ( ) != si . docCount ) { throw new CorruptIndexException ( "doc counts differ for segment " + si . name + ": fieldsReader shows " + fieldsReader . size ( ) + " but segmentInfo shows " + si . docCount ) ; } tis = new TermInfosReader ( cfsDir , segment , fieldInfos , readBufferSize ) ; if ( hasDeletions ( si ) ) { deletedDocs = new BitVector ( directory ( ) , si . getDelFileName ( ) ) ; if ( deletedDocs . count ( ) > maxDoc ( ) ) { throw new CorruptIndexException ( "number of deletes (" + deletedDocs . count ( ) + ") exceeds max doc (" + maxDoc ( ) + ") for segment " + si . name ) ; } } freqStream = cfsDir . openInput ( segment + ".frq" , readBufferSize ) ; proxStream = cfsDir . openInput ( segment + ".prx" , readBufferSize ) ; openNorms ( cfsDir , readBufferSize ) ; if ( fieldInfos . hasVectors ( ) ) { termVectorsReaderOrig = new TermVectorsReader ( cfsDir , segment , fieldInfos , readBufferSize ) ; } success = true ; } finally { if ( ! success ) { doClose ( ) ; } } } protected void doCommit ( ) throws IOException { if ( deletedDocsDirty ) { si . advanceDelGen ( ) ; deletedDocs . write ( directory ( ) , si . getDelFileName ( ) ) ; } if ( undeleteAll && si . hasDeletions ( ) ) { si . clearDelGen ( ) ; } if ( normsDirty ) { si . setNumFields ( fieldInfos . size ( ) ) ; Enumeration values = norms . elements ( ) ; while ( values . hasMoreElements ( ) ) { Norm norm = ( Norm ) values . nextElement ( ) ; if ( norm . dirty ) { norm . reWrite ( si ) ; } } } deletedDocsDirty = false ; normsDirty = false ; undeleteAll = false ; } protected void doClose ( ) throws IOException { if ( fieldsReader != null ) { fieldsReader . close ( ) ; } if ( tis != null ) { tis . close ( ) ; } if ( freqStream != null ) freqStream . close ( ) ; if ( proxStream != null ) proxStream . close ( ) ; closeNorms ( ) ; if ( termVectorsReaderOrig != null ) termVectorsReaderOrig . close ( ) ; if ( cfsReader != null ) cfsReader . close ( ) ; } static boolean hasDeletions ( SegmentInfo si ) throws IOException { return si . hasDeletions ( ) ; } public boolean hasDeletions ( ) { return deletedDocs != null ; } static boolean usesCompoundFile ( SegmentInfo si ) throws IOException { return si . getUseCompoundFile ( ) ; } static boolean hasSeparateNorms ( SegmentInfo si ) throws IOException { return si . hasSeparateNorms ( ) ; } protected void doDelete ( int docNum ) { if ( deletedDocs == null ) deletedDocs = new BitVector ( maxDoc ( ) ) ; deletedDocsDirty = true ; undeleteAll = false ; deletedDocs . set ( docNum ) ; } protected void doUndeleteAll ( ) { deletedDocs = null ; deletedDocsDirty = false ; undeleteAll = true ; } Vector files ( ) throws IOException { return new Vector ( si . files ( ) ) ; } public TermEnum terms ( ) { ensureOpen ( ) ; return tis . terms ( ) ; } public TermEnum terms ( Term t ) throws IOException { ensureOpen ( ) ; return tis . terms ( t ) ; } public synchronized Document document ( int n , FieldSelector fieldSelector ) throws CorruptIndexException , IOException { ensureOpen ( ) ; if ( isDeleted ( n ) ) throw new IllegalArgumentException ( "attempt to access a deleted document" ) ; return fieldsReader . doc ( n , fieldSelector ) ; } public synchronized boolean isDeleted ( int n ) { return ( deletedDocs != null && deletedDocs . get ( n ) ) ; } public TermDocs termDocs ( ) throws IOException { ensureOpen ( ) ; return new SegmentTermDocs ( this ) ; } public TermPositions termPositions ( ) throws IOException { ensureOpen ( ) ; return new SegmentTermPositions ( this ) ; } public int docFreq ( Term t ) throws IOException { ensureOpen ( ) ; TermInfo ti = tis . get ( t ) ; if ( ti != null ) return ti . docFreq ; else return 0 ; } public int numDocs ( ) { int n = maxDoc ( ) ; if ( deletedDocs != null ) n -= deletedDocs . count ( ) ; return n ; } public int maxDoc ( ) { return si . docCount ; } public Collection getFieldNames ( IndexReader . FieldOption fieldOption ) { ensureOpen ( ) ; Set fieldSet = new HashSet ( ) ; for ( int i = 0 ; i < fieldInfos . size ( ) ; i ++ ) { FieldInfo fi = fieldInfos . fieldInfo ( i ) ; if ( fieldOption == IndexReader . FieldOption . ALL ) { fieldSet . add ( fi . name ) ; } else if ( ! fi . isIndexed && fieldOption == IndexReader . FieldOption . UNINDEXED ) { fieldSet . add ( fi . name ) ; } else if ( fi . storePayloads && fieldOption == IndexReader . FieldOption . STORES_PAYLOADS ) { fieldSet . add ( fi . name ) ; } else if ( fi . isIndexed && fieldOption == IndexReader . FieldOption . INDEXED ) { fieldSet . add ( fi . name ) ; } else if ( fi . isIndexed && fi . storeTermVector == false && fieldOption == IndexReader . FieldOption . INDEXED_NO_TERMVECTOR ) { fieldSet . add ( fi . name ) ; } else if ( fi . storeTermVector == true && fi . storePositionWithTermVector == false && fi . storeOffsetWithTermVector == false && fieldOption == IndexReader . FieldOption . TERMVECTOR ) { fieldSet . add ( fi . name ) ; } else if ( fi . isIndexed && fi . storeTermVector && fieldOption == IndexReader . FieldOption . INDEXED_WITH_TERMVECTOR ) { fieldSet . add ( fi . name ) ; } else if ( fi . storePositionWithTermVector && fi . storeOffsetWithTermVector == false && fieldOption == IndexReader . FieldOption . TERMVECTOR_WITH_POSITION ) { fieldSet . add ( fi . name ) ; } else if ( fi . storeOffsetWithTermVector && fi . storePositionWithTermVector == false && fieldOption == IndexReader . FieldOption . TERMVECTOR_WITH_OFFSET ) { fieldSet . add ( fi . name ) ; } else if ( ( fi . storeOffsetWithTermVector && fi . storePositionWithTermVector ) && fieldOption == IndexReader . FieldOption . TERMVECTOR_WITH_POSITION_OFFSET ) { fieldSet . add ( fi . name ) ; } } return fieldSet ; } public synchronized boolean hasNorms ( String field ) { ensureOpen ( ) ; return norms . containsKey ( field ) ; } static byte [ ] createFakeNorms ( int size ) { byte [ ] ones = new byte [ size ] ; Arrays . fill ( ones , DefaultSimilarity . encodeNorm ( 1.0f ) ) ; return ones ; } private byte [ ] ones ; private byte [ ] fakeNorms ( ) { if ( ones == null ) ones = createFakeNorms ( maxDoc ( ) ) ; return ones ; } protected synchronized byte [ ] getNorms ( String field ) throws IOException { Norm norm = ( Norm ) norms . get ( field ) ; if ( norm == null ) return null ; if ( norm . bytes == null ) { byte [ ] bytes = new byte [ maxDoc ( ) ] ; norms ( field , bytes , 0 ) ; norm . bytes = bytes ; norm . close ( ) ; } return norm . bytes ; } public synchronized byte [ ] norms ( String field ) throws IOException { ensureOpen ( ) ; byte [ ] bytes = getNorms ( field ) ; if ( bytes == null ) bytes = fakeNorms ( ) ; return bytes ; } protected void doSetNorm ( int doc , String field , byte value ) throws IOException { Norm norm = ( Norm ) norms . get ( field ) ; if ( norm == null ) return ; norm . dirty = true ; normsDirty = true ; norms ( field ) [ doc ] = value ; } public synchronized void norms ( String field , byte [ ] bytes , int offset ) throws IOException { ensureOpen ( ) ; Norm norm = ( Norm ) norms . get ( field ) ; if ( norm == null ) { System . arraycopy ( fakeNorms ( ) , 0 , bytes , offset , maxDoc ( ) ) ; return ; } if ( norm . bytes != null ) { System . arraycopy ( norm . bytes , 0 , bytes , offset , maxDoc ( ) ) ; return ; } norm . in . seek ( norm . normSeek ) ; norm . in . readBytes ( bytes , offset , maxDoc ( ) ) ; } private void openNorms ( Directory cfsDir , int readBufferSize ) throws IOException { long nextNormSeek = SegmentMerger . NORMS_HEADER . length ; int maxDoc = maxDoc ( ) ; for ( int i = 0 ; i < fieldInfos . size ( ) ; i ++ ) { FieldInfo fi = fieldInfos . fieldInfo ( i ) ; if ( fi . isIndexed && ! fi . omitNorms ) { Directory d = directory ( ) ; String fileName = si . getNormFileName ( fi . number ) ; if ( ! si . hasSeparateNorms ( fi . number ) ) { d = cfsDir ; } boolean singleNormFile = fileName . endsWith ( "." + IndexFileNames . NORMS_EXTENSION ) ; IndexInput normInput = null ; long normSeek ; if ( singleNormFile ) { normSeek = nextNormSeek ; if ( singleNormStream == null ) { singleNormStream = d . openInput ( fileName , readBufferSize ) ; } normInput = singleNormStream ; } else { normSeek = 0 ; normInput = d . openInput ( fileName ) ; } norms . put ( fi . name , new Norm ( normInput , fi . number , normSeek ) ) ; nextNormSeek += maxDoc ; } } } private void closeNorms ( ) throws IOException { synchronized ( norms ) { Enumeration enumerator = norms . elements ( ) ; while ( enumerator . hasMoreElements ( ) ) { Norm norm = ( Norm ) enumerator . nextElement ( ) ; norm . close ( ) ; } if ( singleNormStream != null ) { singleNormStream . close ( ) ; singleNormStream = null ; } } } private TermVectorsReader getTermVectorsReader ( ) { TermVectorsReader tvReader = ( TermVectorsReader ) termVectorsLocal . get ( ) ; if ( tvReader == null ) { tvReader = ( TermVectorsReader ) termVectorsReaderOrig . clone ( ) ; termVectorsLocal . set ( tvReader ) ; } return tvReader ; } public TermFreqVector getTermFreqVector ( int docNumber , String field ) throws IOException { ensureOpen ( ) ; FieldInfo fi = fieldInfos . fieldInfo ( field ) ; if ( fi == null || ! fi . storeTermVector || termVectorsReaderOrig == null ) return null ; TermVectorsReader termVectorsReader = getTermVectorsReader ( ) ; if ( termVectorsReader == null ) return null ; return termVectorsReader . get ( docNumber , field ) ; } public TermFreqVector [ ] getTermFreqVectors ( int docNumber ) throws IOException { ensureOpen ( ) ; if ( termVectorsReaderOrig == null ) return null ; TermVectorsReader termVectorsReader = getTermVectorsReader ( ) ; if ( termVectorsReader == null ) return null ; return termVectorsReader . get ( docNumber ) ; } FieldInfos fieldInfos ( ) { return fieldInfos ; } String getSegmentName ( ) { return segment ; } void setSegmentInfo ( SegmentInfo info ) { si = info ; } void startCommit ( ) { super . startCommit ( ) ; rollbackDeletedDocsDirty = deletedDocsDirty ; rollbackNormsDirty = normsDirty ; rollbackUndeleteAll = undeleteAll ; Enumeration values = norms . elements ( ) ; while ( values . hasMoreElements ( ) ) { Norm norm = ( Norm ) values . nextElement ( ) ; norm . rollbackDirty = norm . dirty ; } } void rollbackCommit ( ) { super . rollbackCommit ( ) ; deletedDocsDirty = rollbackDeletedDocsDirty ; normsDirty = rollbackNormsDirty ; undeleteAll = rollbackUndeleteAll ; Enumeration values = norms . elements ( ) ; while ( values . hasMoreElements ( ) ) { Norm norm = ( Norm ) values . nextElement ( ) ; norm . dirty = norm . rollbackDirty ; } } } 	1	['45', '2', '0', '31', '137', '732', '6', '28', '20', '0.899793388', '1253', '0.590909091', '10', '0.582524272', '0.131118881', '1', '8', '26.35555556', '28', '1.7556', '11']
package org . apache . lucene . search ; import java . io . IOException ; import java . io . StringReader ; import java . util . ArrayList ; import java . util . Arrays ; import java . util . HashMap ; import java . util . Iterator ; import java . util . List ; import java . util . Map ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . analysis . TokenStream ; import org . apache . lucene . index . TermFreqVector ; public class QueryTermVector implements TermFreqVector { private String [ ] terms = new String [ 0 ] ; private int [ ] termFreqs = new int [ 0 ] ; public String getField ( ) { return null ; } public QueryTermVector ( String [ ] queryTerms ) { processTerms ( queryTerms ) ; } public QueryTermVector ( String queryString , Analyzer analyzer ) { if ( analyzer != null ) { TokenStream stream = analyzer . tokenStream ( "" , new StringReader ( queryString ) ) ; if ( stream != null ) { List terms = new ArrayList ( ) ; try { final Token reusableToken = new Token ( ) ; for ( Token nextToken = stream . next ( reusableToken ) ; nextToken != null ; nextToken = stream . next ( reusableToken ) ) { terms . add ( nextToken . term ( ) ) ; } processTerms ( ( String [ ] ) terms . toArray ( new String [ terms . size ( ) ] ) ) ; } catch ( IOException e ) { } } } } private void processTerms ( String [ ] queryTerms ) { if ( queryTerms != null ) { Arrays . sort ( queryTerms ) ; Map tmpSet = new HashMap ( queryTerms . length ) ; List tmpList = new ArrayList ( queryTerms . length ) ; List tmpFreqs = new ArrayList ( queryTerms . length ) ; int j = 0 ; for ( int i = 0 ; i < queryTerms . length ; i ++ ) { String term = queryTerms [ i ] ; Integer position = ( Integer ) tmpSet . get ( term ) ; if ( position == null ) { tmpSet . put ( term , new Integer ( j ++ ) ) ; tmpList . add ( term ) ; tmpFreqs . add ( new Integer ( 1 ) ) ; } else { Integer integer = ( Integer ) tmpFreqs . get ( position . intValue ( ) ) ; tmpFreqs . set ( position . intValue ( ) , new Integer ( integer . intValue ( ) + 1 ) ) ; } } terms = ( String [ ] ) tmpList . toArray ( terms ) ; termFreqs = new int [ tmpFreqs . size ( ) ] ; int i = 0 ; for ( Iterator iter = tmpFreqs . iterator ( ) ; iter . hasNext ( ) ; ) { Integer integer = ( Integer ) iter . next ( ) ; termFreqs [ i ++ ] = integer . intValue ( ) ; } } } public final String toString ( ) { StringBuffer sb = new StringBuffer ( ) ; sb . append ( '{' ) ; for ( int i = 0 ; i < terms . length ; i ++ ) { if ( i > 0 ) sb . append ( ", " ) ; sb . append ( terms [ i ] ) . append ( '/' ) . append ( termFreqs [ i ] ) ; } sb . append ( '}' ) ; return sb . toString ( ) ; } public int size ( ) { return terms . length ; } public String [ ] getTerms ( ) { return terms ; } public int [ ] getTermFrequencies ( ) { return termFreqs ; } public int indexOf ( String term ) { int res = Arrays . binarySearch ( terms , term ) ; return res >= 0 ? res : - 1 ; } public int [ ] indexesOf ( String [ ] terms , int start , int len ) { int res [ ] = new int [ len ] ; for ( int i = 0 ; i < len ; i ++ ) { res [ i ] = indexOf ( terms [ i ] ) ; } return res ; } } 	0	['10', '1', '0', '4', '38', '0', '0', '4', '9', '0.388888889', '287', '1', '0', '0', '0.34', '0', '0', '27.5', '5', '1.6', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . PriorityQueue ; import java . io . IOException ; import java . util . Locale ; import java . text . Collator ; public class FieldSortedHitQueue extends PriorityQueue { public FieldSortedHitQueue ( IndexReader reader , SortField [ ] fields , int size ) throws IOException { final int n = fields . length ; comparators = new ScoreDocComparator [ n ] ; this . fields = new SortField [ n ] ; for ( int i = 0 ; i < n ; ++ i ) { String fieldname = fields [ i ] . getField ( ) ; comparators [ i ] = getCachedComparator ( reader , fieldname , fields [ i ] . getType ( ) , fields [ i ] . getLocale ( ) , fields [ i ] . getFactory ( ) ) ; if ( comparators [ i ] . sortType ( ) == SortField . STRING ) { this . fields [ i ] = new SortField ( fieldname , fields [ i ] . getLocale ( ) , fields [ i ] . getReverse ( ) ) ; } else { this . fields [ i ] = new SortField ( fieldname , comparators [ i ] . sortType ( ) , fields [ i ] . getReverse ( ) ) ; } } initialize ( size ) ; } protected ScoreDocComparator [ ] comparators ; protected SortField [ ] fields ; protected float maxscore = Float . NEGATIVE_INFINITY ; public float getMaxScore ( ) { return maxscore ; } public boolean insert ( FieldDoc fdoc ) { maxscore = Math . max ( maxscore , fdoc . score ) ; return super . insert ( fdoc ) ; } public boolean insert ( Object fdoc ) { return insert ( ( FieldDoc ) fdoc ) ; } protected boolean lessThan ( final Object a , final Object b ) { final ScoreDoc docA = ( ScoreDoc ) a ; final ScoreDoc docB = ( ScoreDoc ) b ; final int n = comparators . length ; int c = 0 ; for ( int i = 0 ; i < n && c == 0 ; ++ i ) { c = ( fields [ i ] . reverse ) ? comparators [ i ] . compare ( docB , docA ) : comparators [ i ] . compare ( docA , docB ) ; } if ( c == 0 ) return docA . doc > docB . doc ; return c > 0 ; } FieldDoc fillFields ( final FieldDoc doc ) { final int n = comparators . length ; final Comparable [ ] fields = new Comparable [ n ] ; for ( int i = 0 ; i < n ; ++ i ) fields [ i ] = comparators [ i ] . sortValue ( doc ) ; doc . fields = fields ; return doc ; } SortField [ ] getFields ( ) { return fields ; } static ScoreDocComparator getCachedComparator ( IndexReader reader , String field , int type , Locale locale , SortComparatorSource factory ) throws IOException { if ( type == SortField . DOC ) return ScoreDocComparator . INDEXORDER ; if ( type == SortField . SCORE ) return ScoreDocComparator . RELEVANCE ; FieldCacheImpl . Entry entry = ( factory != null ) ? new FieldCacheImpl . Entry ( field , factory ) : new FieldCacheImpl . Entry ( field , type , locale ) ; return ( ScoreDocComparator ) Comparators . get ( reader , entry ) ; } static final FieldCacheImpl . Cache Comparators = new FieldCacheImpl . Cache ( ) { protected Object createValue ( IndexReader reader , Object entryKey ) throws IOException { FieldCacheImpl . Entry entry = ( FieldCacheImpl . Entry ) entryKey ; String fieldname = entry . field ; int type = entry . type ; Locale locale = entry . locale ; SortComparatorSource factory = ( SortComparatorSource ) entry . custom ; ScoreDocComparator comparator ; switch ( type ) { case SortField . AUTO : comparator = comparatorAuto ( reader , fieldname ) ; break ; case SortField . INT : comparator = comparatorInt ( reader , fieldname ) ; break ; case SortField . FLOAT : comparator = comparatorFloat ( reader , fieldname ) ; break ; case SortField . STRING : if ( locale != null ) comparator = comparatorStringLocale ( reader , fieldname , locale ) ; else comparator = comparatorString ( reader , fieldname ) ; break ; case SortField . CUSTOM : comparator = factory . newComparator ( reader , fieldname ) ; break ; default : throw new RuntimeException ( "unknown field type: " + type ) ; } return comparator ; } } ; static ScoreDocComparator comparatorInt ( final IndexReader reader , final String fieldname ) throws IOException { final String field = fieldname . intern ( ) ; final int [ ] fieldOrder = FieldCache . DEFAULT . getInts ( reader , field ) ; return new ScoreDocComparator ( ) { public final int compare ( final ScoreDoc i , final ScoreDoc j ) { final int fi = fieldOrder [ i . doc ] ; final int fj = fieldOrder [ j . doc ] ; if ( fi < fj ) return - 1 ; if ( fi > fj ) return 1 ; return 0 ; } public Comparable sortValue ( final ScoreDoc i ) { return new Integer ( fieldOrder [ i . doc ] ) ; } public int sortType ( ) { return SortField . INT ; } } ; } static ScoreDocComparator comparatorFloat ( final IndexReader reader , final String fieldname ) throws IOException { final String field = fieldname . intern ( ) ; final float [ ] fieldOrder = FieldCache . DEFAULT . getFloats ( reader , field ) ; return new ScoreDocComparator ( ) { public final int compare ( final ScoreDoc i , final ScoreDoc j ) { final float fi = fieldOrder [ i . doc ] ; final float fj = fieldOrder [ j . doc ] ; if ( fi < fj ) return - 1 ; if ( fi > fj ) return 1 ; return 0 ; } public Comparable sortValue ( final ScoreDoc i ) { return new Float ( fieldOrder [ i . doc ] ) ; } public int sortType ( ) { return SortField . FLOAT ; } } ; } static ScoreDocComparator comparatorString ( final IndexReader reader , final String fieldname ) throws IOException { final String field = fieldname . intern ( ) ; final FieldCache . StringIndex index = FieldCache . DEFAULT . getStringIndex ( reader , field ) ; return new ScoreDocComparator ( ) { public final int compare ( final ScoreDoc i , final ScoreDoc j ) { final int fi = index . order [ i . doc ] ; final int fj = index . order [ j . doc ] ; if ( fi < fj ) return - 1 ; if ( fi > fj ) return 1 ; return 0 ; } public Comparable sortValue ( final ScoreDoc i ) { return index . lookup [ index . order [ i . doc ] ] ; } public int sortType ( ) { return SortField . STRING ; } } ; } static ScoreDocComparator comparatorStringLocale ( final IndexReader reader , final String fieldname , final Locale locale ) throws IOException { final Collator collator = Collator . getInstance ( locale ) ; final String field = fieldname . intern ( ) ; final String [ ] index = FieldCache . DEFAULT . getStrings ( reader , field ) ; return new ScoreDocComparator ( ) { public final int compare ( final ScoreDoc i , final ScoreDoc j ) { String is = index [ i . doc ] ; String js = index [ j . doc ] ; if ( is == js ) { return 0 ; } else if ( is == null ) { return - 1 ; } else if ( js == null ) { return 1 ; } else { return collator . compare ( is , js ) ; } } public Comparable sortValue ( final ScoreDoc i ) { return index [ i . doc ] ; } public int sortType ( ) { return SortField . STRING ; } } ; } static ScoreDocComparator comparatorAuto ( final IndexReader reader , final String fieldname ) throws IOException { final String field = fieldname . intern ( ) ; Object lookupArray = FieldCache . DEFAULT . getAuto ( reader , field ) ; if ( lookupArray instanceof FieldCache . StringIndex ) { return comparatorString ( reader , field ) ; } else if ( lookupArray instanceof int [ ] ) { return comparatorInt ( reader , field ) ; } else if ( lookupArray instanceof float [ ] ) { return comparatorFloat ( reader , field ) ; } else if ( lookupArray instanceof String [ ] ) { return comparatorString ( reader , field ) ; } else { throw new RuntimeException ( "unknown data type in field '" + field + "'" ) ; } } } 	1	['14', '2', '0', '17', '47', '73', '2', '16', '4', '0.826923077', '361', '0.75', '3', '0.47826087', '0.256410256', '1', '3', '24.5', '7', '1.3571', '5']
package org . apache . lucene . index ; class ReadOnlySegmentReader extends SegmentReader { static void noWrite ( ) { throw new UnsupportedOperationException ( "This IndexReader cannot make any changes to the index (it was opened with readOnly = true)" ) ; } protected void acquireWriteLock ( ) { noWrite ( ) ; } public boolean isDeleted ( int n ) { return deletedDocs != null && deletedDocs . get ( n ) ; } } 	0	['4', '4', '0', '3', '7', '6', '2', '2', '1', '2', '26', '0', '0', '0.98125', '0.5', '2', '3', '5.5', '3', '1.25', '0']
package org . apache . lucene . analysis . standard ; import org . apache . lucene . analysis . * ; public final class StandardFilter extends TokenFilter implements StandardTokenizerConstants { public StandardFilter ( TokenStream in ) { super ( in ) ; } private static final String APOSTROPHE_TYPE = tokenImage [ APOSTROPHE ] ; private static final String ACRONYM_TYPE = tokenImage [ ACRONYM ] ; public final org . apache . lucene . analysis . Token next ( ) throws java . io . IOException { org . apache . lucene . analysis . Token t = input . next ( ) ; if ( t == null ) return null ; String text = t . termText ( ) ; String type = t . type ( ) ; if ( type == APOSTROPHE_TYPE && ( text . endsWith ( "'s" ) || text . endsWith ( "'S" ) ) ) { return new org . apache . lucene . analysis . Token ( text . substring ( 0 , text . length ( ) - 2 ) , t . startOffset ( ) , t . endOffset ( ) , type ) ; } else if ( type == ACRONYM_TYPE ) { StringBuffer trimmed = new StringBuffer ( ) ; for ( int i = 0 ; i < text . length ( ) ; i ++ ) { char c = text . charAt ( i ) ; if ( c != '.' ) trimmed . append ( c ) ; } return new org . apache . lucene . analysis . Token ( trimmed . toString ( ) , t . startOffset ( ) , t . endOffset ( ) , type ) ; } else { return t ; } } } 	1	['3', '3', '0', '5', '17', '1', '1', '4', '2', '0.5', '98', '1', '0', '0.8', '0.75', '0', '0', '31', '1', '0.3333', '1']
package org . apache . lucene . index ; import java . io . IOException ; abstract class DocFieldConsumerPerThread { abstract void startDocument ( ) throws IOException ; abstract DocumentsWriter . DocWriter finishDocument ( ) throws IOException ; abstract DocFieldConsumerPerField addField ( FieldInfo fi ) ; abstract void abort ( ) ; } 	0	['5', '1', '3', '13', '6', '10', '10', '3', '0', '2', '8', '0', '0', '0', '0.6', '0', '0', '0.6', '1', '0.8', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; import java . util . BitSet ; import java . util . Set ; public class ConstantScoreQuery extends Query { protected final Filter filter ; public ConstantScoreQuery ( Filter filter ) { this . filter = filter ; } public Filter getFilter ( ) { return filter ; } public Query rewrite ( IndexReader reader ) throws IOException { return this ; } public void extractTerms ( Set terms ) { } protected class ConstantWeight implements Weight { private Similarity similarity ; private float queryNorm ; private float queryWeight ; public ConstantWeight ( Searcher searcher ) { this . similarity = getSimilarity ( searcher ) ; } public Query getQuery ( ) { return ConstantScoreQuery . this ; } public float getValue ( ) { return queryWeight ; } public float sumOfSquaredWeights ( ) throws IOException { queryWeight = getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float norm ) { this . queryNorm = norm ; queryWeight *= this . queryNorm ; } public Scorer scorer ( IndexReader reader ) throws IOException { return new ConstantScorer ( similarity , reader , this ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { ConstantScorer cs = ( ConstantScorer ) scorer ( reader ) ; boolean exists = cs . bits . get ( doc ) ; ComplexExplanation result = new ComplexExplanation ( ) ; if ( exists ) { result . setDescription ( "ConstantScoreQuery(" + filter + "), product of:" ) ; result . setValue ( queryWeight ) ; result . setMatch ( Boolean . TRUE ) ; result . addDetail ( new Explanation ( getBoost ( ) , "boost" ) ) ; result . addDetail ( new Explanation ( queryNorm , "queryNorm" ) ) ; } else { result . setDescription ( "ConstantScoreQuery(" + filter + ") doesn't match id " + doc ) ; result . setValue ( 0 ) ; result . setMatch ( Boolean . FALSE ) ; } return result ; } } protected class ConstantScorer extends Scorer { final BitSet bits ; final float theScore ; int doc = - 1 ; public ConstantScorer ( Similarity similarity , IndexReader reader , Weight w ) throws IOException { super ( similarity ) ; theScore = w . getValue ( ) ; bits = filter . bits ( reader ) ; } public boolean next ( ) throws IOException { doc = bits . nextSetBit ( doc + 1 ) ; return doc >= 0 ; } public int doc ( ) { return doc ; } public float score ( ) throws IOException { return theScore ; } public boolean skipTo ( int target ) throws IOException { doc = bits . nextSetBit ( target ) ; return doc >= 0 ; } public Explanation explain ( int doc ) throws IOException { throw new UnsupportedOperationException ( ) ; } } protected Weight createWeight ( Searcher searcher ) { return new ConstantScoreQuery . ConstantWeight ( searcher ) ; } public String toString ( String field ) { return "ConstantScore(" + filter . toString ( ) + ( getBoost ( ) == 1.0 ? ")" : "^" + getBoost ( ) ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof ConstantScoreQuery ) ) return false ; ConstantScoreQuery other = ( ConstantScoreQuery ) o ; return this . getBoost ( ) == other . getBoost ( ) && filter . equals ( other . filter ) ; } public int hashCode ( ) { return filter . hashCode ( ) + Float . floatToIntBits ( getBoost ( ) ) ; } } 	1	['8', '2', '0', '8', '19', '8', '3', '6', '7', '0.428571429', '93', '1', '1', '0.631578947', '0.25', '2', '3', '10.5', '5', '1.5', '1']
package org . apache . lucene . index ; import org . apache . lucene . util . PriorityQueue ; import java . io . IOException ; import java . util . Arrays ; import java . util . Iterator ; import java . util . LinkedList ; import java . util . List ; public class MultipleTermPositions implements TermPositions { private static final class TermPositionsQueue extends PriorityQueue { TermPositionsQueue ( List termPositions ) throws IOException { initialize ( termPositions . size ( ) ) ; Iterator i = termPositions . iterator ( ) ; while ( i . hasNext ( ) ) { TermPositions tp = ( TermPositions ) i . next ( ) ; if ( tp . next ( ) ) put ( tp ) ; } } final TermPositions peek ( ) { return ( TermPositions ) top ( ) ; } public final boolean lessThan ( Object a , Object b ) { return ( ( TermPositions ) a ) . doc ( ) < ( ( TermPositions ) b ) . doc ( ) ; } } private static final class IntQueue { private int _arraySize = 16 ; private int _index = 0 ; private int _lastIndex = 0 ; private int [ ] _array = new int [ _arraySize ] ; final void add ( int i ) { if ( _lastIndex == _arraySize ) growArray ( ) ; _array [ _lastIndex ++ ] = i ; } final int next ( ) { return _array [ _index ++ ] ; } final void sort ( ) { Arrays . sort ( _array , _index , _lastIndex ) ; } final void clear ( ) { _index = 0 ; _lastIndex = 0 ; } final int size ( ) { return ( _lastIndex - _index ) ; } private void growArray ( ) { int [ ] newArray = new int [ _arraySize * 2 ] ; System . arraycopy ( _array , 0 , newArray , 0 , _arraySize ) ; _array = newArray ; _arraySize *= 2 ; } } private int _doc ; private int _freq ; private TermPositionsQueue _termPositionsQueue ; private IntQueue _posList ; public MultipleTermPositions ( IndexReader indexReader , Term [ ] terms ) throws IOException { List termPositions = new LinkedList ( ) ; for ( int i = 0 ; i < terms . length ; i ++ ) termPositions . add ( indexReader . termPositions ( terms [ i ] ) ) ; _termPositionsQueue = new TermPositionsQueue ( termPositions ) ; _posList = new IntQueue ( ) ; } public final boolean next ( ) throws IOException { if ( _termPositionsQueue . size ( ) == 0 ) return false ; _posList . clear ( ) ; _doc = _termPositionsQueue . peek ( ) . doc ( ) ; TermPositions tp ; do { tp = _termPositionsQueue . peek ( ) ; for ( int i = 0 ; i < tp . freq ( ) ; i ++ ) _posList . add ( tp . nextPosition ( ) ) ; if ( tp . next ( ) ) _termPositionsQueue . adjustTop ( ) ; else { _termPositionsQueue . pop ( ) ; tp . close ( ) ; } } while ( _termPositionsQueue . size ( ) > 0 && _termPositionsQueue . peek ( ) . doc ( ) == _doc ) ; _posList . sort ( ) ; _freq = _posList . size ( ) ; return true ; } public final int nextPosition ( ) { return _posList . next ( ) ; } public final boolean skipTo ( int target ) throws IOException { while ( _termPositionsQueue . peek ( ) != null && target > _termPositionsQueue . peek ( ) . doc ( ) ) { TermPositions tp = ( TermPositions ) _termPositionsQueue . pop ( ) ; if ( tp . skipTo ( target ) ) _termPositionsQueue . put ( tp ) ; else tp . close ( ) ; } return next ( ) ; } public final int doc ( ) { return _doc ; } public final int freq ( ) { return _freq ; } public final void close ( ) throws IOException { while ( _termPositionsQueue . size ( ) > 0 ) ( ( TermPositions ) _termPositionsQueue . pop ( ) ) . close ( ) ; } public void seek ( Term arg0 ) throws IOException { throw new UnsupportedOperationException ( ) ; } public void seek ( TermEnum termEnum ) throws IOException { throw new UnsupportedOperationException ( ) ; } public int read ( int [ ] arg0 , int [ ] arg1 ) throws IOException { throw new UnsupportedOperationException ( ) ; } public int getPayloadLength ( ) { throw new UnsupportedOperationException ( ) ; } public byte [ ] getPayload ( byte [ ] data , int offset ) throws IOException { throw new UnsupportedOperationException ( ) ; } public boolean isPayloadAvailable ( ) { return false ; } } 	0	['13', '1', '0', '8', '36', '58', '1', '7', '13', '0.791666667', '191', '1', '2', '0', '0.201923077', '0', '0', '13.38461538', '1', '0.9231', '0']
package org . apache . lucene . index ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . FieldSelector ; import java . io . IOException ; import java . util . Collection ; public class FilterIndexReader extends IndexReader { public static class FilterTermDocs implements TermDocs { protected TermDocs in ; public FilterTermDocs ( TermDocs in ) { this . in = in ; } public void seek ( Term term ) throws IOException { in . seek ( term ) ; } public void seek ( TermEnum termEnum ) throws IOException { in . seek ( termEnum ) ; } public int doc ( ) { return in . doc ( ) ; } public int freq ( ) { return in . freq ( ) ; } public boolean next ( ) throws IOException { return in . next ( ) ; } public int read ( int [ ] docs , int [ ] freqs ) throws IOException { return in . read ( docs , freqs ) ; } public boolean skipTo ( int i ) throws IOException { return in . skipTo ( i ) ; } public void close ( ) throws IOException { in . close ( ) ; } } public static class FilterTermPositions extends FilterTermDocs implements TermPositions { public FilterTermPositions ( TermPositions in ) { super ( in ) ; } public int nextPosition ( ) throws IOException { return ( ( TermPositions ) this . in ) . nextPosition ( ) ; } public int getPayloadLength ( ) { return ( ( TermPositions ) this . in ) . getPayloadLength ( ) ; } public byte [ ] getPayload ( byte [ ] data , int offset ) throws IOException { return ( ( TermPositions ) this . in ) . getPayload ( data , offset ) ; } public boolean isPayloadAvailable ( ) { return ( ( TermPositions ) this . in ) . isPayloadAvailable ( ) ; } } public static class FilterTermEnum extends TermEnum { protected TermEnum in ; public FilterTermEnum ( TermEnum in ) { this . in = in ; } public boolean next ( ) throws IOException { return in . next ( ) ; } public Term term ( ) { return in . term ( ) ; } public int docFreq ( ) { return in . docFreq ( ) ; } public void close ( ) throws IOException { in . close ( ) ; } } protected IndexReader in ; public FilterIndexReader ( IndexReader in ) { super ( in . directory ( ) ) ; this . in = in ; } public TermFreqVector [ ] getTermFreqVectors ( int docNumber ) throws IOException { ensureOpen ( ) ; return in . getTermFreqVectors ( docNumber ) ; } public TermFreqVector getTermFreqVector ( int docNumber , String field ) throws IOException { ensureOpen ( ) ; return in . getTermFreqVector ( docNumber , field ) ; } public int numDocs ( ) { return in . numDocs ( ) ; } public int maxDoc ( ) { return in . maxDoc ( ) ; } public Document document ( int n , FieldSelector fieldSelector ) throws CorruptIndexException , IOException { ensureOpen ( ) ; return in . document ( n , fieldSelector ) ; } public boolean isDeleted ( int n ) { return in . isDeleted ( n ) ; } public boolean hasDeletions ( ) { return in . hasDeletions ( ) ; } protected void doUndeleteAll ( ) throws CorruptIndexException , IOException { in . undeleteAll ( ) ; } public boolean hasNorms ( String field ) throws IOException { ensureOpen ( ) ; return in . hasNorms ( field ) ; } public byte [ ] norms ( String f ) throws IOException { ensureOpen ( ) ; return in . norms ( f ) ; } public void norms ( String f , byte [ ] bytes , int offset ) throws IOException { ensureOpen ( ) ; in . norms ( f , bytes , offset ) ; } protected void doSetNorm ( int d , String f , byte b ) throws CorruptIndexException , IOException { in . setNorm ( d , f , b ) ; } public TermEnum terms ( ) throws IOException { ensureOpen ( ) ; return in . terms ( ) ; } public TermEnum terms ( Term t ) throws IOException { ensureOpen ( ) ; return in . terms ( t ) ; } public int docFreq ( Term t ) throws IOException { ensureOpen ( ) ; return in . docFreq ( t ) ; } public TermDocs termDocs ( ) throws IOException { ensureOpen ( ) ; return in . termDocs ( ) ; } public TermPositions termPositions ( ) throws IOException { ensureOpen ( ) ; return in . termPositions ( ) ; } protected void doDelete ( int n ) throws CorruptIndexException , IOException { in . deleteDocument ( n ) ; } protected void doCommit ( ) throws IOException { in . commit ( ) ; } protected void doClose ( ) throws IOException { in . close ( ) ; } public Collection getFieldNames ( IndexReader . FieldOption fieldNames ) { ensureOpen ( ) ; return in . getFieldNames ( fieldNames ) ; } public long getVersion ( ) { ensureOpen ( ) ; return in . getVersion ( ) ; } public boolean isCurrent ( ) throws CorruptIndexException , IOException { ensureOpen ( ) ; return in . isCurrent ( ) ; } } 	1	['24', '2', '0', '11', '50', '0', '0', '11', '19', '0', '171', '1', '1', '0.722891566', '0.199074074', '1', '8', '6.083333333', '1', '0.9583', '2']
package org . apache . lucene . index ; import java . io . IOException ; public interface TermDocs { void seek ( Term term ) throws IOException ; void seek ( TermEnum termEnum ) throws IOException ; int doc ( ) ; int freq ( ) ; boolean next ( ) throws IOException ; int read ( int [ ] docs , int [ ] freqs ) throws IOException ; boolean skipTo ( int target ) throws IOException ; void close ( ) throws IOException ; } 	0	['8', '1', '0', '30', '8', '28', '28', '2', '8', '2', '8', '0', '0', '0', '0.3', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . document ; import java . text . ParseException ; import java . text . SimpleDateFormat ; import java . util . Calendar ; import java . util . Date ; import java . util . TimeZone ; public class DateTools { private final static TimeZone GMT = TimeZone . getTimeZone ( "GMT" ) ; private static final SimpleDateFormat YEAR_FORMAT = new SimpleDateFormat ( "yyyy" ) ; private static final SimpleDateFormat MONTH_FORMAT = new SimpleDateFormat ( "yyyyMM" ) ; private static final SimpleDateFormat DAY_FORMAT = new SimpleDateFormat ( "yyyyMMdd" ) ; private static final SimpleDateFormat HOUR_FORMAT = new SimpleDateFormat ( "yyyyMMddHH" ) ; private static final SimpleDateFormat MINUTE_FORMAT = new SimpleDateFormat ( "yyyyMMddHHmm" ) ; private static final SimpleDateFormat SECOND_FORMAT = new SimpleDateFormat ( "yyyyMMddHHmmss" ) ; private static final SimpleDateFormat MILLISECOND_FORMAT = new SimpleDateFormat ( "yyyyMMddHHmmssSSS" ) ; static { YEAR_FORMAT . setTimeZone ( GMT ) ; MONTH_FORMAT . setTimeZone ( GMT ) ; DAY_FORMAT . setTimeZone ( GMT ) ; HOUR_FORMAT . setTimeZone ( GMT ) ; MINUTE_FORMAT . setTimeZone ( GMT ) ; SECOND_FORMAT . setTimeZone ( GMT ) ; MILLISECOND_FORMAT . setTimeZone ( GMT ) ; } private DateTools ( ) { } public static String dateToString ( Date date , Resolution resolution ) { return timeToString ( date . getTime ( ) , resolution ) ; } public static String timeToString ( long time , Resolution resolution ) { Calendar cal = Calendar . getInstance ( GMT ) ; cal . setTime ( new Date ( round ( time , resolution ) ) ) ; String result ; if ( resolution == Resolution . YEAR ) { synchronized ( YEAR_FORMAT ) { result = YEAR_FORMAT . format ( cal . getTime ( ) ) ; } } else if ( resolution == Resolution . MONTH ) { synchronized ( MONTH_FORMAT ) { result = MONTH_FORMAT . format ( cal . getTime ( ) ) ; } } else if ( resolution == Resolution . DAY ) { synchronized ( DAY_FORMAT ) { result = DAY_FORMAT . format ( cal . getTime ( ) ) ; } } else if ( resolution == Resolution . HOUR ) { synchronized ( HOUR_FORMAT ) { result = HOUR_FORMAT . format ( cal . getTime ( ) ) ; } } else if ( resolution == Resolution . MINUTE ) { synchronized ( MINUTE_FORMAT ) { result = MINUTE_FORMAT . format ( cal . getTime ( ) ) ; } } else if ( resolution == Resolution . SECOND ) { synchronized ( SECOND_FORMAT ) { result = SECOND_FORMAT . format ( cal . getTime ( ) ) ; } } else if ( resolution == Resolution . MILLISECOND ) { synchronized ( MILLISECOND_FORMAT ) { result = MILLISECOND_FORMAT . format ( cal . getTime ( ) ) ; } } else { throw new IllegalArgumentException ( "unknown resolution " + resolution ) ; } return result ; } public static long stringToTime ( String dateString ) throws ParseException { return stringToDate ( dateString ) . getTime ( ) ; } public static Date stringToDate ( String dateString ) throws ParseException { Date date ; if ( dateString . length ( ) == 4 ) { synchronized ( YEAR_FORMAT ) { date = YEAR_FORMAT . parse ( dateString ) ; } } else if ( dateString . length ( ) == 6 ) { synchronized ( MONTH_FORMAT ) { date = MONTH_FORMAT . parse ( dateString ) ; } } else if ( dateString . length ( ) == 8 ) { synchronized ( DAY_FORMAT ) { date = DAY_FORMAT . parse ( dateString ) ; } } else if ( dateString . length ( ) == 10 ) { synchronized ( HOUR_FORMAT ) { date = HOUR_FORMAT . parse ( dateString ) ; } } else if ( dateString . length ( ) == 12 ) { synchronized ( MINUTE_FORMAT ) { date = MINUTE_FORMAT . parse ( dateString ) ; } } else if ( dateString . length ( ) == 14 ) { synchronized ( SECOND_FORMAT ) { date = SECOND_FORMAT . parse ( dateString ) ; } } else if ( dateString . length ( ) == 17 ) { synchronized ( MILLISECOND_FORMAT ) { date = MILLISECOND_FORMAT . parse ( dateString ) ; } } else { throw new ParseException ( "Input is not valid date string: " + dateString , 0 ) ; } return date ; } public static Date round ( Date date , Resolution resolution ) { return new Date ( round ( date . getTime ( ) , resolution ) ) ; } public static long round ( long time , Resolution resolution ) { Calendar cal = Calendar . getInstance ( GMT ) ; cal . setTime ( new Date ( time ) ) ; if ( resolution == Resolution . YEAR ) { cal . set ( Calendar . MONTH , 0 ) ; cal . set ( Calendar . DAY_OF_MONTH , 1 ) ; cal . set ( Calendar . HOUR_OF_DAY , 0 ) ; cal . set ( Calendar . MINUTE , 0 ) ; cal . set ( Calendar . SECOND , 0 ) ; cal . set ( Calendar . MILLISECOND , 0 ) ; } else if ( resolution == Resolution . MONTH ) { cal . set ( Calendar . DAY_OF_MONTH , 1 ) ; cal . set ( Calendar . HOUR_OF_DAY , 0 ) ; cal . set ( Calendar . MINUTE , 0 ) ; cal . set ( Calendar . SECOND , 0 ) ; cal . set ( Calendar . MILLISECOND , 0 ) ; } else if ( resolution == Resolution . DAY ) { cal . set ( Calendar . HOUR_OF_DAY , 0 ) ; cal . set ( Calendar . MINUTE , 0 ) ; cal . set ( Calendar . SECOND , 0 ) ; cal . set ( Calendar . MILLISECOND , 0 ) ; } else if ( resolution == Resolution . HOUR ) { cal . set ( Calendar . MINUTE , 0 ) ; cal . set ( Calendar . SECOND , 0 ) ; cal . set ( Calendar . MILLISECOND , 0 ) ; } else if ( resolution == Resolution . MINUTE ) { cal . set ( Calendar . SECOND , 0 ) ; cal . set ( Calendar . MILLISECOND , 0 ) ; } else if ( resolution == Resolution . SECOND ) { cal . set ( Calendar . MILLISECOND , 0 ) ; } else if ( resolution == Resolution . MILLISECOND ) { } else { throw new IllegalArgumentException ( "unknown resolution " + resolution ) ; } return cal . getTime ( ) . getTime ( ) ; } public static class Resolution { public static final Resolution YEAR = new Resolution ( "year" ) ; public static final Resolution MONTH = new Resolution ( "month" ) ; public static final Resolution DAY = new Resolution ( "day" ) ; public static final Resolution HOUR = new Resolution ( "hour" ) ; public static final Resolution MINUTE = new Resolution ( "minute" ) ; public static final Resolution SECOND = new Resolution ( "second" ) ; public static final Resolution MILLISECOND = new Resolution ( "millisecond" ) ; private String resolution ; private Resolution ( ) { } private Resolution ( String resolution ) { this . resolution = resolution ; } public String toString ( ) { return resolution ; } } } 	1	['8', '1', '0', '2', '27', '18', '1', '1', '6', '0.428571429', '567', '1', '0', '0', '0.314285714', '0', '0', '68.875', '8', '2.5', '1']
package org . apache . lucene . index ; final class IntBlockPool { public int [ ] [ ] buffers = new int [ 10 ] [ ] ; int bufferUpto = - 1 ; public int intUpto = DocumentsWriter . INT_BLOCK_SIZE ; public int [ ] buffer ; public int intOffset = - DocumentsWriter . INT_BLOCK_SIZE ; final private DocumentsWriter docWriter ; final boolean trackAllocations ; public IntBlockPool ( DocumentsWriter docWriter , boolean trackAllocations ) { this . docWriter = docWriter ; this . trackAllocations = trackAllocations ; } public void reset ( ) { if ( bufferUpto != - 1 ) { if ( bufferUpto > 0 ) docWriter . recycleIntBlocks ( buffers , 1 , 1 + bufferUpto ) ; bufferUpto = 0 ; intUpto = 0 ; intOffset = 0 ; buffer = buffers [ 0 ] ; } } public void nextBuffer ( ) { if ( 1 + bufferUpto == buffers . length ) { int [ ] [ ] newBuffers = new int [ ( int ) ( buffers . length * 1.5 ) ] [ ] ; System . arraycopy ( buffers , 0 , newBuffers , 0 , buffers . length ) ; buffers = newBuffers ; } buffer = buffers [ 1 + bufferUpto ] = docWriter . getIntBlock ( trackAllocations ) ; bufferUpto ++ ; intUpto = 0 ; intOffset += DocumentsWriter . INT_BLOCK_SIZE ; } } 	0	['3', '1', '0', '3', '7', '0', '2', '1', '3', '0.142857143', '125', '0.142857143', '1', '0', '0.555555556', '0', '0', '38.33333333', '3', '1.6667', '0']
package org . apache . lucene . search ; public interface ScoreDocComparator { static final ScoreDocComparator RELEVANCE = new ScoreDocComparator ( ) { public int compare ( ScoreDoc i , ScoreDoc j ) { if ( i . score > j . score ) return - 1 ; if ( i . score < j . score ) return 1 ; return 0 ; } public Comparable sortValue ( ScoreDoc i ) { return new Float ( i . score ) ; } public int sortType ( ) { return SortField . SCORE ; } } ; static final ScoreDocComparator INDEXORDER = new ScoreDocComparator ( ) { public int compare ( ScoreDoc i , ScoreDoc j ) { if ( i . doc < j . doc ) return - 1 ; if ( i . doc > j . doc ) return 1 ; return 0 ; } public Comparable sortValue ( ScoreDoc i ) { return new Integer ( i . doc ) ; } public int sortType ( ) { return SortField . DOC ; } } ; int compare ( ScoreDoc i , ScoreDoc j ) ; Comparable sortValue ( ScoreDoc i ) ; int sortType ( ) ; } 	1	['4', '1', '0', '12', '6', '6', '11', '3', '3', '1', '15', '0', '2', '0', '0.833333333', '0', '0', '2.25', '1', '0.75', '1']
package org . apache . lucene . store ; import java . io . IOException ; import java . util . zip . CRC32 ; import java . util . zip . Checksum ; public class ChecksumIndexInput extends IndexInput { IndexInput main ; Checksum digest ; public ChecksumIndexInput ( IndexInput main ) { this . main = main ; digest = new CRC32 ( ) ; } public byte readByte ( ) throws IOException { final byte b = main . readByte ( ) ; digest . update ( b ) ; return b ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { main . readBytes ( b , offset , len ) ; digest . update ( b , offset , len ) ; } public long getChecksum ( ) { return digest . getValue ( ) ; } public void close ( ) throws IOException { main . close ( ) ; } public long getFilePointer ( ) { return main . getFilePointer ( ) ; } public void seek ( long pos ) { throw new RuntimeException ( "not allowed" ) ; } public long length ( ) { return main . length ( ) ; } } 	0	['8', '2', '0', '2', '19', '0', '1', '1', '8', '0.428571429', '65', '0', '1', '0.708333333', '0.3', '1', '4', '6.875', '1', '0.875', '0']
package org . apache . lucene . index ; import java . io . Serializable ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . analysis . TokenStream ; public class Payload implements Serializable { protected byte [ ] data ; protected int offset ; protected int length ; protected Payload ( ) { } public Payload ( byte [ ] data ) { this ( data , 0 , data . length ) ; } public Payload ( byte [ ] data , int offset , int length ) { if ( offset < 0 || offset + length > data . length ) { throw new IllegalArgumentException ( ) ; } this . data = data ; this . offset = offset ; this . length = length ; } public int length ( ) { return this . length ; } public byte byteAt ( int index ) { if ( 0 <= index && index < this . length ) { return this . data [ this . offset + index ] ; } throw new ArrayIndexOutOfBoundsException ( index ) ; } public byte [ ] toByteArray ( ) { byte [ ] retArray = new byte [ this . length ] ; System . arraycopy ( this . data , this . offset , retArray , 0 , this . length ) ; return retArray ; } public void copyTo ( byte [ ] target , int targetOffset ) { if ( this . length > target . length + targetOffset ) { throw new ArrayIndexOutOfBoundsException ( ) ; } System . arraycopy ( this . data , this . offset , target , targetOffset , this . length ) ; } } 	1	['7', '1', '0', '3', '12', '1', '3', '0', '6', '0.277777778', '103', '1', '0', '0', '0.619047619', '0', '0', '13.28571429', '3', '1', '2']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Collection ; abstract class DocConsumer { abstract DocConsumerPerThread addThread ( DocumentsWriterThreadState perThread ) throws IOException ; abstract void flush ( final Collection threads , final DocumentsWriter . FlushState state ) throws IOException ; abstract void closeDocStore ( final DocumentsWriter . FlushState state ) throws IOException ; abstract void abort ( ) ; abstract boolean freeRAM ( ) ; } 	0	['6', '1', '1', '5', '7', '15', '3', '3', '0', '2', '9', '0', '0', '0', '0.416666667', '0', '0', '0.5', '1', '0.8333', '0']
package org . apache . lucene . search ; import java . util . BitSet ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; public abstract class Filter implements java . io . Serializable { public abstract BitSet bits ( IndexReader reader ) throws IOException ; } 	1	['2', '1', '5', '25', '3', '1', '24', '1', '2', '2', '5', '0', '0', '0', '0.75', '0', '0', '1.5', '1', '0.5', '1']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . analysis . Token ; abstract class TermsHashConsumerPerField { abstract boolean start ( Fieldable [ ] fields , int count ) throws IOException ; abstract void finish ( ) throws IOException ; abstract void skippingLongTerm ( Token t ) throws IOException ; abstract void newTerm ( Token t , RawPostingList p ) throws IOException ; abstract void addTerm ( Token t , RawPostingList p ) throws IOException ; abstract int getStreamCount ( ) ; } 	0	['7', '1', '2', '10', '8', '21', '7', '3', '0', '2', '10', '0', '0', '0', '0.4', '0', '0', '0.428571429', '1', '0.8571', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; public interface FieldCache { public static final int STRING_INDEX = - 1 ; public static class StringIndex { public final String [ ] lookup ; public final int [ ] order ; public StringIndex ( int [ ] values , String [ ] lookup ) { this . order = values ; this . lookup = lookup ; } } public interface ByteParser { public byte parseByte ( String string ) ; } public interface ShortParser { public short parseShort ( String string ) ; } public interface IntParser { public int parseInt ( String string ) ; } public interface FloatParser { public float parseFloat ( String string ) ; } public static FieldCache DEFAULT = new FieldCacheImpl ( ) ; public byte [ ] getBytes ( IndexReader reader , String field ) throws IOException ; public byte [ ] getBytes ( IndexReader reader , String field , ByteParser parser ) throws IOException ; public short [ ] getShorts ( IndexReader reader , String field ) throws IOException ; public short [ ] getShorts ( IndexReader reader , String field , ShortParser parser ) throws IOException ; public int [ ] getInts ( IndexReader reader , String field ) throws IOException ; public int [ ] getInts ( IndexReader reader , String field , IntParser parser ) throws IOException ; public float [ ] getFloats ( IndexReader reader , String field ) throws IOException ; public float [ ] getFloats ( IndexReader reader , String field , FloatParser parser ) throws IOException ; public String [ ] getStrings ( IndexReader reader , String field ) throws IOException ; public StringIndex getStringIndex ( IndexReader reader , String field ) throws IOException ; public Object getAuto ( IndexReader reader , String field ) throws IOException ; public Comparable [ ] getCustom ( IndexReader reader , String field , SortComparator comparator ) throws IOException ; } 	1	['13', '1', '0', '16', '14', '78', '10', '8', '12', '1.041666667', '20', '0', '1', '0', '0.427083333', '0', '0', '0.384615385', '1', '0.9231', '1']
package org . apache . lucene . index ; final class TermInfo { int docFreq = 0 ; long freqPointer = 0 ; long proxPointer = 0 ; int skipOffset ; TermInfo ( ) { } TermInfo ( int df , long fp , long pp ) { docFreq = df ; freqPointer = fp ; proxPointer = pp ; } TermInfo ( TermInfo ti ) { docFreq = ti . docFreq ; freqPointer = ti . freqPointer ; proxPointer = ti . proxPointer ; skipOffset = ti . skipOffset ; } final void set ( int docFreq , long freqPointer , long proxPointer , int skipOffset ) { this . docFreq = docFreq ; this . freqPointer = freqPointer ; this . proxPointer = proxPointer ; this . skipOffset = skipOffset ; } final void set ( TermInfo ti ) { docFreq = ti . docFreq ; freqPointer = ti . freqPointer ; proxPointer = ti . proxPointer ; skipOffset = ti . skipOffset ; } } 	0	['5', '1', '0', '8', '6', '0', '8', '0', '0', '0.125', '100', '0', '0', '0', '0.55', '0', '0', '18.2', '1', '0.4', '0']
package org . apache . lucene . util ; public class SmallFloat { public static byte floatToByte ( float f , int numMantissaBits , int zeroExp ) { int fzero = ( 63 - zeroExp ) << numMantissaBits ; int bits = Float . floatToRawIntBits ( f ) ; int smallfloat = bits > > ( 24 - numMantissaBits ) ; if ( smallfloat < fzero ) { return ( bits <= 0 ) ? ( byte ) 0 : ( byte ) 1 ; } else if ( smallfloat >= fzero + 0x100 ) { return - 1 ; } else { return ( byte ) ( smallfloat - fzero ) ; } } public static float byteToFloat ( byte b , int numMantissaBits , int zeroExp ) { if ( b == 0 ) return 0.0f ; int bits = ( b & 0xff ) << ( 24 - numMantissaBits ) ; bits += ( 63 - zeroExp ) << 24 ; return Float . intBitsToFloat ( bits ) ; } public static byte floatToByte315 ( float f ) { int bits = Float . floatToRawIntBits ( f ) ; int smallfloat = bits > > ( 24 - 3 ) ; if ( smallfloat < ( 63 - 15 ) << 3 ) { return ( bits <= 0 ) ? ( byte ) 0 : ( byte ) 1 ; } if ( smallfloat >= ( ( 63 - 15 ) << 3 ) + 0x100 ) { return - 1 ; } return ( byte ) ( smallfloat - ( ( 63 - 15 ) << 3 ) ) ; } public static float byte315ToFloat ( byte b ) { if ( b == 0 ) return 0.0f ; int bits = ( b & 0xff ) << ( 24 - 3 ) ; bits += ( 63 - 15 ) << 24 ; return Float . intBitsToFloat ( bits ) ; } public static byte floatToByte52 ( float f ) { int bits = Float . floatToRawIntBits ( f ) ; int smallfloat = bits > > ( 24 - 5 ) ; if ( smallfloat < ( 63 - 2 ) << 5 ) { return ( bits <= 0 ) ? ( byte ) 0 : ( byte ) 1 ; } if ( smallfloat >= ( ( 63 - 2 ) << 5 ) + 0x100 ) { return - 1 ; } return ( byte ) ( smallfloat - ( ( 63 - 2 ) << 5 ) ) ; } public static float byte52ToFloat ( byte b ) { if ( b == 0 ) return 0.0f ; int bits = ( b & 0xff ) << ( 24 - 5 ) ; bits += ( 63 - 2 ) << 24 ; return Float . intBitsToFloat ( bits ) ; } } 	1	['7', '1', '0', '1', '10', '21', '1', '0', '7', '2', '155', '0', '0', '0', '0.321428571', '0', '0', '21.14285714', '4', '2.5714', '1']
package org . apache . lucene . index ; import java . io . IOException ; public class CorruptIndexException extends IOException { public CorruptIndexException ( String message ) { super ( message ) ; } } 	0	['1', '4', '0', '36', '2', '0', '36', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . index ; import org . apache . lucene . index . IndexFileNames ; import org . apache . lucene . index . SegmentInfos ; import org . apache . lucene . index . SegmentInfo ; import org . apache . lucene . store . Directory ; import java . io . IOException ; import java . io . PrintStream ; import java . util . Map ; import java . util . HashMap ; import java . util . Iterator ; import java . util . List ; import java . util . ArrayList ; import java . util . Collections ; final class IndexFileDeleter { private List deletable ; private Map refCounts = new HashMap ( ) ; private List commits = new ArrayList ( ) ; private List lastFiles = new ArrayList ( ) ; private List commitsToDelete = new ArrayList ( ) ; private PrintStream infoStream ; private Directory directory ; private IndexDeletionPolicy policy ; void setInfoStream ( PrintStream infoStream ) { this . infoStream = infoStream ; } private void message ( String message ) { infoStream . println ( this + " " + Thread . currentThread ( ) . getName ( ) + ": " + message ) ; } public IndexFileDeleter ( Directory directory , IndexDeletionPolicy policy , SegmentInfos segmentInfos , PrintStream infoStream ) throws CorruptIndexException , IOException { this . infoStream = infoStream ; this . policy = policy ; this . directory = directory ; long currentGen = segmentInfos . getGeneration ( ) ; IndexFileNameFilter filter = IndexFileNameFilter . getFilter ( ) ; String [ ] files = directory . list ( ) ; if ( files == null ) throw new IOException ( "cannot read directory " + directory + ": list() returned null" ) ; CommitPoint currentCommitPoint = null ; for ( int i = 0 ; i < files . length ; i ++ ) { String fileName = files [ i ] ; if ( filter . accept ( null , fileName ) && ! fileName . equals ( IndexFileNames . SEGMENTS_GEN ) ) { getRefCount ( fileName ) ; if ( fileName . startsWith ( IndexFileNames . SEGMENTS ) ) { if ( SegmentInfos . generationFromSegmentsFileName ( fileName ) <= currentGen ) { if ( infoStream != null ) { message ( "init: load commit \"" + fileName + "\"" ) ; } SegmentInfos sis = new SegmentInfos ( ) ; sis . read ( directory , fileName ) ; CommitPoint commitPoint = new CommitPoint ( sis ) ; if ( sis . getGeneration ( ) == segmentInfos . getGeneration ( ) ) { currentCommitPoint = commitPoint ; } commits . add ( commitPoint ) ; incRef ( sis , true ) ; } } } } if ( currentCommitPoint == null ) { throw new CorruptIndexException ( "failed to locate current segments_N file" ) ; } Collections . sort ( commits ) ; Iterator it = refCounts . keySet ( ) . iterator ( ) ; while ( it . hasNext ( ) ) { String fileName = ( String ) it . next ( ) ; RefCount rc = ( RefCount ) refCounts . get ( fileName ) ; if ( 0 == rc . count ) { if ( infoStream != null ) { message ( "init: removing unreferenced file \"" + fileName + "\"" ) ; } deleteFile ( fileName ) ; } } policy . onInit ( commits ) ; if ( currentCommitPoint . deleted ) { checkpoint ( segmentInfos , false ) ; } deleteCommits ( ) ; } private void deleteCommits ( ) throws IOException { int size = commitsToDelete . size ( ) ; if ( size > 0 ) { for ( int i = 0 ; i < size ; i ++ ) { CommitPoint commit = ( CommitPoint ) commitsToDelete . get ( i ) ; if ( infoStream != null ) { message ( "deleteCommits: now remove commit \"" + commit . getSegmentsFileName ( ) + "\"" ) ; } int size2 = commit . files . size ( ) ; for ( int j = 0 ; j < size2 ; j ++ ) { decRef ( ( List ) commit . files . get ( j ) ) ; } decRef ( commit . getSegmentsFileName ( ) ) ; } commitsToDelete . clear ( ) ; size = commits . size ( ) ; int readFrom = 0 ; int writeTo = 0 ; while ( readFrom < size ) { CommitPoint commit = ( CommitPoint ) commits . get ( readFrom ) ; if ( ! commit . deleted ) { if ( writeTo != readFrom ) { commits . set ( writeTo , commits . get ( readFrom ) ) ; } writeTo ++ ; } readFrom ++ ; } while ( size > writeTo ) { commits . remove ( size - 1 ) ; size -- ; } } } public void refresh ( ) throws IOException { String [ ] files = directory . list ( ) ; if ( files == null ) throw new IOException ( "cannot read directory " + directory + ": list() returned null" ) ; IndexFileNameFilter filter = IndexFileNameFilter . getFilter ( ) ; for ( int i = 0 ; i < files . length ; i ++ ) { String fileName = files [ i ] ; if ( filter . accept ( null , fileName ) && ! refCounts . containsKey ( fileName ) && ! fileName . equals ( IndexFileNames . SEGMENTS_GEN ) ) { if ( infoStream != null ) { message ( "refresh: removing newly created unreferenced file \"" + fileName + "\"" ) ; } deleteFile ( fileName ) ; } } } public void checkpoint ( SegmentInfos segmentInfos , boolean isCommit ) throws IOException { if ( infoStream != null ) { message ( "now checkpoint \"" + segmentInfos . getCurrentSegmentFileName ( ) + "\" [isCommit = " + isCommit + "]" ) ; } if ( deletable != null ) { List oldDeletable = deletable ; deletable = null ; int size = oldDeletable . size ( ) ; for ( int i = 0 ; i < size ; i ++ ) { deleteFile ( ( String ) oldDeletable . get ( i ) ) ; } } incRef ( segmentInfos , isCommit ) ; if ( isCommit ) { commits . add ( new CommitPoint ( segmentInfos ) ) ; policy . onCommit ( commits ) ; deleteCommits ( ) ; } int size = lastFiles . size ( ) ; if ( size > 0 ) { for ( int i = 0 ; i < size ; i ++ ) { decRef ( ( List ) lastFiles . get ( i ) ) ; } lastFiles . clear ( ) ; } if ( ! isCommit ) { size = segmentInfos . size ( ) ; for ( int i = 0 ; i < size ; i ++ ) { SegmentInfo segmentInfo = segmentInfos . info ( i ) ; if ( segmentInfo . dir == directory ) { lastFiles . add ( segmentInfo . files ( ) ) ; } } } } void incRef ( SegmentInfos segmentInfos , boolean isCommit ) throws IOException { int size = segmentInfos . size ( ) ; for ( int i = 0 ; i < size ; i ++ ) { SegmentInfo segmentInfo = segmentInfos . info ( i ) ; if ( segmentInfo . dir == directory ) { incRef ( segmentInfo . files ( ) ) ; } } if ( isCommit ) { getRefCount ( segmentInfos . getCurrentSegmentFileName ( ) ) . IncRef ( ) ; } } private void incRef ( List files ) throws IOException { int size = files . size ( ) ; for ( int i = 0 ; i < size ; i ++ ) { String fileName = ( String ) files . get ( i ) ; RefCount rc = getRefCount ( fileName ) ; if ( infoStream != null ) { message ( "  IncRef \"" + fileName + "\": pre-incr count is " + rc . count ) ; } rc . IncRef ( ) ; } } private void decRef ( List files ) throws IOException { int size = files . size ( ) ; for ( int i = 0 ; i < size ; i ++ ) { decRef ( ( String ) files . get ( i ) ) ; } } private void decRef ( String fileName ) throws IOException { RefCount rc = getRefCount ( fileName ) ; if ( infoStream != null ) { message ( "  DecRef \"" + fileName + "\": pre-decr count is " + rc . count ) ; } if ( 0 == rc . DecRef ( ) ) { deleteFile ( fileName ) ; refCounts . remove ( fileName ) ; } } void decRef ( SegmentInfos segmentInfos ) throws IOException { final int size = segmentInfos . size ( ) ; for ( int i = 0 ; i < size ; i ++ ) { SegmentInfo segmentInfo = segmentInfos . info ( i ) ; if ( segmentInfo . dir == directory ) { decRef ( segmentInfo . files ( ) ) ; } } } private RefCount getRefCount ( String fileName ) { RefCount rc ; if ( ! refCounts . containsKey ( fileName ) ) { rc = new RefCount ( ) ; refCounts . put ( fileName , rc ) ; } else { rc = ( RefCount ) refCounts . get ( fileName ) ; } return rc ; } private void deleteFile ( String fileName ) throws IOException { try { if ( infoStream != null ) { message ( "delete \"" + fileName + "\"" ) ; } directory . deleteFile ( fileName ) ; } catch ( IOException e ) { if ( directory . fileExists ( fileName ) ) { if ( infoStream != null ) { message ( "IndexFileDeleter: unable to remove file \"" + fileName + "\": " + e . toString ( ) + "; Will re-try later." ) ; } if ( deletable == null ) { deletable = new ArrayList ( ) ; } deletable . add ( fileName ) ; } } } public void deleteDirect ( Directory otherDir , List segments ) throws IOException { int size = segments . size ( ) ; for ( int i = 0 ; i < size ; i ++ ) { List filestoDelete = ( ( SegmentInfo ) segments . get ( i ) ) . files ( ) ; int size2 = filestoDelete . size ( ) ; for ( int j = 0 ; j < size2 ; j ++ ) { otherDir . deleteFile ( ( String ) filestoDelete . get ( j ) ) ; } } } final private static class RefCount { int count ; final private int IncRef ( ) { return ++ count ; } final private int DecRef ( ) { return -- count ; } } final private class CommitPoint implements Comparable , IndexCommitPoint { long gen ; List files ; String segmentsFileName ; boolean deleted ; public CommitPoint ( SegmentInfos segmentInfos ) throws IOException { segmentsFileName = segmentInfos . getCurrentSegmentFileName ( ) ; int size = segmentInfos . size ( ) ; files = new ArrayList ( size ) ; gen = segmentInfos . getGeneration ( ) ; for ( int i = 0 ; i < size ; i ++ ) { SegmentInfo segmentInfo = segmentInfos . info ( i ) ; if ( segmentInfo . dir == directory ) { files . add ( segmentInfo . files ( ) ) ; } } } public String getSegmentsFileName ( ) { return segmentsFileName ; } public void delete ( ) { if ( ! deleted ) { deleted = true ; commitsToDelete . add ( this ) ; } } public int compareTo ( Object obj ) { CommitPoint commit = ( CommitPoint ) obj ; if ( gen < commit . gen ) { return - 1 ; } else if ( gen > commit . gen ) { return 1 ; } else { return 0 ; } } } } 	1	['16', '1', '0', '11', '68', '8', '3', '9', '4', '0.683333333', '814', '1', '2', '0', '0.236111111', '0', '0', '49.375', '2', '1', '11']
package org . apache . lucene . queryParser ; import java . util . ArrayList ; import java . util . List ; import java . util . Map ; import java . util . Vector ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . search . BooleanClause ; import org . apache . lucene . search . BooleanQuery ; import org . apache . lucene . search . MultiPhraseQuery ; import org . apache . lucene . search . PhraseQuery ; import org . apache . lucene . search . Query ; public class MultiFieldQueryParser extends QueryParser { protected String [ ] fields ; protected Map boosts ; public MultiFieldQueryParser ( String [ ] fields , Analyzer analyzer , Map boosts ) { this ( fields , analyzer ) ; this . boosts = boosts ; } public MultiFieldQueryParser ( String [ ] fields , Analyzer analyzer ) { super ( null , analyzer ) ; this . fields = fields ; } protected Query getFieldQuery ( String field , String queryText , int slop ) throws ParseException { if ( field == null ) { List clauses = new ArrayList ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { Query q = super . getFieldQuery ( fields [ i ] , queryText ) ; if ( q != null ) { if ( boosts != null ) { Float boost = ( Float ) boosts . get ( fields [ i ] ) ; if ( boost != null ) { q . setBoost ( boost . floatValue ( ) ) ; } } applySlop ( q , slop ) ; clauses . add ( new BooleanClause ( q , BooleanClause . Occur . SHOULD ) ) ; } } if ( clauses . size ( ) == 0 ) return null ; return getBooleanQuery ( clauses , true ) ; } Query q = super . getFieldQuery ( field , queryText ) ; applySlop ( q , slop ) ; return q ; } private void applySlop ( Query q , int slop ) { if ( q instanceof PhraseQuery ) { ( ( PhraseQuery ) q ) . setSlop ( slop ) ; } else if ( q instanceof MultiPhraseQuery ) { ( ( MultiPhraseQuery ) q ) . setSlop ( slop ) ; } } protected Query getFieldQuery ( String field , String queryText ) throws ParseException { return getFieldQuery ( field , queryText , 0 ) ; } protected Query getFuzzyQuery ( String field , String termStr , float minSimilarity ) throws ParseException { if ( field == null ) { List clauses = new ArrayList ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( getFuzzyQuery ( fields [ i ] , termStr , minSimilarity ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getFuzzyQuery ( field , termStr , minSimilarity ) ; } protected Query getPrefixQuery ( String field , String termStr ) throws ParseException { if ( field == null ) { List clauses = new ArrayList ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( getPrefixQuery ( fields [ i ] , termStr ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getPrefixQuery ( field , termStr ) ; } protected Query getWildcardQuery ( String field , String termStr ) throws ParseException { if ( field == null ) { List clauses = new ArrayList ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( getWildcardQuery ( fields [ i ] , termStr ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getWildcardQuery ( field , termStr ) ; } protected Query getRangeQuery ( String field , String part1 , String part2 , boolean inclusive ) throws ParseException { if ( field == null ) { List clauses = new ArrayList ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( getRangeQuery ( fields [ i ] , part1 , part2 , inclusive ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getRangeQuery ( field , part1 , part2 , inclusive ) ; } public static Query parse ( String [ ] queries , String [ ] fields , Analyzer analyzer ) throws ParseException { if ( queries . length != fields . length ) throw new IllegalArgumentException ( "queries.length != fields.length" ) ; BooleanQuery bQuery = new BooleanQuery ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { QueryParser qp = new QueryParser ( fields [ i ] , analyzer ) ; Query q = qp . parse ( queries [ i ] ) ; if ( q != null && ( ! ( q instanceof BooleanQuery ) || ( ( BooleanQuery ) q ) . getClauses ( ) . length > 0 ) ) { bQuery . add ( q , BooleanClause . Occur . SHOULD ) ; } } return bQuery ; } public static Query parse ( String query , String [ ] fields , BooleanClause . Occur [ ] flags , Analyzer analyzer ) throws ParseException { if ( fields . length != flags . length ) throw new IllegalArgumentException ( "fields.length != flags.length" ) ; BooleanQuery bQuery = new BooleanQuery ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { QueryParser qp = new QueryParser ( fields [ i ] , analyzer ) ; Query q = qp . parse ( query ) ; if ( q != null && ( ! ( q instanceof BooleanQuery ) || ( ( BooleanQuery ) q ) . getClauses ( ) . length > 0 ) ) { bQuery . add ( q , flags [ i ] ) ; } } return bQuery ; } public static Query parse ( String [ ] queries , String [ ] fields , BooleanClause . Occur [ ] flags , Analyzer analyzer ) throws ParseException { if ( ! ( queries . length == fields . length && queries . length == flags . length ) ) throw new IllegalArgumentException ( "queries, fields, and flags array have have different length" ) ; BooleanQuery bQuery = new BooleanQuery ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { QueryParser qp = new QueryParser ( fields [ i ] , analyzer ) ; Query q = qp . parse ( queries [ i ] ) ; if ( q != null && ( ! ( q instanceof BooleanQuery ) || ( ( BooleanQuery ) q ) . getClauses ( ) . length > 0 ) ) { bQuery . add ( q , flags [ i ] ) ; } } return bQuery ; } } 	0	['12', '2', '0', '9', '33', '34', '0', '9', '5', '0.590909091', '453', '1', '0', '0.885057471', '0.283333333', '1', '5', '36.58333333', '3', '1', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; import java . io . Reader ; public class KeywordTokenizer extends Tokenizer { private static final int DEFAULT_BUFFER_SIZE = 256 ; private boolean done ; private final char [ ] buffer ; public KeywordTokenizer ( Reader input ) { this ( input , DEFAULT_BUFFER_SIZE ) ; } public KeywordTokenizer ( Reader input , int bufferSize ) { super ( input ) ; this . buffer = new char [ bufferSize ] ; this . done = false ; } public Token next ( ) throws IOException { if ( ! done ) { done = true ; StringBuffer buffer = new StringBuffer ( ) ; int length ; while ( true ) { length = input . read ( this . buffer ) ; if ( length == - 1 ) break ; buffer . append ( this . buffer , 0 , length ) ; } String text = buffer . toString ( ) ; return new Token ( text , 0 , text . length ( ) ) ; } return null ; } } 	1	['3', '3', '0', '3', '10', '1', '1', '2', '3', '0.5', '63', '1', '0', '0.8', '0.666666667', '0', '0', '19', '1', '0.3333', '3']
package org . apache . lucene . search ; public interface ScoreDocComparator { static final ScoreDocComparator RELEVANCE = new ScoreDocComparator ( ) { public int compare ( ScoreDoc i , ScoreDoc j ) { if ( i . score > j . score ) return - 1 ; if ( i . score < j . score ) return 1 ; return 0 ; } public Comparable sortValue ( ScoreDoc i ) { return new Float ( i . score ) ; } public int sortType ( ) { return SortField . SCORE ; } } ; static final ScoreDocComparator INDEXORDER = new ScoreDocComparator ( ) { public int compare ( ScoreDoc i , ScoreDoc j ) { if ( i . doc < j . doc ) return - 1 ; if ( i . doc > j . doc ) return 1 ; return 0 ; } public Comparable sortValue ( ScoreDoc i ) { return new Integer ( i . doc ) ; } public int sortType ( ) { return SortField . DOC ; } } ; int compare ( ScoreDoc i , ScoreDoc j ) ; Comparable sortValue ( ScoreDoc i ) ; int sortType ( ) ; } 	0	['4', '1', '0', '16', '6', '6', '15', '3', '3', '1', '15', '0', '2', '0', '0.833333333', '0', '0', '2.25', '1', '0.75', '0']
package org . apache . lucene . queryParser ; public interface QueryParserConstants { int EOF = 0 ; int _NUM_CHAR = 1 ; int _ESCAPED_CHAR = 2 ; int _TERM_START_CHAR = 3 ; int _TERM_CHAR = 4 ; int _WHITESPACE = 5 ; int AND = 7 ; int OR = 8 ; int NOT = 9 ; int PLUS = 10 ; int MINUS = 11 ; int LPAREN = 12 ; int RPAREN = 13 ; int COLON = 14 ; int STAR = 15 ; int CARAT = 16 ; int QUOTED = 17 ; int TERM = 18 ; int FUZZY_SLOP = 19 ; int PREFIXTERM = 20 ; int WILDTERM = 21 ; int RANGEIN_START = 22 ; int RANGEEX_START = 23 ; int NUMBER = 24 ; int RANGEIN_TO = 25 ; int RANGEIN_END = 26 ; int RANGEIN_QUOTED = 27 ; int RANGEIN_GOOP = 28 ; int RANGEEX_TO = 29 ; int RANGEEX_END = 30 ; int RANGEEX_QUOTED = 31 ; int RANGEEX_GOOP = 32 ; int Boost = 0 ; int RangeEx = 1 ; int RangeIn = 2 ; int DEFAULT = 3 ; String [ ] tokenImage = { "<EOF>" , "<_NUM_CHAR>" , "<_ESCAPED_CHAR>" , "<_TERM_START_CHAR>" , "<_TERM_CHAR>" , "<_WHITESPACE>" , "<token of kind 6>" , "<AND>" , "<OR>" , "<NOT>" , "\"+\"" , "\"-\"" , "\"(\"" , "\")\"" , "\":\"" , "\"*\"" , "\"^\"" , "<QUOTED>" , "<TERM>" , "<FUZZY_SLOP>" , "<PREFIXTERM>" , "<WILDTERM>" , "\"[\"" , "\"{\"" , "<NUMBER>" , "\"TO\"" , "\"]\"" , "<RANGEIN_QUOTED>" , "<RANGEIN_GOOP>" , "\"TO\"" , "\"}\"" , "<RANGEEX_QUOTED>" , "<RANGEEX_GOOP>" , } ; } 	1	['1', '1', '0', '2', '1', '0', '2', '0', '0', '2', '174', '0', '0', '0', '0', '0', '0', '136', '0', '0', '2']
package org . apache . lucene . index ; import java . io . IOException ; abstract class TermsHashConsumerPerThread { abstract void startDocument ( ) throws IOException ; abstract DocumentsWriter . DocWriter finishDocument ( ) throws IOException ; abstract public TermsHashConsumerPerField addField ( TermsHashPerField termsHashPerField , FieldInfo fieldInfo ) ; abstract public void abort ( ) ; } 	0	['5', '1', '2', '11', '6', '10', '8', '4', '2', '2', '8', '0', '0', '0', '0.466666667', '0', '0', '0.6', '1', '0.8', '0']
package org . apache . lucene . index ; final class IndexFileNames { static final String SEGMENTS = "segments" ; static final String SEGMENTS_GEN = "segments.gen" ; static final String DELETABLE = "deletable" ; static final String NORMS_EXTENSION = "nrm" ; static final String COMPOUND_FILE_EXTENSION = "cfs" ; static final String DELETES_EXTENSION = "del" ; static final String PLAIN_NORMS_EXTENSION = "f" ; static final String SEPARATE_NORMS_EXTENSION = "s" ; static final String INDEX_EXTENSIONS [ ] = new String [ ] { "cfs" , "fnm" , "fdx" , "fdt" , "tii" , "tis" , "frq" , "prx" , "del" , "tvx" , "tvd" , "tvf" , "gen" , "nrm" } ; static final String [ ] INDEX_EXTENSIONS_IN_COMPOUND_FILE = new String [ ] { "fnm" , "fdx" , "fdt" , "tii" , "tis" , "frq" , "prx" , "tvx" , "tvd" , "tvf" , "nrm" } ; static final String COMPOUND_EXTENSIONS [ ] = new String [ ] { "fnm" , "frq" , "prx" , "fdx" , "fdt" , "tii" , "tis" } ; static final String VECTOR_EXTENSIONS [ ] = new String [ ] { "tvx" , "tvd" , "tvf" } ; static final String fileNameFromGeneration ( String base , String extension , long gen ) { if ( gen == SegmentInfo . NO ) { return null ; } else if ( gen == SegmentInfo . WITHOUT_GEN ) { return base + extension ; } else { return base + "_" + Long . toString ( gen , Character . MAX_RADIX ) + extension ; } } } 	1	['3', '1', '0', '5', '8', '3', '5', '0', '0', '1.333333333', '205', '0', '0', '0', '0.5', '0', '0', '63.33333333', '3', '1', '3']
package org . apache . lucene . analysis ; import java . io . IOException ; public final class LowerCaseFilter extends TokenFilter { public LowerCaseFilter ( TokenStream in ) { super ( in ) ; } public final Token next ( final Token reusableToken ) throws IOException { assert reusableToken != null ; Token nextToken = input . next ( reusableToken ) ; if ( nextToken != null ) { final char [ ] buffer = nextToken . termBuffer ( ) ; final int length = nextToken . termLength ( ) ; for ( int i = 0 ; i < length ; i ++ ) buffer [ i ] = Character . toLowerCase ( buffer [ i ] ) ; return nextToken ; } else return null ; } } 	0	['4', '3', '0', '4', '14', '4', '1', '3', '2', '0.833333333', '74', '0', '0', '0.777777778', '0.416666667', '1', '2', '17', '1', '0.5', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . IndexOutput ; import java . io . File ; import java . io . FileNotFoundException ; import java . io . IOException ; import java . io . PrintStream ; import java . util . Vector ; final class SegmentInfos extends Vector { public static final int FORMAT = - 1 ; public static final int FORMAT_LOCKLESS = - 2 ; public static final int FORMAT_SINGLE_NORM_FILE = - 3 ; private static final int CURRENT_FORMAT = FORMAT_SINGLE_NORM_FILE ; public int counter = 0 ; private long version = System . currentTimeMillis ( ) ; private long generation = 0 ; private long lastGeneration = 0 ; private static PrintStream infoStream ; public final SegmentInfo info ( int i ) { return ( SegmentInfo ) elementAt ( i ) ; } public static long getCurrentSegmentGeneration ( String [ ] files ) { if ( files == null ) { return - 1 ; } long max = - 1 ; for ( int i = 0 ; i < files . length ; i ++ ) { String file = files [ i ] ; if ( file . startsWith ( IndexFileNames . SEGMENTS ) && ! file . equals ( IndexFileNames . SEGMENTS_GEN ) ) { long gen = generationFromSegmentsFileName ( file ) ; if ( gen > max ) { max = gen ; } } } return max ; } public static long getCurrentSegmentGeneration ( Directory directory ) throws IOException { String [ ] files = directory . list ( ) ; if ( files == null ) throw new IOException ( "cannot read directory " + directory + ": list() returned null" ) ; return getCurrentSegmentGeneration ( files ) ; } public static String getCurrentSegmentFileName ( String [ ] files ) throws IOException { return IndexFileNames . fileNameFromGeneration ( IndexFileNames . SEGMENTS , "" , getCurrentSegmentGeneration ( files ) ) ; } public static String getCurrentSegmentFileName ( Directory directory ) throws IOException { return IndexFileNames . fileNameFromGeneration ( IndexFileNames . SEGMENTS , "" , getCurrentSegmentGeneration ( directory ) ) ; } public String getCurrentSegmentFileName ( ) { return IndexFileNames . fileNameFromGeneration ( IndexFileNames . SEGMENTS , "" , lastGeneration ) ; } public static long generationFromSegmentsFileName ( String fileName ) { if ( fileName . equals ( IndexFileNames . SEGMENTS ) ) { return 0 ; } else if ( fileName . startsWith ( IndexFileNames . SEGMENTS ) ) { return Long . parseLong ( fileName . substring ( 1 + IndexFileNames . SEGMENTS . length ( ) ) , Character . MAX_RADIX ) ; } else { throw new IllegalArgumentException ( "fileName \"" + fileName + "\" is not a segments file" ) ; } } public String getNextSegmentFileName ( ) { long nextGeneration ; if ( generation == - 1 ) { nextGeneration = 1 ; } else { nextGeneration = generation + 1 ; } return IndexFileNames . fileNameFromGeneration ( IndexFileNames . SEGMENTS , "" , nextGeneration ) ; } public final void read ( Directory directory , String segmentFileName ) throws CorruptIndexException , IOException { boolean success = false ; IndexInput input = directory . openInput ( segmentFileName ) ; generation = generationFromSegmentsFileName ( segmentFileName ) ; lastGeneration = generation ; try { int format = input . readInt ( ) ; if ( format < 0 ) { if ( format < CURRENT_FORMAT ) throw new CorruptIndexException ( "Unknown format version: " + format ) ; version = input . readLong ( ) ; counter = input . readInt ( ) ; } else { counter = format ; } for ( int i = input . readInt ( ) ; i > 0 ; i -- ) { addElement ( new SegmentInfo ( directory , format , input ) ) ; } if ( format >= 0 ) { if ( input . getFilePointer ( ) >= input . length ( ) ) version = System . currentTimeMillis ( ) ; else version = input . readLong ( ) ; } success = true ; } finally { input . close ( ) ; if ( ! success ) { clear ( ) ; } } } public final void read ( Directory directory ) throws CorruptIndexException , IOException { generation = lastGeneration = - 1 ; new FindSegmentsFile ( directory ) { protected Object doBody ( String segmentFileName ) throws CorruptIndexException , IOException { read ( directory , segmentFileName ) ; return null ; } } . run ( ) ; } public final void write ( Directory directory ) throws IOException { String segmentFileName = getNextSegmentFileName ( ) ; if ( generation == - 1 ) { generation = 1 ; } else { generation ++ ; } IndexOutput output = directory . createOutput ( segmentFileName ) ; boolean success = false ; try { output . writeInt ( CURRENT_FORMAT ) ; output . writeLong ( ++ version ) ; output . writeInt ( counter ) ; output . writeInt ( size ( ) ) ; for ( int i = 0 ; i < size ( ) ; i ++ ) { info ( i ) . write ( output ) ; } } finally { try { output . close ( ) ; success = true ; } finally { if ( ! success ) { directory . deleteFile ( segmentFileName ) ; } } } try { output = directory . createOutput ( IndexFileNames . SEGMENTS_GEN ) ; try { output . writeInt ( FORMAT_LOCKLESS ) ; output . writeLong ( generation ) ; output . writeLong ( generation ) ; } finally { output . close ( ) ; } } catch ( IOException e ) { } lastGeneration = generation ; } public Object clone ( ) { SegmentInfos sis = ( SegmentInfos ) super . clone ( ) ; for ( int i = 0 ; i < sis . size ( ) ; i ++ ) { sis . setElementAt ( ( ( SegmentInfo ) sis . elementAt ( i ) ) . clone ( ) , i ) ; } return sis ; } public long getVersion ( ) { return version ; } public long getGeneration ( ) { return generation ; } public static long readCurrentVersion ( Directory directory ) throws CorruptIndexException , IOException { return ( ( Long ) new FindSegmentsFile ( directory ) { protected Object doBody ( String segmentFileName ) throws CorruptIndexException , IOException { IndexInput input = directory . openInput ( segmentFileName ) ; int format = 0 ; long version = 0 ; try { format = input . readInt ( ) ; if ( format < 0 ) { if ( format < CURRENT_FORMAT ) throw new CorruptIndexException ( "Unknown format version: " + format ) ; version = input . readLong ( ) ; } } finally { input . close ( ) ; } if ( format < 0 ) return new Long ( version ) ; SegmentInfos sis = new SegmentInfos ( ) ; sis . read ( directory , segmentFileName ) ; return new Long ( sis . getVersion ( ) ) ; } } . run ( ) ) . longValue ( ) ; } public static void setInfoStream ( PrintStream infoStream ) { SegmentInfos . infoStream = infoStream ; } private static int defaultGenFileRetryCount = 10 ; private static int defaultGenFileRetryPauseMsec = 50 ; private static int defaultGenLookaheadCount = 10 ; public static void setDefaultGenFileRetryCount ( int count ) { defaultGenFileRetryCount = count ; } public static int getDefaultGenFileRetryCount ( ) { return defaultGenFileRetryCount ; } public static void setDefaultGenFileRetryPauseMsec ( int msec ) { defaultGenFileRetryPauseMsec = msec ; } public static int getDefaultGenFileRetryPauseMsec ( ) { return defaultGenFileRetryPauseMsec ; } public static void setDefaultGenLookaheadCount ( int count ) { defaultGenLookaheadCount = count ; } public static int getDefaultGenLookahedCount ( ) { return defaultGenLookaheadCount ; } public static PrintStream getInfoStream ( ) { return infoStream ; } private static void message ( String message ) { if ( infoStream != null ) { infoStream . println ( Thread . currentThread ( ) . getName ( ) + ": " + message ) ; } } public abstract static class FindSegmentsFile { File fileDirectory ; Directory directory ; public FindSegmentsFile ( File directory ) { this . fileDirectory = directory ; } public FindSegmentsFile ( Directory directory ) { this . directory = directory ; } public Object run ( ) throws CorruptIndexException , IOException { String segmentFileName = null ; long lastGen = - 1 ; long gen = 0 ; int genLookaheadCount = 0 ; IOException exc = null ; boolean retry = false ; int method = 0 ; while ( true ) { String [ ] files = null ; if ( 0 == method ) { if ( directory != null ) { files = directory . list ( ) ; if ( files == null ) throw new FileNotFoundException ( "cannot read directory " + directory + ": list() returned null" ) ; } else { files = fileDirectory . list ( ) ; if ( files == null ) throw new FileNotFoundException ( "cannot read directory " + fileDirectory + ": list() returned null" ) ; } gen = getCurrentSegmentGeneration ( files ) ; if ( gen == - 1 ) { String s = "" ; for ( int i = 0 ; i < files . length ; i ++ ) { s += " " + files [ i ] ; } throw new FileNotFoundException ( "no segments* file found in " + directory + ": files:" + s ) ; } } if ( 1 == method || ( 0 == method && lastGen == gen && retry ) ) { method = 1 ; for ( int i = 0 ; i < defaultGenFileRetryCount ; i ++ ) { IndexInput genInput = null ; try { genInput = directory . openInput ( IndexFileNames . SEGMENTS_GEN ) ; } catch ( IOException e ) { message ( "segments.gen open: IOException " + e ) ; } if ( genInput != null ) { try { int version = genInput . readInt ( ) ; if ( version == FORMAT_LOCKLESS ) { long gen0 = genInput . readLong ( ) ; long gen1 = genInput . readLong ( ) ; message ( "fallback check: " + gen0 + "; " + gen1 ) ; if ( gen0 == gen1 ) { if ( gen0 > gen ) { message ( "fallback to '" + IndexFileNames . SEGMENTS_GEN + "' check: now try generation " + gen0 + " > " + gen ) ; gen = gen0 ; } break ; } } } catch ( IOException err2 ) { } finally { genInput . close ( ) ; } } try { Thread . sleep ( defaultGenFileRetryPauseMsec ) ; } catch ( InterruptedException e ) { } } } if ( 2 == method || ( 1 == method && lastGen == gen && retry ) ) { method = 2 ; if ( genLookaheadCount < defaultGenLookaheadCount ) { gen ++ ; genLookaheadCount ++ ; message ( "look ahead increment gen to " + gen ) ; } } if ( lastGen == gen ) { if ( retry ) { throw exc ; } else { retry = true ; } } else { retry = false ; } lastGen = gen ; segmentFileName = IndexFileNames . fileNameFromGeneration ( IndexFileNames . SEGMENTS , "" , gen ) ; try { Object v = doBody ( segmentFileName ) ; if ( exc != null ) { message ( "success on " + segmentFileName ) ; } return v ; } catch ( IOException err ) { if ( exc == null ) { exc = err ; } message ( "primary Exception on '" + segmentFileName + "': " + err + "'; will retry: retry=" + retry + "; gen = " + gen ) ; if ( ! retry && gen > 1 ) { String prevSegmentFileName = IndexFileNames . fileNameFromGeneration ( IndexFileNames . SEGMENTS , "" , gen - 1 ) ; if ( directory . fileExists ( prevSegmentFileName ) ) { message ( "fallback to prior segment file '" + prevSegmentFileName + "'" ) ; try { Object v = doBody ( prevSegmentFileName ) ; if ( exc != null ) { message ( "success on fallback " + prevSegmentFileName ) ; } return v ; } catch ( IOException err2 ) { message ( "secondary Exception on '" + prevSegmentFileName + "': " + err2 + "'; will retry" ) ; } } } } } } protected abstract Object doBody ( String segmentFileName ) throws CorruptIndexException , IOException ; } } 	1	['30', '4', '0', '16', '75', '349', '10', '8', '23', '0.936781609', '515', '0.666666667', '0', '0.738317757', '0.155172414', '1', '3', '15.76666667', '6', '1.2667', '8']
package org . apache . lucene . index ; import org . apache . lucene . util . ArrayUtil ; import org . apache . lucene . search . Similarity ; final class NormsWriterPerField extends InvertedDocEndConsumerPerField implements Comparable { final NormsWriterPerThread perThread ; final FieldInfo fieldInfo ; final DocumentsWriter . DocState docState ; int [ ] docIDs = new int [ 1 ] ; byte [ ] norms = new byte [ 1 ] ; int upto ; final DocInverter . FieldInvertState fieldState ; public void reset ( ) { docIDs = ArrayUtil . shrink ( docIDs , upto ) ; norms = ArrayUtil . shrink ( norms , upto ) ; upto = 0 ; } public NormsWriterPerField ( final DocInverterPerField docInverterPerField , final NormsWriterPerThread perThread , final FieldInfo fieldInfo ) { this . perThread = perThread ; this . fieldInfo = fieldInfo ; docState = perThread . docState ; fieldState = docInverterPerField . fieldState ; } void abort ( ) { upto = 0 ; } public int compareTo ( Object other ) { return fieldInfo . name . compareTo ( ( ( NormsWriterPerField ) other ) . fieldInfo . name ) ; } void finish ( ) { assert docIDs . length == norms . length ; if ( fieldInfo . isIndexed && ! fieldInfo . omitNorms ) { if ( docIDs . length <= upto ) { assert docIDs . length == upto ; docIDs = ArrayUtil . grow ( docIDs , 1 + upto ) ; norms = ArrayUtil . grow ( norms , 1 + upto ) ; } final float norm = fieldState . boost * docState . similarity . lengthNorm ( fieldInfo . name , fieldState . length ) ; norms [ upto ] = Similarity . encodeNorm ( norm ) ; docIDs [ upto ] = docState . docID ; upto ++ ; } } } 	0	['7', '2', '0', '9', '20', '5', '2', '8', '3', '0.796296296', '191', '0', '4', '0.285714286', '0.277777778', '0', '0', '25', '8', '1.7143', '0']
package org . apache . lucene . document ; import org . apache . lucene . analysis . TokenStream ; import org . apache . lucene . index . IndexWriter ; import org . apache . lucene . util . Parameter ; import java . io . Reader ; import java . io . Serializable ; public final class Field extends AbstractField implements Fieldable , Serializable { public static final class Store extends Parameter implements Serializable { private Store ( String name ) { super ( name ) ; } public static final Store COMPRESS = new Store ( "COMPRESS" ) ; public static final Store YES = new Store ( "YES" ) ; public static final Store NO = new Store ( "NO" ) ; } public static final class Index extends Parameter implements Serializable { private Index ( String name ) { super ( name ) ; } public static final Index NO = new Index ( "NO" ) ; public static final Index TOKENIZED = new Index ( "TOKENIZED" ) ; public static final Index UN_TOKENIZED = new Index ( "UN_TOKENIZED" ) ; public static final Index NO_NORMS = new Index ( "NO_NORMS" ) ; } public static final class TermVector extends Parameter implements Serializable { private TermVector ( String name ) { super ( name ) ; } public static final TermVector NO = new TermVector ( "NO" ) ; public static final TermVector YES = new TermVector ( "YES" ) ; public static final TermVector WITH_POSITIONS = new TermVector ( "WITH_POSITIONS" ) ; public static final TermVector WITH_OFFSETS = new TermVector ( "WITH_OFFSETS" ) ; public static final TermVector WITH_POSITIONS_OFFSETS = new TermVector ( "WITH_POSITIONS_OFFSETS" ) ; } public String stringValue ( ) { return fieldsData instanceof String ? ( String ) fieldsData : null ; } public Reader readerValue ( ) { return fieldsData instanceof Reader ? ( Reader ) fieldsData : null ; } public byte [ ] binaryValue ( ) { return fieldsData instanceof byte [ ] ? ( byte [ ] ) fieldsData : null ; } public TokenStream tokenStreamValue ( ) { return fieldsData instanceof TokenStream ? ( TokenStream ) fieldsData : null ; } public Field ( String name , String value , Store store , Index index ) { this ( name , value , store , index , TermVector . NO ) ; } public Field ( String name , String value , Store store , Index index , TermVector termVector ) { if ( name == null ) throw new NullPointerException ( "name cannot be null" ) ; if ( value == null ) throw new NullPointerException ( "value cannot be null" ) ; if ( name . length ( ) == 0 && value . length ( ) == 0 ) throw new IllegalArgumentException ( "name and value cannot both be empty" ) ; if ( index == Index . NO && store == Store . NO ) throw new IllegalArgumentException ( "it doesn't make sense to have a field that " + "is neither indexed nor stored" ) ; if ( index == Index . NO && termVector != TermVector . NO ) throw new IllegalArgumentException ( "cannot store term vector information " + "for a field that is not indexed" ) ; this . name = name . intern ( ) ; this . fieldsData = value ; if ( store == Store . YES ) { this . isStored = true ; this . isCompressed = false ; } else if ( store == Store . COMPRESS ) { this . isStored = true ; this . isCompressed = true ; } else if ( store == Store . NO ) { this . isStored = false ; this . isCompressed = false ; } else throw new IllegalArgumentException ( "unknown store parameter " + store ) ; if ( index == Index . NO ) { this . isIndexed = false ; this . isTokenized = false ; } else if ( index == Index . TOKENIZED ) { this . isIndexed = true ; this . isTokenized = true ; } else if ( index == Index . UN_TOKENIZED ) { this . isIndexed = true ; this . isTokenized = false ; } else if ( index == Index . NO_NORMS ) { this . isIndexed = true ; this . isTokenized = false ; this . omitNorms = true ; } else { throw new IllegalArgumentException ( "unknown index parameter " + index ) ; } this . isBinary = false ; setStoreTermVector ( termVector ) ; } public Field ( String name , Reader reader ) { this ( name , reader , TermVector . NO ) ; } public Field ( String name , Reader reader , TermVector termVector ) { if ( name == null ) throw new NullPointerException ( "name cannot be null" ) ; if ( reader == null ) throw new NullPointerException ( "reader cannot be null" ) ; this . name = name . intern ( ) ; this . fieldsData = reader ; this . isStored = false ; this . isCompressed = false ; this . isIndexed = true ; this . isTokenized = true ; this . isBinary = false ; setStoreTermVector ( termVector ) ; } public Field ( String name , TokenStream tokenStream ) { this ( name , tokenStream , TermVector . NO ) ; } public Field ( String name , TokenStream tokenStream , TermVector termVector ) { if ( name == null ) throw new NullPointerException ( "name cannot be null" ) ; if ( tokenStream == null ) throw new NullPointerException ( "tokenStream cannot be null" ) ; this . name = name . intern ( ) ; this . fieldsData = tokenStream ; this . isStored = false ; this . isCompressed = false ; this . isIndexed = true ; this . isTokenized = true ; this . isBinary = false ; setStoreTermVector ( termVector ) ; } public Field ( String name , byte [ ] value , Store store ) { if ( name == null ) throw new IllegalArgumentException ( "name cannot be null" ) ; if ( value == null ) throw new IllegalArgumentException ( "value cannot be null" ) ; this . name = name . intern ( ) ; this . fieldsData = value ; if ( store == Store . YES ) { this . isStored = true ; this . isCompressed = false ; } else if ( store == Store . COMPRESS ) { this . isStored = true ; this . isCompressed = true ; } else if ( store == Store . NO ) throw new IllegalArgumentException ( "binary values can't be unstored" ) ; else throw new IllegalArgumentException ( "unknown store parameter " + store ) ; this . isIndexed = false ; this . isTokenized = false ; this . isBinary = true ; setStoreTermVector ( TermVector . NO ) ; } } 	1	['11', '2', '0', '8', '21', '0', '2', '6', '11', '2', '392', '0', '0', '0.8', '0.352272727', '1', '4', '34.63636364', '2', '0.7273', '2']
package org . apache . lucene . index ; import java . io . IOException ; public class SerialMergeScheduler extends MergeScheduler { synchronized public void merge ( IndexWriter writer ) throws CorruptIndexException , IOException { while ( true ) { MergePolicy . OneMerge merge = writer . getNextMerge ( ) ; if ( merge == null ) break ; writer . merge ( merge ) ; } } public void close ( ) { } } 	0	['3', '2', '0', '5', '6', '3', '1', '4', '3', '2', '18', '0', '0', '0.5', '0.666666667', '0', '0', '5', '1', '0.6667', '0']
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; public class TopDocCollector extends HitCollector { private int numHits ; private float minScore = 0.0f ; int totalHits ; PriorityQueue hq ; public TopDocCollector ( int numHits ) { this ( numHits , new HitQueue ( numHits ) ) ; } TopDocCollector ( int numHits , PriorityQueue hq ) { this . numHits = numHits ; this . hq = hq ; } public void collect ( int doc , float score ) { if ( score > 0.0f ) { totalHits ++ ; if ( hq . size ( ) < numHits || score >= minScore ) { hq . insert ( new ScoreDoc ( doc , score ) ) ; minScore = ( ( ScoreDoc ) hq . top ( ) ) . score ; } } } public int getTotalHits ( ) { return totalHits ; } public TopDocs topDocs ( ) { ScoreDoc [ ] scoreDocs = new ScoreDoc [ hq . size ( ) ] ; for ( int i = hq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = ( ScoreDoc ) hq . pop ( ) ; float maxScore = ( totalHits == 0 ) ? Float . NEGATIVE_INFINITY : scoreDocs [ 0 ] . score ; return new TopDocs ( totalHits , scoreDocs , maxScore ) ; } } 	1	['5', '2', '1', '7', '13', '0', '2', '5', '4', '0.4375', '110', '0.5', '1', '0.25', '0.5', '0', '0', '20.2', '4', '1.6', '2']
package org . apache . lucene . search ; public class TopFieldDocs extends TopDocs { public SortField [ ] fields ; TopFieldDocs ( int totalHits , ScoreDoc [ ] scoreDocs , SortField [ ] fields , float maxScore ) { super ( totalHits , scoreDocs , maxScore ) ; this . fields = fields ; } } 	0	['1', '2', '0', '14', '2', '0', '11', '3', '0', '2', '11', '0', '1', '1', '1', '0', '0', '9', '0', '0', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; public class KeywordAnalyzer extends Analyzer { public TokenStream tokenStream ( String fieldName , final Reader reader ) { return new KeywordTokenizer ( reader ) ; } } 	1	['2', '2', '0', '3', '4', '1', '0', '3', '2', '2', '10', '0', '0', '0.666666667', '0.666666667', '0', '0', '4', '1', '0.5', '1']
package org . apache . lucene . document ; import java . util . * ; import org . apache . lucene . search . ScoreDoc ; import org . apache . lucene . search . Searcher ; import org . apache . lucene . index . IndexReader ; public final class Document implements java . io . Serializable { List fields = new ArrayList ( ) ; private float boost = 1.0f ; public Document ( ) { } public void setBoost ( float boost ) { this . boost = boost ; } public float getBoost ( ) { return boost ; } public final void add ( Fieldable field ) { fields . add ( field ) ; } public final void removeField ( String name ) { Iterator it = fields . iterator ( ) ; while ( it . hasNext ( ) ) { Fieldable field = ( Fieldable ) it . next ( ) ; if ( field . name ( ) . equals ( name ) ) { it . remove ( ) ; return ; } } } public final void removeFields ( String name ) { Iterator it = fields . iterator ( ) ; while ( it . hasNext ( ) ) { Fieldable field = ( Fieldable ) it . next ( ) ; if ( field . name ( ) . equals ( name ) ) { it . remove ( ) ; } } } public final Field getField ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Field field = ( Field ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) return field ; } return null ; } public Fieldable getFieldable ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) return field ; } return null ; } public final String get ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( ! field . isBinary ( ) ) ) return field . stringValue ( ) ; } return null ; } public final Enumeration fields ( ) { return new Enumeration ( ) { final Iterator iter = fields . iterator ( ) ; public boolean hasMoreElements ( ) { return iter . hasNext ( ) ; } public Object nextElement ( ) { return iter . next ( ) ; } } ; } public final List getFields ( ) { return fields ; } private final static Field [ ] NO_FIELDS = new Field [ 0 ] ; public final Field [ ] getFields ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Field field = ( Field ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) { result . add ( field ) ; } } if ( result . size ( ) == 0 ) return NO_FIELDS ; return ( Field [ ] ) result . toArray ( new Field [ result . size ( ) ] ) ; } private final static Fieldable [ ] NO_FIELDABLES = new Fieldable [ 0 ] ; public Fieldable [ ] getFieldables ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) { result . add ( field ) ; } } if ( result . size ( ) == 0 ) return NO_FIELDABLES ; return ( Fieldable [ ] ) result . toArray ( new Fieldable [ result . size ( ) ] ) ; } private final static String [ ] NO_STRINGS = new String [ 0 ] ; public final String [ ] getValues ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( ! field . isBinary ( ) ) ) result . add ( field . stringValue ( ) ) ; } if ( result . size ( ) == 0 ) return NO_STRINGS ; return ( String [ ] ) result . toArray ( new String [ result . size ( ) ] ) ; } private final static byte [ ] [ ] NO_BYTES = new byte [ 0 ] [ ] ; public final byte [ ] [ ] getBinaryValues ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( field . isBinary ( ) ) ) result . add ( field . binaryValue ( ) ) ; } if ( result . size ( ) == 0 ) return NO_BYTES ; return ( byte [ ] [ ] ) result . toArray ( new byte [ result . size ( ) ] [ ] ) ; } public final byte [ ] getBinaryValue ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( field . isBinary ( ) ) ) return field . binaryValue ( ) ; } return null ; } public final String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "Document<" ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; buffer . append ( field . toString ( ) ) ; if ( i != fields . size ( ) - 1 ) buffer . append ( " " ) ; } buffer . append ( ">" ) ; return buffer . toString ( ) ; } } 	0	['18', '1', '0', '30', '39', '0', '28', '3', '17', '0.81372549', '432', '0.833333333', '2', '0', '0.426470588', '0', '0', '22.66666667', '5', '2.4444', '0']
package org . apache . lucene . store ; import java . io . File ; import java . io . FileInputStream ; import java . io . FileOutputStream ; import java . io . IOException ; import java . io . RandomAccessFile ; import java . security . MessageDigest ; import java . security . NoSuchAlgorithmException ; import java . util . Hashtable ; import org . apache . lucene . index . IndexFileNameFilter ; import org . apache . lucene . index . IndexWriter ; public class FSDirectory extends Directory { private static final Hashtable DIRECTORIES = new Hashtable ( ) ; private static boolean disableLocks = false ; public static void setDisableLocks ( boolean doDisableLocks ) { FSDirectory . disableLocks = doDisableLocks ; } public static boolean getDisableLocks ( ) { return FSDirectory . disableLocks ; } public static final String LOCK_DIR = System . getProperty ( "org.apache.lucene.lockDir" , System . getProperty ( "java.io.tmpdir" ) ) ; private static Class IMPL ; static { try { String name = System . getProperty ( "org.apache.lucene.FSDirectory.class" , FSDirectory . class . getName ( ) ) ; IMPL = Class . forName ( name ) ; } catch ( ClassNotFoundException e ) { throw new RuntimeException ( "cannot load FSDirectory class: " + e . toString ( ) , e ) ; } catch ( SecurityException se ) { try { IMPL = Class . forName ( FSDirectory . class . getName ( ) ) ; } catch ( ClassNotFoundException e ) { throw new RuntimeException ( "cannot load default FSDirectory class: " + e . toString ( ) , e ) ; } } } private static MessageDigest DIGESTER ; static { try { DIGESTER = MessageDigest . getInstance ( "MD5" ) ; } catch ( NoSuchAlgorithmException e ) { throw new RuntimeException ( e . toString ( ) , e ) ; } } private byte [ ] buffer = null ; public static FSDirectory getDirectory ( String path ) throws IOException { return getDirectory ( new File ( path ) , null ) ; } public static FSDirectory getDirectory ( String path , LockFactory lockFactory ) throws IOException { return getDirectory ( new File ( path ) , lockFactory ) ; } public static FSDirectory getDirectory ( File file ) throws IOException { return getDirectory ( file , null ) ; } public static FSDirectory getDirectory ( File file , LockFactory lockFactory ) throws IOException { file = new File ( file . getCanonicalPath ( ) ) ; if ( file . exists ( ) && ! file . isDirectory ( ) ) throw new IOException ( file + " not a directory" ) ; if ( ! file . exists ( ) ) if ( ! file . mkdirs ( ) ) throw new IOException ( "Cannot create directory: " + file ) ; FSDirectory dir ; synchronized ( DIRECTORIES ) { dir = ( FSDirectory ) DIRECTORIES . get ( file ) ; if ( dir == null ) { try { dir = ( FSDirectory ) IMPL . newInstance ( ) ; } catch ( Exception e ) { throw new RuntimeException ( "cannot load FSDirectory class: " + e . toString ( ) , e ) ; } dir . init ( file , lockFactory ) ; DIRECTORIES . put ( file , dir ) ; } else { if ( lockFactory != null && lockFactory != dir . getLockFactory ( ) ) { throw new IOException ( "Directory was previously created with a different LockFactory instance; please pass null as the lockFactory instance and use setLockFactory to change it" ) ; } } } synchronized ( dir ) { dir . refCount ++ ; } return dir ; } public static FSDirectory getDirectory ( String path , boolean create ) throws IOException { return getDirectory ( new File ( path ) , create ) ; } public static FSDirectory getDirectory ( File file , boolean create ) throws IOException { FSDirectory dir = getDirectory ( file , null ) ; if ( create ) { dir . create ( ) ; } return dir ; } private void create ( ) throws IOException { if ( directory . exists ( ) ) { String [ ] files = directory . list ( IndexFileNameFilter . getFilter ( ) ) ; if ( files == null ) throw new IOException ( "cannot read directory " + directory . getAbsolutePath ( ) + ": list() returned null" ) ; for ( int i = 0 ; i < files . length ; i ++ ) { File file = new File ( directory , files [ i ] ) ; if ( ! file . delete ( ) ) throw new IOException ( "Cannot delete " + file ) ; } } lockFactory . clearLock ( IndexWriter . WRITE_LOCK_NAME ) ; } private File directory = null ; private int refCount ; protected FSDirectory ( ) { } ; private void init ( File path , LockFactory lockFactory ) throws IOException { directory = path ; boolean doClearLockID = false ; if ( lockFactory == null ) { if ( disableLocks ) { lockFactory = NoLockFactory . getNoLockFactory ( ) ; } else { String lockClassName = System . getProperty ( "org.apache.lucene.store.FSDirectoryLockFactoryClass" ) ; if ( lockClassName != null && ! lockClassName . equals ( "" ) ) { Class c ; try { c = Class . forName ( lockClassName ) ; } catch ( ClassNotFoundException e ) { throw new IOException ( "unable to find LockClass " + lockClassName ) ; } try { lockFactory = ( LockFactory ) c . newInstance ( ) ; } catch ( IllegalAccessException e ) { throw new IOException ( "IllegalAccessException when instantiating LockClass " + lockClassName ) ; } catch ( InstantiationException e ) { throw new IOException ( "InstantiationException when instantiating LockClass " + lockClassName ) ; } catch ( ClassCastException e ) { throw new IOException ( "unable to cast LockClass " + lockClassName + " instance to a LockFactory" ) ; } if ( lockFactory instanceof NativeFSLockFactory ) { ( ( NativeFSLockFactory ) lockFactory ) . setLockDir ( path ) ; } else if ( lockFactory instanceof SimpleFSLockFactory ) { ( ( SimpleFSLockFactory ) lockFactory ) . setLockDir ( path ) ; } } else { lockFactory = new SimpleFSLockFactory ( path ) ; doClearLockID = true ; } } } setLockFactory ( lockFactory ) ; if ( doClearLockID ) { lockFactory . setLockPrefix ( null ) ; } } public String [ ] list ( ) { return directory . list ( IndexFileNameFilter . getFilter ( ) ) ; } public boolean fileExists ( String name ) { File file = new File ( directory , name ) ; return file . exists ( ) ; } public long fileModified ( String name ) { File file = new File ( directory , name ) ; return file . lastModified ( ) ; } public static long fileModified ( File directory , String name ) { File file = new File ( directory , name ) ; return file . lastModified ( ) ; } public void touchFile ( String name ) { File file = new File ( directory , name ) ; file . setLastModified ( System . currentTimeMillis ( ) ) ; } public long fileLength ( String name ) { File file = new File ( directory , name ) ; return file . length ( ) ; } public void deleteFile ( String name ) throws IOException { File file = new File ( directory , name ) ; if ( ! file . delete ( ) ) throw new IOException ( "Cannot delete " + file ) ; } public synchronized void renameFile ( String from , String to ) throws IOException { File old = new File ( directory , from ) ; File nu = new File ( directory , to ) ; if ( nu . exists ( ) ) if ( ! nu . delete ( ) ) throw new IOException ( "Cannot delete " + nu ) ; if ( ! old . renameTo ( nu ) ) { java . io . InputStream in = null ; java . io . OutputStream out = null ; try { in = new FileInputStream ( old ) ; out = new FileOutputStream ( nu ) ; if ( buffer == null ) { buffer = new byte [ 1024 ] ; } int len ; while ( ( len = in . read ( buffer ) ) >= 0 ) { out . write ( buffer , 0 , len ) ; } old . delete ( ) ; } catch ( IOException ioe ) { IOException newExc = new IOException ( "Cannot rename " + old + " to " + nu ) ; newExc . initCause ( ioe ) ; throw newExc ; } finally { try { if ( in != null ) { try { in . close ( ) ; } catch ( IOException e ) { throw new RuntimeException ( "Cannot close input stream: " + e . toString ( ) , e ) ; } } } finally { if ( out != null ) { try { out . close ( ) ; } catch ( IOException e ) { throw new RuntimeException ( "Cannot close output stream: " + e . toString ( ) , e ) ; } } } } } } public IndexOutput createOutput ( String name ) throws IOException { File file = new File ( directory , name ) ; if ( file . exists ( ) && ! file . delete ( ) ) throw new IOException ( "Cannot overwrite: " + file ) ; return new FSIndexOutput ( file ) ; } public IndexInput openInput ( String name ) throws IOException { return new FSIndexInput ( new File ( directory , name ) ) ; } public IndexInput openInput ( String name , int bufferSize ) throws IOException { return new FSIndexInput ( new File ( directory , name ) , bufferSize ) ; } private static final char [ ] HEX_DIGITS = { '0' , '1' , '2' , '3' , '4' , '5' , '6' , '7' , '8' , '9' , 'a' , 'b' , 'c' , 'd' , 'e' , 'f' } ; public String getLockID ( ) { String dirName ; try { dirName = directory . getCanonicalPath ( ) ; } catch ( IOException e ) { throw new RuntimeException ( e . toString ( ) , e ) ; } byte digest [ ] ; synchronized ( DIGESTER ) { digest = DIGESTER . digest ( dirName . getBytes ( ) ) ; } StringBuffer buf = new StringBuffer ( ) ; buf . append ( "lucene-" ) ; for ( int i = 0 ; i < digest . length ; i ++ ) { int b = digest [ i ] ; buf . append ( HEX_DIGITS [ ( b > > 4 ) & 0xf ] ) ; buf . append ( HEX_DIGITS [ b & 0xf ] ) ; } return buf . toString ( ) ; } public synchronized void close ( ) { if ( -- refCount <= 0 ) { synchronized ( DIRECTORIES ) { DIRECTORIES . remove ( directory ) ; } } } public File getFile ( ) { return directory ; } public String toString ( ) { return this . getClass ( ) . getName ( ) + "@" + directory ; } protected static class FSIndexInput extends BufferedIndexInput { private static class Descriptor extends RandomAccessFile { private boolean isOpen ; long position ; final long length ; public Descriptor ( File file , String mode ) throws IOException { super ( file , mode ) ; isOpen = true ; length = length ( ) ; } public void close ( ) throws IOException { if ( isOpen ) { isOpen = false ; super . close ( ) ; } } protected void finalize ( ) throws Throwable { try { close ( ) ; } finally { super . finalize ( ) ; } } } private final Descriptor file ; boolean isClone ; public FSIndexInput ( File path ) throws IOException { this ( path , BufferedIndexInput . BUFFER_SIZE ) ; } public FSIndexInput ( File path , int bufferSize ) throws IOException { super ( bufferSize ) ; file = new Descriptor ( path , "r" ) ; } protected void readInternal ( byte [ ] b , int offset , int len ) throws IOException { synchronized ( file ) { long position = getFilePointer ( ) ; if ( position != file . position ) { file . seek ( position ) ; file . position = position ; } int total = 0 ; do { int i = file . read ( b , offset + total , len - total ) ; if ( i == - 1 ) throw new IOException ( "read past EOF" ) ; file . position += i ; total += i ; } while ( total < len ) ; } } public void close ( ) throws IOException { if ( ! isClone ) file . close ( ) ; } protected void seekInternal ( long position ) { } public long length ( ) { return file . length ; } public Object clone ( ) { FSIndexInput clone = ( FSIndexInput ) super . clone ( ) ; clone . isClone = true ; return clone ; } boolean isFDValid ( ) throws IOException { return file . getFD ( ) . valid ( ) ; } } protected static class FSIndexOutput extends BufferedIndexOutput { RandomAccessFile file = null ; private boolean isOpen ; public FSIndexOutput ( File path ) throws IOException { file = new RandomAccessFile ( path , "rw" ) ; isOpen = true ; } public void flushBuffer ( byte [ ] b , int offset , int size ) throws IOException { file . write ( b , offset , size ) ; } public void close ( ) throws IOException { if ( isOpen ) { super . close ( ) ; file . close ( ) ; isOpen = false ; } } public void seek ( long pos ) throws IOException { super . seek ( pos ) ; file . seek ( pos ) ; } public long length ( ) throws IOException { return file . length ( ) ; } } } 	1	['28', '2', '1', '16', '89', '86', '6', '10', '23', '0.844444444', '1021', '0.8', '0', '0.395348837', '0.265432099', '1', '7', '35.10714286', '2', '1', '11']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . * ; final class PhrasePositions { int doc ; int position ; int count ; int offset ; TermPositions tp ; PhrasePositions next ; boolean repeats ; PhrasePositions ( TermPositions t , int o ) { tp = t ; offset = o ; } final boolean next ( ) throws IOException { if ( ! tp . next ( ) ) { tp . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } doc = tp . doc ( ) ; position = 0 ; return true ; } final boolean skipTo ( int target ) throws IOException { if ( ! tp . skipTo ( target ) ) { tp . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } doc = tp . doc ( ) ; position = 0 ; return true ; } final void firstPosition ( ) throws IOException { count = tp . freq ( ) ; nextPosition ( ) ; } final boolean nextPosition ( ) throws IOException { if ( count -- > 0 ) { position = tp . nextPosition ( ) - offset ; return true ; } else return false ; } } 	0	['5', '1', '0', '5', '12', '0', '4', '1', '0', '0.678571429', '95', '0', '2', '0', '0.533333333', '0', '0', '16.6', '1', '0.8', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; import java . util . LinkedList ; import java . util . List ; public class CachingTokenFilter extends TokenFilter { private List cache ; private int index ; public CachingTokenFilter ( TokenStream input ) { super ( input ) ; } public Token next ( ) throws IOException { if ( cache == null ) { cache = new LinkedList ( ) ; fillCache ( ) ; } if ( index == cache . size ( ) ) { return null ; } return ( Token ) cache . get ( index ++ ) ; } public void reset ( ) throws IOException { index = 0 ; } private void fillCache ( ) throws IOException { Token token ; while ( ( token = input . next ( ) ) != null ) { cache . add ( token ) ; } } } 	1	['4', '3', '0', '3', '10', '2', '0', '3', '3', '0.666666667', '57', '1', '0', '0.571428571', '0.625', '0', '0', '12.75', '1', '0.75', '2']
package org . apache . lucene . document ; import java . io . Serializable ; public interface FieldSelector extends Serializable { FieldSelectorResult accept ( String fieldName ) ; } 	0	['1', '1', '0', '19', '1', '0', '18', '1', '1', '2', '1', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; public final class LengthFilter extends TokenFilter { final int min ; final int max ; public LengthFilter ( TokenStream in , int min , int max ) { super ( in ) ; this . min = min ; this . max = max ; } public final Token next ( ) throws IOException { for ( Token token = input . next ( ) ; token != null ; token = input . next ( ) ) { int len = token . termText ( ) . length ( ) ; if ( len >= min && len <= max ) { return token ; } } return null ; } } 	1	['2', '3', '0', '3', '6', '0', '0', '3', '2', '0', '41', '0', '0', '0.8', '0.666666667', '0', '0', '18.5', '1', '0.5', '1']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . IndexInput ; import java . util . LinkedList ; import java . util . HashSet ; import java . util . Iterator ; import java . io . IOException ; final class CompoundFileWriter { private static final class FileEntry { String file ; long directoryOffset ; long dataOffset ; } private Directory directory ; private String fileName ; private HashSet ids ; private LinkedList entries ; private boolean merged = false ; private SegmentMerger . CheckAbort checkAbort ; public CompoundFileWriter ( Directory dir , String name ) { this ( dir , name , null ) ; } CompoundFileWriter ( Directory dir , String name , SegmentMerger . CheckAbort checkAbort ) { if ( dir == null ) throw new NullPointerException ( "directory cannot be null" ) ; if ( name == null ) throw new NullPointerException ( "name cannot be null" ) ; this . checkAbort = checkAbort ; directory = dir ; fileName = name ; ids = new HashSet ( ) ; entries = new LinkedList ( ) ; } public Directory getDirectory ( ) { return directory ; } public String getName ( ) { return fileName ; } public void addFile ( String file ) { if ( merged ) throw new IllegalStateException ( "Can't add extensions after merge has been called" ) ; if ( file == null ) throw new NullPointerException ( "file cannot be null" ) ; if ( ! ids . add ( file ) ) throw new IllegalArgumentException ( "File " + file + " already added" ) ; FileEntry entry = new FileEntry ( ) ; entry . file = file ; entries . add ( entry ) ; } public void close ( ) throws IOException { if ( merged ) throw new IllegalStateException ( "Merge already performed" ) ; if ( entries . isEmpty ( ) ) throw new IllegalStateException ( "No entries to merge have been defined" ) ; merged = true ; IndexOutput os = null ; try { os = directory . createOutput ( fileName ) ; os . writeVInt ( entries . size ( ) ) ; Iterator it = entries . iterator ( ) ; long totalSize = 0 ; while ( it . hasNext ( ) ) { FileEntry fe = ( FileEntry ) it . next ( ) ; fe . directoryOffset = os . getFilePointer ( ) ; os . writeLong ( 0 ) ; os . writeString ( fe . file ) ; totalSize += directory . fileLength ( fe . file ) ; } final long finalLength = totalSize + os . getFilePointer ( ) ; os . setLength ( finalLength ) ; byte buffer [ ] = new byte [ 16384 ] ; it = entries . iterator ( ) ; while ( it . hasNext ( ) ) { FileEntry fe = ( FileEntry ) it . next ( ) ; fe . dataOffset = os . getFilePointer ( ) ; copyFile ( fe , os , buffer ) ; } it = entries . iterator ( ) ; while ( it . hasNext ( ) ) { FileEntry fe = ( FileEntry ) it . next ( ) ; os . seek ( fe . directoryOffset ) ; os . writeLong ( fe . dataOffset ) ; } assert finalLength == os . length ( ) ; IndexOutput tmp = os ; os = null ; tmp . close ( ) ; } finally { if ( os != null ) try { os . close ( ) ; } catch ( IOException e ) { } } } private void copyFile ( FileEntry source , IndexOutput os , byte buffer [ ] ) throws IOException { IndexInput is = null ; try { long startPtr = os . getFilePointer ( ) ; is = directory . openInput ( source . file ) ; long length = is . length ( ) ; long remainder = length ; int chunk = buffer . length ; while ( remainder > 0 ) { int len = ( int ) Math . min ( chunk , remainder ) ; is . readBytes ( buffer , 0 , len , false ) ; os . writeBytes ( buffer , len ) ; remainder -= len ; if ( checkAbort != null ) checkAbort . work ( 80 ) ; } if ( remainder != 0 ) throw new IOException ( "Non-zero remainder length after copying: " + remainder + " (id: " + source . file + ", length: " + length + ", buffer size: " + chunk + ")" ) ; long endPtr = os . getFilePointer ( ) ; long diff = endPtr - startPtr ; if ( diff != length ) throw new IOException ( "Difference in the output file offsets " + diff + " does not match the original file length " + length ) ; } finally { if ( is != null ) is . close ( ) ; } } } 	0	['9', '1', '0', '9', '51', '14', '3', '6', '5', '0.703125', '414', '0.75', '2', '0', '0.303571429', '0', '0', '44.11111111', '4', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . FieldSelector ; import org . apache . lucene . index . CorruptIndexException ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . store . Directory ; import java . io . IOException ; import java . util . BitSet ; public class IndexSearcher extends Searcher { IndexReader reader ; private boolean closeReader ; public IndexSearcher ( String path ) throws CorruptIndexException , IOException { this ( IndexReader . open ( path ) , true ) ; } public IndexSearcher ( Directory directory ) throws CorruptIndexException , IOException { this ( IndexReader . open ( directory ) , true ) ; } public IndexSearcher ( IndexReader r ) { this ( r , false ) ; } private IndexSearcher ( IndexReader r , boolean closeReader ) { reader = r ; this . closeReader = closeReader ; } public IndexReader getIndexReader ( ) { return reader ; } public void close ( ) throws IOException { if ( closeReader ) reader . close ( ) ; } public int docFreq ( Term term ) throws IOException { return reader . docFreq ( term ) ; } public Document doc ( int i ) throws CorruptIndexException , IOException { return reader . document ( i ) ; } public Document doc ( int i , FieldSelector fieldSelector ) throws CorruptIndexException , IOException { return reader . document ( i , fieldSelector ) ; } public int maxDoc ( ) throws IOException { return reader . maxDoc ( ) ; } public TopDocs search ( Weight weight , Filter filter , final int nDocs ) throws IOException { if ( nDocs <= 0 ) throw new IllegalArgumentException ( "nDocs must be > 0" ) ; TopDocCollector collector = new TopDocCollector ( nDocs ) ; search ( weight , filter , collector ) ; return collector . topDocs ( ) ; } public TopFieldDocs search ( Weight weight , Filter filter , final int nDocs , Sort sort ) throws IOException { TopFieldDocCollector collector = new TopFieldDocCollector ( reader , sort , nDocs ) ; search ( weight , filter , collector ) ; return ( TopFieldDocs ) collector . topDocs ( ) ; } public void search ( Weight weight , Filter filter , final HitCollector results ) throws IOException { HitCollector collector = results ; if ( filter != null ) { final BitSet bits = filter . bits ( reader ) ; collector = new HitCollector ( ) { public final void collect ( int doc , float score ) { if ( bits . get ( doc ) ) { results . collect ( doc , score ) ; } } } ; } Scorer scorer = weight . scorer ( reader ) ; if ( scorer == null ) return ; scorer . score ( collector ) ; } public Query rewrite ( Query original ) throws IOException { Query query = original ; for ( Query rewrittenQuery = query . rewrite ( reader ) ; rewrittenQuery != query ; rewrittenQuery = query . rewrite ( reader ) ) { query = rewrittenQuery ; } return query ; } public Explanation explain ( Weight weight , int doc ) throws IOException { return weight . explain ( reader , doc ) ; } } 	1	['15', '2', '0', '21', '34', '0', '3', '19', '14', '0.357142857', '164', '0.5', '1', '0.666666667', '0.18974359', '1', '3', '9.8', '1', '0.7333', '2']
package org . apache . lucene . index ; import org . apache . lucene . document . Fieldable ; final class DocFieldProcessorPerField { final DocFieldConsumerPerField consumer ; final FieldInfo fieldInfo ; DocFieldProcessorPerField next ; int lastGen = - 1 ; int fieldCount ; Fieldable [ ] fields = new Fieldable [ 1 ] ; public DocFieldProcessorPerField ( final DocFieldProcessorPerThread perThread , final FieldInfo fieldInfo ) { this . consumer = perThread . consumer . addField ( fieldInfo ) ; this . fieldInfo = fieldInfo ; } public void abort ( ) { consumer . abort ( ) ; } } 	0	['2', '1', '0', '5', '5', '0', '1', '5', '2', '1.166666667', '31', '0', '4', '0', '0.666666667', '0', '0', '11.5', '1', '0.5', '0']
package org . apache . lucene . queryParser ; public class ParseException extends Exception { public ParseException ( Token currentTokenVal , int [ ] [ ] expectedTokenSequencesVal , String [ ] tokenImageVal ) { super ( "" ) ; specialConstructor = true ; currentToken = currentTokenVal ; expectedTokenSequences = expectedTokenSequencesVal ; tokenImage = tokenImageVal ; } public ParseException ( ) { super ( ) ; specialConstructor = false ; } public ParseException ( String message ) { super ( message ) ; specialConstructor = false ; } protected boolean specialConstructor ; public Token currentToken ; public int [ ] [ ] expectedTokenSequences ; public String [ ] tokenImage ; public String getMessage ( ) { if ( ! specialConstructor ) { return super . getMessage ( ) ; } String expected = "" ; int maxSize = 0 ; for ( int i = 0 ; i < expectedTokenSequences . length ; i ++ ) { if ( maxSize < expectedTokenSequences [ i ] . length ) { maxSize = expectedTokenSequences [ i ] . length ; } for ( int j = 0 ; j < expectedTokenSequences [ i ] . length ; j ++ ) { expected += tokenImage [ expectedTokenSequences [ i ] [ j ] ] + " " ; } if ( expectedTokenSequences [ i ] [ expectedTokenSequences [ i ] . length - 1 ] != 0 ) { expected += "..." ; } expected += eol + "    " ; } String retval = "Encountered \"" ; Token tok = currentToken . next ; for ( int i = 0 ; i < maxSize ; i ++ ) { if ( i != 0 ) retval += " " ; if ( tok . kind == 0 ) { retval += tokenImage [ 0 ] ; break ; } retval += add_escapes ( tok . image ) ; tok = tok . next ; } retval += "\" at line " + currentToken . next . beginLine + ", column " + currentToken . next . beginColumn ; retval += "." + eol ; if ( expectedTokenSequences . length == 1 ) { retval += "Was expecting:" + eol + "    " ; } else { retval += "Was expecting one of:" + eol + "    " ; } retval += expected ; return retval ; } protected String eol = System . getProperty ( "line.separator" , "\n" ) ; protected String add_escapes ( String str ) { StringBuffer retval = new StringBuffer ( ) ; char ch ; for ( int i = 0 ; i < str . length ( ) ; i ++ ) { switch ( str . charAt ( i ) ) { case 0 : continue ; case '\b' : retval . append ( "\\b" ) ; continue ; case '\t' : retval . append ( "\\t" ) ; continue ; case '\n' : retval . append ( "\\n" ) ; continue ; case '\f' : retval . append ( "\\f" ) ; continue ; case '\r' : retval . append ( "\\r" ) ; continue ; case '\"' : retval . append ( "\\\"" ) ; continue ; case '\'' : retval . append ( "\\\'" ) ; continue ; case '\\' : retval . append ( "\\\\" ) ; continue ; default : if ( ( ch = str . charAt ( i ) ) < 0x20 || ch > 0x7e ) { String s = "0000" + Integer . toString ( ch , 16 ) ; retval . append ( "\\u" + s . substring ( s . length ( ) - 4 , s . length ( ) ) ) ; } else { retval . append ( ch ) ; } continue ; } } return retval . toString ( ) ; } } 	1	['5', '3', '0', '3', '18', '0', '2', '1', '4', '0.55', '387', '0.4', '1', '0.866666667', '0.4', '1', '1', '75.4', '14', '4.8', '3']
package org . apache . lucene . index ; import java . util . HashMap ; import java . util . Collection ; import java . util . Iterator ; import java . util . Map ; import java . util . HashSet ; import java . io . IOException ; import org . apache . lucene . util . ArrayUtil ; final class DocFieldConsumers extends DocFieldConsumer { final DocFieldConsumer one ; final DocFieldConsumer two ; public DocFieldConsumers ( DocFieldConsumer one , DocFieldConsumer two ) { this . one = one ; this . two = two ; } void setFieldInfos ( FieldInfos fieldInfos ) { super . setFieldInfos ( fieldInfos ) ; one . setFieldInfos ( fieldInfos ) ; two . setFieldInfos ( fieldInfos ) ; } public void flush ( Map threadsAndFields , DocumentsWriter . FlushState state ) throws IOException { Map oneThreadsAndFields = new HashMap ( ) ; Map twoThreadsAndFields = new HashMap ( ) ; Iterator it = threadsAndFields . entrySet ( ) . iterator ( ) ; while ( it . hasNext ( ) ) { Map . Entry entry = ( Map . Entry ) it . next ( ) ; DocFieldConsumersPerThread perThread = ( DocFieldConsumersPerThread ) entry . getKey ( ) ; Collection fields = ( Collection ) entry . getValue ( ) ; Iterator fieldsIt = fields . iterator ( ) ; Collection oneFields = new HashSet ( ) ; Collection twoFields = new HashSet ( ) ; while ( fieldsIt . hasNext ( ) ) { DocFieldConsumersPerField perField = ( DocFieldConsumersPerField ) fieldsIt . next ( ) ; oneFields . add ( perField . one ) ; twoFields . add ( perField . two ) ; } oneThreadsAndFields . put ( perThread . one , oneFields ) ; twoThreadsAndFields . put ( perThread . two , twoFields ) ; } one . flush ( oneThreadsAndFields , state ) ; two . flush ( twoThreadsAndFields , state ) ; } public void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException { try { one . closeDocStore ( state ) ; } finally { two . closeDocStore ( state ) ; } } public void abort ( ) { try { one . abort ( ) ; } finally { two . abort ( ) ; } } public boolean freeRAM ( ) { boolean any = one . freeRAM ( ) ; any |= two . freeRAM ( ) ; return any ; } public DocFieldConsumerPerThread addThread ( DocFieldProcessorPerThread docFieldProcessorPerThread ) throws IOException { return new DocFieldConsumersPerThread ( docFieldProcessorPerThread , this , one . addThread ( docFieldProcessorPerThread ) , two . addThread ( docFieldProcessorPerThread ) ) ; } PerDoc [ ] docFreeList = new PerDoc [ 1 ] ; int freeCount ; int allocCount ; synchronized PerDoc getPerDoc ( ) { if ( freeCount == 0 ) { allocCount ++ ; if ( allocCount > docFreeList . length ) { assert allocCount == 1 + docFreeList . length ; docFreeList = new PerDoc [ ArrayUtil . getNextSize ( allocCount ) ] ; } return new PerDoc ( ) ; } else return docFreeList [ -- freeCount ] ; } synchronized void freePerDoc ( PerDoc perDoc ) { assert freeCount < docFreeList . length ; docFreeList [ freeCount ++ ] = perDoc ; } class PerDoc extends DocumentsWriter . DocWriter { DocumentsWriter . DocWriter one ; DocumentsWriter . DocWriter two ; public long sizeInBytes ( ) { return one . sizeInBytes ( ) + two . sizeInBytes ( ) ; } public void finish ( ) throws IOException { try { try { one . finish ( ) ; } finally { two . finish ( ) ; } } finally { freePerDoc ( this ) ; } } public void abort ( ) { try { try { one . abort ( ) ; } finally { two . abort ( ) ; } } finally { freePerDoc ( this ) ; } } } } 	0	['11', '2', '0', '11', '37', '3', '3', '10', '6', '0.757142857', '281', '0', '3', '0.4', '0.2125', '0', '0', '23.90909091', '5', '1.5455', '0']
package org . apache . lucene . search ; import java . io . IOException ; public abstract class Scorer { private Similarity similarity ; protected Scorer ( Similarity similarity ) { this . similarity = similarity ; } public Similarity getSimilarity ( ) { return this . similarity ; } public void score ( HitCollector hc ) throws IOException { while ( next ( ) ) { hc . collect ( doc ( ) , score ( ) ) ; } } protected boolean score ( HitCollector hc , int max ) throws IOException { while ( doc ( ) < max ) { hc . collect ( doc ( ) , score ( ) ) ; if ( ! next ( ) ) return false ; } return true ; } public abstract boolean next ( ) throws IOException ; public abstract int doc ( ) ; public abstract float score ( ) throws IOException ; public abstract boolean skipTo ( int target ) throws IOException ; public abstract Explanation explain ( int doc ) throws IOException ; } 	1	['9', '1', '17', '38', '11', '34', '35', '3', '7', '0.875', '47', '1', '1', '0', '0.416666667', '0', '0', '4.111111111', '1', '0.8889', '1']
package org . apache . lucene . index ; abstract class InvertedDocEndConsumerPerThread { abstract void startDocument ( ) ; abstract InvertedDocEndConsumerPerField addField ( DocInverterPerField docInverterPerField , FieldInfo fieldInfo ) ; abstract void finishDocument ( ) ; abstract void abort ( ) ; } 	0	['5', '1', '1', '8', '6', '10', '6', '3', '0', '2', '8', '0', '0', '0', '0.466666667', '0', '0', '0.6', '1', '0.8', '0']
package org . apache . lucene . index ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . FieldSelector ; import org . apache . lucene . search . Similarity ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . FSDirectory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . Lock ; import org . apache . lucene . store . LockObtainFailedException ; import org . apache . lucene . store . AlreadyClosedException ; import java . io . File ; import java . io . FileOutputStream ; import java . io . IOException ; import java . util . Arrays ; import java . util . Collection ; public abstract class IndexReader { public static final class FieldOption { private String option ; private FieldOption ( ) { } private FieldOption ( String option ) { this . option = option ; } public String toString ( ) { return this . option ; } public static final FieldOption ALL = new FieldOption ( "ALL" ) ; public static final FieldOption INDEXED = new FieldOption ( "INDEXED" ) ; public static final FieldOption STORES_PAYLOADS = new FieldOption ( "STORES_PAYLOADS" ) ; public static final FieldOption UNINDEXED = new FieldOption ( "UNINDEXED" ) ; public static final FieldOption INDEXED_WITH_TERMVECTOR = new FieldOption ( "INDEXED_WITH_TERMVECTOR" ) ; public static final FieldOption INDEXED_NO_TERMVECTOR = new FieldOption ( "INDEXED_NO_TERMVECTOR" ) ; public static final FieldOption TERMVECTOR = new FieldOption ( "TERMVECTOR" ) ; public static final FieldOption TERMVECTOR_WITH_POSITION = new FieldOption ( "TERMVECTOR_WITH_POSITION" ) ; public static final FieldOption TERMVECTOR_WITH_OFFSET = new FieldOption ( "TERMVECTOR_WITH_OFFSET" ) ; public static final FieldOption TERMVECTOR_WITH_POSITION_OFFSET = new FieldOption ( "TERMVECTOR_WITH_POSITION_OFFSET" ) ; } protected IndexReader ( Directory directory ) { this . directory = directory ; } IndexReader ( Directory directory , SegmentInfos segmentInfos , boolean closeDirectory ) { init ( directory , segmentInfos , closeDirectory , true ) ; } void init ( Directory directory , SegmentInfos segmentInfos , boolean closeDirectory , boolean directoryOwner ) { this . directory = directory ; this . segmentInfos = segmentInfos ; this . directoryOwner = directoryOwner ; this . closeDirectory = closeDirectory ; } private Directory directory ; private boolean directoryOwner ; private boolean closeDirectory ; private IndexDeletionPolicy deletionPolicy ; private boolean closed ; protected final void ensureOpen ( ) throws AlreadyClosedException { if ( closed ) { throw new AlreadyClosedException ( "this IndexReader is closed" ) ; } } private SegmentInfos segmentInfos ; private Lock writeLock ; private boolean stale ; private boolean hasChanges ; private boolean rollbackHasChanges ; private SegmentInfos rollbackSegmentInfos ; public static IndexReader open ( String path ) throws CorruptIndexException , IOException { return open ( FSDirectory . getDirectory ( path ) , true , null ) ; } public static IndexReader open ( File path ) throws CorruptIndexException , IOException { return open ( FSDirectory . getDirectory ( path ) , true , null ) ; } public static IndexReader open ( final Directory directory ) throws CorruptIndexException , IOException { return open ( directory , false , null ) ; } public static IndexReader open ( final Directory directory , IndexDeletionPolicy deletionPolicy ) throws CorruptIndexException , IOException { return open ( directory , false , deletionPolicy ) ; } private static IndexReader open ( final Directory directory , final boolean closeDirectory , final IndexDeletionPolicy deletionPolicy ) throws CorruptIndexException , IOException { return ( IndexReader ) new SegmentInfos . FindSegmentsFile ( directory ) { protected Object doBody ( String segmentFileName ) throws CorruptIndexException , IOException { SegmentInfos infos = new SegmentInfos ( ) ; infos . read ( directory , segmentFileName ) ; IndexReader reader ; if ( infos . size ( ) == 1 ) { reader = SegmentReader . get ( infos , infos . info ( 0 ) , closeDirectory ) ; } else { IndexReader [ ] readers = new IndexReader [ infos . size ( ) ] ; for ( int i = infos . size ( ) - 1 ; i >= 0 ; i -- ) { try { readers [ i ] = SegmentReader . get ( infos . info ( i ) ) ; } catch ( IOException e ) { for ( i ++ ; i < infos . size ( ) ; i ++ ) { readers [ i ] . close ( ) ; } throw e ; } } reader = new MultiReader ( directory , infos , closeDirectory , readers ) ; } reader . deletionPolicy = deletionPolicy ; return reader ; } } . run ( ) ; } public Directory directory ( ) { ensureOpen ( ) ; return directory ; } public static long lastModified ( String directory ) throws CorruptIndexException , IOException { return lastModified ( new File ( directory ) ) ; } public static long lastModified ( File fileDirectory ) throws CorruptIndexException , IOException { return ( ( Long ) new SegmentInfos . FindSegmentsFile ( fileDirectory ) { public Object doBody ( String segmentFileName ) { return new Long ( FSDirectory . fileModified ( fileDirectory , segmentFileName ) ) ; } } . run ( ) ) . longValue ( ) ; } public static long lastModified ( final Directory directory2 ) throws CorruptIndexException , IOException { return ( ( Long ) new SegmentInfos . FindSegmentsFile ( directory2 ) { public Object doBody ( String segmentFileName ) throws IOException { return new Long ( directory2 . fileModified ( segmentFileName ) ) ; } } . run ( ) ) . longValue ( ) ; } public static long getCurrentVersion ( String directory ) throws CorruptIndexException , IOException { return getCurrentVersion ( new File ( directory ) ) ; } public static long getCurrentVersion ( File directory ) throws CorruptIndexException , IOException { Directory dir = FSDirectory . getDirectory ( directory ) ; long version = getCurrentVersion ( dir ) ; dir . close ( ) ; return version ; } public static long getCurrentVersion ( Directory directory ) throws CorruptIndexException , IOException { return SegmentInfos . readCurrentVersion ( directory ) ; } public long getVersion ( ) { ensureOpen ( ) ; return segmentInfos . getVersion ( ) ; } public boolean isCurrent ( ) throws CorruptIndexException , IOException { ensureOpen ( ) ; return SegmentInfos . readCurrentVersion ( directory ) == segmentInfos . getVersion ( ) ; } public boolean isOptimized ( ) { ensureOpen ( ) ; return segmentInfos . size ( ) == 1 && hasDeletions ( ) == false ; } abstract public TermFreqVector [ ] getTermFreqVectors ( int docNumber ) throws IOException ; abstract public TermFreqVector getTermFreqVector ( int docNumber , String field ) throws IOException ; public static boolean indexExists ( String directory ) { return indexExists ( new File ( directory ) ) ; } public static boolean indexExists ( File directory ) { return SegmentInfos . getCurrentSegmentGeneration ( directory . list ( ) ) != - 1 ; } public static boolean indexExists ( Directory directory ) throws IOException { return SegmentInfos . getCurrentSegmentGeneration ( directory ) != - 1 ; } public abstract int numDocs ( ) ; public abstract int maxDoc ( ) ; public Document document ( int n ) throws CorruptIndexException , IOException { ensureOpen ( ) ; return document ( n , null ) ; } public abstract Document document ( int n , FieldSelector fieldSelector ) throws CorruptIndexException , IOException ; public abstract boolean isDeleted ( int n ) ; public abstract boolean hasDeletions ( ) ; public boolean hasNorms ( String field ) throws IOException { ensureOpen ( ) ; return norms ( field ) != null ; } public abstract byte [ ] norms ( String field ) throws IOException ; public abstract void norms ( String field , byte [ ] bytes , int offset ) throws IOException ; public final synchronized void setNorm ( int doc , String field , byte value ) throws StaleReaderException , CorruptIndexException , LockObtainFailedException , IOException { ensureOpen ( ) ; if ( directoryOwner ) acquireWriteLock ( ) ; hasChanges = true ; doSetNorm ( doc , field , value ) ; } protected abstract void doSetNorm ( int doc , String field , byte value ) throws CorruptIndexException , IOException ; public void setNorm ( int doc , String field , float value ) throws StaleReaderException , CorruptIndexException , LockObtainFailedException , IOException { ensureOpen ( ) ; setNorm ( doc , field , Similarity . encodeNorm ( value ) ) ; } public abstract TermEnum terms ( ) throws IOException ; public abstract TermEnum terms ( Term t ) throws IOException ; public abstract int docFreq ( Term t ) throws IOException ; public TermDocs termDocs ( Term term ) throws IOException { ensureOpen ( ) ; TermDocs termDocs = termDocs ( ) ; termDocs . seek ( term ) ; return termDocs ; } public abstract TermDocs termDocs ( ) throws IOException ; public TermPositions termPositions ( Term term ) throws IOException { ensureOpen ( ) ; TermPositions termPositions = termPositions ( ) ; termPositions . seek ( term ) ; return termPositions ; } public abstract TermPositions termPositions ( ) throws IOException ; private void acquireWriteLock ( ) throws StaleReaderException , CorruptIndexException , LockObtainFailedException , IOException { ensureOpen ( ) ; if ( stale ) throw new StaleReaderException ( "IndexReader out of date and no longer valid for delete, undelete, or setNorm operations" ) ; if ( writeLock == null ) { Lock writeLock = directory . makeLock ( IndexWriter . WRITE_LOCK_NAME ) ; if ( ! writeLock . obtain ( IndexWriter . WRITE_LOCK_TIMEOUT ) ) throw new LockObtainFailedException ( "Index locked for write: " + writeLock ) ; this . writeLock = writeLock ; if ( SegmentInfos . readCurrentVersion ( directory ) > segmentInfos . getVersion ( ) ) { stale = true ; this . writeLock . release ( ) ; this . writeLock = null ; throw new StaleReaderException ( "IndexReader out of date and no longer valid for delete, undelete, or setNorm operations" ) ; } } } public final synchronized void deleteDocument ( int docNum ) throws StaleReaderException , CorruptIndexException , LockObtainFailedException , IOException { ensureOpen ( ) ; if ( directoryOwner ) acquireWriteLock ( ) ; hasChanges = true ; doDelete ( docNum ) ; } protected abstract void doDelete ( int docNum ) throws CorruptIndexException , IOException ; public final int deleteDocuments ( Term term ) throws StaleReaderException , CorruptIndexException , LockObtainFailedException , IOException { ensureOpen ( ) ; TermDocs docs = termDocs ( term ) ; if ( docs == null ) return 0 ; int n = 0 ; try { while ( docs . next ( ) ) { deleteDocument ( docs . doc ( ) ) ; n ++ ; } } finally { docs . close ( ) ; } return n ; } public final synchronized void undeleteAll ( ) throws StaleReaderException , CorruptIndexException , LockObtainFailedException , IOException { ensureOpen ( ) ; if ( directoryOwner ) acquireWriteLock ( ) ; hasChanges = true ; doUndeleteAll ( ) ; } protected abstract void doUndeleteAll ( ) throws CorruptIndexException , IOException ; void startCommit ( ) { if ( directoryOwner ) { rollbackSegmentInfos = ( SegmentInfos ) segmentInfos . clone ( ) ; } rollbackHasChanges = hasChanges ; } void rollbackCommit ( ) { if ( directoryOwner ) { for ( int i = 0 ; i < segmentInfos . size ( ) ; i ++ ) { segmentInfos . info ( i ) . reset ( rollbackSegmentInfos . info ( i ) ) ; } rollbackSegmentInfos = null ; } hasChanges = rollbackHasChanges ; } protected final synchronized void commit ( ) throws IOException { if ( hasChanges ) { if ( directoryOwner ) { IndexFileDeleter deleter = new IndexFileDeleter ( directory , deletionPolicy == null ? new KeepOnlyLastCommitDeletionPolicy ( ) : deletionPolicy , segmentInfos , null ) ; startCommit ( ) ; boolean success = false ; try { doCommit ( ) ; segmentInfos . write ( directory ) ; success = true ; } finally { if ( ! success ) { rollbackCommit ( ) ; deleter . refresh ( ) ; } } deleter . checkpoint ( segmentInfos , true ) ; if ( writeLock != null ) { writeLock . release ( ) ; writeLock = null ; } } else doCommit ( ) ; } hasChanges = false ; } protected abstract void doCommit ( ) throws IOException ; public final synchronized void close ( ) throws IOException { if ( ! closed ) { commit ( ) ; doClose ( ) ; if ( directoryOwner ) closed = true ; if ( closeDirectory ) directory . close ( ) ; } } protected abstract void doClose ( ) throws IOException ; protected void finalize ( ) throws Throwable { try { if ( writeLock != null ) { writeLock . release ( ) ; writeLock = null ; } } finally { super . finalize ( ) ; } } public abstract Collection getFieldNames ( FieldOption fldOption ) ; public static boolean isLocked ( Directory directory ) throws IOException { return directory . makeLock ( IndexWriter . WRITE_LOCK_NAME ) . isLocked ( ) ; } public static boolean isLocked ( String directory ) throws IOException { Directory dir = FSDirectory . getDirectory ( directory ) ; boolean result = isLocked ( dir ) ; dir . close ( ) ; return result ; } public static void unlock ( Directory directory ) throws IOException { directory . makeLock ( IndexWriter . WRITE_LOCK_NAME ) . release ( ) ; } public static void main ( String [ ] args ) { String filename = null ; boolean extract = false ; for ( int i = 0 ; i < args . length ; ++ i ) { if ( args [ i ] . equals ( "-extract" ) ) { extract = true ; } else if ( filename == null ) { filename = args [ i ] ; } } if ( filename == null ) { System . out . println ( "Usage: org.apache.lucene.index.IndexReader [-extract] <cfsfile>" ) ; return ; } Directory dir = null ; CompoundFileReader cfr = null ; try { File file = new File ( filename ) ; String dirname = file . getAbsoluteFile ( ) . getParent ( ) ; filename = file . getName ( ) ; dir = FSDirectory . getDirectory ( dirname ) ; cfr = new CompoundFileReader ( dir , filename ) ; String [ ] files = cfr . list ( ) ; Arrays . sort ( files ) ; for ( int i = 0 ; i < files . length ; ++ i ) { long len = cfr . fileLength ( files [ i ] ) ; if ( extract ) { System . out . println ( "extract " + files [ i ] + " with " + len + " bytes to local directory..." ) ; IndexInput ii = cfr . openInput ( files [ i ] ) ; FileOutputStream f = new FileOutputStream ( files [ i ] ) ; byte [ ] buffer = new byte [ 1024 ] ; int chunk = buffer . length ; while ( len > 0 ) { final int bufLen = ( int ) Math . min ( chunk , len ) ; ii . readBytes ( buffer , 0 , bufLen ) ; f . write ( buffer , 0 , bufLen ) ; len -= bufLen ; } f . close ( ) ; ii . close ( ) ; } else System . out . println ( files [ i ] + ": " + len + " bytes" ) ; } } catch ( IOException ioe ) { ioe . printStackTrace ( ) ; } finally { try { if ( dir != null ) dir . close ( ) ; if ( cfr != null ) cfr . close ( ) ; } catch ( IOException ioe ) { ioe . printStackTrace ( ) ; } } } } 	1	['62', '1', '4', '118', '125', '1757', '93', '27', '46', '0.877794337', '815', '1', '5', '0', '0.102822581', '0', '0', '11.96774194', '11', '1.2258', '11']
package org . apache . lucene . index ; import java . io . IOException ; public abstract class MergeScheduler { abstract void merge ( IndexWriter writer ) throws CorruptIndexException , IOException ; abstract void close ( ) throws CorruptIndexException , IOException ; } 	0	['3', '1', '2', '5', '4', '3', '4', '2', '1', '2', '6', '0', '0', '0', '0.666666667', '0', '0', '1', '1', '0.6667', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . BitSet ; import org . apache . lucene . index . IndexReader ; public class QueryWrapperFilter extends Filter { private Query query ; public QueryWrapperFilter ( Query query ) { this . query = query ; } public BitSet bits ( IndexReader reader ) throws IOException { final BitSet bits = new BitSet ( reader . maxDoc ( ) ) ; new IndexSearcher ( reader ) . search ( query , new HitCollector ( ) { public final void collect ( int doc , float score ) { bits . set ( doc ) ; } } ) ; return bits ; } public String toString ( ) { return "QueryWrapperFilter(" + query + ")" ; } public boolean equals ( Object o ) { if ( ! ( o instanceof QueryWrapperFilter ) ) return false ; return this . query . equals ( ( ( QueryWrapperFilter ) o ) . query ) ; } public int hashCode ( ) { return query . hashCode ( ) ^ 0x923F64B9 ; } } 	1	['5', '2', '0', '7', '17', '0', '2', '6', '5', '0', '62', '1', '1', '0.2', '0.4', '1', '1', '11.2', '2', '1', '1']
package org . apache . lucene . index ; import java . util . Map ; import java . util . HashMap ; import java . util . HashSet ; import java . util . Collection ; import java . util . Iterator ; import java . io . IOException ; final class DocInverter extends DocFieldConsumer { final InvertedDocConsumer consumer ; final InvertedDocEndConsumer endConsumer ; public DocInverter ( InvertedDocConsumer consumer , InvertedDocEndConsumer endConsumer ) { this . consumer = consumer ; this . endConsumer = endConsumer ; } void setFieldInfos ( FieldInfos fieldInfos ) { super . setFieldInfos ( fieldInfos ) ; consumer . setFieldInfos ( fieldInfos ) ; endConsumer . setFieldInfos ( fieldInfos ) ; } void flush ( Map threadsAndFields , DocumentsWriter . FlushState state ) throws IOException { Map childThreadsAndFields = new HashMap ( ) ; Map endChildThreadsAndFields = new HashMap ( ) ; Iterator it = threadsAndFields . entrySet ( ) . iterator ( ) ; while ( it . hasNext ( ) ) { Map . Entry entry = ( Map . Entry ) it . next ( ) ; DocInverterPerThread perThread = ( DocInverterPerThread ) entry . getKey ( ) ; Collection fields = ( Collection ) entry . getValue ( ) ; Iterator fieldsIt = fields . iterator ( ) ; Collection childFields = new HashSet ( ) ; Collection endChildFields = new HashSet ( ) ; while ( fieldsIt . hasNext ( ) ) { DocInverterPerField perField = ( DocInverterPerField ) fieldsIt . next ( ) ; childFields . add ( perField . consumer ) ; endChildFields . add ( perField . endConsumer ) ; } childThreadsAndFields . put ( perThread . consumer , childFields ) ; endChildThreadsAndFields . put ( perThread . endConsumer , endChildFields ) ; } consumer . flush ( childThreadsAndFields , state ) ; endConsumer . flush ( endChildThreadsAndFields , state ) ; } public void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException { consumer . closeDocStore ( state ) ; endConsumer . closeDocStore ( state ) ; } void abort ( ) { consumer . abort ( ) ; endConsumer . abort ( ) ; } public boolean freeRAM ( ) { return consumer . freeRAM ( ) ; } public DocFieldConsumerPerThread addThread ( DocFieldProcessorPerThread docFieldProcessorPerThread ) { return new DocInverterPerThread ( docFieldProcessorPerThread , this ) ; } final static class FieldInvertState { int position ; int length ; int offset ; float boost ; void reset ( float docBoost ) { position = 0 ; length = 0 ; offset = 0 ; boost = docBoost ; } } } 	0	['7', '2', '0', '14', '30', '0', '2', '13', '4', '0.25', '136', '0', '2', '0.5', '0.285714286', '0', '0', '18.14285714', '1', '0.8571', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . BufferedIndexInput ; import java . io . IOException ; class TermVectorsReader implements Cloneable { private FieldInfos fieldInfos ; private IndexInput tvx ; private IndexInput tvd ; private IndexInput tvf ; private int size ; private int tvdFormat ; private int tvfFormat ; TermVectorsReader ( Directory d , String segment , FieldInfos fieldInfos ) throws CorruptIndexException , IOException { this ( d , segment , fieldInfos , BufferedIndexInput . BUFFER_SIZE ) ; } TermVectorsReader ( Directory d , String segment , FieldInfos fieldInfos , int readBufferSize ) throws CorruptIndexException , IOException { if ( d . fileExists ( segment + TermVectorsWriter . TVX_EXTENSION ) ) { tvx = d . openInput ( segment + TermVectorsWriter . TVX_EXTENSION , readBufferSize ) ; checkValidFormat ( tvx ) ; tvd = d . openInput ( segment + TermVectorsWriter . TVD_EXTENSION , readBufferSize ) ; tvdFormat = checkValidFormat ( tvd ) ; tvf = d . openInput ( segment + TermVectorsWriter . TVF_EXTENSION , readBufferSize ) ; tvfFormat = checkValidFormat ( tvf ) ; size = ( int ) tvx . length ( ) / 8 ; } this . fieldInfos = fieldInfos ; } private int checkValidFormat ( IndexInput in ) throws CorruptIndexException , IOException { int format = in . readInt ( ) ; if ( format > TermVectorsWriter . FORMAT_VERSION ) { throw new CorruptIndexException ( "Incompatible format version: " + format + " expected " + TermVectorsWriter . FORMAT_VERSION + " or less" ) ; } return format ; } void close ( ) throws IOException { IOException keep = null ; if ( tvx != null ) try { tvx . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tvd != null ) try { tvd . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tvf != null ) try { tvf . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( keep != null ) throw ( IOException ) keep . fillInStackTrace ( ) ; } int size ( ) { return size ; } TermFreqVector get ( int docNum , String field ) throws IOException { int fieldNumber = fieldInfos . fieldNumber ( field ) ; TermFreqVector result = null ; if ( tvx != null ) { tvx . seek ( ( docNum * 8L ) + TermVectorsWriter . FORMAT_SIZE ) ; long position = tvx . readLong ( ) ; tvd . seek ( position ) ; int fieldCount = tvd . readVInt ( ) ; int number = 0 ; int found = - 1 ; for ( int i = 0 ; i < fieldCount ; i ++ ) { if ( tvdFormat == TermVectorsWriter . FORMAT_VERSION ) number = tvd . readVInt ( ) ; else number += tvd . readVInt ( ) ; if ( number == fieldNumber ) found = i ; } if ( found != - 1 ) { position = 0 ; for ( int i = 0 ; i <= found ; i ++ ) position += tvd . readVLong ( ) ; result = readTermVector ( field , position ) ; } else { } } else { } return result ; } TermFreqVector [ ] get ( int docNum ) throws IOException { TermFreqVector [ ] result = null ; if ( tvx != null ) { tvx . seek ( ( docNum * 8L ) + TermVectorsWriter . FORMAT_SIZE ) ; long position = tvx . readLong ( ) ; tvd . seek ( position ) ; int fieldCount = tvd . readVInt ( ) ; if ( fieldCount != 0 ) { int number = 0 ; String [ ] fields = new String [ fieldCount ] ; for ( int i = 0 ; i < fieldCount ; i ++ ) { if ( tvdFormat == TermVectorsWriter . FORMAT_VERSION ) number = tvd . readVInt ( ) ; else number += tvd . readVInt ( ) ; fields [ i ] = fieldInfos . fieldName ( number ) ; } position = 0 ; long [ ] tvfPointers = new long [ fieldCount ] ; for ( int i = 0 ; i < fieldCount ; i ++ ) { position += tvd . readVLong ( ) ; tvfPointers [ i ] = position ; } result = readTermVectors ( fields , tvfPointers ) ; } } else { } return result ; } private SegmentTermVector [ ] readTermVectors ( String fields [ ] , long tvfPointers [ ] ) throws IOException { SegmentTermVector res [ ] = new SegmentTermVector [ fields . length ] ; for ( int i = 0 ; i < fields . length ; i ++ ) { res [ i ] = readTermVector ( fields [ i ] , tvfPointers [ i ] ) ; } return res ; } private SegmentTermVector readTermVector ( String field , long tvfPointer ) throws IOException { tvf . seek ( tvfPointer ) ; int numTerms = tvf . readVInt ( ) ; if ( numTerms == 0 ) return new SegmentTermVector ( field , null , null ) ; boolean storePositions ; boolean storeOffsets ; if ( tvfFormat == TermVectorsWriter . FORMAT_VERSION ) { byte bits = tvf . readByte ( ) ; storePositions = ( bits & TermVectorsWriter . STORE_POSITIONS_WITH_TERMVECTOR ) != 0 ; storeOffsets = ( bits & TermVectorsWriter . STORE_OFFSET_WITH_TERMVECTOR ) != 0 ; } else { tvf . readVInt ( ) ; storePositions = false ; storeOffsets = false ; } String terms [ ] = new String [ numTerms ] ; int termFreqs [ ] = new int [ numTerms ] ; int positions [ ] [ ] = null ; TermVectorOffsetInfo offsets [ ] [ ] = null ; if ( storePositions ) positions = new int [ numTerms ] [ ] ; if ( storeOffsets ) offsets = new TermVectorOffsetInfo [ numTerms ] [ ] ; int start = 0 ; int deltaLength = 0 ; int totalLength = 0 ; char [ ] buffer = new char [ 10 ] ; char [ ] previousBuffer = { } ; for ( int i = 0 ; i < numTerms ; i ++ ) { start = tvf . readVInt ( ) ; deltaLength = tvf . readVInt ( ) ; totalLength = start + deltaLength ; if ( buffer . length < totalLength ) { buffer = null ; buffer = new char [ totalLength ] ; if ( start > 0 ) System . arraycopy ( previousBuffer , 0 , buffer , 0 , start ) ; } tvf . readChars ( buffer , start , deltaLength ) ; terms [ i ] = new String ( buffer , 0 , totalLength ) ; previousBuffer = buffer ; int freq = tvf . readVInt ( ) ; termFreqs [ i ] = freq ; if ( storePositions ) { int [ ] pos = new int [ freq ] ; positions [ i ] = pos ; int prevPosition = 0 ; for ( int j = 0 ; j < freq ; j ++ ) { pos [ j ] = prevPosition + tvf . readVInt ( ) ; prevPosition = pos [ j ] ; } } if ( storeOffsets ) { TermVectorOffsetInfo [ ] offs = new TermVectorOffsetInfo [ freq ] ; offsets [ i ] = offs ; int prevOffset = 0 ; for ( int j = 0 ; j < freq ; j ++ ) { int startOffset = prevOffset + tvf . readVInt ( ) ; int endOffset = startOffset + tvf . readVInt ( ) ; offs [ j ] = new TermVectorOffsetInfo ( startOffset , endOffset ) ; prevOffset = endOffset ; } } } SegmentTermVector tv ; if ( storePositions || storeOffsets ) { tv = new SegmentTermPositionVector ( field , terms , termFreqs , positions , offsets ) ; } else { tv = new SegmentTermVector ( field , terms , termFreqs ) ; } return tv ; } protected Object clone ( ) { if ( tvx == null || tvd == null || tvf == null ) return null ; TermVectorsReader clone = null ; try { clone = ( TermVectorsReader ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . tvx = ( IndexInput ) tvx . clone ( ) ; clone . tvd = ( IndexInput ) tvd . clone ( ) ; clone . tvf = ( IndexInput ) tvf . clone ( ) ; return clone ; } } 	1	['10', '1', '0', '9', '37', '17', '1', '8', '0', '0.555555556', '659', '1', '4', '0', '0.277777778', '0', '0', '64.2', '4', '1.1', '11']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . document . Fieldable ; final class DocFieldConsumersPerField extends DocFieldConsumerPerField { final DocFieldConsumerPerField one ; final DocFieldConsumerPerField two ; final DocFieldConsumersPerThread perThread ; public DocFieldConsumersPerField ( DocFieldConsumersPerThread perThread , DocFieldConsumerPerField one , DocFieldConsumerPerField two ) { this . perThread = perThread ; this . one = one ; this . two = two ; } public void processFields ( Fieldable [ ] fields , int count ) throws IOException { one . processFields ( fields , count ) ; two . processFields ( fields , count ) ; } public void abort ( ) { try { one . abort ( ) ; } finally { two . abort ( ) ; } } } 	0	['3', '2', '0', '4', '6', '0', '2', '3', '3', '0.333333333', '44', '0', '3', '0.5', '0.466666667', '0', '0', '12.66666667', '3', '1.3333', '0']
package org . apache . lucene . store ; import java . nio . channels . FileChannel ; import java . nio . channels . FileLock ; import java . io . File ; import java . io . RandomAccessFile ; import java . io . IOException ; import java . util . HashSet ; import java . util . Random ; public class NativeFSLockFactory extends LockFactory { private File lockDir ; private void acquireTestLock ( ) throws IOException { String randomLockName = "lucene-" + Long . toString ( new Random ( ) . nextInt ( ) , Character . MAX_RADIX ) + "-test.lock" ; Lock l = makeLock ( randomLockName ) ; try { l . obtain ( ) ; } catch ( IOException e ) { IOException e2 = new IOException ( "Failed to acquire random test lock; please verify filesystem for lock directory '" + lockDir + "' supports locking" ) ; e2 . initCause ( e ) ; throw e2 ; } l . release ( ) ; } NativeFSLockFactory ( ) throws IOException { this ( ( File ) null ) ; } public NativeFSLockFactory ( String lockDirName ) throws IOException { this ( new File ( lockDirName ) ) ; } public NativeFSLockFactory ( File lockDir ) throws IOException { setLockDir ( lockDir ) ; } void setLockDir ( File lockDir ) throws IOException { this . lockDir = lockDir ; if ( lockDir != null ) { if ( ! lockDir . exists ( ) ) { if ( ! lockDir . mkdirs ( ) ) throw new IOException ( "Cannot create directory: " + lockDir . getAbsolutePath ( ) ) ; } else if ( ! lockDir . isDirectory ( ) ) { throw new IOException ( "Found regular file where directory expected: " + lockDir . getAbsolutePath ( ) ) ; } acquireTestLock ( ) ; } } public synchronized Lock makeLock ( String lockName ) { if ( lockPrefix != null ) lockName = lockPrefix + "-n-" + lockName ; return new NativeFSLock ( lockDir , lockName ) ; } public void clearLock ( String lockName ) throws IOException { if ( lockDir . exists ( ) ) { if ( lockPrefix != null ) { lockName = lockPrefix + "-n-" + lockName ; } File lockFile = new File ( lockDir , lockName ) ; if ( lockFile . exists ( ) && ! lockFile . delete ( ) ) { throw new IOException ( "Cannot delete " + lockFile ) ; } } } } ; class NativeFSLock extends Lock { private RandomAccessFile f ; private FileChannel channel ; private FileLock lock ; private File path ; private File lockDir ; private static HashSet LOCK_HELD = new HashSet ( ) ; public NativeFSLock ( File lockDir , String lockFileName ) { this . lockDir = lockDir ; path = new File ( lockDir , lockFileName ) ; } public synchronized boolean obtain ( ) throws IOException { if ( isLocked ( ) ) { return false ; } if ( ! lockDir . exists ( ) ) { if ( ! lockDir . mkdirs ( ) ) throw new IOException ( "Cannot create directory: " + lockDir . getAbsolutePath ( ) ) ; } else if ( ! lockDir . isDirectory ( ) ) { throw new IOException ( "Found regular file where directory expected: " + lockDir . getAbsolutePath ( ) ) ; } String canonicalPath = path . getCanonicalPath ( ) ; boolean markedHeld = false ; try { synchronized ( LOCK_HELD ) { if ( LOCK_HELD . contains ( canonicalPath ) ) { return false ; } else { LOCK_HELD . add ( canonicalPath ) ; markedHeld = true ; } } try { f = new RandomAccessFile ( path , "rw" ) ; } catch ( IOException e ) { failureReason = e ; f = null ; } if ( f != null ) { try { channel = f . getChannel ( ) ; try { lock = channel . tryLock ( ) ; } catch ( IOException e ) { failureReason = e ; } finally { if ( lock == null ) { try { channel . close ( ) ; } finally { channel = null ; } } } } finally { if ( channel == null ) { try { f . close ( ) ; } finally { f = null ; } } } } } finally { if ( markedHeld && ! isLocked ( ) ) { synchronized ( LOCK_HELD ) { if ( LOCK_HELD . contains ( canonicalPath ) ) { LOCK_HELD . remove ( canonicalPath ) ; } } } } return isLocked ( ) ; } public synchronized void release ( ) { try { if ( isLocked ( ) ) { try { lock . release ( ) ; } finally { lock = null ; try { channel . close ( ) ; } finally { channel = null ; try { f . close ( ) ; } finally { f = null ; synchronized ( LOCK_HELD ) { LOCK_HELD . remove ( path . getCanonicalPath ( ) ) ; } } } } path . delete ( ) ; } } catch ( IOException e ) { throw new RuntimeException ( e ) ; } } public boolean isLocked ( ) { return lock != null ; } public String toString ( ) { return "NativeFSLock@" + path ; } public void finalize ( ) throws Throwable { try { if ( isLocked ( ) ) { release ( ) ; } } finally { super . finalize ( ) ; } } } 	1	['7', '2', '0', '4', '27', '9', '1', '3', '4', '0.333333333', '186', '1', '0', '0.5', '0.571428571', '0', '0', '25.42857143', '2', '0.7143', '2']
package org . apache . lucene . document ; public class LoadFirstFieldSelector implements FieldSelector { public FieldSelectorResult accept ( String fieldName ) { return FieldSelectorResult . LOAD_AND_BREAK ; } } 	0	['2', '1', '0', '2', '3', '1', '0', '2', '2', '2', '7', '0', '0', '0', '0.75', '0', '0', '2.5', '1', '0.5', '0']
package org . apache . lucene . analysis ; public class ISOLatin1AccentFilter extends TokenFilter { public ISOLatin1AccentFilter ( TokenStream input ) { super ( input ) ; } public final Token next ( ) throws java . io . IOException { final Token t = input . next ( ) ; if ( t != null ) t . setTermText ( removeAccents ( t . termText ( ) ) ) ; return t ; } public final static String removeAccents ( String input ) { final StringBuffer output = new StringBuffer ( ) ; for ( int i = 0 ; i < input . length ( ) ; i ++ ) { switch ( input . charAt ( i ) ) { case '' : case '' : case '' : case '' : case '' : case '' : output . append ( "A" ) ; break ; case '' : output . append ( "AE" ) ; break ; case '' : output . append ( "C" ) ; break ; case '' : case '' : case '' : case '' : output . append ( "E" ) ; break ; case '' : case '' : case '' : case '' : output . append ( "I" ) ; break ; case '' : output . append ( "D" ) ; break ; case '' : output . append ( "N" ) ; break ; case '' : case '' : case '' : case '' : case '' : case '' : output . append ( "O" ) ; break ; case '' : output . append ( "OE" ) ; break ; case '' : output . append ( "TH" ) ; break ; case '' : case '' : case '' : case '' : output . append ( "U" ) ; break ; case '' : case '' : output . append ( "Y" ) ; break ; case '' : case '' : case '' : case '' : case '' : case '' : output . append ( "a" ) ; break ; case '' : output . append ( "ae" ) ; break ; case '' : output . append ( "c" ) ; break ; case '' : case '' : case '' : case '' : output . append ( "e" ) ; break ; case '' : case '' : case '' : case '' : output . append ( "i" ) ; break ; case '' : output . append ( "d" ) ; break ; case '' : output . append ( "n" ) ; break ; case '' : case '' : case '' : case '' : case '' : case '' : output . append ( "o" ) ; break ; case '' : output . append ( "oe" ) ; break ; case '' : output . append ( "ss" ) ; break ; case '' : output . append ( "th" ) ; break ; case '' : case '' : case '' : case '' : output . append ( "u" ) ; break ; case '' : case '' : output . append ( "y" ) ; break ; default : output . append ( input . charAt ( i ) ) ; break ; } } return output . toString ( ) ; } } 	1	['3', '3', '0', '3', '13', '3', '0', '3', '3', '2', '170', '0', '0', '0.666666667', '0.444444444', '0', '0', '55.66666667', '3', '1.3333', '3']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class BufferedIndexInput extends IndexInput { public static final int BUFFER_SIZE = 1024 ; private int bufferSize = BUFFER_SIZE ; protected byte [ ] buffer ; private long bufferStart = 0 ; private int bufferLength = 0 ; private int bufferPosition = 0 ; public byte readByte ( ) throws IOException { if ( bufferPosition >= bufferLength ) refill ( ) ; return buffer [ bufferPosition ++ ] ; } public BufferedIndexInput ( ) { } public BufferedIndexInput ( int bufferSize ) { checkBufferSize ( bufferSize ) ; this . bufferSize = bufferSize ; } public void setBufferSize ( int newSize ) { assert buffer == null || bufferSize == buffer . length : "buffer=" + buffer + " bufferSize=" + bufferSize + " buffer.length=" + ( buffer != null ? buffer . length : 0 ) ; if ( newSize != bufferSize ) { checkBufferSize ( newSize ) ; bufferSize = newSize ; if ( buffer != null ) { byte [ ] newBuffer = new byte [ newSize ] ; final int leftInBuffer = bufferLength - bufferPosition ; final int numToCopy ; if ( leftInBuffer > newSize ) numToCopy = newSize ; else numToCopy = leftInBuffer ; System . arraycopy ( buffer , bufferPosition , newBuffer , 0 , numToCopy ) ; bufferStart += bufferPosition ; bufferPosition = 0 ; bufferLength = numToCopy ; newBuffer ( newBuffer ) ; } } } protected void newBuffer ( byte [ ] newBuffer ) { buffer = newBuffer ; } public int getBufferSize ( ) { return bufferSize ; } private void checkBufferSize ( int bufferSize ) { if ( bufferSize <= 0 ) throw new IllegalArgumentException ( "bufferSize must be greater than 0 (got " + bufferSize + ")" ) ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { readBytes ( b , offset , len , true ) ; } public void readBytes ( byte [ ] b , int offset , int len , boolean useBuffer ) throws IOException { if ( len <= ( bufferLength - bufferPosition ) ) { if ( len > 0 ) System . arraycopy ( buffer , bufferPosition , b , offset , len ) ; bufferPosition += len ; } else { int available = bufferLength - bufferPosition ; if ( available > 0 ) { System . arraycopy ( buffer , bufferPosition , b , offset , available ) ; offset += available ; len -= available ; bufferPosition += available ; } if ( useBuffer && len < bufferSize ) { refill ( ) ; if ( bufferLength < len ) { System . arraycopy ( buffer , 0 , b , offset , bufferLength ) ; throw new IOException ( "read past EOF" ) ; } else { System . arraycopy ( buffer , 0 , b , offset , len ) ; bufferPosition = len ; } } else { long after = bufferStart + bufferPosition + len ; if ( after > length ( ) ) throw new IOException ( "read past EOF" ) ; readInternal ( b , offset , len ) ; bufferStart = after ; bufferPosition = 0 ; bufferLength = 0 ; } } } private void refill ( ) throws IOException { long start = bufferStart + bufferPosition ; long end = start + bufferSize ; if ( end > length ( ) ) end = length ( ) ; int newLength = ( int ) ( end - start ) ; if ( newLength <= 0 ) throw new IOException ( "read past EOF" ) ; if ( buffer == null ) { newBuffer ( new byte [ bufferSize ] ) ; seekInternal ( bufferStart ) ; } readInternal ( buffer , 0 , newLength ) ; bufferLength = newLength ; bufferStart = start ; bufferPosition = 0 ; } protected abstract void readInternal ( byte [ ] b , int offset , int length ) throws IOException ; public long getFilePointer ( ) { return bufferStart + bufferPosition ; } public void seek ( long pos ) throws IOException { if ( pos >= bufferStart && pos < ( bufferStart + bufferLength ) ) bufferPosition = ( int ) ( pos - bufferStart ) ; else { bufferStart = pos ; bufferPosition = 0 ; bufferLength = 0 ; seekInternal ( pos ) ; } } protected abstract void seekInternal ( long pos ) throws IOException ; public Object clone ( ) { BufferedIndexInput clone = ( BufferedIndexInput ) super . clone ( ) ; clone . buffer = null ; clone . bufferLength = 0 ; clone . bufferPosition = 0 ; clone . bufferStart = getFilePointer ( ) ; return clone ; } } 	0	['17', '2', '2', '4', '33', '42', '3', '1', '10', '0.6953125', '478', '0.625', '0', '0.548387097', '0.302083333', '1', '5', '26.64705882', '8', '1.2941', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . BitSet ; import org . apache . lucene . index . IndexReader ; public class RemoteCachingWrapperFilter extends Filter { protected Filter filter ; public RemoteCachingWrapperFilter ( Filter filter ) { this . filter = filter ; } public BitSet bits ( IndexReader reader ) throws IOException { Filter cachedFilter = FilterManager . getInstance ( ) . getFilter ( filter ) ; return cachedFilter . bits ( reader ) ; } } 	1	['2', '2', '0', '3', '6', '0', '0', '3', '2', '0', '18', '1', '1', '0.5', '0.666666667', '0', '0', '7.5', '1', '0.5', '2']
package org . apache . lucene . index ; import java . io . Reader ; final class ReusableStringReader extends Reader { int upto ; int left ; String s ; void init ( String s ) { this . s = s ; left = s . length ( ) ; this . upto = 0 ; } public int read ( char [ ] c ) { return read ( c , 0 , c . length ) ; } public int read ( char [ ] c , int off , int len ) { if ( left > len ) { s . getChars ( upto , upto + len , c , off ) ; upto += len ; left -= len ; return len ; } else if ( 0 == left ) { return - 1 ; } else { s . getChars ( upto , upto + left , c , off ) ; int r = left ; left = 0 ; upto = s . length ( ) ; return r ; } } public void close ( ) { } ; } 	0	['5', '2', '0', '2', '8', '8', '2', '0', '3', '0.5', '90', '0', '0', '0.714285714', '0.45', '1', '2', '16.4', '3', '1.2', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermEnum ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; public class RangeQuery extends Query { private Term lowerTerm ; private Term upperTerm ; private boolean inclusive ; public RangeQuery ( Term lowerTerm , Term upperTerm , boolean inclusive ) { if ( lowerTerm == null && upperTerm == null ) { throw new IllegalArgumentException ( "At least one term must be non-null" ) ; } if ( lowerTerm != null && upperTerm != null && lowerTerm . field ( ) != upperTerm . field ( ) ) { throw new IllegalArgumentException ( "Both terms must be for the same field" ) ; } if ( lowerTerm != null ) { this . lowerTerm = lowerTerm ; } else { this . lowerTerm = new Term ( upperTerm . field ( ) , "" ) ; } this . upperTerm = upperTerm ; this . inclusive = inclusive ; } public Query rewrite ( IndexReader reader ) throws IOException { BooleanQuery query = new BooleanQuery ( true ) ; TermEnum enumerator = reader . terms ( lowerTerm ) ; try { boolean checkLower = false ; if ( ! inclusive ) checkLower = true ; String testField = getField ( ) ; do { Term term = enumerator . term ( ) ; if ( term != null && term . field ( ) == testField ) { if ( ! checkLower || term . text ( ) . compareTo ( lowerTerm . text ( ) ) > 0 ) { checkLower = false ; if ( upperTerm != null ) { int compare = upperTerm . text ( ) . compareTo ( term . text ( ) ) ; if ( ( compare < 0 ) || ( ! inclusive && compare == 0 ) ) break ; } TermQuery tq = new TermQuery ( term ) ; tq . setBoost ( getBoost ( ) ) ; query . add ( tq , BooleanClause . Occur . SHOULD ) ; } } else { break ; } } while ( enumerator . next ( ) ) ; } finally { enumerator . close ( ) ; } return query ; } public String getField ( ) { return ( lowerTerm != null ? lowerTerm . field ( ) : upperTerm . field ( ) ) ; } public Term getLowerTerm ( ) { return lowerTerm ; } public Term getUpperTerm ( ) { return upperTerm ; } public boolean isInclusive ( ) { return inclusive ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! getField ( ) . equals ( field ) ) { buffer . append ( getField ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( inclusive ? "[" : "{" ) ; buffer . append ( lowerTerm != null ? lowerTerm . text ( ) : "null" ) ; buffer . append ( " TO " ) ; buffer . append ( upperTerm != null ? upperTerm . text ( ) : "null" ) ; buffer . append ( inclusive ? "]" : "}" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof RangeQuery ) ) return false ; final RangeQuery other = ( RangeQuery ) o ; if ( this . getBoost ( ) != other . getBoost ( ) ) return false ; if ( this . inclusive != other . inclusive ) return false ; if ( this . lowerTerm != null ? ! this . lowerTerm . equals ( other . lowerTerm ) : other . lowerTerm != null ) return false ; if ( this . upperTerm != null ? ! this . upperTerm . equals ( other . upperTerm ) : other . upperTerm != null ) return false ; return true ; } public int hashCode ( ) { int h = Float . floatToIntBits ( getBoost ( ) ) ; h ^= lowerTerm != null ? lowerTerm . hashCode ( ) : 0 ; h ^= ( h << 25 ) | ( h > > > 8 ) ; h ^= upperTerm != null ? upperTerm . hashCode ( ) : 0 ; h ^= this . inclusive ? 0x2742E74A : 0 ; return h ; } } 	1	['9', '2', '0', '9', '32', '0', '1', '8', '9', '0.291666667', '340', '1', '2', '0.6', '0.259259259', '2', '3', '36.44444444', '11', '3', '2']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . document . Fieldable ; abstract class DocFieldConsumerPerField { abstract void processFields ( Fieldable [ ] fields , int count ) throws IOException ; abstract void abort ( ) ; } 	0	['3', '1', '3', '11', '4', '3', '10', '1', '0', '2', '6', '0', '0', '0', '0.555555556', '0', '0', '1', '1', '0.6667', '0']
package org . apache . lucene . analysis . standard ; import java . io . * ; public class StandardTokenizer extends org . apache . lucene . analysis . Tokenizer implements StandardTokenizerConstants { public StandardTokenizer ( Reader reader ) { this ( new FastCharStream ( reader ) ) ; this . input = reader ; } final public org . apache . lucene . analysis . Token next ( ) throws ParseException , IOException { Token token = null ; switch ( ( jj_ntk == - 1 ) ? jj_ntk ( ) : jj_ntk ) { case ALPHANUM : token = jj_consume_token ( ALPHANUM ) ; break ; case APOSTROPHE : token = jj_consume_token ( APOSTROPHE ) ; break ; case ACRONYM : token = jj_consume_token ( ACRONYM ) ; break ; case COMPANY : token = jj_consume_token ( COMPANY ) ; break ; case EMAIL : token = jj_consume_token ( EMAIL ) ; break ; case HOST : token = jj_consume_token ( HOST ) ; break ; case NUM : token = jj_consume_token ( NUM ) ; break ; case CJ : token = jj_consume_token ( CJ ) ; break ; case 0 : token = jj_consume_token ( 0 ) ; break ; default : jj_la1 [ 0 ] = jj_gen ; jj_consume_token ( - 1 ) ; throw new ParseException ( ) ; } if ( token . kind == EOF ) { { if ( true ) return null ; } } else { { if ( true ) return new org . apache . lucene . analysis . Token ( token . image , token . beginColumn , token . endColumn , tokenImage [ token . kind ] ) ; } } throw new Error ( "Missing return statement in function" ) ; } public StandardTokenizerTokenManager token_source ; public Token token , jj_nt ; private int jj_ntk ; private int jj_gen ; final private int [ ] jj_la1 = new int [ 1 ] ; static private int [ ] jj_la1_0 ; static { jj_la1_0 ( ) ; } private static void jj_la1_0 ( ) { jj_la1_0 = new int [ ] { 0x10ff , } ; } public StandardTokenizer ( CharStream stream ) { token_source = new StandardTokenizerTokenManager ( stream ) ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 1 ; i ++ ) jj_la1 [ i ] = - 1 ; } public void ReInit ( CharStream stream ) { token_source . ReInit ( stream ) ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 1 ; i ++ ) jj_la1 [ i ] = - 1 ; } public StandardTokenizer ( StandardTokenizerTokenManager tm ) { token_source = tm ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 1 ; i ++ ) jj_la1 [ i ] = - 1 ; } public void ReInit ( StandardTokenizerTokenManager tm ) { token_source = tm ; token = new Token ( ) ; jj_ntk = - 1 ; jj_gen = 0 ; for ( int i = 0 ; i < 1 ; i ++ ) jj_la1 [ i ] = - 1 ; } final private Token jj_consume_token ( int kind ) throws ParseException { Token oldToken ; if ( ( oldToken = token ) . next != null ) token = token . next ; else token = token . next = token_source . getNextToken ( ) ; jj_ntk = - 1 ; if ( token . kind == kind ) { jj_gen ++ ; return token ; } token = oldToken ; jj_kind = kind ; throw generateParseException ( ) ; } final public Token getNextToken ( ) { if ( token . next != null ) token = token . next ; else token = token . next = token_source . getNextToken ( ) ; jj_ntk = - 1 ; jj_gen ++ ; return token ; } final public Token getToken ( int index ) { Token t = token ; for ( int i = 0 ; i < index ; i ++ ) { if ( t . next != null ) t = t . next ; else t = t . next = token_source . getNextToken ( ) ; } return t ; } final private int jj_ntk ( ) { if ( ( jj_nt = token . next ) == null ) return ( jj_ntk = ( token . next = token_source . getNextToken ( ) ) . kind ) ; else return ( jj_ntk = jj_nt . kind ) ; } private java . util . Vector jj_expentries = new java . util . Vector ( ) ; private int [ ] jj_expentry ; private int jj_kind = - 1 ; public ParseException generateParseException ( ) { jj_expentries . removeAllElements ( ) ; boolean [ ] la1tokens = new boolean [ 16 ] ; for ( int i = 0 ; i < 16 ; i ++ ) { la1tokens [ i ] = false ; } if ( jj_kind >= 0 ) { la1tokens [ jj_kind ] = true ; jj_kind = - 1 ; } for ( int i = 0 ; i < 1 ; i ++ ) { if ( jj_la1 [ i ] == jj_gen ) { for ( int j = 0 ; j < 32 ; j ++ ) { if ( ( jj_la1_0 [ i ] & ( 1 << j ) ) != 0 ) { la1tokens [ j ] = true ; } } } } for ( int i = 0 ; i < 16 ; i ++ ) { if ( la1tokens [ i ] ) { jj_expentry = new int [ 1 ] ; jj_expentry [ 0 ] = i ; jj_expentries . addElement ( jj_expentry ) ; } } int [ ] [ ] exptokseq = new int [ jj_expentries . size ( ) ] [ ] ; for ( int i = 0 ; i < jj_expentries . size ( ) ; i ++ ) { exptokseq [ i ] = ( int [ ] ) jj_expentries . elementAt ( i ) ; } return new ParseException ( token , exptokseq , tokenImage ) ; } final public void enable_tracing ( ) { } final public void disable_tracing ( ) { } } 	1	['15', '3', '0', '9', '29', '15', '1', '8', '11', '0.6', '524', '0.7', '3', '0.266666667', '0.285714286', '1', '1', '33.26666667', '10', '1.7333', '6']
package org . apache . lucene . index ; class SegmentTermPositionVector extends SegmentTermVector implements TermPositionVector { protected int [ ] [ ] positions ; protected TermVectorOffsetInfo [ ] [ ] offsets ; public static final int [ ] EMPTY_TERM_POS = new int [ 0 ] ; public SegmentTermPositionVector ( String field , String terms [ ] , int termFreqs [ ] , int [ ] [ ] positions , TermVectorOffsetInfo [ ] [ ] offsets ) { super ( field , terms , termFreqs ) ; this . offsets = offsets ; this . positions = positions ; } public TermVectorOffsetInfo [ ] getOffsets ( int index ) { TermVectorOffsetInfo [ ] result = TermVectorOffsetInfo . EMPTY_OFFSET_INFO ; if ( offsets == null ) return null ; if ( index >= 0 && index < offsets . length ) { result = offsets [ index ] ; } return result ; } public int [ ] getTermPositions ( int index ) { int [ ] result = EMPTY_TERM_POS ; if ( positions == null ) return null ; if ( index >= 0 && index < positions . length ) { result = positions [ index ] ; } return result ; } } 	0	['4', '2', '0', '4', '5', '0', '1', '3', '3', '0.666666667', '65', '0.666666667', '1', '0.777777778', '0.476190476', '0', '0', '14.5', '4', '2', '0']
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . FieldCache ; import org . apache . lucene . search . function . DocValues ; import java . io . IOException ; public class ByteFieldSource extends FieldCacheSource { private FieldCache . ByteParser parser ; public ByteFieldSource ( String field ) { this ( field , null ) ; } public ByteFieldSource ( String field , FieldCache . ByteParser parser ) { super ( field ) ; this . parser = parser ; } public String description ( ) { return "byte(" + super . description ( ) + ')' ; } public DocValues getCachedFieldValues ( FieldCache cache , String field , IndexReader reader ) throws IOException { final byte [ ] arr = ( parser == null ) ? cache . getBytes ( reader , field ) : cache . getBytes ( reader , field , parser ) ; return new DocValues ( reader . maxDoc ( ) ) { public float floatVal ( int doc ) { return ( float ) arr [ doc ] ; } public int intVal ( int doc ) { return arr [ doc ] ; } public String toString ( int doc ) { return description ( ) + '=' + intVal ( doc ) ; } Object getInnerArray ( ) { return arr ; } } ; } public boolean cachedFieldSourceEquals ( FieldCacheSource o ) { if ( o . getClass ( ) != ByteFieldSource . class ) { return false ; } ByteFieldSource other = ( ByteFieldSource ) o ; return this . parser == null ? other . parser == null : this . parser . getClass ( ) == other . parser . getClass ( ) ; } public int cachedFieldSourceHashCode ( ) { return parser == null ? Byte . class . hashCode ( ) : parser . getClass ( ) . hashCode ( ) ; } } 	1	['7', '3', '0', '7', '22', '9', '2', '6', '6', '0.777777778', '122', '0.333333333', '1', '0.705882353', '0.333333333', '2', '3', '16', '6', '1.7143', '1']
package org . apache . lucene . index ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . IndexOutput ; import java . io . IOException ; final class ByteSliceReader extends IndexInput { ByteBlockPool pool ; int bufferUpto ; byte [ ] buffer ; public int upto ; int limit ; int level ; public int bufferOffset ; public int endIndex ; public void init ( ByteBlockPool pool , int startIndex , int endIndex ) { assert endIndex - startIndex >= 0 ; assert startIndex >= 0 ; assert endIndex >= 0 ; this . pool = pool ; this . endIndex = endIndex ; level = 0 ; bufferUpto = startIndex / DocumentsWriter . BYTE_BLOCK_SIZE ; bufferOffset = bufferUpto * DocumentsWriter . BYTE_BLOCK_SIZE ; buffer = pool . buffers [ bufferUpto ] ; upto = startIndex & DocumentsWriter . BYTE_BLOCK_MASK ; final int firstSize = ByteBlockPool . levelSizeArray [ 0 ] ; if ( startIndex + firstSize >= endIndex ) { limit = endIndex & DocumentsWriter . BYTE_BLOCK_MASK ; } else limit = upto + firstSize - 4 ; } public boolean eof ( ) { assert upto + bufferOffset <= endIndex ; return upto + bufferOffset == endIndex ; } public byte readByte ( ) { assert ! eof ( ) ; assert upto <= limit ; if ( upto == limit ) nextSlice ( ) ; return buffer [ upto ++ ] ; } public long writeTo ( IndexOutput out ) throws IOException { long size = 0 ; while ( true ) { if ( limit + bufferOffset == endIndex ) { assert endIndex - bufferOffset >= upto ; out . writeBytes ( buffer , upto , limit - upto ) ; size += limit - upto ; break ; } else { out . writeBytes ( buffer , upto , limit - upto ) ; size += limit - upto ; nextSlice ( ) ; } } return size ; } public void nextSlice ( ) { final int nextIndex = ( ( buffer [ limit ] & 0xff ) << 24 ) + ( ( buffer [ 1 + limit ] & 0xff ) << 16 ) + ( ( buffer [ 2 + limit ] & 0xff ) << 8 ) + ( buffer [ 3 + limit ] & 0xff ) ; level = ByteBlockPool . nextLevelArray [ level ] ; final int newSize = ByteBlockPool . levelSizeArray [ level ] ; bufferUpto = nextIndex / DocumentsWriter . BYTE_BLOCK_SIZE ; bufferOffset = bufferUpto * DocumentsWriter . BYTE_BLOCK_SIZE ; buffer = pool . buffers [ bufferUpto ] ; upto = nextIndex & DocumentsWriter . BYTE_BLOCK_MASK ; if ( nextIndex + newSize >= endIndex ) { assert endIndex - nextIndex > 0 ; limit = endIndex - bufferOffset ; } else { limit = upto + newSize - 4 ; } } public void readBytes ( byte [ ] b , int offset , int len ) { while ( len > 0 ) { final int numLeft = limit - upto ; if ( numLeft < len ) { System . arraycopy ( buffer , upto , b , offset , numLeft ) ; offset += numLeft ; len -= numLeft ; nextSlice ( ) ; } else { System . arraycopy ( buffer , upto , b , offset , len ) ; upto += len ; break ; } } } public long getFilePointer ( ) { throw new RuntimeException ( "not implemented" ) ; } public long length ( ) { throw new RuntimeException ( "not implemented" ) ; } public void seek ( long pos ) { throw new RuntimeException ( "not implemented" ) ; } public void close ( ) { throw new RuntimeException ( "not implemented" ) ; } } 	0	['13', '2', '0', '8', '22', '38', '5', '3', '10', '0.658333333', '447', '0', '1', '0.607142857', '0.214285714', '1', '4', '32.61538462', '8', '2.3846', '0']
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . FieldCache ; import java . io . IOException ; public class OrdFieldSource extends ValueSource { protected String field ; public OrdFieldSource ( String field ) { this . field = field ; } public String description ( ) { return "ord(" + field + ')' ; } public DocValues getValues ( IndexReader reader ) throws IOException { final int [ ] arr = FieldCache . DEFAULT . getStringIndex ( reader , field ) . order ; return new DocValues ( arr . length ) { public float floatVal ( int doc ) { return ( float ) arr [ doc ] ; } public String strVal ( int doc ) { return Integer . toString ( arr [ doc ] ) ; } public String toString ( int doc ) { return description ( ) + '=' + intVal ( doc ) ; } Object getInnerArray ( ) { return arr ; } } ; } public boolean equals ( Object o ) { if ( o . getClass ( ) != OrdFieldSource . class ) return false ; OrdFieldSource other = ( OrdFieldSource ) o ; return this . field . equals ( other . field ) ; } private static final int hcode = OrdFieldSource . class . hashCode ( ) ; public int hashCode ( ) { return hcode + field . hashCode ( ) ; } } 	1	['7', '2', '0', '6', '21', '0', '1', '6', '5', '0.666666667', '92', '0.666666667', '0', '0.5', '0.375', '2', '2', '11.71428571', '3', '1', '2']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . util . UnicodeUtil ; final class TermBuffer implements Cloneable { private String field ; private Term term ; private boolean preUTF8Strings ; private boolean dirty ; private UnicodeUtil . UTF16Result text = new UnicodeUtil . UTF16Result ( ) ; private UnicodeUtil . UTF8Result bytes = new UnicodeUtil . UTF8Result ( ) ; public final int compareTo ( TermBuffer other ) { if ( field == other . field ) return compareChars ( text . result , text . length , other . text . result , other . text . length ) ; else return field . compareTo ( other . field ) ; } private static final int compareChars ( char [ ] chars1 , int len1 , char [ ] chars2 , int len2 ) { final int end = len1 < len2 ? len1 : len2 ; for ( int k = 0 ; k < end ; k ++ ) { char c1 = chars1 [ k ] ; char c2 = chars2 [ k ] ; if ( c1 != c2 ) { return c1 - c2 ; } } return len1 - len2 ; } void setPreUTF8Strings ( ) { preUTF8Strings = true ; } public final void read ( IndexInput input , FieldInfos fieldInfos ) throws IOException { this . term = null ; int start = input . readVInt ( ) ; int length = input . readVInt ( ) ; int totalLength = start + length ; if ( preUTF8Strings ) { text . setLength ( totalLength ) ; input . readChars ( text . result , start , length ) ; } else { if ( dirty ) { UnicodeUtil . UTF16toUTF8 ( text . result , 0 , text . length , bytes ) ; bytes . setLength ( totalLength ) ; input . readBytes ( bytes . result , start , length ) ; UnicodeUtil . UTF8toUTF16 ( bytes . result , 0 , totalLength , text ) ; dirty = false ; } else { bytes . setLength ( totalLength ) ; input . readBytes ( bytes . result , start , length ) ; UnicodeUtil . UTF8toUTF16 ( bytes . result , start , length , text ) ; } } this . field = fieldInfos . fieldName ( input . readVInt ( ) ) ; } public final void set ( Term term ) { if ( term == null ) { reset ( ) ; return ; } final String termText = term . text ( ) ; final int termLen = termText . length ( ) ; text . setLength ( termLen ) ; termText . getChars ( 0 , termLen , text . result , 0 ) ; dirty = true ; field = term . field ( ) ; this . term = term ; } public final void set ( TermBuffer other ) { text . copyText ( other . text ) ; dirty = true ; field = other . field ; term = other . term ; } public void reset ( ) { field = null ; text . setLength ( 0 ) ; term = null ; dirty = true ; } public Term toTerm ( ) { if ( field == null ) return null ; if ( term == null ) term = new Term ( field , new String ( text . result , 0 , text . length ) , false ) ; return term ; } protected Object clone ( ) { TermBuffer clone = null ; try { clone = ( TermBuffer ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . dirty = true ; clone . bytes = new UnicodeUtil . UTF8Result ( ) ; clone . text = new UnicodeUtil . UTF16Result ( ) ; clone . text . copyText ( text ) ; return clone ; } } 	0	['10', '1', '0', '7', '30', '0', '1', '6', '6', '0.574074074', '303', '1', '3', '0', '0.228571429', '0', '0', '28.7', '4', '1.6', '0']
package org . apache . lucene . document ; import java . util . HashMap ; import java . util . List ; import java . util . Map ; public class MapFieldSelector implements FieldSelector { Map fieldSelections ; public MapFieldSelector ( Map fieldSelections ) { this . fieldSelections = fieldSelections ; } public MapFieldSelector ( List fields ) { fieldSelections = new HashMap ( fields . size ( ) * 5 / 3 ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) fieldSelections . put ( fields . get ( i ) , FieldSelectorResult . LOAD ) ; } public MapFieldSelector ( String [ ] fields ) { fieldSelections = new HashMap ( fields . length * 5 / 3 ) ; for ( int i = 0 ; i < fields . length ; i ++ ) fieldSelections . put ( fields [ i ] , FieldSelectorResult . LOAD ) ; } public FieldSelectorResult accept ( String field ) { FieldSelectorResult selection = ( FieldSelectorResult ) fieldSelections . get ( field ) ; return selection != null ? selection : FieldSelectorResult . NO_LOAD ; } } 	1	['4', '1', '0', '2', '10', '0', '0', '2', '4', '0', '83', '0', '0', '0', '0.4', '0', '0', '19.5', '2', '0.5', '1']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Map ; abstract class DocFieldConsumer { FieldInfos fieldInfos ; abstract void flush ( Map threadsAndFields , DocumentsWriter . FlushState state ) throws IOException ; abstract void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException ; abstract void abort ( ) ; abstract DocFieldConsumerPerThread addThread ( DocFieldProcessorPerThread docFieldProcessorPerThread ) throws IOException ; abstract boolean freeRAM ( ) ; void setFieldInfos ( FieldInfos fieldInfos ) { this . fieldInfos = fieldInfos ; } } 	0	['7', '1', '3', '9', '8', '21', '6', '4', '0', '1', '15', '0', '1', '0', '0.342857143', '0', '0', '1', '1', '0.8571', '0']
package org . apache . lucene . index ; import java . io . File ; import java . io . FilenameFilter ; import java . util . HashSet ; public class IndexFileNameFilter implements FilenameFilter { static IndexFileNameFilter singleton = new IndexFileNameFilter ( ) ; private HashSet extensions ; private HashSet extensionsInCFS ; public IndexFileNameFilter ( ) { extensions = new HashSet ( ) ; for ( int i = 0 ; i < IndexFileNames . INDEX_EXTENSIONS . length ; i ++ ) { extensions . add ( IndexFileNames . INDEX_EXTENSIONS [ i ] ) ; } extensionsInCFS = new HashSet ( ) ; for ( int i = 0 ; i < IndexFileNames . INDEX_EXTENSIONS_IN_COMPOUND_FILE . length ; i ++ ) { extensionsInCFS . add ( IndexFileNames . INDEX_EXTENSIONS_IN_COMPOUND_FILE [ i ] ) ; } } public boolean accept ( File dir , String name ) { int i = name . lastIndexOf ( '.' ) ; if ( i != - 1 ) { String extension = name . substring ( 1 + i ) ; if ( extensions . contains ( extension ) ) { return true ; } else if ( extension . startsWith ( "f" ) && extension . matches ( "f\\d+" ) ) { return true ; } else if ( extension . startsWith ( "s" ) && extension . matches ( "s\\d+" ) ) { return true ; } } else { if ( name . equals ( IndexFileNames . DELETABLE ) ) return true ; else if ( name . startsWith ( IndexFileNames . SEGMENTS ) ) return true ; } return false ; } public boolean isCFSFile ( String name ) { int i = name . lastIndexOf ( '.' ) ; if ( i != - 1 ) { String extension = name . substring ( 1 + i ) ; if ( extensionsInCFS . contains ( extension ) ) { return true ; } if ( extension . startsWith ( "f" ) && extension . matches ( "f\\d+" ) ) { return true ; } } return false ; } public static IndexFileNameFilter getFilter ( ) { return singleton ; } } 	1	['5', '1', '0', '3', '14', '4', '2', '1', '4', '0.583333333', '145', '0.666666667', '1', '0', '0.5', '0', '0', '27.4', '7', '2.6', '1']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class BufferedIndexOutput extends IndexOutput { static final int BUFFER_SIZE = 16384 ; private final byte [ ] buffer = new byte [ BUFFER_SIZE ] ; private long bufferStart = 0 ; private int bufferPosition = 0 ; public void writeByte ( byte b ) throws IOException { if ( bufferPosition >= BUFFER_SIZE ) flush ( ) ; buffer [ bufferPosition ++ ] = b ; } public void writeBytes ( byte [ ] b , int offset , int length ) throws IOException { int bytesLeft = BUFFER_SIZE - bufferPosition ; if ( bytesLeft >= length ) { System . arraycopy ( b , offset , buffer , bufferPosition , length ) ; bufferPosition += length ; if ( BUFFER_SIZE - bufferPosition == 0 ) flush ( ) ; } else { if ( length > BUFFER_SIZE ) { if ( bufferPosition > 0 ) flush ( ) ; flushBuffer ( b , offset , length ) ; bufferStart += length ; } else { int pos = 0 ; int pieceLength ; while ( pos < length ) { pieceLength = ( length - pos < bytesLeft ) ? length - pos : bytesLeft ; System . arraycopy ( b , pos + offset , buffer , bufferPosition , pieceLength ) ; pos += pieceLength ; bufferPosition += pieceLength ; bytesLeft = BUFFER_SIZE - bufferPosition ; if ( bytesLeft == 0 ) { flush ( ) ; bytesLeft = BUFFER_SIZE ; } } } } } public void flush ( ) throws IOException { flushBuffer ( buffer , bufferPosition ) ; bufferStart += bufferPosition ; bufferPosition = 0 ; } private void flushBuffer ( byte [ ] b , int len ) throws IOException { flushBuffer ( b , 0 , len ) ; } protected abstract void flushBuffer ( byte [ ] b , int offset , int len ) throws IOException ; public void close ( ) throws IOException { flush ( ) ; } public long getFilePointer ( ) { return bufferStart + bufferPosition ; } public void seek ( long pos ) throws IOException { flush ( ) ; bufferStart = pos ; } public abstract long length ( ) throws IOException ; } 	0	['10', '2', '1', '2', '12', '17', '1', '1', '8', '0.555555556', '185', '0.75', '0', '0.653846154', '0.36', '1', '5', '17.1', '1', '0.9', '0']
package org . apache . lucene . document ; public abstract class AbstractField implements Fieldable { protected String name = "body" ; protected boolean storeTermVector = false ; protected boolean storeOffsetWithTermVector = false ; protected boolean storePositionWithTermVector = false ; protected boolean omitNorms = false ; protected boolean isStored = false ; protected boolean isIndexed = true ; protected boolean isTokenized = true ; protected boolean isBinary = false ; protected boolean isCompressed = false ; protected boolean lazy = false ; protected float boost = 1.0f ; protected Object fieldsData = null ; protected AbstractField ( ) { } protected AbstractField ( String name , Field . Store store , Field . Index index , Field . TermVector termVector ) { if ( name == null ) throw new NullPointerException ( "name cannot be null" ) ; this . name = name . intern ( ) ; if ( store == Field . Store . YES ) { this . isStored = true ; this . isCompressed = false ; } else if ( store == Field . Store . COMPRESS ) { this . isStored = true ; this . isCompressed = true ; } else if ( store == Field . Store . NO ) { this . isStored = false ; this . isCompressed = false ; } else throw new IllegalArgumentException ( "unknown store parameter " + store ) ; if ( index == Field . Index . NO ) { this . isIndexed = false ; this . isTokenized = false ; } else if ( index == Field . Index . TOKENIZED ) { this . isIndexed = true ; this . isTokenized = true ; } else if ( index == Field . Index . UN_TOKENIZED ) { this . isIndexed = true ; this . isTokenized = false ; } else if ( index == Field . Index . NO_NORMS ) { this . isIndexed = true ; this . isTokenized = false ; this . omitNorms = true ; } else { throw new IllegalArgumentException ( "unknown index parameter " + index ) ; } this . isBinary = false ; setStoreTermVector ( termVector ) ; } public void setBoost ( float boost ) { this . boost = boost ; } public float getBoost ( ) { return boost ; } public String name ( ) { return name ; } protected void setStoreTermVector ( Field . TermVector termVector ) { if ( termVector == Field . TermVector . NO ) { this . storeTermVector = false ; this . storePositionWithTermVector = false ; this . storeOffsetWithTermVector = false ; } else if ( termVector == Field . TermVector . YES ) { this . storeTermVector = true ; this . storePositionWithTermVector = false ; this . storeOffsetWithTermVector = false ; } else if ( termVector == Field . TermVector . WITH_POSITIONS ) { this . storeTermVector = true ; this . storePositionWithTermVector = true ; this . storeOffsetWithTermVector = false ; } else if ( termVector == Field . TermVector . WITH_OFFSETS ) { this . storeTermVector = true ; this . storePositionWithTermVector = false ; this . storeOffsetWithTermVector = true ; } else if ( termVector == Field . TermVector . WITH_POSITIONS_OFFSETS ) { this . storeTermVector = true ; this . storePositionWithTermVector = true ; this . storeOffsetWithTermVector = true ; } else { throw new IllegalArgumentException ( "unknown termVector parameter " + termVector ) ; } } public final boolean isStored ( ) { return isStored ; } public final boolean isIndexed ( ) { return isIndexed ; } public final boolean isTokenized ( ) { return isTokenized ; } public final boolean isCompressed ( ) { return isCompressed ; } public final boolean isTermVectorStored ( ) { return storeTermVector ; } public boolean isStoreOffsetWithTermVector ( ) { return storeOffsetWithTermVector ; } public boolean isStorePositionWithTermVector ( ) { return storePositionWithTermVector ; } public final boolean isBinary ( ) { return isBinary ; } public boolean getOmitNorms ( ) { return omitNorms ; } public void setOmitNorms ( boolean omitNorms ) { this . omitNorms = omitNorms ; } public boolean isLazy ( ) { return lazy ; } public final String toString ( ) { StringBuffer result = new StringBuffer ( ) ; if ( isStored ) { result . append ( "stored" ) ; if ( isCompressed ) result . append ( "/compressed" ) ; else result . append ( "/uncompressed" ) ; } if ( isIndexed ) { if ( result . length ( ) > 0 ) result . append ( "," ) ; result . append ( "indexed" ) ; } if ( isTokenized ) { if ( result . length ( ) > 0 ) result . append ( "," ) ; result . append ( "tokenized" ) ; } if ( storeTermVector ) { if ( result . length ( ) > 0 ) result . append ( "," ) ; result . append ( "termVector" ) ; } if ( storeOffsetWithTermVector ) { if ( result . length ( ) > 0 ) result . append ( "," ) ; result . append ( "termVectorOffsets" ) ; } if ( storePositionWithTermVector ) { if ( result . length ( ) > 0 ) result . append ( "," ) ; result . append ( "termVectorPosition" ) ; } if ( isBinary ) { if ( result . length ( ) > 0 ) result . append ( "," ) ; result . append ( "binary" ) ; } if ( omitNorms ) { result . append ( ",omitNorms" ) ; } if ( lazy ) { result . append ( ",lazy" ) ; } result . append ( '<' ) ; result . append ( name ) ; result . append ( ':' ) ; if ( fieldsData != null && lazy == false ) { result . append ( fieldsData ) ; } result . append ( '>' ) ; return result . toString ( ) ; } } 	1	['18', '1', '3', '7', '28', '51', '3', '4', '15', '0.809954751', '503', '1', '0', '0', '0.198412698', '0', '0', '26.22222222', '19', '2.1667', '2']
package org . apache . lucene . index ; final class IndexFileNames { static final String SEGMENTS = "segments" ; static final String SEGMENTS_GEN = "segments.gen" ; static final String DELETABLE = "deletable" ; static final String NORMS_EXTENSION = "nrm" ; static final String FREQ_EXTENSION = "frq" ; static final String PROX_EXTENSION = "prx" ; static final String TERMS_EXTENSION = "tis" ; static final String TERMS_INDEX_EXTENSION = "tii" ; static final String FIELDS_INDEX_EXTENSION = "fdx" ; static final String FIELDS_EXTENSION = "fdt" ; static final String VECTORS_FIELDS_EXTENSION = "tvf" ; static final String VECTORS_DOCUMENTS_EXTENSION = "tvd" ; static final String VECTORS_INDEX_EXTENSION = "tvx" ; static final String COMPOUND_FILE_EXTENSION = "cfs" ; static final String COMPOUND_FILE_STORE_EXTENSION = "cfx" ; static final String DELETES_EXTENSION = "del" ; static final String FIELD_INFOS_EXTENSION = "fnm" ; static final String PLAIN_NORMS_EXTENSION = "f" ; static final String SEPARATE_NORMS_EXTENSION = "s" ; static final String GEN_EXTENSION = "gen" ; static final String INDEX_EXTENSIONS [ ] = new String [ ] { COMPOUND_FILE_EXTENSION , FIELD_INFOS_EXTENSION , FIELDS_INDEX_EXTENSION , FIELDS_EXTENSION , TERMS_INDEX_EXTENSION , TERMS_EXTENSION , FREQ_EXTENSION , PROX_EXTENSION , DELETES_EXTENSION , VECTORS_INDEX_EXTENSION , VECTORS_DOCUMENTS_EXTENSION , VECTORS_FIELDS_EXTENSION , GEN_EXTENSION , NORMS_EXTENSION , COMPOUND_FILE_STORE_EXTENSION , } ; static final String [ ] INDEX_EXTENSIONS_IN_COMPOUND_FILE = new String [ ] { FIELD_INFOS_EXTENSION , FIELDS_INDEX_EXTENSION , FIELDS_EXTENSION , TERMS_INDEX_EXTENSION , TERMS_EXTENSION , FREQ_EXTENSION , PROX_EXTENSION , VECTORS_INDEX_EXTENSION , VECTORS_DOCUMENTS_EXTENSION , VECTORS_FIELDS_EXTENSION , NORMS_EXTENSION } ; static final String [ ] STORE_INDEX_EXTENSIONS = new String [ ] { VECTORS_INDEX_EXTENSION , VECTORS_FIELDS_EXTENSION , VECTORS_DOCUMENTS_EXTENSION , FIELDS_INDEX_EXTENSION , FIELDS_EXTENSION } ; static final String [ ] NON_STORE_INDEX_EXTENSIONS = new String [ ] { FIELD_INFOS_EXTENSION , FREQ_EXTENSION , PROX_EXTENSION , TERMS_EXTENSION , TERMS_INDEX_EXTENSION , NORMS_EXTENSION } ; static final String COMPOUND_EXTENSIONS [ ] = new String [ ] { FIELD_INFOS_EXTENSION , FREQ_EXTENSION , PROX_EXTENSION , FIELDS_INDEX_EXTENSION , FIELDS_EXTENSION , TERMS_INDEX_EXTENSION , TERMS_EXTENSION } ; static final String VECTOR_EXTENSIONS [ ] = new String [ ] { VECTORS_INDEX_EXTENSION , VECTORS_DOCUMENTS_EXTENSION , VECTORS_FIELDS_EXTENSION } ; static final String fileNameFromGeneration ( String base , String extension , long gen ) { if ( gen == SegmentInfo . NO ) { return null ; } else if ( gen == SegmentInfo . WITHOUT_GEN ) { return base + extension ; } else { return base + "_" + Long . toString ( gen , Character . MAX_RADIX ) + extension ; } } static final boolean isDocStoreFile ( String fileName ) { if ( fileName . endsWith ( COMPOUND_FILE_STORE_EXTENSION ) ) return true ; for ( int i = 0 ; i < STORE_INDEX_EXTENSIONS . length ; i ++ ) if ( fileName . endsWith ( STORE_INDEX_EXTENSIONS [ i ] ) ) return true ; return false ; } } 	0	['4', '1', '0', '5', '10', '4', '5', '0', '0', '1.243589744', '298', '0', '0', '0', '0.444444444', '0', '0', '67', '4', '1.75', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; import java . util . ArrayList ; import java . util . Iterator ; import java . util . Collection ; import java . util . Set ; public class DisjunctionMaxQuery extends Query { private ArrayList disjuncts = new ArrayList ( ) ; private float tieBreakerMultiplier = 0.0f ; public DisjunctionMaxQuery ( float tieBreakerMultiplier ) { this . tieBreakerMultiplier = tieBreakerMultiplier ; } public DisjunctionMaxQuery ( Collection disjuncts , float tieBreakerMultiplier ) { this . tieBreakerMultiplier = tieBreakerMultiplier ; add ( disjuncts ) ; } public void add ( Query query ) { disjuncts . add ( query ) ; } public void add ( Collection disjuncts ) { this . disjuncts . addAll ( disjuncts ) ; } public Iterator iterator ( ) { return disjuncts . iterator ( ) ; } private class DisjunctionMaxWeight implements Weight { private Searcher searcher ; private ArrayList weights = new ArrayList ( ) ; public DisjunctionMaxWeight ( Searcher searcher ) throws IOException { this . searcher = searcher ; for ( int i = 0 ; i < disjuncts . size ( ) ; i ++ ) weights . add ( ( ( Query ) disjuncts . get ( i ) ) . createWeight ( searcher ) ) ; } public Query getQuery ( ) { return DisjunctionMaxQuery . this ; } public float getValue ( ) { return getBoost ( ) ; } public float sumOfSquaredWeights ( ) throws IOException { float max = 0.0f , sum = 0.0f ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { float sub = ( ( Weight ) weights . get ( i ) ) . sumOfSquaredWeights ( ) ; sum += sub ; max = Math . max ( max , sub ) ; } return ( ( ( sum - max ) * tieBreakerMultiplier * tieBreakerMultiplier ) + max ) * getBoost ( ) * getBoost ( ) ; } public void normalize ( float norm ) { norm *= getBoost ( ) ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) ( ( Weight ) weights . get ( i ) ) . normalize ( norm ) ; } public Scorer scorer ( IndexReader reader ) throws IOException { DisjunctionMaxScorer result = new DisjunctionMaxScorer ( tieBreakerMultiplier , getSimilarity ( searcher ) ) ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { Weight w = ( Weight ) weights . get ( i ) ; Scorer subScorer = w . scorer ( reader ) ; if ( subScorer == null ) return null ; result . add ( subScorer ) ; } return result ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { if ( disjuncts . size ( ) == 1 ) return ( ( Weight ) weights . get ( 0 ) ) . explain ( reader , doc ) ; ComplexExplanation result = new ComplexExplanation ( ) ; float max = 0.0f , sum = 0.0f ; result . setDescription ( tieBreakerMultiplier == 0.0f ? "max of:" : "max plus " + tieBreakerMultiplier + " times others of:" ) ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { Explanation e = ( ( Weight ) weights . get ( i ) ) . explain ( reader , doc ) ; if ( e . isMatch ( ) ) { result . setMatch ( Boolean . TRUE ) ; result . addDetail ( e ) ; sum += e . getValue ( ) ; max = Math . max ( max , e . getValue ( ) ) ; } } result . setValue ( max + ( sum - max ) * tieBreakerMultiplier ) ; return result ; } } protected Weight createWeight ( Searcher searcher ) throws IOException { return new DisjunctionMaxWeight ( searcher ) ; } public Query rewrite ( IndexReader reader ) throws IOException { if ( disjuncts . size ( ) == 1 ) { Query singleton = ( Query ) disjuncts . get ( 0 ) ; Query result = singleton . rewrite ( reader ) ; if ( getBoost ( ) != 1.0f ) { if ( result == singleton ) result = ( Query ) result . clone ( ) ; result . setBoost ( getBoost ( ) * result . getBoost ( ) ) ; } return result ; } DisjunctionMaxQuery clone = null ; for ( int i = 0 ; i < disjuncts . size ( ) ; i ++ ) { Query clause = ( Query ) disjuncts . get ( i ) ; Query rewrite = clause . rewrite ( reader ) ; if ( rewrite != clause ) { if ( clone == null ) clone = ( DisjunctionMaxQuery ) this . clone ( ) ; clone . disjuncts . set ( i , rewrite ) ; } } if ( clone != null ) return clone ; else return this ; } public Object clone ( ) { DisjunctionMaxQuery clone = ( DisjunctionMaxQuery ) super . clone ( ) ; clone . disjuncts = ( ArrayList ) this . disjuncts . clone ( ) ; return clone ; } public void extractTerms ( Set terms ) { for ( int i = 0 ; i < disjuncts . size ( ) ; i ++ ) { ( ( Query ) disjuncts . get ( i ) ) . extractTerms ( terms ) ; } } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "(" ) ; for ( int i = 0 ; i < disjuncts . size ( ) ; i ++ ) { Query subquery = ( Query ) disjuncts . get ( i ) ; if ( subquery instanceof BooleanQuery ) { buffer . append ( "(" ) ; buffer . append ( subquery . toString ( field ) ) ; buffer . append ( ")" ) ; } else buffer . append ( subquery . toString ( field ) ) ; if ( i != disjuncts . size ( ) - 1 ) buffer . append ( " | " ) ; } buffer . append ( ")" ) ; if ( tieBreakerMultiplier != 0.0f ) { buffer . append ( "~" ) ; buffer . append ( tieBreakerMultiplier ) ; } if ( getBoost ( ) != 1.0 ) { buffer . append ( "^" ) ; buffer . append ( getBoost ( ) ) ; } return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof DisjunctionMaxQuery ) ) return false ; DisjunctionMaxQuery other = ( DisjunctionMaxQuery ) o ; return this . getBoost ( ) == other . getBoost ( ) && this . tieBreakerMultiplier == other . tieBreakerMultiplier && this . disjuncts . equals ( other . disjuncts ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) + Float . floatToIntBits ( tieBreakerMultiplier ) + disjuncts . hashCode ( ) ; } } 	1	['14', '2', '0', '6', '38', '0', '1', '6', '11', '0.384615385', '318', '1', '0', '0.5', '0.171428571', '2', '5', '21.57142857', '6', '1.5714', '2']
package org . apache . lucene . index ; import java . util . List ; import java . io . IOException ; public interface IndexDeletionPolicy { public void onInit ( List commits ) throws IOException ; public void onCommit ( List commits ) throws IOException ; } 	0	['2', '1', '0', '8', '2', '1', '8', '0', '2', '2', '2', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . util . PriorityQueue ; public class ParallelMultiSearcher extends MultiSearcher { private Searchable [ ] searchables ; private int [ ] starts ; public ParallelMultiSearcher ( Searchable [ ] searchables ) throws IOException { super ( searchables ) ; this . searchables = searchables ; this . starts = getStarts ( ) ; } public int docFreq ( Term term ) throws IOException { return super . docFreq ( term ) ; } public TopDocs search ( Weight weight , Filter filter , int nDocs ) throws IOException { HitQueue hq = new HitQueue ( nDocs ) ; int totalHits = 0 ; MultiSearcherThread [ ] msta = new MultiSearcherThread [ searchables . length ] ; for ( int i = 0 ; i < searchables . length ; i ++ ) { msta [ i ] = new MultiSearcherThread ( searchables [ i ] , weight , filter , nDocs , hq , i , starts , "MultiSearcher thread #" + ( i + 1 ) ) ; msta [ i ] . start ( ) ; } for ( int i = 0 ; i < searchables . length ; i ++ ) { try { msta [ i ] . join ( ) ; } catch ( InterruptedException ie ) { ; } IOException ioe = msta [ i ] . getIOException ( ) ; if ( ioe == null ) { totalHits += msta [ i ] . hits ( ) ; } else { throw ioe ; } } ScoreDoc [ ] scoreDocs = new ScoreDoc [ hq . size ( ) ] ; for ( int i = hq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = ( ScoreDoc ) hq . pop ( ) ; float maxScore = ( totalHits == 0 ) ? Float . NEGATIVE_INFINITY : scoreDocs [ 0 ] . score ; return new TopDocs ( totalHits , scoreDocs , maxScore ) ; } public TopFieldDocs search ( Weight weight , Filter filter , int nDocs , Sort sort ) throws IOException { FieldDocSortedHitQueue hq = new FieldDocSortedHitQueue ( null , nDocs ) ; int totalHits = 0 ; MultiSearcherThread [ ] msta = new MultiSearcherThread [ searchables . length ] ; for ( int i = 0 ; i < searchables . length ; i ++ ) { msta [ i ] = new MultiSearcherThread ( searchables [ i ] , weight , filter , nDocs , hq , sort , i , starts , "MultiSearcher thread #" + ( i + 1 ) ) ; msta [ i ] . start ( ) ; } float maxScore = Float . NEGATIVE_INFINITY ; for ( int i = 0 ; i < searchables . length ; i ++ ) { try { msta [ i ] . join ( ) ; } catch ( InterruptedException ie ) { ; } IOException ioe = msta [ i ] . getIOException ( ) ; if ( ioe == null ) { totalHits += msta [ i ] . hits ( ) ; maxScore = Math . max ( maxScore , msta [ i ] . getMaxScore ( ) ) ; } else { throw ioe ; } } ScoreDoc [ ] scoreDocs = new ScoreDoc [ hq . size ( ) ] ; for ( int i = hq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = ( ScoreDoc ) hq . pop ( ) ; return new TopFieldDocs ( totalHits , scoreDocs , hq . getFields ( ) , maxScore ) ; } public void search ( Weight weight , Filter filter , final HitCollector results ) throws IOException { for ( int i = 0 ; i < searchables . length ; i ++ ) { final int start = starts [ i ] ; searchables [ i ] . search ( weight , filter , new HitCollector ( ) { public void collect ( int doc , float score ) { results . collect ( doc + start , score ) ; } } ) ; } } public Query rewrite ( Query original ) throws IOException { return super . rewrite ( original ) ; } } class MultiSearcherThread extends Thread { private Searchable searchable ; private Weight weight ; private Filter filter ; private int nDocs ; private TopDocs docs ; private int i ; private PriorityQueue hq ; private int [ ] starts ; private IOException ioe ; private Sort sort ; public MultiSearcherThread ( Searchable searchable , Weight weight , Filter filter , int nDocs , HitQueue hq , int i , int [ ] starts , String name ) { super ( name ) ; this . searchable = searchable ; this . weight = weight ; this . filter = filter ; this . nDocs = nDocs ; this . hq = hq ; this . i = i ; this . starts = starts ; } public MultiSearcherThread ( Searchable searchable , Weight weight , Filter filter , int nDocs , FieldDocSortedHitQueue hq , Sort sort , int i , int [ ] starts , String name ) { super ( name ) ; this . searchable = searchable ; this . weight = weight ; this . filter = filter ; this . nDocs = nDocs ; this . hq = hq ; this . i = i ; this . starts = starts ; this . sort = sort ; } public void run ( ) { try { docs = ( sort == null ) ? searchable . search ( weight , filter , nDocs ) : searchable . search ( weight , filter , nDocs , sort ) ; } catch ( IOException ioe ) { this . ioe = ioe ; } if ( ioe == null ) { if ( sort != null ) { ( ( FieldDocSortedHitQueue ) hq ) . setFields ( ( ( TopFieldDocs ) docs ) . fields ) ; } ScoreDoc [ ] scoreDocs = docs . scoreDocs ; for ( int j = 0 ; j < scoreDocs . length ; j ++ ) { ScoreDoc scoreDoc = scoreDocs [ j ] ; scoreDoc . doc += starts [ i ] ; synchronized ( hq ) { if ( ! hq . insert ( scoreDoc ) ) break ; } } } } public int hits ( ) { return docs . totalHits ; } public float getMaxScore ( ) { return docs . getMaxScore ( ) ; } public IOException getIOException ( ) { return ioe ; } } 	1	['6', '3', '0', '16', '33', '3', '1', '16', '6', '0.4', '297', '1', '1', '0.880952381', '0.351851852', '2', '3', '48.16666667', '1', '0.8333', '1']
package org . apache . lucene . analysis ; import java . io . Reader ; public class LetterTokenizer extends CharTokenizer { public LetterTokenizer ( Reader in ) { super ( in ) ; } protected boolean isTokenChar ( char c ) { return Character . isLetter ( c ) ; } } 	0	['2', '4', '1', '2', '4', '1', '1', '1', '1', '2', '9', '0', '0', '0.923076923', '0.666666667', '1', '1', '3.5', '1', '0.5', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . Arrays ; import java . util . Comparator ; class ConjunctionScorer extends Scorer { private Scorer [ ] scorers = new Scorer [ 2 ] ; private int length = 0 ; private int first = 0 ; private int last = - 1 ; private boolean firstTime = true ; private boolean more = true ; private float coord ; public ConjunctionScorer ( Similarity similarity ) { super ( similarity ) ; } final void add ( Scorer scorer ) { if ( length >= scorers . length ) { Scorer [ ] temps = new Scorer [ scorers . length * 2 ] ; System . arraycopy ( scorers , 0 , temps , 0 , length ) ; scorers = temps ; } last += 1 ; length += 1 ; scorers [ last ] = scorer ; } public int doc ( ) { return scorers [ first ] . doc ( ) ; } public boolean next ( ) throws IOException { if ( firstTime ) { init ( true ) ; } else if ( more ) { more = scorers [ last ] . next ( ) ; } return doNext ( ) ; } private boolean doNext ( ) throws IOException { while ( more && scorers [ first ] . doc ( ) < scorers [ last ] . doc ( ) ) { more = scorers [ first ] . skipTo ( scorers [ last ] . doc ( ) ) ; last = first ; first = ( first == length - 1 ) ? 0 : first + 1 ; } return more ; } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { init ( false ) ; } for ( int i = 0 , pos = first ; i < length ; i ++ ) { if ( ! more ) break ; more = scorers [ pos ] . skipTo ( target ) ; pos = ( pos == length - 1 ) ? 0 : pos + 1 ; } if ( more ) sortScorers ( ) ; return doNext ( ) ; } public float score ( ) throws IOException { float sum = 0.0f ; for ( int i = 0 ; i < length ; i ++ ) { sum += scorers [ i ] . score ( ) ; } return sum * coord ; } private void init ( boolean initScorers ) throws IOException { coord = getSimilarity ( ) . coord ( length , length ) ; more = length > 0 ; if ( initScorers ) { for ( int i = 0 , pos = first ; i < length ; i ++ ) { if ( ! more ) break ; more = scorers [ pos ] . next ( ) ; pos = ( pos == length - 1 ) ? 0 : pos + 1 ; } if ( more ) sortScorers ( ) ; } firstTime = false ; } private void sortScorers ( ) { if ( length != scorers . length ) { Scorer [ ] temps = new Scorer [ length ] ; System . arraycopy ( scorers , 0 , temps , 0 , length ) ; scorers = temps ; } Arrays . sort ( scorers , new Comparator ( ) { public int compare ( Object o1 , Object o2 ) { return ( ( Scorer ) o1 ) . doc ( ) - ( ( Scorer ) o2 ) . doc ( ) ; } } ) ; first = 0 ; last = length - 1 ; } public Explanation explain ( int doc ) { throw new UnsupportedOperationException ( ) ; } } 	1	['10', '2', '1', '6', '21', '0', '3', '4', '6', '0.412698413', '340', '1', '1', '0.470588235', '0.3', '1', '3', '32.3', '2', '1.1', '1']
package org . apache . lucene . index ; import org . apache . lucene . util . UnicodeUtil ; final class TermVectorsTermsWriterPerThread extends TermsHashConsumerPerThread { final TermVectorsTermsWriter termsWriter ; final TermsHashPerThread termsHashPerThread ; final DocumentsWriter . DocState docState ; TermVectorsTermsWriter . PerDoc doc ; public TermVectorsTermsWriterPerThread ( TermsHashPerThread termsHashPerThread , TermVectorsTermsWriter termsWriter ) { this . termsWriter = termsWriter ; this . termsHashPerThread = termsHashPerThread ; docState = termsHashPerThread . docState ; } final ByteSliceReader vectorSliceReader = new ByteSliceReader ( ) ; final UnicodeUtil . UTF8Result utf8Results [ ] = { new UnicodeUtil . UTF8Result ( ) , new UnicodeUtil . UTF8Result ( ) } ; public void startDocument ( ) { assert clearLastVectorFieldName ( ) ; if ( doc != null ) { doc . reset ( ) ; doc . docID = docState . docID ; } } public DocumentsWriter . DocWriter finishDocument ( ) { try { return doc ; } finally { doc = null ; } } public TermsHashConsumerPerField addField ( TermsHashPerField termsHashPerField , FieldInfo fieldInfo ) { return new TermVectorsTermsWriterPerField ( termsHashPerField , this , fieldInfo ) ; } public void abort ( ) { if ( doc != null ) { doc . abort ( ) ; doc = null ; } } final boolean clearLastVectorFieldName ( ) { lastVectorFieldName = null ; return true ; } String lastVectorFieldName ; final boolean vectorFieldsInOrder ( FieldInfo fi ) { try { if ( lastVectorFieldName != null ) return lastVectorFieldName . compareTo ( fi . name ) < 0 ; else return true ; } finally { lastVectorFieldName = fi . name ; } } } 	0	['9', '2', '0', '12', '21', '24', '2', '12', '5', '0.916666667', '167', '0', '6', '0.363636364', '0.270833333', '0', '0', '16.55555556', '6', '2', '0']
package org . apache . lucene . analysis ; import org . apache . lucene . index . Payload ; import org . apache . lucene . index . TermPositions ; public class Token implements Cloneable { String termText ; int startOffset ; int endOffset ; String type = "word" ; Payload payload ; private int positionIncrement = 1 ; public Token ( String text , int start , int end ) { termText = text ; startOffset = start ; endOffset = end ; } public Token ( String text , int start , int end , String typ ) { termText = text ; startOffset = start ; endOffset = end ; type = typ ; } public void setPositionIncrement ( int positionIncrement ) { if ( positionIncrement < 0 ) throw new IllegalArgumentException ( "Increment must be zero or greater: " + positionIncrement ) ; this . positionIncrement = positionIncrement ; } public int getPositionIncrement ( ) { return positionIncrement ; } public void setTermText ( String text ) { termText = text ; } public final String termText ( ) { return termText ; } public final int startOffset ( ) { return startOffset ; } public final int endOffset ( ) { return endOffset ; } public final String type ( ) { return type ; } public void setPayload ( Payload payload ) { this . payload = payload ; } public Payload getPayload ( ) { return this . payload ; } public String toString ( ) { StringBuffer sb = new StringBuffer ( ) ; sb . append ( "(" + termText + "," + startOffset + "," + endOffset ) ; if ( ! type . equals ( "word" ) ) sb . append ( ",type=" + type ) ; if ( positionIncrement != 1 ) sb . append ( ",posIncr=" + positionIncrement ) ; sb . append ( ")" ) ; return sb . toString ( ) ; } public Object clone ( ) { try { return super . clone ( ) ; } catch ( CloneNotSupportedException e ) { throw new RuntimeException ( e ) ; } } } 	1	['13', '1', '0', '15', '22', '24', '14', '1', '13', '0.75', '177', '0.166666667', '1', '0', '0.384615385', '1', '1', '12.15384615', '3', '1.0769', '8']
package org . apache . lucene . index ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . analysis . Token ; import java . io . IOException ; abstract class InvertedDocConsumerPerField { abstract boolean start ( Fieldable [ ] fields , int count ) throws IOException ; abstract void add ( Token token ) throws IOException ; abstract void finish ( ) throws IOException ; abstract void abort ( ) ; } 	0	['5', '1', '1', '7', '6', '10', '5', '2', '0', '2', '8', '0', '0', '0', '0.4', '0', '0', '0.6', '1', '0.8', '0']
package org . apache . lucene . search ; import java . io . Serializable ; public class Sort implements Serializable { public static final Sort RELEVANCE = new Sort ( ) ; public static final Sort INDEXORDER = new Sort ( SortField . FIELD_DOC ) ; SortField [ ] fields ; public Sort ( ) { this ( new SortField [ ] { SortField . FIELD_SCORE , SortField . FIELD_DOC } ) ; } public Sort ( String field ) { setSort ( field , false ) ; } public Sort ( String field , boolean reverse ) { setSort ( field , reverse ) ; } public Sort ( String [ ] fields ) { setSort ( fields ) ; } public Sort ( SortField field ) { setSort ( field ) ; } public Sort ( SortField [ ] fields ) { setSort ( fields ) ; } public final void setSort ( String field ) { setSort ( field , false ) ; } public void setSort ( String field , boolean reverse ) { SortField [ ] nfields = new SortField [ ] { new SortField ( field , SortField . AUTO , reverse ) , SortField . FIELD_DOC } ; fields = nfields ; } public void setSort ( String [ ] fieldnames ) { final int n = fieldnames . length ; SortField [ ] nfields = new SortField [ n ] ; for ( int i = 0 ; i < n ; ++ i ) { nfields [ i ] = new SortField ( fieldnames [ i ] , SortField . AUTO ) ; } fields = nfields ; } public void setSort ( SortField field ) { this . fields = new SortField [ ] { field } ; } public void setSort ( SortField [ ] fields ) { this . fields = fields ; } public SortField [ ] getSort ( ) { return fields ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { buffer . append ( fields [ i ] . toString ( ) ) ; if ( ( i + 1 ) < fields . length ) buffer . append ( ',' ) ; } return buffer . toString ( ) ; } } 	1	['14', '1', '0', '12', '22', '61', '11', '1', '13', '0.692307692', '175', '0', '3', '0', '0.320512821', '0', '0', '11.28571429', '3', '0.7143', '2']
package org . apache . lucene . analysis . standard ; import org . apache . lucene . analysis . Token ; class StandardTokenizerImpl { public static final int YYEOF = - 1 ; private static final int ZZ_BUFFERSIZE = 16384 ; public static final int YYINITIAL = 0 ; private static final String ZZ_CMAP_PACKED = "\11\0\1\0\1\15\1\0\1\0\1\14\22\0\1\0\5\0\1\5" + "\1\3\4\0\1\11\1\7\1\4\1\11\12\2\6\0\1\6\32\12" + "\4\0\1\10\1\0\32\12\57\0\1\12\12\0\1\12\4\0\1\12" + "\5\0\27\12\1\0\37\12\1\0\12\2\0\22\12\34\0\136\12" + "\2\0\11\12\2\0\7\12\16\0\2\12\16\0\5\12\11\0\1\12" + "\213\0\1\12\13\0\1\12\1\0\3\12\1\0\1\12\1\0\24\12" + "\1\0\54\12\1\0\10\12\2\0\32\12\14\0\202\12\12\0\71\12" + "\2\0\2\12\2\0\2\12\3\0\46\12\2\0\2\12\67\0\46\12" + "\2\0\1\12\7\0\47\12\110\0\33\12\5\0\3\12\56\0\32\12" + "\5\0\13\12\25\0\12\2\7\0\143\12\1\0\1\12\17\0\2\12" + "\11\0\12\2\3\12\23\0\1\12\1\0\33\12\123\0\46\12\0" + "\65\12\3\0\1\12\22\0\1\12\7\0\12\12\4\0\12\2\25\0" + "\10\12\2\0\2\12\2\0\26\12\1\0\7\12\1\0\1\12\3\0" + "\4\12\42\0\2\12\1\0\3\12\4\0\12\2\2\12\23\0\6\12" + "\4\0\2\12\2\0\26\12\1\0\7\12\1\0\2\12\1\0\2\12" + "\1\0\2\12\37\0\4\12\1\0\1\12\7\0\12\2\2\0\3\12" + "\20\0\7\12\1\0\1\12\1\0\3\12\1\0\26\12\1\0\7\12" + "\1\0\2\12\1\0\5\12\3\0\1\12\22\0\1\12\17\0\1\12" + "\5\0\12\2\25\0\10\12\2\0\2\12\2\0\26\12\1\0\7\12" + "\1\0\2\12\2\0\4\12\3\0\1\12\36\0\2\12\1\0\3\12" + "\4\0\12\2\25\0\6\12\3\0\3\12\1\0\4\12\3\0\2\12" + "\1\0\1\12\1\0\2\12\3\0\2\12\3\0\3\12\3\0\10\12" + "\1\0\3\12\55\0\11\2\25\0\10\12\1\0\3\12\1\0\27\12" + "\1\0\12\12\1\0\5\12\46\0\2\12\4\0\12\2\25\0\10\12" + "\1\0\3\12\1\0\27\12\1\0\12\12\1\0\5\12\44\0\1\12" + "\1\0\2\12\4\0\12\2\25\0\10\12\1\0\3\12\1\0\27\12" + "\1\0\20\12\46\0\2\12\4\0\12\2\25\0\22\12\3\0\30\12" + "\1\0\11\12\1\0\1\12\2\0\7\12\71\0\1\1\60\12\1\1" + "\2\12\14\1\7\12\11\1\12\2\47\0\2\12\1\0\1\12\2\0" + "\2\12\1\0\1\12\2\0\1\12\6\0\4\12\1\0\7\12\1\0" + "\3\12\1\0\1\12\1\0\1\12\2\0\2\12\1\0\4\12\1\0" + "\2\12\11\0\1\12\2\0\5\12\1\0\1\12\11\0\12\2\2\0" + "\2\12\42\0\1\12\37\0\12\2\26\0\10\12\1\0\42\12\35\0" + "\4\12\164\0\42\12\1\0\5\12\1\0\2\12\25\0\12\2\6\0" + "\6\12\112\0\46\12\12\0\47\12\11\0\132\12\5\0\104\12\5\0" + "\122\12\6\0\7\12\1\0\77\12\1\0\1\12\1\0\4\12\2\0" + "\7\12\1\0\1\12\1\0\4\12\2\0\47\12\1\0\1\12\1\0" + "\4\12\2\0\37\12\1\0\1\12\1\0\4\12\2\0\7\12\1\0" + "\1\12\1\0\4\12\2\0\7\12\1\0\7\12\1\0\27\12\1\0" + "\37\12\1\0\1\12\1\0\4\12\2\0\7\12\1\0\47\12\1\0" + "\23\12\16\0\11\2\56\0\125\12\14\0\12\2\0\10\12\12\0" + "\32\12\5\0\113\12\225\0\64\12\54\0\12\2\46\0\12\2\6\0" + "\130\12\10\0\51\12\0\234\12\4\0\132\12\6\0\26\12\2\0" + "\6\12\2\0\46\12\2\0\6\12\2\0\10\12\1\0\1\12\1\0" + "\1\12\1\0\1\12\1\0\37\12\2\0\65\12\1\0\7\12\1\0" + "\1\12\3\0\3\12\1\0\7\12\3\0\4\12\2\0\6\12\4\0" + "\15\12\5\0\3\12\1\0\7\12\202\0\1\12\202\0\1\12\4\0" + "\1\12\2\0\12\12\1\0\1\12\3\0\5\12\6\0\1\12\1\0" + "\1\12\1\0\1\12\1\0\4\12\1\0\3\12\1\0\7\12\0" + "\2\12\52\0\5\12\12\0\1\13\124\13\10\13\2\13\2\13\132\13" + "\1\13\3\13\6\13\50\13\3\13\1\0\136\12\21\0\30\12\70\0" + "\20\13\0\200\13\200\0\13\12\13\100\0\13\132\13\12" + "\0\12\0\13\322\13\7\12\14\0\5\12\5\0\1\12" + "\1\0\12\12\1\0\15\12\1\0\5\12\1\0\1\12\1\0\2\12" + "\1\0\2\12\1\0\154\12\41\0\12\22\0\100\12\2\0\66\12" + "\50\0\14\12\164\0\3\12\1\0\1\12\1\0\207\12\23\0\12\2" + "\7\0\32\12\6\0\32\12\12\0\1\13\72\13\37\12\3\0\6\12" + "\2\0\6\12\2\0\6\12\2\0\3\12\43\0" ; private static final char [ ] ZZ_CMAP = zzUnpackCMap ( ZZ_CMAP_PACKED ) ; private static final int [ ] ZZ_ACTION = zzUnpackAction ( ) ; private static final String ZZ_ACTION_PACKED_0 = "\1\0\1\1\3\2\1\3\1\1\13\0\1\2\3\4" + "\2\0\1\5\1\0\1\5\3\4\6\5\1\6\1\4" + "\2\7\1\10\1\0\1\10\3\0\2\10\1\11\1\12" + "\1\4" ; private static int [ ] zzUnpackAction ( ) { int [ ] result = new int [ 51 ] ; int offset = 0 ; offset = zzUnpackAction ( ZZ_ACTION_PACKED_0 , offset , result ) ; return result ; } private static int zzUnpackAction ( String packed , int offset , int [ ] result ) { int i = 0 ; int j = offset ; int l = packed . length ( ) ; while ( i < l ) { int count = packed . charAt ( i ++ ) ; int value = packed . charAt ( i ++ ) ; do result [ j ++ ] = value ; while ( -- count > 0 ) ; } return j ; } private static final int [ ] ZZ_ROWMAP = zzUnpackRowMap ( ) ; private static final String ZZ_ROWMAP_PACKED_0 = "\0\0\0\16\0\34\0\52\0\70\0\16\0\106\0\124" + "\0\142\0\160\0\176\0\214\0\232\0\250\0\266\0\304" + "\0\322\0\340\0\356\0\374\0\0\0\0" + "\0\0\0\0\0\0\0\0" + "\0\0\0\0\0\0\0\322\0" + "\0\0\0\0\0\0\0\124\0\214" + "\0\0\0" ; private static int [ ] zzUnpackRowMap ( ) { int [ ] result = new int [ 51 ] ; int offset = 0 ; offset = zzUnpackRowMap ( ZZ_ROWMAP_PACKED_0 , offset , result ) ; return result ; } private static int zzUnpackRowMap ( String packed , int offset , int [ ] result ) { int i = 0 ; int j = offset ; int l = packed . length ( ) ; while ( i < l ) { int high = packed . charAt ( i ++ ) << 16 ; result [ j ++ ] = high | packed . charAt ( i ++ ) ; } return j ; } private static final int [ ] ZZ_TRANS = zzUnpackTrans ( ) ; private static final String ZZ_TRANS_PACKED_0 = "\1\2\1\3\1\4\7\2\1\5\1\6\1\7\1\2" + "\17\0\2\3\1\0\1\10\1\0\1\11\2\12\1\13" + "\1\3\4\0\1\3\1\4\1\0\1\14\1\0\1\11" + "\2\15\1\16\1\4\4\0\1\3\1\4\1\17\1\20" + "\1\21\1\22\2\12\1\13\1\23\20\0\1\2\1\0" + "\1\24\1\25\7\0\1\26\4\0\2\27\7\0\1\27" + "\4\0\1\30\1\31\7\0\1\32\5\0\1\33\7\0" + "\1\13\4\0\1\34\1\35\7\0\1\36\4\0\1\37" + "\1\40\7\0\1\41\4\0\1\42\1\43\7\0\1\44" + "\15\0\1\45\4\0\1\24\1\25\7\0\1\46\15\0" + "\1\47\4\0\2\27\7\0\1\50\4\0\1\3\1\4" + "\1\17\1\10\1\21\1\22\2\12\1\13\1\23\4\0" + "\2\24\1\0\1\51\1\0\1\11\2\52\1\0\1\24" + "\4\0\1\24\1\25\1\0\1\53\1\0\1\11\2\54" + "\1\55\1\25\4\0\1\24\1\25\1\0\1\51\1\0" + "\1\11\2\52\1\0\1\26\4\0\2\27\1\0\1\56" + "\2\0\1\56\2\0\1\27\4\0\2\30\1\0\1\52" + "\1\0\1\11\2\52\1\0\1\30\4\0\1\30\1\31" + "\1\0\1\54\1\0\1\11\2\54\1\55\1\31\4\0" + "\1\30\1\31\1\0\1\52\1\0\1\11\2\52\1\0" + "\1\32\5\0\1\33\1\0\1\55\2\0\3\55\1\33" + "\4\0\2\34\1\0\1\57\1\0\1\11\2\12\1\13" + "\1\34\4\0\1\34\1\35\1\0\1\60\1\0\1\11" + "\2\15\1\16\1\35\4\0\1\34\1\35\1\0\1\57" + "\1\0\1\11\2\12\1\13\1\36\4\0\2\37\1\0" + "\1\12\1\0\1\11\2\12\1\13\1\37\4\0\1\37" + "\1\40\1\0\1\15\1\0\1\11\2\15\1\16\1\40" + "\4\0\1\37\1\40\1\0\1\12\1\0\1\11\2\12" + "\1\13\1\41\4\0\2\42\1\0\1\13\2\0\3\13" + "\1\42\4\0\1\42\1\43\1\0\1\16\2\0\3\16" + "\1\43\4\0\1\42\1\43\1\0\1\13\2\0\3\13" + "\1\44\6\0\1\17\6\0\1\45\4\0\1\24\1\25" + "\1\0\1\61\1\0\1\11\2\52\1\0\1\26\4\0" + "\2\27\1\0\1\56\2\0\1\56\2\0\1\50\4\0" + "\2\24\7\0\1\24\4\0\2\30\7\0\1\30\4\0" + "\2\34\7\0\1\34\4\0\2\37\7\0\1\37\4\0" + "\2\42\7\0\1\42\4\0\2\62\7\0\1\62\4\0" + "\2\24\7\0\1\63\4\0\2\62\1\0\1\56\2\0" + "\1\56\2\0\1\62\4\0\2\24\1\0\1\61\1\0" + "\1\11\2\52\1\0\1\24\3\0" ; private static int [ ] zzUnpackTrans ( ) { int [ ] result = new int [ 658 ] ; int offset = 0 ; offset = zzUnpackTrans ( ZZ_TRANS_PACKED_0 , offset , result ) ; return result ; } private static int zzUnpackTrans ( String packed , int offset , int [ ] result ) { int i = 0 ; int j = offset ; int l = packed . length ( ) ; while ( i < l ) { int count = packed . charAt ( i ++ ) ; int value = packed . charAt ( i ++ ) ; value -- ; do result [ j ++ ] = value ; while ( -- count > 0 ) ; } return j ; } private static final int ZZ_UNKNOWN_ERROR = 0 ; private static final int ZZ_NO_MATCH = 1 ; private static final int ZZ_PUSHBACK_2BIG = 2 ; private static final String ZZ_ERROR_MSG [ ] = { "Unkown internal scanner error" , "Error: could not match input" , "Error: pushback value was too large" } ; private static final int [ ] ZZ_ATTRIBUTE = zzUnpackAttribute ( ) ; private static final String ZZ_ATTRIBUTE_PACKED_0 = "\1\0\1\11\3\1\1\11\1\1\13\0\4\1\2\0" + "\1\1\1\0\17\1\1\0\1\1\3\0\5\1" ; private static int [ ] zzUnpackAttribute ( ) { int [ ] result = new int [ 51 ] ; int offset = 0 ; offset = zzUnpackAttribute ( ZZ_ATTRIBUTE_PACKED_0 , offset , result ) ; return result ; } private static int zzUnpackAttribute ( String packed , int offset , int [ ] result ) { int i = 0 ; int j = offset ; int l = packed . length ( ) ; while ( i < l ) { int count = packed . charAt ( i ++ ) ; int value = packed . charAt ( i ++ ) ; do result [ j ++ ] = value ; while ( -- count > 0 ) ; } return j ; } private java . io . Reader zzReader ; private int zzState ; private int zzLexicalState = YYINITIAL ; private char zzBuffer [ ] = new char [ ZZ_BUFFERSIZE ] ; private int zzMarkedPos ; private int zzPushbackPos ; private int zzCurrentPos ; private int zzStartRead ; private int zzEndRead ; private int yyline ; private int yychar ; private int yycolumn ; private boolean zzAtBOL = true ; private boolean zzAtEOF ; public static final int ALPHANUM = StandardTokenizer . ALPHANUM ; public static final int APOSTROPHE = StandardTokenizer . APOSTROPHE ; public static final int ACRONYM = StandardTokenizer . ACRONYM ; public static final int COMPANY = StandardTokenizer . COMPANY ; public static final int EMAIL = StandardTokenizer . EMAIL ; public static final int HOST = StandardTokenizer . HOST ; public static final int NUM = StandardTokenizer . NUM ; public static final int CJ = StandardTokenizer . CJ ; public static final int ACRONYM_DEP = StandardTokenizer . ACRONYM_DEP ; public static final String [ ] TOKEN_TYPES = StandardTokenizer . TOKEN_TYPES ; public final int yychar ( ) { return yychar ; } final void getText ( Token t ) { t . setTermBuffer ( zzBuffer , zzStartRead , zzMarkedPos - zzStartRead ) ; } StandardTokenizerImpl ( java . io . Reader in ) { this . zzReader = in ; } StandardTokenizerImpl ( java . io . InputStream in ) { this ( new java . io . InputStreamReader ( in ) ) ; } private static char [ ] zzUnpackCMap ( String packed ) { char [ ] map = new char [ 0x10000 ] ; int i = 0 ; int j = 0 ; while ( i < 1154 ) { int count = packed . charAt ( i ++ ) ; char value = packed . charAt ( i ++ ) ; do map [ j ++ ] = value ; while ( -- count > 0 ) ; } return map ; } private boolean zzRefill ( ) throws java . io . IOException { if ( zzStartRead > 0 ) { System . arraycopy ( zzBuffer , zzStartRead , zzBuffer , 0 , zzEndRead - zzStartRead ) ; zzEndRead -= zzStartRead ; zzCurrentPos -= zzStartRead ; zzMarkedPos -= zzStartRead ; zzPushbackPos -= zzStartRead ; zzStartRead = 0 ; } if ( zzCurrentPos >= zzBuffer . length ) { char newBuffer [ ] = new char [ zzCurrentPos * 2 ] ; System . arraycopy ( zzBuffer , 0 , newBuffer , 0 , zzBuffer . length ) ; zzBuffer = newBuffer ; } int numRead = zzReader . read ( zzBuffer , zzEndRead , zzBuffer . length - zzEndRead ) ; if ( numRead < 0 ) { return true ; } else { zzEndRead += numRead ; return false ; } } public final void yyclose ( ) throws java . io . IOException { zzAtEOF = true ; zzEndRead = zzStartRead ; if ( zzReader != null ) zzReader . close ( ) ; } public final void yyreset ( java . io . Reader reader ) { zzReader = reader ; zzAtBOL = true ; zzAtEOF = false ; zzEndRead = zzStartRead = 0 ; zzCurrentPos = zzMarkedPos = zzPushbackPos = 0 ; yyline = yychar = yycolumn = 0 ; zzLexicalState = YYINITIAL ; } public final int yystate ( ) { return zzLexicalState ; } public final void yybegin ( int newState ) { zzLexicalState = newState ; } public final String yytext ( ) { return new String ( zzBuffer , zzStartRead , zzMarkedPos - zzStartRead ) ; } public final char yycharat ( int pos ) { return zzBuffer [ zzStartRead + pos ] ; } public final int yylength ( ) { return zzMarkedPos - zzStartRead ; } private void zzScanError ( int errorCode ) { String message ; try { message = ZZ_ERROR_MSG [ errorCode ] ; } catch ( ArrayIndexOutOfBoundsException e ) { message = ZZ_ERROR_MSG [ ZZ_UNKNOWN_ERROR ] ; } throw new Error ( message ) ; } public void yypushback ( int number ) { if ( number > yylength ( ) ) zzScanError ( ZZ_PUSHBACK_2BIG ) ; zzMarkedPos -= number ; } public int getNextToken ( ) throws java . io . IOException { int zzInput ; int zzAction ; int zzCurrentPosL ; int zzMarkedPosL ; int zzEndReadL = zzEndRead ; char [ ] zzBufferL = zzBuffer ; char [ ] zzCMapL = ZZ_CMAP ; int [ ] zzTransL = ZZ_TRANS ; int [ ] zzRowMapL = ZZ_ROWMAP ; int [ ] zzAttrL = ZZ_ATTRIBUTE ; while ( true ) { zzMarkedPosL = zzMarkedPos ; yychar += zzMarkedPosL - zzStartRead ; zzAction = - 1 ; zzCurrentPosL = zzCurrentPos = zzStartRead = zzMarkedPosL ; zzState = zzLexicalState ; zzForAction : { while ( true ) { if ( zzCurrentPosL < zzEndReadL ) zzInput = zzBufferL [ zzCurrentPosL ++ ] ; else if ( zzAtEOF ) { zzInput = YYEOF ; break zzForAction ; } else { zzCurrentPos = zzCurrentPosL ; zzMarkedPos = zzMarkedPosL ; boolean eof = zzRefill ( ) ; zzCurrentPosL = zzCurrentPos ; zzMarkedPosL = zzMarkedPos ; zzBufferL = zzBuffer ; zzEndReadL = zzEndRead ; if ( eof ) { zzInput = YYEOF ; break zzForAction ; } else { zzInput = zzBufferL [ zzCurrentPosL ++ ] ; } } int zzNext = zzTransL [ zzRowMapL [ zzState ] + zzCMapL [ zzInput ] ] ; if ( zzNext == - 1 ) break zzForAction ; zzState = zzNext ; int zzAttributes = zzAttrL [ zzState ] ; if ( ( zzAttributes & 1 ) == 1 ) { zzAction = zzState ; zzMarkedPosL = zzCurrentPosL ; if ( ( zzAttributes & 8 ) == 8 ) break zzForAction ; } } } zzMarkedPos = zzMarkedPosL ; switch ( zzAction < 0 ? zzAction : ZZ_ACTION [ zzAction ] ) { case 4 : { return HOST ; } case 11 : break ; case 9 : { return ACRONYM ; } case 12 : break ; case 8 : { return ACRONYM_DEP ; } case 13 : break ; case 1 : { } case 14 : break ; case 5 : { return NUM ; } case 15 : break ; case 3 : { return CJ ; } case 16 : break ; case 2 : { return ALPHANUM ; } case 17 : break ; case 7 : { return COMPANY ; } case 18 : break ; case 6 : { return APOSTROPHE ; } case 19 : break ; case 10 : { return EMAIL ; } case 20 : break ; default : if ( zzInput == YYEOF && zzStartRead == zzCurrentPos ) { zzAtEOF = true ; return YYEOF ; } else { zzScanError ( ZZ_NO_MATCH ) ; } } } } } 	0	['25', '1', '0', '3', '35', '196', '2', '2', '10', '0.968495935', '729', '0.707317073', '0', '0', '0.214285714', '0', '0', '26.52', '3', '1.28', '0']
package org . apache . lucene . index ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . document . Document ; import org . apache . lucene . search . Similarity ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . FSDirectory ; import org . apache . lucene . store . Lock ; import org . apache . lucene . store . LockObtainFailedException ; import org . apache . lucene . store . AlreadyClosedException ; import org . apache . lucene . store . RAMDirectory ; import java . io . File ; import java . io . IOException ; import java . io . PrintStream ; import java . util . ArrayList ; import java . util . List ; import java . util . HashMap ; import java . util . Iterator ; import java . util . Map . Entry ; public class IndexWriter { public static long WRITE_LOCK_TIMEOUT = 1000 ; private long writeLockTimeout = WRITE_LOCK_TIMEOUT ; public static final String WRITE_LOCK_NAME = "write.lock" ; public final static int DEFAULT_MERGE_FACTOR = 10 ; public final static int DEFAULT_MAX_BUFFERED_DOCS = 10 ; public final static int DEFAULT_MAX_BUFFERED_DELETE_TERMS = 1000 ; public final static int DEFAULT_MAX_MERGE_DOCS = Integer . MAX_VALUE ; public final static int DEFAULT_MAX_FIELD_LENGTH = 10000 ; public final static int DEFAULT_TERM_INDEX_INTERVAL = 128 ; private final static int MERGE_READ_BUFFER_SIZE = 4096 ; private Directory directory ; private Analyzer analyzer ; private Similarity similarity = Similarity . getDefault ( ) ; private boolean commitPending ; private SegmentInfos rollbackSegmentInfos ; private SegmentInfos localRollbackSegmentInfos ; private boolean localAutoCommit ; private boolean autoCommit = true ; SegmentInfos segmentInfos = new SegmentInfos ( ) ; SegmentInfos ramSegmentInfos = new SegmentInfos ( ) ; private final RAMDirectory ramDirectory = new RAMDirectory ( ) ; private IndexFileDeleter deleter ; private Lock writeLock ; private int termIndexInterval = DEFAULT_TERM_INDEX_INTERVAL ; private int maxBufferedDeleteTerms = DEFAULT_MAX_BUFFERED_DELETE_TERMS ; private HashMap bufferedDeleteTerms = new HashMap ( ) ; private int numBufferedDeleteTerms = 0 ; private boolean useCompoundFile = true ; private boolean closeDir ; private boolean closed ; protected final void ensureOpen ( ) throws AlreadyClosedException { if ( closed ) { throw new AlreadyClosedException ( "this IndexWriter is closed" ) ; } } public boolean getUseCompoundFile ( ) { ensureOpen ( ) ; return useCompoundFile ; } public void setUseCompoundFile ( boolean value ) { ensureOpen ( ) ; useCompoundFile = value ; } public void setSimilarity ( Similarity similarity ) { ensureOpen ( ) ; this . similarity = similarity ; } public Similarity getSimilarity ( ) { ensureOpen ( ) ; return this . similarity ; } public void setTermIndexInterval ( int interval ) { ensureOpen ( ) ; this . termIndexInterval = interval ; } public int getTermIndexInterval ( ) { ensureOpen ( ) ; return termIndexInterval ; } public IndexWriter ( String path , Analyzer a , boolean create ) throws CorruptIndexException , LockObtainFailedException , IOException { init ( FSDirectory . getDirectory ( path ) , a , create , true , null , true ) ; } public IndexWriter ( File path , Analyzer a , boolean create ) throws CorruptIndexException , LockObtainFailedException , IOException { init ( FSDirectory . getDirectory ( path ) , a , create , true , null , true ) ; } public IndexWriter ( Directory d , Analyzer a , boolean create ) throws CorruptIndexException , LockObtainFailedException , IOException { init ( d , a , create , false , null , true ) ; } public IndexWriter ( String path , Analyzer a ) throws CorruptIndexException , LockObtainFailedException , IOException { init ( FSDirectory . getDirectory ( path ) , a , true , null , true ) ; } public IndexWriter ( File path , Analyzer a ) throws CorruptIndexException , LockObtainFailedException , IOException { init ( FSDirectory . getDirectory ( path ) , a , true , null , true ) ; } public IndexWriter ( Directory d , Analyzer a ) throws CorruptIndexException , LockObtainFailedException , IOException { init ( d , a , false , null , true ) ; } public IndexWriter ( Directory d , boolean autoCommit , Analyzer a ) throws CorruptIndexException , LockObtainFailedException , IOException { init ( d , a , false , null , autoCommit ) ; } public IndexWriter ( Directory d , boolean autoCommit , Analyzer a , boolean create ) throws CorruptIndexException , LockObtainFailedException , IOException { init ( d , a , create , false , null , autoCommit ) ; } public IndexWriter ( Directory d , boolean autoCommit , Analyzer a , IndexDeletionPolicy deletionPolicy ) throws CorruptIndexException , LockObtainFailedException , IOException { init ( d , a , false , deletionPolicy , autoCommit ) ; } public IndexWriter ( Directory d , boolean autoCommit , Analyzer a , boolean create , IndexDeletionPolicy deletionPolicy ) throws CorruptIndexException , LockObtainFailedException , IOException { init ( d , a , create , false , deletionPolicy , autoCommit ) ; } private void init ( Directory d , Analyzer a , boolean closeDir , IndexDeletionPolicy deletionPolicy , boolean autoCommit ) throws CorruptIndexException , LockObtainFailedException , IOException { if ( IndexReader . indexExists ( d ) ) { init ( d , a , false , closeDir , deletionPolicy , autoCommit ) ; } else { init ( d , a , true , closeDir , deletionPolicy , autoCommit ) ; } } private void init ( Directory d , Analyzer a , final boolean create , boolean closeDir , IndexDeletionPolicy deletionPolicy , boolean autoCommit ) throws CorruptIndexException , LockObtainFailedException , IOException { this . closeDir = closeDir ; directory = d ; analyzer = a ; this . infoStream = defaultInfoStream ; if ( create ) { directory . clearLock ( IndexWriter . WRITE_LOCK_NAME ) ; } Lock writeLock = directory . makeLock ( IndexWriter . WRITE_LOCK_NAME ) ; if ( ! writeLock . obtain ( writeLockTimeout ) ) throw new LockObtainFailedException ( "Index locked for write: " + writeLock ) ; this . writeLock = writeLock ; try { if ( create ) { try { segmentInfos . read ( directory ) ; segmentInfos . clear ( ) ; } catch ( IOException e ) { } segmentInfos . write ( directory ) ; } else { segmentInfos . read ( directory ) ; } this . autoCommit = autoCommit ; if ( ! autoCommit ) { rollbackSegmentInfos = ( SegmentInfos ) segmentInfos . clone ( ) ; } deleter = new IndexFileDeleter ( directory , deletionPolicy == null ? new KeepOnlyLastCommitDeletionPolicy ( ) : deletionPolicy , segmentInfos , infoStream ) ; } catch ( IOException e ) { this . writeLock . release ( ) ; this . writeLock = null ; throw e ; } } public void setMaxMergeDocs ( int maxMergeDocs ) { ensureOpen ( ) ; this . maxMergeDocs = maxMergeDocs ; } public int getMaxMergeDocs ( ) { ensureOpen ( ) ; return maxMergeDocs ; } public void setMaxFieldLength ( int maxFieldLength ) { ensureOpen ( ) ; this . maxFieldLength = maxFieldLength ; } public int getMaxFieldLength ( ) { ensureOpen ( ) ; return maxFieldLength ; } public void setMaxBufferedDocs ( int maxBufferedDocs ) { ensureOpen ( ) ; if ( maxBufferedDocs < 2 ) throw new IllegalArgumentException ( "maxBufferedDocs must at least be 2" ) ; this . minMergeDocs = maxBufferedDocs ; } public int getMaxBufferedDocs ( ) { ensureOpen ( ) ; return minMergeDocs ; } public void setMaxBufferedDeleteTerms ( int maxBufferedDeleteTerms ) { ensureOpen ( ) ; if ( maxBufferedDeleteTerms < 1 ) throw new IllegalArgumentException ( "maxBufferedDeleteTerms must at least be 1" ) ; this . maxBufferedDeleteTerms = maxBufferedDeleteTerms ; } public int getMaxBufferedDeleteTerms ( ) { ensureOpen ( ) ; return maxBufferedDeleteTerms ; } public void setMergeFactor ( int mergeFactor ) { ensureOpen ( ) ; if ( mergeFactor < 2 ) throw new IllegalArgumentException ( "mergeFactor cannot be less than 2" ) ; this . mergeFactor = mergeFactor ; } public int getMergeFactor ( ) { ensureOpen ( ) ; return mergeFactor ; } public static void setDefaultInfoStream ( PrintStream infoStream ) { IndexWriter . defaultInfoStream = infoStream ; } public static PrintStream getDefaultInfoStream ( ) { return IndexWriter . defaultInfoStream ; } public void setInfoStream ( PrintStream infoStream ) { ensureOpen ( ) ; this . infoStream = infoStream ; deleter . setInfoStream ( infoStream ) ; } public PrintStream getInfoStream ( ) { ensureOpen ( ) ; return infoStream ; } public void setWriteLockTimeout ( long writeLockTimeout ) { ensureOpen ( ) ; this . writeLockTimeout = writeLockTimeout ; } public long getWriteLockTimeout ( ) { ensureOpen ( ) ; return writeLockTimeout ; } public static void setDefaultWriteLockTimeout ( long writeLockTimeout ) { IndexWriter . WRITE_LOCK_TIMEOUT = writeLockTimeout ; } public static long getDefaultWriteLockTimeout ( ) { return IndexWriter . WRITE_LOCK_TIMEOUT ; } public synchronized void close ( ) throws CorruptIndexException , IOException { if ( ! closed ) { flushRamSegments ( ) ; if ( commitPending ) { segmentInfos . write ( directory ) ; deleter . checkpoint ( segmentInfos , true ) ; commitPending = false ; rollbackSegmentInfos = null ; } ramDirectory . close ( ) ; if ( writeLock != null ) { writeLock . release ( ) ; writeLock = null ; } closed = true ; if ( closeDir ) directory . close ( ) ; } } protected void finalize ( ) throws Throwable { try { if ( writeLock != null ) { writeLock . release ( ) ; writeLock = null ; } } finally { super . finalize ( ) ; } } public Directory getDirectory ( ) { ensureOpen ( ) ; return directory ; } public Analyzer getAnalyzer ( ) { ensureOpen ( ) ; return analyzer ; } public synchronized int docCount ( ) { ensureOpen ( ) ; int count = ramSegmentInfos . size ( ) ; for ( int i = 0 ; i < segmentInfos . size ( ) ; i ++ ) { SegmentInfo si = segmentInfos . info ( i ) ; count += si . docCount ; } return count ; } private int maxFieldLength = DEFAULT_MAX_FIELD_LENGTH ; public void addDocument ( Document doc ) throws CorruptIndexException , IOException { addDocument ( doc , analyzer ) ; } public void addDocument ( Document doc , Analyzer analyzer ) throws CorruptIndexException , IOException { ensureOpen ( ) ; SegmentInfo newSegmentInfo = buildSingleDocSegment ( doc , analyzer ) ; synchronized ( this ) { ramSegmentInfos . addElement ( newSegmentInfo ) ; maybeFlushRamSegments ( ) ; } } SegmentInfo buildSingleDocSegment ( Document doc , Analyzer analyzer ) throws CorruptIndexException , IOException { DocumentWriter dw = new DocumentWriter ( ramDirectory , analyzer , this ) ; dw . setInfoStream ( infoStream ) ; String segmentName = newRamSegmentName ( ) ; dw . addDocument ( segmentName , doc ) ; SegmentInfo si = new SegmentInfo ( segmentName , 1 , ramDirectory , false , false ) ; si . setNumFields ( dw . getNumFields ( ) ) ; return si ; } public synchronized void deleteDocuments ( Term term ) throws CorruptIndexException , IOException { ensureOpen ( ) ; bufferDeleteTerm ( term ) ; maybeFlushRamSegments ( ) ; } public synchronized void deleteDocuments ( Term [ ] terms ) throws CorruptIndexException , IOException { ensureOpen ( ) ; for ( int i = 0 ; i < terms . length ; i ++ ) { bufferDeleteTerm ( terms [ i ] ) ; } maybeFlushRamSegments ( ) ; } public void updateDocument ( Term term , Document doc ) throws CorruptIndexException , IOException { ensureOpen ( ) ; updateDocument ( term , doc , getAnalyzer ( ) ) ; } public void updateDocument ( Term term , Document doc , Analyzer analyzer ) throws CorruptIndexException , IOException { ensureOpen ( ) ; SegmentInfo newSegmentInfo = buildSingleDocSegment ( doc , analyzer ) ; synchronized ( this ) { bufferDeleteTerm ( term ) ; ramSegmentInfos . addElement ( newSegmentInfo ) ; maybeFlushRamSegments ( ) ; } } final synchronized String newRamSegmentName ( ) { return "_ram_" + Integer . toString ( ramSegmentInfos . counter ++ , Character . MAX_RADIX ) ; } final synchronized int getSegmentCount ( ) { return segmentInfos . size ( ) ; } final synchronized int getRamSegmentCount ( ) { return ramSegmentInfos . size ( ) ; } final synchronized int getDocCount ( int i ) { if ( i >= 0 && i < segmentInfos . size ( ) ) { return segmentInfos . info ( i ) . docCount ; } else { return - 1 ; } } final synchronized String newSegmentName ( ) { return "_" + Integer . toString ( segmentInfos . counter ++ , Character . MAX_RADIX ) ; } private int mergeFactor = DEFAULT_MERGE_FACTOR ; private int minMergeDocs = DEFAULT_MAX_BUFFERED_DOCS ; private int maxMergeDocs = DEFAULT_MAX_MERGE_DOCS ; private PrintStream infoStream = null ; private static PrintStream defaultInfoStream = null ; public synchronized void optimize ( ) throws CorruptIndexException , IOException { ensureOpen ( ) ; flushRamSegments ( ) ; while ( segmentInfos . size ( ) > 1 || ( segmentInfos . size ( ) == 1 && ( SegmentReader . hasDeletions ( segmentInfos . info ( 0 ) ) || SegmentReader . hasSeparateNorms ( segmentInfos . info ( 0 ) ) || segmentInfos . info ( 0 ) . dir != directory || ( useCompoundFile && ( ! SegmentReader . usesCompoundFile ( segmentInfos . info ( 0 ) ) ) ) ) ) ) { int minSegment = segmentInfos . size ( ) - mergeFactor ; mergeSegments ( segmentInfos , minSegment < 0 ? 0 : minSegment , segmentInfos . size ( ) ) ; } } private void startTransaction ( ) throws IOException { localRollbackSegmentInfos = ( SegmentInfos ) segmentInfos . clone ( ) ; localAutoCommit = autoCommit ; if ( localAutoCommit ) { flushRamSegments ( ) ; autoCommit = false ; } else deleter . incRef ( segmentInfos , false ) ; } private void rollbackTransaction ( ) throws IOException { autoCommit = localAutoCommit ; segmentInfos . clear ( ) ; segmentInfos . addAll ( localRollbackSegmentInfos ) ; localRollbackSegmentInfos = null ; deleter . checkpoint ( segmentInfos , false ) ; if ( ! autoCommit ) deleter . decRef ( segmentInfos ) ; deleter . refresh ( ) ; } private void commitTransaction ( ) throws IOException { autoCommit = localAutoCommit ; boolean success = false ; try { checkpoint ( ) ; success = true ; } finally { if ( ! success ) { rollbackTransaction ( ) ; } } if ( ! autoCommit ) deleter . decRef ( localRollbackSegmentInfos ) ; localRollbackSegmentInfos = null ; deleter . checkpoint ( segmentInfos , autoCommit ) ; } public synchronized void abort ( ) throws IOException { ensureOpen ( ) ; if ( ! autoCommit ) { segmentInfos . clear ( ) ; segmentInfos . addAll ( rollbackSegmentInfos ) ; deleter . checkpoint ( segmentInfos , false ) ; deleter . refresh ( ) ; ramSegmentInfos = new SegmentInfos ( ) ; bufferedDeleteTerms . clear ( ) ; numBufferedDeleteTerms = 0 ; commitPending = false ; close ( ) ; } else { throw new IllegalStateException ( "abort() can only be called when IndexWriter was opened with autoCommit=false" ) ; } } private void checkpoint ( ) throws IOException { if ( autoCommit ) { segmentInfos . write ( directory ) ; } else { commitPending = true ; } } public synchronized void addIndexes ( Directory [ ] dirs ) throws CorruptIndexException , IOException { ensureOpen ( ) ; optimize ( ) ; int start = segmentInfos . size ( ) ; boolean success = false ; startTransaction ( ) ; try { for ( int i = 0 ; i < dirs . length ; i ++ ) { SegmentInfos sis = new SegmentInfos ( ) ; sis . read ( dirs [ i ] ) ; for ( int j = 0 ; j < sis . size ( ) ; j ++ ) { segmentInfos . addElement ( sis . info ( j ) ) ; } } while ( segmentInfos . size ( ) > start + mergeFactor ) { for ( int base = start ; base < segmentInfos . size ( ) ; base ++ ) { int end = Math . min ( segmentInfos . size ( ) , base + mergeFactor ) ; if ( end - base > 1 ) { mergeSegments ( segmentInfos , base , end ) ; } } } success = true ; } finally { if ( success ) { commitTransaction ( ) ; } else { rollbackTransaction ( ) ; } } optimize ( ) ; } public synchronized void addIndexesNoOptimize ( Directory [ ] dirs ) throws CorruptIndexException , IOException { ensureOpen ( ) ; flushRamSegments ( ) ; int startUpperBound = minMergeDocs ; boolean success = false ; startTransaction ( ) ; try { for ( int i = 0 ; i < dirs . length ; i ++ ) { if ( directory == dirs [ i ] ) { throw new IllegalArgumentException ( "Cannot add this index to itself" ) ; } SegmentInfos sis = new SegmentInfos ( ) ; sis . read ( dirs [ i ] ) ; for ( int j = 0 ; j < sis . size ( ) ; j ++ ) { SegmentInfo info = sis . info ( j ) ; segmentInfos . addElement ( info ) ; while ( startUpperBound < info . docCount ) { startUpperBound *= mergeFactor ; if ( startUpperBound > maxMergeDocs ) { throw new IllegalArgumentException ( "Upper bound cannot exceed maxMergeDocs" ) ; } } } } maybeMergeSegments ( startUpperBound ) ; int segmentCount = segmentInfos . size ( ) ; int numTailSegments = 0 ; while ( numTailSegments < segmentCount && startUpperBound >= segmentInfos . info ( segmentCount - 1 - numTailSegments ) . docCount ) { numTailSegments ++ ; } if ( numTailSegments == 0 ) { success = true ; return ; } if ( checkNonDecreasingLevels ( segmentCount - numTailSegments ) ) { int numSegmentsToCopy = 0 ; while ( numSegmentsToCopy < segmentCount && directory != segmentInfos . info ( segmentCount - 1 - numSegmentsToCopy ) . dir ) { numSegmentsToCopy ++ ; } if ( numSegmentsToCopy == 0 ) { success = true ; return ; } for ( int i = segmentCount - numSegmentsToCopy ; i < segmentCount ; i ++ ) { mergeSegments ( segmentInfos , i , i + 1 ) ; } if ( checkNonDecreasingLevels ( segmentCount - numSegmentsToCopy ) ) { success = true ; return ; } } mergeSegments ( segmentInfos , segmentCount - numTailSegments , segmentCount ) ; if ( segmentInfos . info ( segmentInfos . size ( ) - 1 ) . docCount > startUpperBound ) { maybeMergeSegments ( startUpperBound * mergeFactor ) ; } success = true ; } finally { if ( success ) { commitTransaction ( ) ; } else { rollbackTransaction ( ) ; } } } public synchronized void addIndexes ( IndexReader [ ] readers ) throws CorruptIndexException , IOException { ensureOpen ( ) ; optimize ( ) ; final String mergedName = newSegmentName ( ) ; SegmentMerger merger = new SegmentMerger ( this , mergedName ) ; SegmentInfo info ; IndexReader sReader = null ; try { if ( segmentInfos . size ( ) == 1 ) { sReader = SegmentReader . get ( segmentInfos . info ( 0 ) ) ; merger . add ( sReader ) ; } for ( int i = 0 ; i < readers . length ; i ++ ) merger . add ( readers [ i ] ) ; boolean success = false ; startTransaction ( ) ; try { int docCount = merger . merge ( ) ; if ( sReader != null ) { sReader . close ( ) ; sReader = null ; } segmentInfos . setSize ( 0 ) ; info = new SegmentInfo ( mergedName , docCount , directory , false , true ) ; segmentInfos . addElement ( info ) ; success = true ; } finally { if ( ! success ) { rollbackTransaction ( ) ; } else { commitTransaction ( ) ; } } } finally { if ( sReader != null ) { sReader . close ( ) ; } } if ( useCompoundFile ) { boolean success = false ; startTransaction ( ) ; try { merger . createCompoundFile ( mergedName + ".cfs" ) ; info . setUseCompoundFile ( true ) ; } finally { if ( ! success ) { rollbackTransaction ( ) ; } else { commitTransaction ( ) ; } } } } void doAfterFlush ( ) throws IOException { } protected final void maybeFlushRamSegments ( ) throws CorruptIndexException , IOException { if ( ramSegmentInfos . size ( ) >= minMergeDocs || numBufferedDeleteTerms >= maxBufferedDeleteTerms ) { flushRamSegments ( ) ; } } private final synchronized void flushRamSegments ( ) throws CorruptIndexException , IOException { flushRamSegments ( true ) ; } protected final synchronized void flushRamSegments ( boolean triggerMerge ) throws CorruptIndexException , IOException { if ( ramSegmentInfos . size ( ) > 0 || bufferedDeleteTerms . size ( ) > 0 ) { mergeSegments ( ramSegmentInfos , 0 , ramSegmentInfos . size ( ) ) ; if ( triggerMerge ) maybeMergeSegments ( minMergeDocs ) ; } } public final synchronized void flush ( ) throws CorruptIndexException , IOException { ensureOpen ( ) ; flushRamSegments ( ) ; } public final long ramSizeInBytes ( ) { ensureOpen ( ) ; return ramDirectory . sizeInBytes ( ) ; } public final synchronized int numRamDocs ( ) { ensureOpen ( ) ; return ramSegmentInfos . size ( ) ; } private final void maybeMergeSegments ( int startUpperBound ) throws CorruptIndexException , IOException { long lowerBound = - 1 ; long upperBound = startUpperBound ; while ( upperBound < maxMergeDocs ) { int minSegment = segmentInfos . size ( ) ; int maxSegment = - 1 ; while ( -- minSegment >= 0 ) { SegmentInfo si = segmentInfos . info ( minSegment ) ; if ( maxSegment == - 1 && si . docCount > lowerBound && si . docCount <= upperBound ) { maxSegment = minSegment ; } else if ( si . docCount > upperBound ) { break ; } } minSegment ++ ; maxSegment ++ ; int numSegments = maxSegment - minSegment ; if ( numSegments < mergeFactor ) { break ; } else { boolean exceedsUpperLimit = false ; while ( numSegments >= mergeFactor ) { int docCount = mergeSegments ( segmentInfos , minSegment , minSegment + mergeFactor ) ; numSegments -= mergeFactor ; if ( docCount > upperBound ) { minSegment ++ ; exceedsUpperLimit = true ; } else { numSegments ++ ; } } if ( ! exceedsUpperLimit ) { break ; } } lowerBound = upperBound ; upperBound *= mergeFactor ; } } private final int mergeSegments ( SegmentInfos sourceSegments , int minSegment , int end ) throws CorruptIndexException , IOException { boolean doMerge = end > 0 ; final String mergedName = newSegmentName ( ) ; SegmentMerger merger = null ; final List ramSegmentsToDelete = new ArrayList ( ) ; SegmentInfo newSegment = null ; int mergedDocCount = 0 ; boolean anyDeletes = ( bufferedDeleteTerms . size ( ) != 0 ) ; try { if ( doMerge ) { if ( infoStream != null ) infoStream . print ( "merging segments" ) ; merger = new SegmentMerger ( this , mergedName ) ; for ( int i = minSegment ; i < end ; i ++ ) { SegmentInfo si = sourceSegments . info ( i ) ; if ( infoStream != null ) infoStream . print ( " " + si . name + " (" + si . docCount + " docs)" ) ; IndexReader reader = SegmentReader . get ( si , MERGE_READ_BUFFER_SIZE ) ; merger . add ( reader ) ; if ( reader . directory ( ) == this . ramDirectory ) { ramSegmentsToDelete . add ( si ) ; } } } SegmentInfos rollback = null ; boolean success = false ; try { if ( doMerge ) { mergedDocCount = merger . merge ( ) ; if ( infoStream != null ) { infoStream . println ( " into " + mergedName + " (" + mergedDocCount + " docs)" ) ; } newSegment = new SegmentInfo ( mergedName , mergedDocCount , directory , false , true ) ; } if ( sourceSegments != ramSegmentInfos || anyDeletes ) { rollback = ( SegmentInfos ) segmentInfos . clone ( ) ; } if ( doMerge ) { if ( sourceSegments == ramSegmentInfos ) { segmentInfos . addElement ( newSegment ) ; } else { for ( int i = end - 1 ; i > minSegment ; i -- ) sourceSegments . remove ( i ) ; segmentInfos . set ( minSegment , newSegment ) ; } } if ( sourceSegments == ramSegmentInfos ) { maybeApplyDeletes ( doMerge ) ; doAfterFlush ( ) ; } checkpoint ( ) ; success = true ; } finally { if ( success ) { if ( sourceSegments == ramSegmentInfos ) { ramSegmentInfos . removeAllElements ( ) ; } } else { if ( sourceSegments == ramSegmentInfos && ! anyDeletes ) { if ( newSegment != null && segmentInfos . size ( ) > 0 && segmentInfos . info ( segmentInfos . size ( ) - 1 ) == newSegment ) { segmentInfos . remove ( segmentInfos . size ( ) - 1 ) ; } } else if ( rollback != null ) { segmentInfos . clear ( ) ; segmentInfos . addAll ( rollback ) ; } deleter . refresh ( ) ; } } } finally { if ( doMerge ) merger . closeReaders ( ) ; } deleter . deleteDirect ( ramDirectory , ramSegmentsToDelete ) ; deleter . checkpoint ( segmentInfos , autoCommit ) ; if ( useCompoundFile && doMerge ) { boolean success = false ; try { merger . createCompoundFile ( mergedName + ".cfs" ) ; newSegment . setUseCompoundFile ( true ) ; checkpoint ( ) ; success = true ; } finally { if ( ! success ) { newSegment . setUseCompoundFile ( false ) ; deleter . refresh ( ) ; } } deleter . checkpoint ( segmentInfos , autoCommit ) ; } return mergedDocCount ; } private final void maybeApplyDeletes ( boolean doMerge ) throws CorruptIndexException , IOException { if ( bufferedDeleteTerms . size ( ) > 0 ) { if ( infoStream != null ) infoStream . println ( "flush " + numBufferedDeleteTerms + " buffered deleted terms on " + segmentInfos . size ( ) + " segments." ) ; if ( doMerge ) { IndexReader reader = null ; try { reader = SegmentReader . get ( segmentInfos . info ( segmentInfos . size ( ) - 1 ) ) ; applyDeletesSelectively ( bufferedDeleteTerms , reader ) ; } finally { if ( reader != null ) { try { reader . doCommit ( ) ; } finally { reader . doClose ( ) ; } } } } int infosEnd = segmentInfos . size ( ) ; if ( doMerge ) { infosEnd -- ; } for ( int i = 0 ; i < infosEnd ; i ++ ) { IndexReader reader = null ; try { reader = SegmentReader . get ( segmentInfos . info ( i ) ) ; applyDeletes ( bufferedDeleteTerms , reader ) ; } finally { if ( reader != null ) { try { reader . doCommit ( ) ; } finally { reader . doClose ( ) ; } } } } bufferedDeleteTerms . clear ( ) ; numBufferedDeleteTerms = 0 ; } } private final boolean checkNonDecreasingLevels ( int start ) { int lowerBound = - 1 ; int upperBound = minMergeDocs ; for ( int i = segmentInfos . size ( ) - 1 ; i >= start ; i -- ) { int docCount = segmentInfos . info ( i ) . docCount ; if ( docCount <= lowerBound ) { return false ; } while ( docCount > upperBound ) { lowerBound = upperBound ; upperBound *= mergeFactor ; } } return true ; } final synchronized int getBufferedDeleteTermsSize ( ) { return bufferedDeleteTerms . size ( ) ; } final synchronized int getNumBufferedDeleteTerms ( ) { return numBufferedDeleteTerms ; } private static class Num { private int num ; Num ( int num ) { this . num = num ; } int getNum ( ) { return num ; } void setNum ( int num ) { this . num = num ; } } private void bufferDeleteTerm ( Term term ) { Num num = ( Num ) bufferedDeleteTerms . get ( term ) ; if ( num == null ) { bufferedDeleteTerms . put ( term , new Num ( ramSegmentInfos . size ( ) ) ) ; } else { num . setNum ( ramSegmentInfos . size ( ) ) ; } numBufferedDeleteTerms ++ ; } private final void applyDeletesSelectively ( HashMap deleteTerms , IndexReader reader ) throws CorruptIndexException , IOException { Iterator iter = deleteTerms . entrySet ( ) . iterator ( ) ; while ( iter . hasNext ( ) ) { Entry entry = ( Entry ) iter . next ( ) ; Term term = ( Term ) entry . getKey ( ) ; TermDocs docs = reader . termDocs ( term ) ; if ( docs != null ) { int num = ( ( Num ) entry . getValue ( ) ) . getNum ( ) ; try { while ( docs . next ( ) ) { int doc = docs . doc ( ) ; if ( doc >= num ) { break ; } reader . deleteDocument ( doc ) ; } } finally { docs . close ( ) ; } } } } private final void applyDeletes ( HashMap deleteTerms , IndexReader reader ) throws CorruptIndexException , IOException { Iterator iter = deleteTerms . entrySet ( ) . iterator ( ) ; while ( iter . hasNext ( ) ) { Entry entry = ( Entry ) iter . next ( ) ; reader . deleteDocuments ( ( Term ) entry . getKey ( ) ) ; } } } 	1	['80', '1', '0', '23', '171', '1344', '4', '22', '52', '0.843178622', '2846', '0.722222222', '10', '0', '0.1005996', '0', '0', '34.125', '4', '0.9875', '47']
package org . apache . lucene . store ; import java . io . IOException ; import java . util . zip . CRC32 ; import java . util . zip . Checksum ; public class ChecksumIndexOutput extends IndexOutput { IndexOutput main ; Checksum digest ; public ChecksumIndexOutput ( IndexOutput main ) { this . main = main ; digest = new CRC32 ( ) ; } public void writeByte ( byte b ) throws IOException { digest . update ( b ) ; main . writeByte ( b ) ; } public void writeBytes ( byte [ ] b , int offset , int length ) throws IOException { digest . update ( b , offset , length ) ; main . writeBytes ( b , offset , length ) ; } public long getChecksum ( ) { return digest . getValue ( ) ; } public void flush ( ) throws IOException { main . flush ( ) ; } public void close ( ) throws IOException { main . close ( ) ; } public long getFilePointer ( ) { return main . getFilePointer ( ) ; } public void seek ( long pos ) { throw new RuntimeException ( "not allowed" ) ; } public void prepareCommit ( ) throws IOException { final long checksum = getChecksum ( ) ; final long pos = main . getFilePointer ( ) ; main . writeLong ( checksum - 1 ) ; main . flush ( ) ; main . seek ( pos ) ; } public void finishCommit ( ) throws IOException { main . writeLong ( getChecksum ( ) ) ; } public long length ( ) throws IOException { return main . length ( ) ; } } 	0	['11', '2', '0', '2', '25', '0', '1', '1', '11', '0.35', '98', '0', '1', '0.62962963', '0.242424242', '1', '5', '7.727272727', '1', '0.9091', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . BufferedIndexInput ; final class TermInfosReader { private Directory directory ; private String segment ; private FieldInfos fieldInfos ; private ThreadLocal enumerators = new ThreadLocal ( ) ; private SegmentTermEnum origEnum ; private long size ; private Term [ ] indexTerms = null ; private TermInfo [ ] indexInfos ; private long [ ] indexPointers ; private SegmentTermEnum indexEnum ; TermInfosReader ( Directory dir , String seg , FieldInfos fis ) throws CorruptIndexException , IOException { this ( dir , seg , fis , BufferedIndexInput . BUFFER_SIZE ) ; } TermInfosReader ( Directory dir , String seg , FieldInfos fis , int readBufferSize ) throws CorruptIndexException , IOException { directory = dir ; segment = seg ; fieldInfos = fis ; origEnum = new SegmentTermEnum ( directory . openInput ( segment + ".tis" , readBufferSize ) , fieldInfos , false ) ; size = origEnum . size ; indexEnum = new SegmentTermEnum ( directory . openInput ( segment + ".tii" , readBufferSize ) , fieldInfos , true ) ; } public int getSkipInterval ( ) { return origEnum . skipInterval ; } public int getMaxSkipLevels ( ) { return origEnum . maxSkipLevels ; } final void close ( ) throws IOException { if ( origEnum != null ) origEnum . close ( ) ; if ( indexEnum != null ) indexEnum . close ( ) ; enumerators . set ( null ) ; } final long size ( ) { return size ; } private SegmentTermEnum getEnum ( ) { SegmentTermEnum termEnum = ( SegmentTermEnum ) enumerators . get ( ) ; if ( termEnum == null ) { termEnum = terms ( ) ; enumerators . set ( termEnum ) ; } return termEnum ; } private synchronized void ensureIndexIsRead ( ) throws IOException { if ( indexTerms != null ) return ; try { int indexSize = ( int ) indexEnum . size ; indexTerms = new Term [ indexSize ] ; indexInfos = new TermInfo [ indexSize ] ; indexPointers = new long [ indexSize ] ; for ( int i = 0 ; indexEnum . next ( ) ; i ++ ) { indexTerms [ i ] = indexEnum . term ( ) ; indexInfos [ i ] = indexEnum . termInfo ( ) ; indexPointers [ i ] = indexEnum . indexPointer ; } } finally { indexEnum . close ( ) ; indexEnum = null ; } } private final int getIndexOffset ( Term term ) { int lo = 0 ; int hi = indexTerms . length - 1 ; while ( hi >= lo ) { int mid = ( lo + hi ) > > 1 ; int delta = term . compareTo ( indexTerms [ mid ] ) ; if ( delta < 0 ) hi = mid - 1 ; else if ( delta > 0 ) lo = mid + 1 ; else return mid ; } return hi ; } private final void seekEnum ( int indexOffset ) throws IOException { getEnum ( ) . seek ( indexPointers [ indexOffset ] , ( indexOffset * getEnum ( ) . indexInterval ) - 1 , indexTerms [ indexOffset ] , indexInfos [ indexOffset ] ) ; } TermInfo get ( Term term ) throws IOException { if ( size == 0 ) return null ; ensureIndexIsRead ( ) ; SegmentTermEnum enumerator = getEnum ( ) ; if ( enumerator . term ( ) != null && ( ( enumerator . prev ( ) != null && term . compareTo ( enumerator . prev ( ) ) > 0 ) || term . compareTo ( enumerator . term ( ) ) >= 0 ) ) { int enumOffset = ( int ) ( enumerator . position / enumerator . indexInterval ) + 1 ; if ( indexTerms . length == enumOffset || term . compareTo ( indexTerms [ enumOffset ] ) < 0 ) return scanEnum ( term ) ; } seekEnum ( getIndexOffset ( term ) ) ; return scanEnum ( term ) ; } private final TermInfo scanEnum ( Term term ) throws IOException { SegmentTermEnum enumerator = getEnum ( ) ; enumerator . scanTo ( term ) ; if ( enumerator . term ( ) != null && term . compareTo ( enumerator . term ( ) ) == 0 ) return enumerator . termInfo ( ) ; else return null ; } final Term get ( int position ) throws IOException { if ( size == 0 ) return null ; SegmentTermEnum enumerator = getEnum ( ) ; if ( enumerator != null && enumerator . term ( ) != null && position >= enumerator . position && position < ( enumerator . position + enumerator . indexInterval ) ) return scanEnum ( position ) ; seekEnum ( position / enumerator . indexInterval ) ; return scanEnum ( position ) ; } private final Term scanEnum ( int position ) throws IOException { SegmentTermEnum enumerator = getEnum ( ) ; while ( enumerator . position < position ) if ( ! enumerator . next ( ) ) return null ; return enumerator . term ( ) ; } final long getPosition ( Term term ) throws IOException { if ( size == 0 ) return - 1 ; ensureIndexIsRead ( ) ; int indexOffset = getIndexOffset ( term ) ; seekEnum ( indexOffset ) ; SegmentTermEnum enumerator = getEnum ( ) ; while ( term . compareTo ( enumerator . term ( ) ) > 0 && enumerator . next ( ) ) { } if ( term . compareTo ( enumerator . term ( ) ) == 0 ) return enumerator . position ; else return - 1 ; } public SegmentTermEnum terms ( ) { return ( SegmentTermEnum ) origEnum . clone ( ) ; } public SegmentTermEnum terms ( Term term ) throws IOException { get ( term ) ; return ( SegmentTermEnum ) getEnum ( ) . clone ( ) ; } } 	1	['17', '1', '0', '9', '35', '72', '2', '7', '4', '0.65', '474', '1', '6', '0', '0.31372549', '0', '0', '26.29411765', '4', '1.1176', '4']
package org . apache . lucene . index ; import java . io . IOException ; abstract class DocConsumerPerThread { abstract DocumentsWriter . DocWriter processDocument ( ) throws IOException ; abstract void abort ( ) ; } 	0	['3', '1', '1', '6', '4', '3', '5', '1', '0', '2', '6', '0', '0', '0', '1', '0', '0', '1', '1', '0.6667', '0']
package org . apache . lucene . store ; import java . io . IOException ; import java . io . File ; import java . io . RandomAccessFile ; import java . nio . ByteBuffer ; import java . nio . channels . FileChannel ; import java . nio . channels . FileChannel . MapMode ; public class MMapDirectory extends FSDirectory { private static class MMapIndexInput extends IndexInput { private ByteBuffer buffer ; private final long length ; private MMapIndexInput ( RandomAccessFile raf ) throws IOException { this . length = raf . length ( ) ; this . buffer = raf . getChannel ( ) . map ( MapMode . READ_ONLY , 0 , length ) ; } public byte readByte ( ) throws IOException { return buffer . get ( ) ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { buffer . get ( b , offset , len ) ; } public long getFilePointer ( ) { return buffer . position ( ) ; } public void seek ( long pos ) throws IOException { buffer . position ( ( int ) pos ) ; } public long length ( ) { return length ; } public Object clone ( ) { MMapIndexInput clone = ( MMapIndexInput ) super . clone ( ) ; clone . buffer = buffer . duplicate ( ) ; return clone ; } public void close ( ) throws IOException { } } private static class MultiMMapIndexInput extends IndexInput { private ByteBuffer [ ] buffers ; private int [ ] bufSizes ; private final long length ; private int curBufIndex ; private final int maxBufSize ; private ByteBuffer curBuf ; private int curAvail ; public MultiMMapIndexInput ( RandomAccessFile raf , int maxBufSize ) throws IOException { this . length = raf . length ( ) ; this . maxBufSize = maxBufSize ; if ( maxBufSize <= 0 ) throw new IllegalArgumentException ( "Non positive maxBufSize: " + maxBufSize ) ; if ( ( length / maxBufSize ) > Integer . MAX_VALUE ) throw new IllegalArgumentException ( "RandomAccessFile too big for maximum buffer size: " + raf . toString ( ) ) ; int nrBuffers = ( int ) ( length / maxBufSize ) ; if ( ( nrBuffers * maxBufSize ) < length ) nrBuffers ++ ; this . buffers = new ByteBuffer [ nrBuffers ] ; this . bufSizes = new int [ nrBuffers ] ; long bufferStart = 0 ; FileChannel rafc = raf . getChannel ( ) ; for ( int bufNr = 0 ; bufNr < nrBuffers ; bufNr ++ ) { int bufSize = ( length > ( bufferStart + maxBufSize ) ) ? maxBufSize : ( int ) ( length - bufferStart ) ; this . buffers [ bufNr ] = rafc . map ( MapMode . READ_ONLY , bufferStart , bufSize ) ; this . bufSizes [ bufNr ] = bufSize ; bufferStart += bufSize ; } seek ( 0L ) ; } public byte readByte ( ) throws IOException { if ( curAvail == 0 ) { curBufIndex ++ ; curBuf = buffers [ curBufIndex ] ; curBuf . position ( 0 ) ; curAvail = bufSizes [ curBufIndex ] ; } curAvail -- ; return curBuf . get ( ) ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { while ( len > curAvail ) { curBuf . get ( b , offset , curAvail ) ; len -= curAvail ; offset += curAvail ; curBufIndex ++ ; curBuf = buffers [ curBufIndex ] ; curBuf . position ( 0 ) ; curAvail = bufSizes [ curBufIndex ] ; } curBuf . get ( b , offset , len ) ; curAvail -= len ; } public long getFilePointer ( ) { return ( curBufIndex * ( long ) maxBufSize ) + curBuf . position ( ) ; } public void seek ( long pos ) throws IOException { curBufIndex = ( int ) ( pos / maxBufSize ) ; curBuf = buffers [ curBufIndex ] ; int bufOffset = ( int ) ( pos - ( curBufIndex * maxBufSize ) ) ; curBuf . position ( bufOffset ) ; curAvail = bufSizes [ curBufIndex ] - bufOffset ; } public long length ( ) { return length ; } public Object clone ( ) { MultiMMapIndexInput clone = ( MultiMMapIndexInput ) super . clone ( ) ; clone . buffers = new ByteBuffer [ buffers . length ] ; for ( int bufNr = 0 ; bufNr < buffers . length ; bufNr ++ ) { clone . buffers [ bufNr ] = buffers [ bufNr ] . duplicate ( ) ; } try { clone . seek ( getFilePointer ( ) ) ; } catch ( IOException ioe ) { RuntimeException newException = new RuntimeException ( ioe ) ; newException . initCause ( ioe ) ; throw newException ; } ; return clone ; } public void close ( ) throws IOException { } } private final int MAX_BBUF = Integer . MAX_VALUE ; public IndexInput openInput ( String name ) throws IOException { File f = new File ( getFile ( ) , name ) ; RandomAccessFile raf = new RandomAccessFile ( f , "r" ) ; try { return ( raf . length ( ) <= MAX_BBUF ) ? ( IndexInput ) new MMapIndexInput ( raf ) : ( IndexInput ) new MultiMMapIndexInput ( raf , MAX_BBUF ) ; } finally { raf . close ( ) ; } } } 	1	['2', '3', '0', '5', '10', '1', '0', '5', '2', '1', '48', '1', '0', '0.977272727', '0.75', '1', '2', '22.5', '1', '0.5', '1']
package org . apache . lucene . analysis ; import java . io . IOException ; import java . util . ArrayList ; import java . util . Iterator ; import java . util . List ; public class SinkTokenizer extends Tokenizer { protected List lst = new ArrayList ( ) ; protected Iterator iter ; public SinkTokenizer ( List input ) { this . lst = input ; if ( this . lst == null ) this . lst = new ArrayList ( ) ; } public SinkTokenizer ( ) { this . lst = new ArrayList ( ) ; } public SinkTokenizer ( int initCap ) { this . lst = new ArrayList ( initCap ) ; } public List getTokens ( ) { return lst ; } public Token next ( final Token reusableToken ) throws IOException { assert reusableToken != null ; if ( iter == null ) iter = lst . iterator ( ) ; if ( iter . hasNext ( ) ) { Token nextToken = ( Token ) iter . next ( ) ; return ( Token ) nextToken . clone ( ) ; } return null ; } public void add ( Token t ) { if ( t == null ) return ; lst . add ( ( Token ) t . clone ( ) ) ; } public void close ( ) throws IOException { input = null ; lst = null ; } public void reset ( ) throws IOException { iter = lst . iterator ( ) ; } } 	0	['10', '3', '0', '3', '23', '0', '1', '2', '8', '0.75', '143', '0.5', '0', '0.538461538', '0.288888889', '2', '2', '12.9', '2', '0.7', '0']
package org . apache . lucene . index ; final class FieldInfo { String name ; boolean isIndexed ; int number ; boolean storeTermVector ; boolean storeOffsetWithTermVector ; boolean storePositionWithTermVector ; boolean omitNorms ; boolean storePayloads ; FieldInfo ( String na , boolean tk , int nu , boolean storeTermVector , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector , boolean omitNorms , boolean storePayloads ) { name = na ; isIndexed = tk ; number = nu ; this . storeTermVector = storeTermVector ; this . storeOffsetWithTermVector = storeOffsetWithTermVector ; this . storePositionWithTermVector = storePositionWithTermVector ; this . omitNorms = omitNorms ; this . storePayloads = storePayloads ; } } 	1	['1', '1', '0', '8', '2', '0', '8', '0', '0', '2', '36', '0', '0', '0', '1', '0', '0', '27', '0', '0', '1']
package org . apache . lucene . index ; import java . util . Comparator ; public class TermVectorEntryFreqSortedComparator implements Comparator { public int compare ( Object object , Object object1 ) { int result = 0 ; TermVectorEntry entry = ( TermVectorEntry ) object ; TermVectorEntry entry1 = ( TermVectorEntry ) object1 ; result = entry1 . getFrequency ( ) - entry . getFrequency ( ) ; if ( result == 0 ) { result = entry . getTerm ( ) . compareTo ( entry1 . getTerm ( ) ) ; if ( result == 0 ) { result = entry . getField ( ) . compareTo ( entry1 . getField ( ) ) ; } } return result ; } } 	0	['2', '1', '0', '1', '7', '1', '0', '1', '2', '2', '37', '0', '0', '0', '0.75', '0', '0', '17.5', '3', '1.5', '0']
package org . apache . lucene . search ; import java . util . Iterator ; import java . util . NoSuchElementException ; public class HitIterator implements Iterator { private Hits hits ; private int hitNumber = 0 ; HitIterator ( Hits hits ) { this . hits = hits ; } public boolean hasNext ( ) { return hitNumber < hits . length ( ) ; } public Object next ( ) { if ( hitNumber == hits . length ( ) ) throw new NoSuchElementException ( ) ; Object next = new Hit ( hits , hitNumber ) ; hitNumber ++ ; return next ; } public void remove ( ) { throw new UnsupportedOperationException ( ) ; } public int length ( ) { return hits . length ( ) ; } } 	1	['5', '1', '0', '2', '10', '0', '1', '2', '4', '0.375', '60', '1', '1', '0', '0.6', '0', '0', '10.6', '2', '1.2', '2']
package org . apache . lucene ; public final class LucenePackage { private LucenePackage ( ) { } public static Package get ( ) { return LucenePackage . class . getPackage ( ) ; } } 	0	['3', '1', '0', '0', '8', '3', '0', '0', '1', '1', '27', '0', '0', '0', '0.333333333', '0', '0', '7.666666667', '2', '1', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . ArrayList ; import java . util . List ; import java . util . Iterator ; class BooleanScorer2 extends Scorer { private ArrayList requiredScorers = new ArrayList ( ) ; private ArrayList optionalScorers = new ArrayList ( ) ; private ArrayList prohibitedScorers = new ArrayList ( ) ; private class Coordinator { int maxCoord = 0 ; private float [ ] coordFactors = null ; void init ( ) { coordFactors = new float [ maxCoord + 1 ] ; Similarity sim = getSimilarity ( ) ; for ( int i = 0 ; i <= maxCoord ; i ++ ) { coordFactors [ i ] = sim . coord ( i , maxCoord ) ; } } int nrMatchers ; void initDoc ( ) { nrMatchers = 0 ; } float coordFactor ( ) { return coordFactors [ nrMatchers ] ; } } private final Coordinator coordinator ; private Scorer countingSumScorer = null ; private final int minNrShouldMatch ; private boolean allowDocsOutOfOrder ; public BooleanScorer2 ( Similarity similarity , int minNrShouldMatch , boolean allowDocsOutOfOrder ) { super ( similarity ) ; if ( minNrShouldMatch < 0 ) { throw new IllegalArgumentException ( "Minimum number of optional scorers should not be negative" ) ; } coordinator = new Coordinator ( ) ; this . minNrShouldMatch = minNrShouldMatch ; this . allowDocsOutOfOrder = allowDocsOutOfOrder ; } public BooleanScorer2 ( Similarity similarity , int minNrShouldMatch ) { this ( similarity , minNrShouldMatch , false ) ; } public BooleanScorer2 ( Similarity similarity ) { this ( similarity , 0 , false ) ; } public void add ( final Scorer scorer , boolean required , boolean prohibited ) { if ( ! prohibited ) { coordinator . maxCoord ++ ; } if ( required ) { if ( prohibited ) { throw new IllegalArgumentException ( "scorer cannot be required and prohibited" ) ; } requiredScorers . add ( scorer ) ; } else if ( prohibited ) { prohibitedScorers . add ( scorer ) ; } else { optionalScorers . add ( scorer ) ; } } private void initCountingSumScorer ( ) { coordinator . init ( ) ; countingSumScorer = makeCountingSumScorer ( ) ; } private class SingleMatchScorer extends Scorer { private Scorer scorer ; private int lastScoredDoc = - 1 ; SingleMatchScorer ( Scorer scorer ) { super ( scorer . getSimilarity ( ) ) ; this . scorer = scorer ; } public float score ( ) throws IOException { if ( this . doc ( ) >= lastScoredDoc ) { lastScoredDoc = this . doc ( ) ; coordinator . nrMatchers ++ ; } return scorer . score ( ) ; } public int doc ( ) { return scorer . doc ( ) ; } public boolean next ( ) throws IOException { return scorer . next ( ) ; } public boolean skipTo ( int docNr ) throws IOException { return scorer . skipTo ( docNr ) ; } public Explanation explain ( int docNr ) throws IOException { return scorer . explain ( docNr ) ; } } private Scorer countingDisjunctionSumScorer ( final List scorers , int minNrShouldMatch ) { return new DisjunctionSumScorer ( scorers , minNrShouldMatch ) { private int lastScoredDoc = - 1 ; public float score ( ) throws IOException { if ( this . doc ( ) >= lastScoredDoc ) { lastScoredDoc = this . doc ( ) ; coordinator . nrMatchers += super . nrMatchers ; } return super . score ( ) ; } } ; } private static Similarity defaultSimilarity = new DefaultSimilarity ( ) ; private Scorer countingConjunctionSumScorer ( List requiredScorers ) { final int requiredNrMatchers = requiredScorers . size ( ) ; ConjunctionScorer cs = new ConjunctionScorer ( defaultSimilarity ) { private int lastScoredDoc = - 1 ; public float score ( ) throws IOException { if ( this . doc ( ) >= lastScoredDoc ) { lastScoredDoc = this . doc ( ) ; coordinator . nrMatchers += requiredNrMatchers ; } return super . score ( ) ; } } ; Iterator rsi = requiredScorers . iterator ( ) ; while ( rsi . hasNext ( ) ) { cs . add ( ( Scorer ) rsi . next ( ) ) ; } return cs ; } private Scorer dualConjunctionSumScorer ( Scorer req1 , Scorer req2 ) { ConjunctionScorer cs = new ConjunctionScorer ( defaultSimilarity ) ; cs . add ( req1 ) ; cs . add ( req2 ) ; return cs ; } private Scorer makeCountingSumScorer ( ) { return ( requiredScorers . size ( ) == 0 ) ? makeCountingSumScorerNoReq ( ) : makeCountingSumScorerSomeReq ( ) ; } private Scorer makeCountingSumScorerNoReq ( ) { if ( optionalScorers . size ( ) == 0 ) { return new NonMatchingScorer ( ) ; } else { int nrOptRequired = ( minNrShouldMatch < 1 ) ? 1 : minNrShouldMatch ; if ( optionalScorers . size ( ) < nrOptRequired ) { return new NonMatchingScorer ( ) ; } else { Scorer requiredCountingSumScorer = ( optionalScorers . size ( ) > nrOptRequired ) ? countingDisjunctionSumScorer ( optionalScorers , nrOptRequired ) : ( optionalScorers . size ( ) == 1 ) ? new SingleMatchScorer ( ( Scorer ) optionalScorers . get ( 0 ) ) : countingConjunctionSumScorer ( optionalScorers ) ; return addProhibitedScorers ( requiredCountingSumScorer ) ; } } } private Scorer makeCountingSumScorerSomeReq ( ) { if ( optionalScorers . size ( ) < minNrShouldMatch ) { return new NonMatchingScorer ( ) ; } else if ( optionalScorers . size ( ) == minNrShouldMatch ) { ArrayList allReq = new ArrayList ( requiredScorers ) ; allReq . addAll ( optionalScorers ) ; return addProhibitedScorers ( countingConjunctionSumScorer ( allReq ) ) ; } else { Scorer requiredCountingSumScorer = ( requiredScorers . size ( ) == 1 ) ? new SingleMatchScorer ( ( Scorer ) requiredScorers . get ( 0 ) ) : countingConjunctionSumScorer ( requiredScorers ) ; if ( minNrShouldMatch > 0 ) { return addProhibitedScorers ( dualConjunctionSumScorer ( requiredCountingSumScorer , countingDisjunctionSumScorer ( optionalScorers , minNrShouldMatch ) ) ) ; } else { return new ReqOptSumScorer ( addProhibitedScorers ( requiredCountingSumScorer ) , ( ( optionalScorers . size ( ) == 1 ) ? new SingleMatchScorer ( ( Scorer ) optionalScorers . get ( 0 ) ) : countingDisjunctionSumScorer ( optionalScorers , 1 ) ) ) ; } } } private Scorer addProhibitedScorers ( Scorer requiredCountingSumScorer ) { return ( prohibitedScorers . size ( ) == 0 ) ? requiredCountingSumScorer : new ReqExclScorer ( requiredCountingSumScorer , ( ( prohibitedScorers . size ( ) == 1 ) ? ( Scorer ) prohibitedScorers . get ( 0 ) : new DisjunctionSumScorer ( prohibitedScorers ) ) ) ; } public void score ( HitCollector hc ) throws IOException { if ( allowDocsOutOfOrder && requiredScorers . size ( ) == 0 && prohibitedScorers . size ( ) < 32 ) { BooleanScorer bs = new BooleanScorer ( getSimilarity ( ) , minNrShouldMatch ) ; Iterator si = optionalScorers . iterator ( ) ; while ( si . hasNext ( ) ) { bs . add ( ( Scorer ) si . next ( ) , false , false ) ; } si = prohibitedScorers . iterator ( ) ; while ( si . hasNext ( ) ) { bs . add ( ( Scorer ) si . next ( ) , false , true ) ; } bs . score ( hc ) ; } else { if ( countingSumScorer == null ) { initCountingSumScorer ( ) ; } while ( countingSumScorer . next ( ) ) { hc . collect ( countingSumScorer . doc ( ) , score ( ) ) ; } } } protected boolean score ( HitCollector hc , int max ) throws IOException { int docNr = countingSumScorer . doc ( ) ; while ( docNr < max ) { hc . collect ( docNr , score ( ) ) ; if ( ! countingSumScorer . next ( ) ) { return false ; } docNr = countingSumScorer . doc ( ) ; } return true ; } public int doc ( ) { return countingSumScorer . doc ( ) ; } public boolean next ( ) throws IOException { if ( countingSumScorer == null ) { initCountingSumScorer ( ) ; } return countingSumScorer . next ( ) ; } public float score ( ) throws IOException { coordinator . initDoc ( ) ; float sum = countingSumScorer . score ( ) ; return sum * coordinator . coordFactor ( ) ; } public boolean skipTo ( int target ) throws IOException { if ( countingSumScorer == null ) { initCountingSumScorer ( ) ; } return countingSumScorer . skipTo ( target ) ; } public Explanation explain ( int doc ) { throw new UnsupportedOperationException ( ) ; } } 	1	['21', '2', '0', '16', '58', '104', '5', '15', '10', '0.65625', '511', '1', '3', '0.32', '0.2375', '1', '3', '22.95238095', '6', '1.6667', '1']
package org . apache . lucene . index ; abstract class RawPostingList { final static int BYTES_SIZE = DocumentsWriter . OBJECT_HEADER_BYTES + 3 * DocumentsWriter . INT_NUM_BYTE ; int textStart ; int intStart ; int byteStart ; } 	0	['1', '1', '2', '12', '2', '0', '12', '0', '0', '2', '8', '0', '0', '0', '1', '0', '0', '3', '0', '0', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . analysis . Token ; final class DocInverterPerThread extends DocFieldConsumerPerThread { final DocInverter docInverter ; final InvertedDocConsumerPerThread consumer ; final InvertedDocEndConsumerPerThread endConsumer ; final Token localToken = new Token ( ) ; final DocumentsWriter . DocState docState ; final DocInverter . FieldInvertState fieldState = new DocInverter . FieldInvertState ( ) ; final ReusableStringReader stringReader = new ReusableStringReader ( ) ; public DocInverterPerThread ( DocFieldProcessorPerThread docFieldProcessorPerThread , DocInverter docInverter ) { this . docInverter = docInverter ; docState = docFieldProcessorPerThread . docState ; consumer = docInverter . consumer . addThread ( this ) ; endConsumer = docInverter . endConsumer . addThread ( this ) ; } public void startDocument ( ) throws IOException { consumer . startDocument ( ) ; endConsumer . startDocument ( ) ; } public DocumentsWriter . DocWriter finishDocument ( ) throws IOException { endConsumer . finishDocument ( ) ; return consumer . finishDocument ( ) ; } void abort ( ) { try { consumer . abort ( ) ; } finally { endConsumer . abort ( ) ; } } public DocFieldConsumerPerField addField ( FieldInfo fi ) { return new DocInverterPerField ( this , fi ) ; } } 	1	['5', '2', '0', '19', '18', '0', '8', '15', '4', '0.785714286', '84', '0', '7', '0.5', '0.4', '0', '0', '14.4', '3', '1.2', '1']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . document . Document ; import org . apache . lucene . index . CorruptIndexException ; public class Hit implements java . io . Serializable { private Document doc = null ; private boolean resolved = false ; private Hits hits = null ; private int hitNumber ; Hit ( Hits hits , int hitNumber ) { this . hits = hits ; this . hitNumber = hitNumber ; } public Document getDocument ( ) throws CorruptIndexException , IOException { if ( ! resolved ) fetchTheHit ( ) ; return doc ; } public float getScore ( ) throws IOException { return hits . score ( hitNumber ) ; } public int getId ( ) throws IOException { return hits . id ( hitNumber ) ; } private void fetchTheHit ( ) throws CorruptIndexException , IOException { doc = hits . doc ( hitNumber ) ; resolved = true ; } public float getBoost ( ) throws CorruptIndexException , IOException { return getDocument ( ) . getBoost ( ) ; } public String get ( String name ) throws CorruptIndexException , IOException { return getDocument ( ) . get ( name ) ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "Hit<" ) ; buffer . append ( hits . toString ( ) ) ; buffer . append ( " [" ) ; buffer . append ( hitNumber ) ; buffer . append ( "] " ) ; if ( resolved ) { buffer . append ( "resolved" ) ; } else { buffer . append ( "unresolved" ) ; } buffer . append ( ">" ) ; return buffer . toString ( ) ; } } 	0	['8', '1', '0', '4', '19', '2', '1', '3', '6', '0.178571429', '116', '1', '2', '0', '0.34375', '0', '0', '13', '2', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . util . PriorityQueue ; import org . apache . lucene . util . ToStringUtils ; import java . io . IOException ; public class FuzzyQuery extends MultiTermQuery { public final static float defaultMinSimilarity = 0.5f ; public final static int defaultPrefixLength = 0 ; private float minimumSimilarity ; private int prefixLength ; public FuzzyQuery ( Term term , float minimumSimilarity , int prefixLength ) throws IllegalArgumentException { super ( term ) ; if ( minimumSimilarity >= 1.0f ) throw new IllegalArgumentException ( "minimumSimilarity >= 1" ) ; else if ( minimumSimilarity < 0.0f ) throw new IllegalArgumentException ( "minimumSimilarity < 0" ) ; if ( prefixLength < 0 ) throw new IllegalArgumentException ( "prefixLength < 0" ) ; this . minimumSimilarity = minimumSimilarity ; this . prefixLength = prefixLength ; } public FuzzyQuery ( Term term , float minimumSimilarity ) throws IllegalArgumentException { this ( term , minimumSimilarity , defaultPrefixLength ) ; } public FuzzyQuery ( Term term ) { this ( term , defaultMinSimilarity , defaultPrefixLength ) ; } public float getMinSimilarity ( ) { return minimumSimilarity ; } public int getPrefixLength ( ) { return prefixLength ; } protected FilteredTermEnum getEnum ( IndexReader reader ) throws IOException { return new FuzzyTermEnum ( reader , getTerm ( ) , minimumSimilarity , prefixLength ) ; } public Query rewrite ( IndexReader reader ) throws IOException { FilteredTermEnum enumerator = getEnum ( reader ) ; int maxClauseCount = BooleanQuery . getMaxClauseCount ( ) ; ScoreTermQueue stQueue = new ScoreTermQueue ( maxClauseCount ) ; ScoreTerm reusableST = null ; try { do { float score = 0.0f ; Term t = enumerator . term ( ) ; if ( t != null ) { score = enumerator . difference ( ) ; if ( reusableST == null ) { reusableST = new ScoreTerm ( t , score ) ; } else if ( score >= reusableST . score ) { reusableST . score = score ; reusableST . term = t ; } else { continue ; } reusableST = ( ScoreTerm ) stQueue . insertWithOverflow ( reusableST ) ; } } while ( enumerator . next ( ) ) ; } finally { enumerator . close ( ) ; } BooleanQuery query = new BooleanQuery ( true ) ; int size = stQueue . size ( ) ; for ( int i = 0 ; i < size ; i ++ ) { ScoreTerm st = ( ScoreTerm ) stQueue . pop ( ) ; TermQuery tq = new TermQuery ( st . term ) ; tq . setBoost ( getBoost ( ) * st . score ) ; query . add ( tq , BooleanClause . Occur . SHOULD ) ; } return query ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; Term term = getTerm ( ) ; if ( ! term . field ( ) . equals ( field ) ) { buffer . append ( term . field ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( term . text ( ) ) ; buffer . append ( '~' ) ; buffer . append ( Float . toString ( minimumSimilarity ) ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } protected static class ScoreTerm { public Term term ; public float score ; public ScoreTerm ( Term term , float score ) { this . term = term ; this . score = score ; } } protected static class ScoreTermQueue extends PriorityQueue { public ScoreTermQueue ( int size ) { initialize ( size ) ; } protected boolean lessThan ( Object a , Object b ) { ScoreTerm termA = ( ScoreTerm ) a ; ScoreTerm termB = ( ScoreTerm ) b ; if ( termA . score == termB . score ) return termA . term . compareTo ( termB . term ) > 0 ; else return termA . score < termB . score ; } } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof FuzzyQuery ) ) return false ; if ( ! super . equals ( o ) ) return false ; final FuzzyQuery fuzzyQuery = ( FuzzyQuery ) o ; if ( minimumSimilarity != fuzzyQuery . minimumSimilarity ) return false ; if ( prefixLength != fuzzyQuery . prefixLength ) return false ; return true ; } public int hashCode ( ) { int result = super . hashCode ( ) ; result = 29 * result + minimumSimilarity != + 0.0f ? Float . floatToIntBits ( minimumSimilarity ) : 0 ; result = 29 * result + prefixLength ; return result ; } } 	1	['10', '3', '0', '13', '41', '7', '1', '12', '9', '0.638888889', '286', '0.5', '0', '0.72', '0.285714286', '2', '6', '27.2', '6', '1.4', '3']
package org . apache . lucene . util ; import java . io . ObjectStreamException ; import java . io . Serializable ; import java . io . StreamCorruptedException ; import java . util . HashMap ; import java . util . Map ; public abstract class Parameter implements Serializable { static Map allParameters = new HashMap ( ) ; private String name ; private Parameter ( ) { } protected Parameter ( String name ) { this . name = name ; String key = makeKey ( name ) ; if ( allParameters . containsKey ( key ) ) throw new IllegalArgumentException ( "Parameter name " + key + " already used!" ) ; allParameters . put ( key , this ) ; } private String makeKey ( String name ) { return getClass ( ) + " " + name ; } public String toString ( ) { return name ; } protected Object readResolve ( ) throws ObjectStreamException { Object par = allParameters . get ( makeKey ( name ) ) ; if ( par == null ) throw new StreamCorruptedException ( "Unknown parameter value: " + name ) ; return par ; } } 	0	['6', '1', '5', '5', '18', '5', '5', '0', '1', '0.6', '88', '0.5', '0', '0', '0.7', '0', '0', '13.33333333', '1', '0.5', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . io . FileNotFoundException ; import java . util . HashSet ; import java . util . Collection ; import java . util . ArrayList ; import java . util . List ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . Lock ; import org . apache . lucene . store . LockObtainFailedException ; abstract class DirectoryIndexReader extends IndexReader { protected Directory directory ; protected boolean closeDirectory ; private IndexDeletionPolicy deletionPolicy ; private SegmentInfos segmentInfos ; private Lock writeLock ; private boolean stale ; private final HashSet synced = new HashSet ( ) ; private boolean rollbackHasChanges ; private SegmentInfos rollbackSegmentInfos ; protected boolean readOnly ; void init ( Directory directory , SegmentInfos segmentInfos , boolean closeDirectory , boolean readOnly ) throws IOException { this . directory = directory ; this . segmentInfos = segmentInfos ; this . closeDirectory = closeDirectory ; this . readOnly = readOnly ; if ( ! readOnly && segmentInfos != null ) { for ( int i = 0 ; i < segmentInfos . size ( ) ; i ++ ) { final SegmentInfo info = segmentInfos . info ( i ) ; List files = info . files ( ) ; for ( int j = 0 ; j < files . size ( ) ; j ++ ) synced . add ( files . get ( j ) ) ; } } } protected DirectoryIndexReader ( ) { } DirectoryIndexReader ( Directory directory , SegmentInfos segmentInfos , boolean closeDirectory , boolean readOnly ) throws IOException { super ( ) ; init ( directory , segmentInfos , closeDirectory , readOnly ) ; } static DirectoryIndexReader open ( final Directory directory , final boolean closeDirectory , final IndexDeletionPolicy deletionPolicy ) throws CorruptIndexException , IOException { return open ( directory , closeDirectory , deletionPolicy , null , false ) ; } static DirectoryIndexReader open ( final Directory directory , final boolean closeDirectory , final IndexDeletionPolicy deletionPolicy , final IndexCommit commit , final boolean readOnly ) throws CorruptIndexException , IOException { SegmentInfos . FindSegmentsFile finder = new SegmentInfos . FindSegmentsFile ( directory ) { protected Object doBody ( String segmentFileName ) throws CorruptIndexException , IOException { SegmentInfos infos = new SegmentInfos ( ) ; infos . read ( directory , segmentFileName ) ; DirectoryIndexReader reader ; if ( infos . size ( ) == 1 ) { reader = SegmentReader . get ( readOnly , infos , infos . info ( 0 ) , closeDirectory ) ; } else if ( readOnly ) { reader = new ReadOnlyMultiSegmentReader ( directory , infos , closeDirectory ) ; } else { reader = new MultiSegmentReader ( directory , infos , closeDirectory , false ) ; } reader . setDeletionPolicy ( deletionPolicy ) ; return reader ; } } ; if ( commit == null ) return ( DirectoryIndexReader ) finder . run ( ) ; else { if ( directory != commit . getDirectory ( ) ) throw new IOException ( "the specified commit does not match the specified Directory" ) ; return ( DirectoryIndexReader ) finder . doBody ( commit . getSegmentsFileName ( ) ) ; } } public final synchronized IndexReader reopen ( ) throws CorruptIndexException , IOException { ensureOpen ( ) ; if ( this . hasChanges || this . isCurrent ( ) ) { return this ; } return ( DirectoryIndexReader ) new SegmentInfos . FindSegmentsFile ( directory ) { protected Object doBody ( String segmentFileName ) throws CorruptIndexException , IOException { SegmentInfos infos = new SegmentInfos ( ) ; infos . read ( directory , segmentFileName ) ; DirectoryIndexReader newReader = doReopen ( infos ) ; if ( DirectoryIndexReader . this != newReader ) { newReader . init ( directory , infos , closeDirectory , readOnly ) ; newReader . deletionPolicy = deletionPolicy ; } return newReader ; } } . run ( ) ; } protected abstract DirectoryIndexReader doReopen ( SegmentInfos infos ) throws CorruptIndexException , IOException ; public void setDeletionPolicy ( IndexDeletionPolicy deletionPolicy ) { this . deletionPolicy = deletionPolicy ; } public Directory directory ( ) { ensureOpen ( ) ; return directory ; } public long getVersion ( ) { ensureOpen ( ) ; return segmentInfos . getVersion ( ) ; } public boolean isCurrent ( ) throws CorruptIndexException , IOException { ensureOpen ( ) ; return SegmentInfos . readCurrentVersion ( directory ) == segmentInfos . getVersion ( ) ; } public boolean isOptimized ( ) { ensureOpen ( ) ; return segmentInfos . size ( ) == 1 && hasDeletions ( ) == false ; } protected void doClose ( ) throws IOException { if ( closeDirectory ) directory . close ( ) ; } protected void doCommit ( ) throws IOException { if ( hasChanges ) { if ( segmentInfos != null ) { IndexFileDeleter deleter = new IndexFileDeleter ( directory , deletionPolicy == null ? new KeepOnlyLastCommitDeletionPolicy ( ) : deletionPolicy , segmentInfos , null , null ) ; startCommit ( ) ; boolean success = false ; try { commitChanges ( ) ; for ( int i = 0 ; i < segmentInfos . size ( ) ; i ++ ) { final SegmentInfo info = segmentInfos . info ( i ) ; final List files = info . files ( ) ; for ( int j = 0 ; j < files . size ( ) ; j ++ ) { final String fileName = ( String ) files . get ( j ) ; if ( ! synced . contains ( fileName ) ) { assert directory . fileExists ( fileName ) ; directory . sync ( fileName ) ; synced . add ( fileName ) ; } } } segmentInfos . commit ( directory ) ; success = true ; } finally { if ( ! success ) { rollbackCommit ( ) ; deleter . refresh ( ) ; } } deleter . checkpoint ( segmentInfos , true ) ; if ( writeLock != null ) { writeLock . release ( ) ; writeLock = null ; } } else commitChanges ( ) ; } hasChanges = false ; } protected abstract void commitChanges ( ) throws IOException ; protected void acquireWriteLock ( ) throws StaleReaderException , CorruptIndexException , LockObtainFailedException , IOException { if ( segmentInfos != null ) { ensureOpen ( ) ; if ( stale ) throw new StaleReaderException ( "IndexReader out of date and no longer valid for delete, undelete, or setNorm operations" ) ; if ( writeLock == null ) { Lock writeLock = directory . makeLock ( IndexWriter . WRITE_LOCK_NAME ) ; if ( ! writeLock . obtain ( IndexWriter . WRITE_LOCK_TIMEOUT ) ) throw new LockObtainFailedException ( "Index locked for write: " + writeLock ) ; this . writeLock = writeLock ; if ( SegmentInfos . readCurrentVersion ( directory ) > segmentInfos . getVersion ( ) ) { stale = true ; this . writeLock . release ( ) ; this . writeLock = null ; throw new StaleReaderException ( "IndexReader out of date and no longer valid for delete, undelete, or setNorm operations" ) ; } } } } void startCommit ( ) { if ( segmentInfos != null ) { rollbackSegmentInfos = ( SegmentInfos ) segmentInfos . clone ( ) ; } rollbackHasChanges = hasChanges ; } void rollbackCommit ( ) { if ( segmentInfos != null ) { for ( int i = 0 ; i < segmentInfos . size ( ) ; i ++ ) { segmentInfos . info ( i ) . reset ( rollbackSegmentInfos . info ( i ) ) ; } rollbackSegmentInfos = null ; } hasChanges = rollbackHasChanges ; } protected void finalize ( ) throws Throwable { try { if ( writeLock != null ) { writeLock . release ( ) ; writeLock = null ; } } finally { super . finalize ( ) ; } } private static class ReaderCommit extends IndexCommit { private String segmentsFileName ; Collection files ; Directory dir ; long generation ; long version ; final boolean isOptimized ; ReaderCommit ( SegmentInfos infos , Directory dir ) throws IOException { segmentsFileName = infos . getCurrentSegmentFileName ( ) ; this . dir = dir ; final int size = infos . size ( ) ; files = new ArrayList ( size ) ; files . add ( segmentsFileName ) ; for ( int i = 0 ; i < size ; i ++ ) { SegmentInfo info = infos . info ( i ) ; if ( info . dir == dir ) files . addAll ( info . files ( ) ) ; } version = infos . getVersion ( ) ; generation = infos . getGeneration ( ) ; isOptimized = infos . size ( ) == 1 && ! infos . info ( 0 ) . hasDeletions ( ) ; } public boolean isOptimized ( ) { return isOptimized ; } public String getSegmentsFileName ( ) { return segmentsFileName ; } public Collection getFileNames ( ) { return files ; } public Directory getDirectory ( ) { return dir ; } public long getVersion ( ) { return version ; } public long getGeneration ( ) { return generation ; } public boolean isDeleted ( ) { return false ; } } public IndexCommit getIndexCommit ( ) throws IOException { return new ReaderCommit ( segmentInfos , directory ) ; } public static Collection listCommits ( Directory dir ) throws IOException { final String [ ] files = dir . list ( ) ; if ( files == null ) throw new IOException ( "cannot read directory " + dir + ": list() returned null" ) ; Collection commits = new ArrayList ( ) ; SegmentInfos latest = new SegmentInfos ( ) ; latest . read ( dir ) ; final long currentGen = latest . getGeneration ( ) ; commits . add ( new ReaderCommit ( latest , dir ) ) ; for ( int i = 0 ; i < files . length ; i ++ ) { final String fileName = files [ i ] ; if ( fileName . startsWith ( IndexFileNames . SEGMENTS ) && ! fileName . equals ( IndexFileNames . SEGMENTS_GEN ) && SegmentInfos . generationFromSegmentsFileName ( fileName ) < currentGen ) { SegmentInfos sis = new SegmentInfos ( ) ; try { sis . read ( dir , fileName ) ; } catch ( FileNotFoundException fnfe ) { sis = null ; } if ( sis != null ) commits . add ( new ReaderCommit ( sis , dir ) ) ; } } return commits ; } } 	1	['25', '2', '2', '20', '82', '160', '5', '18', '8', '0.881944444', '596', '0.833333333', '5', '0.768421053', '0.197916667', '2', '8', '22.36', '3', '1.08', '10']
package org . apache . lucene . search ; public class QueryFilter extends CachingWrapperFilter { public QueryFilter ( Query query ) { super ( new QueryWrapperFilter ( query ) ) ; } public boolean equals ( Object o ) { return super . equals ( ( QueryFilter ) o ) ; } public int hashCode ( ) { return super . hashCode ( ) ^ 0x923F64B9 ; } } 	0	['3', '3', '0', '4', '7', '3', '0', '4', '3', '2', '20', '0', '0', '0.777777778', '0.555555556', '2', '3', '5.666666667', '1', '0.6667', '0']
package org . apache . lucene . index ; import java . io . ByteArrayOutputStream ; import java . io . IOException ; import java . util . Iterator ; import java . util . zip . Deflater ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . RAMOutputStream ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . IndexInput ; final class FieldsWriter { static final byte FIELD_IS_TOKENIZED = 0x1 ; static final byte FIELD_IS_BINARY = 0x2 ; static final byte FIELD_IS_COMPRESSED = 0x4 ; static final int FORMAT = 0 ; static final int FORMAT_VERSION_UTF8_LENGTH_IN_BYTES = 1 ; static final int FORMAT_CURRENT = FORMAT_VERSION_UTF8_LENGTH_IN_BYTES ; private FieldInfos fieldInfos ; private IndexOutput fieldsStream ; private IndexOutput indexStream ; private boolean doClose ; FieldsWriter ( Directory d , String segment , FieldInfos fn ) throws IOException { fieldInfos = fn ; boolean success = false ; final String fieldsName = segment + "." + IndexFileNames . FIELDS_EXTENSION ; try { fieldsStream = d . createOutput ( fieldsName ) ; fieldsStream . writeInt ( FORMAT_CURRENT ) ; success = true ; } finally { if ( ! success ) { try { close ( ) ; } catch ( Throwable t ) { } try { d . deleteFile ( fieldsName ) ; } catch ( Throwable t ) { } } } success = false ; final String indexName = segment + "." + IndexFileNames . FIELDS_INDEX_EXTENSION ; try { indexStream = d . createOutput ( indexName ) ; indexStream . writeInt ( FORMAT_CURRENT ) ; success = true ; } finally { if ( ! success ) { try { close ( ) ; } catch ( IOException ioe ) { } try { d . deleteFile ( fieldsName ) ; } catch ( Throwable t ) { } try { d . deleteFile ( indexName ) ; } catch ( Throwable t ) { } } } doClose = true ; } FieldsWriter ( IndexOutput fdx , IndexOutput fdt , FieldInfos fn ) { fieldInfos = fn ; fieldsStream = fdt ; indexStream = fdx ; doClose = false ; } void setFieldsStream ( IndexOutput stream ) { this . fieldsStream = stream ; } void flushDocument ( int numStoredFields , RAMOutputStream buffer ) throws IOException { indexStream . writeLong ( fieldsStream . getFilePointer ( ) ) ; fieldsStream . writeVInt ( numStoredFields ) ; buffer . writeTo ( fieldsStream ) ; } void skipDocument ( ) throws IOException { indexStream . writeLong ( fieldsStream . getFilePointer ( ) ) ; fieldsStream . writeVInt ( 0 ) ; } void flush ( ) throws IOException { indexStream . flush ( ) ; fieldsStream . flush ( ) ; } final void close ( ) throws IOException { if ( doClose ) { try { if ( fieldsStream != null ) { try { fieldsStream . close ( ) ; } finally { fieldsStream = null ; } } } catch ( IOException ioe ) { try { if ( indexStream != null ) { try { indexStream . close ( ) ; } finally { indexStream = null ; } } } catch ( IOException ioe2 ) { } throw ioe ; } finally { if ( indexStream != null ) { try { indexStream . close ( ) ; } finally { indexStream = null ; } } } } } final void writeField ( FieldInfo fi , Fieldable field ) throws IOException { boolean disableCompression = ( field instanceof FieldsReader . FieldForMerge ) ; fieldsStream . writeVInt ( fi . number ) ; byte bits = 0 ; if ( field . isTokenized ( ) ) bits |= FieldsWriter . FIELD_IS_TOKENIZED ; if ( field . isBinary ( ) ) bits |= FieldsWriter . FIELD_IS_BINARY ; if ( field . isCompressed ( ) ) bits |= FieldsWriter . FIELD_IS_COMPRESSED ; fieldsStream . writeByte ( bits ) ; if ( field . isCompressed ( ) ) { final byte [ ] data ; final int len ; final int offset ; if ( disableCompression ) { data = field . getBinaryValue ( ) ; assert data != null ; len = field . getBinaryLength ( ) ; offset = field . getBinaryOffset ( ) ; } else { if ( field . isBinary ( ) ) { data = compress ( field . getBinaryValue ( ) , field . getBinaryOffset ( ) , field . getBinaryLength ( ) ) ; } else { byte x [ ] = field . stringValue ( ) . getBytes ( "UTF-8" ) ; data = compress ( x , 0 , x . length ) ; } len = data . length ; offset = 0 ; } fieldsStream . writeVInt ( len ) ; fieldsStream . writeBytes ( data , offset , len ) ; } else { if ( field . isBinary ( ) ) { final byte [ ] data ; final int len ; final int offset ; data = field . getBinaryValue ( ) ; len = field . getBinaryLength ( ) ; offset = field . getBinaryOffset ( ) ; fieldsStream . writeVInt ( len ) ; fieldsStream . writeBytes ( data , offset , len ) ; } else { fieldsStream . writeString ( field . stringValue ( ) ) ; } } } final void addRawDocuments ( IndexInput stream , int [ ] lengths , int numDocs ) throws IOException { long position = fieldsStream . getFilePointer ( ) ; long start = position ; for ( int i = 0 ; i < numDocs ; i ++ ) { indexStream . writeLong ( position ) ; position += lengths [ i ] ; } fieldsStream . copyBytes ( stream , position - start ) ; assert fieldsStream . getFilePointer ( ) == position ; } final void addDocument ( Document doc ) throws IOException { indexStream . writeLong ( fieldsStream . getFilePointer ( ) ) ; int storedCount = 0 ; Iterator fieldIterator = doc . getFields ( ) . iterator ( ) ; while ( fieldIterator . hasNext ( ) ) { Fieldable field = ( Fieldable ) fieldIterator . next ( ) ; if ( field . isStored ( ) ) storedCount ++ ; } fieldsStream . writeVInt ( storedCount ) ; fieldIterator = doc . getFields ( ) . iterator ( ) ; while ( fieldIterator . hasNext ( ) ) { Fieldable field = ( Fieldable ) fieldIterator . next ( ) ; if ( field . isStored ( ) ) writeField ( fieldInfos . fieldInfo ( field . name ( ) ) , field ) ; } } private final byte [ ] compress ( byte [ ] input , int offset , int length ) { Deflater compressor = new Deflater ( ) ; compressor . setLevel ( Deflater . BEST_COMPRESSION ) ; compressor . setInput ( input , offset , length ) ; compressor . finish ( ) ; ByteArrayOutputStream bos = new ByteArrayOutputStream ( length ) ; try { compressor . setLevel ( Deflater . BEST_COMPRESSION ) ; compressor . setInput ( input ) ; compressor . finish ( ) ; byte [ ] buf = new byte [ 1024 ] ; while ( ! compressor . finished ( ) ) { int count = compressor . deflate ( buf ) ; bos . write ( buf , 0 , count ) ; } } finally { compressor . end ( ) ; } return bos . toByteArray ( ) ; } } 	1	['13', '1', '0', '13', '61', '0', '4', '9', '0', '0.881944444', '553', '0.333333333', '3', '0', '0.179487179', '0', '0', '40.61538462', '4', '1', '1']
package org . apache . lucene . index ; import java . util . * ; public class FieldSortedTermVectorMapper extends TermVectorMapper { private Map fieldToTerms = new HashMap ( ) ; private SortedSet currentSet ; private String currentField ; private Comparator comparator ; public FieldSortedTermVectorMapper ( Comparator comparator ) { this ( false , false , comparator ) ; } public FieldSortedTermVectorMapper ( boolean ignoringPositions , boolean ignoringOffsets , Comparator comparator ) { super ( ignoringPositions , ignoringOffsets ) ; this . comparator = comparator ; } public void map ( String term , int frequency , TermVectorOffsetInfo [ ] offsets , int [ ] positions ) { TermVectorEntry entry = new TermVectorEntry ( currentField , term , frequency , offsets , positions ) ; currentSet . add ( entry ) ; } public void setExpectations ( String field , int numTerms , boolean storeOffsets , boolean storePositions ) { currentSet = new TreeSet ( comparator ) ; currentField = field ; fieldToTerms . put ( field , currentSet ) ; } public Map getFieldToTerms ( ) { return fieldToTerms ; } public Comparator getComparator ( ) { return comparator ; } } 	0	['6', '2', '0', '3', '12', '3', '0', '3', '6', '0.6', '69', '1', '0', '0.555555556', '0.380952381', '0', '0', '9.833333333', '1', '0.6667', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . IndexInput ; import java . io . IOException ; final class SegmentTermPositions extends SegmentTermDocs implements TermPositions { private IndexInput proxStream ; private int proxCount ; private int position ; private int payloadLength ; private boolean needToLoadPayload ; private long lazySkipPointer = - 1 ; private int lazySkipProxCount = 0 ; SegmentTermPositions ( SegmentReader p ) { super ( p ) ; this . proxStream = null ; } final void seek ( TermInfo ti , Term term ) throws IOException { super . seek ( ti , term ) ; if ( ti != null ) lazySkipPointer = ti . proxPointer ; lazySkipProxCount = 0 ; proxCount = 0 ; payloadLength = 0 ; needToLoadPayload = false ; } public final void close ( ) throws IOException { super . close ( ) ; if ( proxStream != null ) proxStream . close ( ) ; } public final int nextPosition ( ) throws IOException { if ( currentFieldOmitTf ) return 0 ; lazySkip ( ) ; proxCount -- ; return position += readDeltaPosition ( ) ; } private final int readDeltaPosition ( ) throws IOException { int delta = proxStream . readVInt ( ) ; if ( currentFieldStoresPayloads ) { if ( ( delta & 1 ) != 0 ) { payloadLength = proxStream . readVInt ( ) ; } delta >>>= 1 ; needToLoadPayload = true ; } return delta ; } protected final void skippingDoc ( ) throws IOException { lazySkipProxCount += freq ; } public final boolean next ( ) throws IOException { lazySkipProxCount += proxCount ; if ( super . next ( ) ) { proxCount = freq ; position = 0 ; return true ; } return false ; } public final int read ( final int [ ] docs , final int [ ] freqs ) { throw new UnsupportedOperationException ( "TermPositions does not support processing multiple documents in one call. Use TermDocs instead." ) ; } protected void skipProx ( long proxPointer , int payloadLength ) throws IOException { lazySkipPointer = proxPointer ; lazySkipProxCount = 0 ; proxCount = 0 ; this . payloadLength = payloadLength ; needToLoadPayload = false ; } private void skipPositions ( int n ) throws IOException { assert ! currentFieldOmitTf ; for ( int f = n ; f > 0 ; f -- ) { readDeltaPosition ( ) ; skipPayload ( ) ; } } private void skipPayload ( ) throws IOException { if ( needToLoadPayload && payloadLength > 0 ) { proxStream . seek ( proxStream . getFilePointer ( ) + payloadLength ) ; } needToLoadPayload = false ; } private void lazySkip ( ) throws IOException { if ( proxStream == null ) { proxStream = ( IndexInput ) parent . proxStream . clone ( ) ; } skipPayload ( ) ; if ( lazySkipPointer != - 1 ) { proxStream . seek ( lazySkipPointer ) ; lazySkipPointer = - 1 ; } if ( lazySkipProxCount != 0 ) { skipPositions ( lazySkipProxCount ) ; lazySkipProxCount = 0 ; } } public int getPayloadLength ( ) { return payloadLength ; } public byte [ ] getPayload ( byte [ ] data , int offset ) throws IOException { if ( ! needToLoadPayload ) { throw new IOException ( "Payload cannot be loaded more than once for the same term position." ) ; } byte [ ] retArray ; int retOffset ; if ( data == null || data . length - offset < payloadLength ) { retArray = new byte [ payloadLength ] ; retOffset = 0 ; } else { retArray = data ; retOffset = offset ; } proxStream . readBytes ( retArray , retOffset , payloadLength ) ; needToLoadPayload = false ; return retArray ; } public boolean isPayloadAvailable ( ) { return needToLoadPayload && payloadLength > 0 ; } } 	1	['17', '2', '0', '6', '34', '34', '1', '6', '7', '0.722222222', '323', '0.777777778', '1', '0.444444444', '0.173611111', '1', '2', '17.47058824', '3', '1', '3']
package org . apache . lucene . util ; public class SmallFloat { public static byte floatToByte ( float f , int numMantissaBits , int zeroExp ) { int fzero = ( 63 - zeroExp ) << numMantissaBits ; int bits = Float . floatToRawIntBits ( f ) ; int smallfloat = bits > > ( 24 - numMantissaBits ) ; if ( smallfloat < fzero ) { return ( bits <= 0 ) ? ( byte ) 0 : ( byte ) 1 ; } else if ( smallfloat >= fzero + 0x100 ) { return - 1 ; } else { return ( byte ) ( smallfloat - fzero ) ; } } public static float byteToFloat ( byte b , int numMantissaBits , int zeroExp ) { if ( b == 0 ) return 0.0f ; int bits = ( b & 0xff ) << ( 24 - numMantissaBits ) ; bits += ( 63 - zeroExp ) << 24 ; return Float . intBitsToFloat ( bits ) ; } public static byte floatToByte315 ( float f ) { int bits = Float . floatToRawIntBits ( f ) ; int smallfloat = bits > > ( 24 - 3 ) ; if ( smallfloat < ( 63 - 15 ) << 3 ) { return ( bits <= 0 ) ? ( byte ) 0 : ( byte ) 1 ; } if ( smallfloat >= ( ( 63 - 15 ) << 3 ) + 0x100 ) { return - 1 ; } return ( byte ) ( smallfloat - ( ( 63 - 15 ) << 3 ) ) ; } public static float byte315ToFloat ( byte b ) { if ( b == 0 ) return 0.0f ; int bits = ( b & 0xff ) << ( 24 - 3 ) ; bits += ( 63 - 15 ) << 24 ; return Float . intBitsToFloat ( bits ) ; } public static byte floatToByte52 ( float f ) { int bits = Float . floatToRawIntBits ( f ) ; int smallfloat = bits > > ( 24 - 5 ) ; if ( smallfloat < ( 63 - 2 ) << 5 ) { return ( bits <= 0 ) ? ( byte ) 0 : ( byte ) 1 ; } if ( smallfloat >= ( ( 63 - 2 ) << 5 ) + 0x100 ) { return - 1 ; } return ( byte ) ( smallfloat - ( ( 63 - 2 ) << 5 ) ) ; } public static float byte52ToFloat ( byte b ) { if ( b == 0 ) return 0.0f ; int bits = ( b & 0xff ) << ( 24 - 5 ) ; bits += ( 63 - 2 ) << 24 ; return Float . intBitsToFloat ( bits ) ; } } 	0	['7', '1', '0', '1', '10', '21', '1', '0', '7', '2', '155', '0', '0', '0', '0.321428571', '0', '0', '21.14285714', '4', '2.5714', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Collection ; import java . util . Map ; import java . util . HashMap ; import java . util . Iterator ; final class DocFieldProcessor extends DocConsumer { final DocumentsWriter docWriter ; final FieldInfos fieldInfos = new FieldInfos ( ) ; final DocFieldConsumer consumer ; public DocFieldProcessor ( DocumentsWriter docWriter , DocFieldConsumer consumer ) { this . docWriter = docWriter ; this . consumer = consumer ; consumer . setFieldInfos ( fieldInfos ) ; } public void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException { consumer . closeDocStore ( state ) ; } public void flush ( Collection threads , DocumentsWriter . FlushState state ) throws IOException { Map childThreadsAndFields = new HashMap ( ) ; Iterator it = threads . iterator ( ) ; while ( it . hasNext ( ) ) { DocFieldProcessorPerThread perThread = ( DocFieldProcessorPerThread ) it . next ( ) ; childThreadsAndFields . put ( perThread . consumer , perThread . fields ( ) ) ; perThread . trimFields ( state ) ; } consumer . flush ( childThreadsAndFields , state ) ; fieldInfos . write ( state . directory , state . segmentName + ".fnm" ) ; } public void abort ( ) { consumer . abort ( ) ; } public boolean freeRAM ( ) { return consumer . freeRAM ( ) ; } public DocConsumerPerThread addThread ( DocumentsWriterThreadState threadState ) throws IOException { return new DocFieldProcessorPerThread ( threadState , this ) ; } } 	1	['6', '2', '0', '10', '25', '0', '2', '10', '6', '0.666666667', '91', '0', '3', '0.5', '0.333333333', '0', '0', '13.66666667', '1', '0.8333', '1']
package org . apache . lucene . store ; import java . io . IOException ; import java . io . File ; public class LockStressTest { public static void main ( String [ ] args ) throws Exception { if ( args . length != 6 ) { System . out . println ( "\nUsage: java org.apache.lucene.store.LockStressTest myID verifierHostOrIP verifierPort lockFactoryClassName lockDirName sleepTime\n" + "\n" + "  myID = int from 0 .. 255 (should be unique for test process)\n" + "  verifierHostOrIP = host name or IP address where LockVerifyServer is running\n" + "  verifierPort = port that LockVerifyServer is listening on\n" + "  lockFactoryClassName = primary LockFactory class that we will use\n" + "  lockDirName = path to the lock directory (only set for Simple/NativeFSLockFactory\n" + "  sleepTimeMS = milliseconds to pause betweeen each lock obtain/release\n" + "\n" + "You should run multiple instances of this process, each with its own\n" + "unique ID, and each pointing to the same lock directory, to verify\n" + "that locking is working correctly.\n" + "\n" + "Make sure you are first running LockVerifyServer.\n" + "\n" ) ; System . exit ( 1 ) ; } final int myID = Integer . parseInt ( args [ 0 ] ) ; if ( myID < 0 || myID > 255 ) { System . out . println ( "myID must be a unique int 0..255" ) ; System . exit ( 1 ) ; } final String verifierHost = args [ 1 ] ; final int verifierPort = Integer . parseInt ( args [ 2 ] ) ; final String lockFactoryClassName = args [ 3 ] ; final String lockDirName = args [ 4 ] ; final int sleepTimeMS = Integer . parseInt ( args [ 5 ] ) ; Class c ; try { c = Class . forName ( lockFactoryClassName ) ; } catch ( ClassNotFoundException e ) { throw new IOException ( "unable to find LockClass " + lockFactoryClassName ) ; } LockFactory lockFactory ; try { lockFactory = ( LockFactory ) c . newInstance ( ) ; } catch ( IllegalAccessException e ) { throw new IOException ( "IllegalAccessException when instantiating LockClass " + lockFactoryClassName ) ; } catch ( InstantiationException e ) { throw new IOException ( "InstantiationException when instantiating LockClass " + lockFactoryClassName ) ; } catch ( ClassCastException e ) { throw new IOException ( "unable to cast LockClass " + lockFactoryClassName + " instance to a LockFactory" ) ; } File lockDir = new File ( lockDirName ) ; if ( lockFactory instanceof NativeFSLockFactory ) { ( ( NativeFSLockFactory ) lockFactory ) . setLockDir ( lockDir ) ; } else if ( lockFactory instanceof SimpleFSLockFactory ) { ( ( SimpleFSLockFactory ) lockFactory ) . setLockDir ( lockDir ) ; } lockFactory . setLockPrefix ( "test" ) ; LockFactory verifyLF = new VerifyingLockFactory ( ( byte ) myID , lockFactory , verifierHost , verifierPort ) ; Lock l = verifyLF . makeLock ( "test.lock" ) ; while ( true ) { boolean obtained = false ; try { obtained = l . obtain ( 10 ) ; } catch ( LockObtainFailedException e ) { System . out . print ( "x" ) ; } if ( obtained ) { System . out . print ( "l" ) ; l . release ( ) ; } Thread . sleep ( sleepTimeMS ) ; } } } 	0	['2', '1', '0', '6', '22', '1', '0', '6', '2', '2', '172', '0', '0', '0', '0.5', '0', '0', '85', '1', '0.5', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; import java . io . IOException ; import java . util . Map ; import java . util . HashMap ; public class PerFieldAnalyzerWrapper extends Analyzer { private Analyzer defaultAnalyzer ; private Map analyzerMap = new HashMap ( ) ; public PerFieldAnalyzerWrapper ( Analyzer defaultAnalyzer ) { this . defaultAnalyzer = defaultAnalyzer ; } public void addAnalyzer ( String fieldName , Analyzer analyzer ) { analyzerMap . put ( fieldName , analyzer ) ; } public TokenStream tokenStream ( String fieldName , Reader reader ) { Analyzer analyzer = ( Analyzer ) analyzerMap . get ( fieldName ) ; if ( analyzer == null ) { analyzer = defaultAnalyzer ; } return analyzer . tokenStream ( fieldName , reader ) ; } public TokenStream reusableTokenStream ( String fieldName , Reader reader ) throws IOException { Analyzer analyzer = ( Analyzer ) analyzerMap . get ( fieldName ) ; if ( analyzer == null ) analyzer = defaultAnalyzer ; return analyzer . reusableTokenStream ( fieldName , reader ) ; } public int getPositionIncrementGap ( String fieldName ) { Analyzer analyzer = ( Analyzer ) analyzerMap . get ( fieldName ) ; if ( analyzer == null ) analyzer = defaultAnalyzer ; return analyzer . getPositionIncrementGap ( fieldName ) ; } public String toString ( ) { return "PerFieldAnalyzerWrapper(" + analyzerMap + ", default=" + defaultAnalyzer + ")" ; } } 	1	['6', '2', '0', '2', '17', '0', '0', '2', '6', '0.1', '90', '1', '1', '0.5', '0.583333333', '1', '1', '13.66666667', '2', '1.1667', '1']
package org . apache . lucene . index ; final class NormsWriterPerThread extends InvertedDocEndConsumerPerThread { final NormsWriter normsWriter ; final DocumentsWriter . DocState docState ; public NormsWriterPerThread ( DocInverterPerThread docInverterPerThread , NormsWriter normsWriter ) { this . normsWriter = normsWriter ; docState = docInverterPerThread . docState ; } InvertedDocEndConsumerPerField addField ( DocInverterPerField docInverterPerField , final FieldInfo fieldInfo ) { return new NormsWriterPerField ( docInverterPerField , this , fieldInfo ) ; } void abort ( ) { } void startDocument ( ) { } void finishDocument ( ) { } boolean freeRAM ( ) { return false ; } } 	0	['6', '2', '0', '8', '8', '15', '2', '8', '1', '1', '30', '0', '2', '0.444444444', '0.333333333', '0', '0', '3.666666667', '1', '0.8333', '0']
package org . apache . lucene . index ; import java . io . IOException ; final class DocumentsWriterThreadState { boolean isIdle = true ; int numThreads = 1 ; boolean doFlushAfter ; final DocConsumerPerThread consumer ; final DocumentsWriter . DocState docState ; final DocumentsWriter docWriter ; public DocumentsWriterThreadState ( DocumentsWriter docWriter ) throws IOException { this . docWriter = docWriter ; docState = new DocumentsWriter . DocState ( ) ; docState . maxFieldLength = docWriter . maxFieldLength ; docState . infoStream = docWriter . infoStream ; docState . similarity = docWriter . similarity ; docState . docWriter = docWriter ; consumer = docWriter . consumer . addThread ( this ) ; } void doAfterFlush ( ) { numThreads = 0 ; doFlushAfter = false ; } } 	1	['2', '1', '0', '7', '5', '0', '4', '5', '1', '0.833333333', '57', '0', '3', '0', '0.75', '0', '0', '24.5', '1', '0.5', '1']
package org . apache . lucene . index ; import java . util . List ; public final class KeepOnlyLastCommitDeletionPolicy implements IndexDeletionPolicy { public void onInit ( List commits ) { onCommit ( commits ) ; } public void onCommit ( List commits ) { int size = commits . size ( ) ; for ( int i = 0 ; i < size - 1 ; i ++ ) { ( ( IndexCommit ) commits . get ( i ) ) . delete ( ) ; } } } 	0	['3', '1', '0', '4', '7', '3', '2', '2', '3', '2', '28', '0', '0', '0', '0.833333333', '0', '0', '8.333333333', '2', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; import org . apache . lucene . search . BooleanClause . Occur ; import java . io . IOException ; import java . util . * ; public class BooleanQuery extends Query { private static int maxClauseCount = 1024 ; public static class TooManyClauses extends RuntimeException { public TooManyClauses ( ) { } public String getMessage ( ) { return "maxClauseCount is set to " + maxClauseCount ; } } public static int getMaxClauseCount ( ) { return maxClauseCount ; } public static void setMaxClauseCount ( int maxClauseCount ) { if ( maxClauseCount < 1 ) throw new IllegalArgumentException ( "maxClauseCount must be >= 1" ) ; BooleanQuery . maxClauseCount = maxClauseCount ; } private ArrayList clauses = new ArrayList ( ) ; private boolean disableCoord ; public BooleanQuery ( ) { } public BooleanQuery ( boolean disableCoord ) { this . disableCoord = disableCoord ; } public boolean isCoordDisabled ( ) { return disableCoord ; } public Similarity getSimilarity ( Searcher searcher ) { Similarity result = super . getSimilarity ( searcher ) ; if ( disableCoord ) { result = new SimilarityDelegator ( result ) { public float coord ( int overlap , int maxOverlap ) { return 1.0f ; } } ; } return result ; } public void setMinimumNumberShouldMatch ( int min ) { this . minNrShouldMatch = min ; } protected int minNrShouldMatch = 0 ; public int getMinimumNumberShouldMatch ( ) { return minNrShouldMatch ; } public void add ( Query query , BooleanClause . Occur occur ) { add ( new BooleanClause ( query , occur ) ) ; } public void add ( BooleanClause clause ) { if ( clauses . size ( ) >= maxClauseCount ) throw new TooManyClauses ( ) ; clauses . add ( clause ) ; } public BooleanClause [ ] getClauses ( ) { return ( BooleanClause [ ] ) clauses . toArray ( new BooleanClause [ clauses . size ( ) ] ) ; } public List clauses ( ) { return clauses ; } private class BooleanWeight implements Weight { protected Similarity similarity ; protected ArrayList weights = new ArrayList ( ) ; public BooleanWeight ( Searcher searcher ) throws IOException { this . similarity = getSimilarity ( searcher ) ; for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . get ( i ) ; weights . add ( c . getQuery ( ) . createWeight ( searcher ) ) ; } } public Query getQuery ( ) { return BooleanQuery . this ; } public float getValue ( ) { return getBoost ( ) ; } public float sumOfSquaredWeights ( ) throws IOException { float sum = 0.0f ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . get ( i ) ; Weight w = ( Weight ) weights . get ( i ) ; float s = w . sumOfSquaredWeights ( ) ; if ( ! c . isProhibited ( ) ) sum += s ; } sum *= getBoost ( ) * getBoost ( ) ; return sum ; } public void normalize ( float norm ) { norm *= getBoost ( ) ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { Weight w = ( Weight ) weights . get ( i ) ; w . normalize ( norm ) ; } } public Scorer scorer ( IndexReader reader ) throws IOException { BooleanScorer2 result = new BooleanScorer2 ( similarity , minNrShouldMatch , allowDocsOutOfOrder ) ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . get ( i ) ; Weight w = ( Weight ) weights . get ( i ) ; Scorer subScorer = w . scorer ( reader ) ; if ( subScorer != null ) result . add ( subScorer , c . isRequired ( ) , c . isProhibited ( ) ) ; else if ( c . isRequired ( ) ) return null ; } return result ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { final int minShouldMatch = BooleanQuery . this . getMinimumNumberShouldMatch ( ) ; ComplexExplanation sumExpl = new ComplexExplanation ( ) ; sumExpl . setDescription ( "sum of:" ) ; int coord = 0 ; int maxCoord = 0 ; float sum = 0.0f ; boolean fail = false ; int shouldMatchCount = 0 ; for ( int i = 0 ; i < weights . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . get ( i ) ; Weight w = ( Weight ) weights . get ( i ) ; Explanation e = w . explain ( reader , doc ) ; if ( ! c . isProhibited ( ) ) maxCoord ++ ; if ( e . isMatch ( ) ) { if ( ! c . isProhibited ( ) ) { sumExpl . addDetail ( e ) ; sum += e . getValue ( ) ; coord ++ ; } else { Explanation r = new Explanation ( 0.0f , "match on prohibited clause (" + c . getQuery ( ) . toString ( ) + ")" ) ; r . addDetail ( e ) ; sumExpl . addDetail ( r ) ; fail = true ; } if ( c . getOccur ( ) . equals ( Occur . SHOULD ) ) shouldMatchCount ++ ; } else if ( c . isRequired ( ) ) { Explanation r = new Explanation ( 0.0f , "no match on required clause (" + c . getQuery ( ) . toString ( ) + ")" ) ; r . addDetail ( e ) ; sumExpl . addDetail ( r ) ; fail = true ; } } if ( fail ) { sumExpl . setMatch ( Boolean . FALSE ) ; sumExpl . setValue ( 0.0f ) ; sumExpl . setDescription ( "Failure to meet condition(s) of required/prohibited clause(s)" ) ; return sumExpl ; } else if ( shouldMatchCount < minShouldMatch ) { sumExpl . setMatch ( Boolean . FALSE ) ; sumExpl . setValue ( 0.0f ) ; sumExpl . setDescription ( "Failure to match minimum number " + "of optional clauses: " + minShouldMatch ) ; return sumExpl ; } sumExpl . setMatch ( 0 < coord ? Boolean . TRUE : Boolean . FALSE ) ; sumExpl . setValue ( sum ) ; float coordFactor = similarity . coord ( coord , maxCoord ) ; if ( coordFactor == 1.0f ) return sumExpl ; else { ComplexExplanation result = new ComplexExplanation ( sumExpl . isMatch ( ) , sum * coordFactor , "product of:" ) ; result . addDetail ( sumExpl ) ; result . addDetail ( new Explanation ( coordFactor , "coord(" + coord + "/" + maxCoord + ")" ) ) ; return result ; } } } private static boolean allowDocsOutOfOrder = false ; public static void setAllowDocsOutOfOrder ( boolean allow ) { allowDocsOutOfOrder = allow ; } public static boolean getAllowDocsOutOfOrder ( ) { return allowDocsOutOfOrder ; } public static void setUseScorer14 ( boolean use14 ) { setAllowDocsOutOfOrder ( use14 ) ; } public static boolean getUseScorer14 ( ) { return getAllowDocsOutOfOrder ( ) ; } protected Weight createWeight ( Searcher searcher ) throws IOException { return new BooleanWeight ( searcher ) ; } public Query rewrite ( IndexReader reader ) throws IOException { if ( minNrShouldMatch == 0 && clauses . size ( ) == 1 ) { BooleanClause c = ( BooleanClause ) clauses . get ( 0 ) ; if ( ! c . isProhibited ( ) ) { Query query = c . getQuery ( ) . rewrite ( reader ) ; if ( getBoost ( ) != 1.0f ) { if ( query == c . getQuery ( ) ) query = ( Query ) query . clone ( ) ; query . setBoost ( getBoost ( ) * query . getBoost ( ) ) ; } return query ; } } BooleanQuery clone = null ; for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . get ( i ) ; Query query = c . getQuery ( ) . rewrite ( reader ) ; if ( query != c . getQuery ( ) ) { if ( clone == null ) clone = ( BooleanQuery ) this . clone ( ) ; clone . clauses . set ( i , new BooleanClause ( query , c . getOccur ( ) ) ) ; } } if ( clone != null ) { return clone ; } else return this ; } public void extractTerms ( Set terms ) { for ( Iterator i = clauses . iterator ( ) ; i . hasNext ( ) ; ) { BooleanClause clause = ( BooleanClause ) i . next ( ) ; clause . getQuery ( ) . extractTerms ( terms ) ; } } public Object clone ( ) { BooleanQuery clone = ( BooleanQuery ) super . clone ( ) ; clone . clauses = ( ArrayList ) this . clauses . clone ( ) ; return clone ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; boolean needParens = ( getBoost ( ) != 1.0 ) || ( getMinimumNumberShouldMatch ( ) > 0 ) ; if ( needParens ) { buffer . append ( "(" ) ; } for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { BooleanClause c = ( BooleanClause ) clauses . get ( i ) ; if ( c . isProhibited ( ) ) buffer . append ( "-" ) ; else if ( c . isRequired ( ) ) buffer . append ( "+" ) ; Query subQuery = c . getQuery ( ) ; if ( subQuery instanceof BooleanQuery ) { buffer . append ( "(" ) ; buffer . append ( c . getQuery ( ) . toString ( field ) ) ; buffer . append ( ")" ) ; } else buffer . append ( c . getQuery ( ) . toString ( field ) ) ; if ( i != clauses . size ( ) - 1 ) buffer . append ( " " ) ; } if ( needParens ) { buffer . append ( ")" ) ; } if ( getMinimumNumberShouldMatch ( ) > 0 ) { buffer . append ( '~' ) ; buffer . append ( getMinimumNumberShouldMatch ( ) ) ; } if ( getBoost ( ) != 1.0f ) { buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; } return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof BooleanQuery ) ) return false ; BooleanQuery other = ( BooleanQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && this . clauses . equals ( other . clauses ) && this . getMinimumNumberShouldMatch ( ) == other . getMinimumNumberShouldMatch ( ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ clauses . hashCode ( ) + getMinimumNumberShouldMatch ( ) ; } } 	1	['27', '2', '0', '20', '64', '167', '13', '11', '22', '0.761538462', '446', '1', '0', '0.333333333', '0.102564103', '2', '6', '15.33333333', '12', '1.5926', '10']
package org . apache . lucene . document ; import java . util . Set ; public class SetBasedFieldSelector implements FieldSelector { private Set fieldsToLoad ; private Set lazyFieldsToLoad ; public SetBasedFieldSelector ( Set fieldsToLoad , Set lazyFieldsToLoad ) { this . fieldsToLoad = fieldsToLoad ; this . lazyFieldsToLoad = lazyFieldsToLoad ; } public FieldSelectorResult accept ( String fieldName ) { FieldSelectorResult result = FieldSelectorResult . NO_LOAD ; if ( fieldsToLoad . contains ( fieldName ) == true ) { result = FieldSelectorResult . LOAD ; } if ( lazyFieldsToLoad . contains ( fieldName ) == true ) { result = FieldSelectorResult . LAZY_LOAD ; } return result ; } } 	0	['2', '1', '0', '2', '4', '0', '0', '2', '2', '0', '33', '1', '0', '0', '0.666666667', '0', '0', '14.5', '3', '1.5', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; public class WhitespaceTokenizer extends CharTokenizer { public WhitespaceTokenizer ( Reader in ) { super ( in ) ; } protected boolean isTokenChar ( char c ) { return ! Character . isWhitespace ( c ) ; } } 	1	['2', '4', '0', '2', '4', '1', '1', '1', '1', '2', '13', '0', '0', '0.923076923', '0.666666667', '1', '1', '5.5', '2', '1', '1']
package org . apache . lucene . index ; import java . util . Arrays ; final class ByteBlockPool { abstract static class Allocator { abstract void recycleByteBlocks ( byte [ ] [ ] blocks , int start , int end ) ; abstract byte [ ] getByteBlock ( boolean trackAllocations ) ; } public byte [ ] [ ] buffers = new byte [ 10 ] [ ] ; int bufferUpto = - 1 ; public int byteUpto = DocumentsWriter . BYTE_BLOCK_SIZE ; public byte [ ] buffer ; public int byteOffset = - DocumentsWriter . BYTE_BLOCK_SIZE ; private final boolean trackAllocations ; private final Allocator allocator ; public ByteBlockPool ( Allocator allocator , boolean trackAllocations ) { this . allocator = allocator ; this . trackAllocations = trackAllocations ; } public void reset ( ) { if ( bufferUpto != - 1 ) { for ( int i = 0 ; i < bufferUpto ; i ++ ) Arrays . fill ( buffers [ i ] , ( byte ) 0 ) ; Arrays . fill ( buffers [ bufferUpto ] , 0 , byteUpto , ( byte ) 0 ) ; if ( bufferUpto > 0 ) allocator . recycleByteBlocks ( buffers , 1 , 1 + bufferUpto ) ; bufferUpto = 0 ; byteUpto = 0 ; byteOffset = 0 ; buffer = buffers [ 0 ] ; } } public void nextBuffer ( ) { if ( 1 + bufferUpto == buffers . length ) { byte [ ] [ ] newBuffers = new byte [ ( int ) ( buffers . length * 1.5 ) ] [ ] ; System . arraycopy ( buffers , 0 , newBuffers , 0 , buffers . length ) ; buffers = newBuffers ; } buffer = buffers [ 1 + bufferUpto ] = allocator . getByteBlock ( trackAllocations ) ; bufferUpto ++ ; byteUpto = 0 ; byteOffset += DocumentsWriter . BYTE_BLOCK_SIZE ; } public int newSlice ( final int size ) { if ( byteUpto > DocumentsWriter . BYTE_BLOCK_SIZE - size ) nextBuffer ( ) ; final int upto = byteUpto ; byteUpto += size ; buffer [ byteUpto - 1 ] = 16 ; return upto ; } final static int [ ] nextLevelArray = { 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 9 } ; final static int [ ] levelSizeArray = { 5 , 14 , 20 , 30 , 40 , 40 , 80 , 80 , 120 , 200 } ; final static int FIRST_LEVEL_SIZE = levelSizeArray [ 0 ] ; public int allocSlice ( final byte [ ] slice , final int upto ) { final int level = slice [ upto ] & 15 ; final int newLevel = nextLevelArray [ level ] ; final int newSize = levelSizeArray [ newLevel ] ; if ( byteUpto > DocumentsWriter . BYTE_BLOCK_SIZE - newSize ) nextBuffer ( ) ; final int newUpto = byteUpto ; final int offset = newUpto + byteOffset ; byteUpto += newSize ; buffer [ newUpto ] = slice [ upto - 3 ] ; buffer [ newUpto + 1 ] = slice [ upto - 2 ] ; buffer [ newUpto + 2 ] = slice [ upto - 1 ] ; slice [ upto - 3 ] = ( byte ) ( offset > > > 24 ) ; slice [ upto - 2 ] = ( byte ) ( offset > > > 16 ) ; slice [ upto - 1 ] = ( byte ) ( offset > > > 8 ) ; slice [ upto ] = ( byte ) offset ; buffer [ byteUpto - 1 ] = ( byte ) ( 16 | newLevel ) ; return newUpto + 3 ; } } 	0	['6', '1', '0', '5', '12', '0', '4', '1', '5', '0.44', '387', '0.2', '1', '0', '0.4', '0', '0', '61.83333333', '4', '1.6667', '0']
package org . apache . lucene . index ; public final class Term implements Comparable , java . io . Serializable { String field ; String text ; public Term ( String fld , String txt ) { this ( fld , txt , true ) ; } public Term ( String fld ) { this ( fld , "" , true ) ; } Term ( String fld , String txt , boolean intern ) { field = intern ? fld . intern ( ) : fld ; text = txt ; } public final String field ( ) { return field ; } public final String text ( ) { return text ; } public Term createTerm ( String text ) { return new Term ( field , text , false ) ; } public final boolean equals ( Object o ) { if ( o == this ) return true ; if ( o == null ) return false ; if ( ! ( o instanceof Term ) ) return false ; Term other = ( Term ) o ; return field == other . field && text . equals ( other . text ) ; } public final int hashCode ( ) { return field . hashCode ( ) + text . hashCode ( ) ; } public int compareTo ( Object other ) { return compareTo ( ( Term ) other ) ; } public final int compareTo ( Term other ) { if ( field == other . field ) return text . compareTo ( other . text ) ; else return field . compareTo ( other . field ) ; } final void set ( String fld , String txt ) { field = fld ; text = txt ; } public final String toString ( ) { return field + ":" + text ; } private void readObject ( java . io . ObjectInputStream in ) throws java . io . IOException , ClassNotFoundException { in . defaultReadObject ( ) ; field = field . intern ( ) ; } } 	1	['13', '1', '0', '79', '22', '0', '79', '0', '10', '0.125', '145', '0', '0', '0', '0.294871795', '1', '1', '10', '6', '1.2308', '2']
package org . apache . lucene . store ; import java . net . ServerSocket ; import java . net . Socket ; import java . io . OutputStream ; import java . io . InputStream ; import java . io . IOException ; public class LockVerifyServer { private static String getTime ( long startTime ) { return "[" + ( ( System . currentTimeMillis ( ) - startTime ) / 1000 ) + "s] " ; } public static void main ( String [ ] args ) throws IOException { if ( args . length != 1 ) { System . out . println ( "\nUsage: java org.apache.lucene.store.LockVerifyServer port\n" ) ; System . exit ( 1 ) ; } final int port = Integer . parseInt ( args [ 0 ] ) ; ServerSocket s = new ServerSocket ( port ) ; s . setReuseAddress ( true ) ; System . out . println ( "\nReady on port " + port + "..." ) ; int lockedID = 0 ; long startTime = System . currentTimeMillis ( ) ; while ( true ) { Socket cs = s . accept ( ) ; OutputStream out = cs . getOutputStream ( ) ; InputStream in = cs . getInputStream ( ) ; int id = in . read ( ) ; int command = in . read ( ) ; boolean err = false ; if ( command == 1 ) { if ( lockedID != 0 ) { err = true ; System . out . println ( getTime ( startTime ) + " ERROR: id " + id + " got lock, but " + lockedID + " already holds the lock" ) ; } lockedID = id ; } else if ( command == 0 ) { if ( lockedID != id ) { err = true ; System . out . println ( getTime ( startTime ) + " ERROR: id " + id + " released the lock, but " + lockedID + " is the one holding the lock" ) ; } lockedID = 0 ; } else throw new RuntimeException ( "unrecognized command " + command ) ; System . out . print ( "." ) ; if ( err ) out . write ( 1 ) ; else out . write ( 0 ) ; out . close ( ) ; in . close ( ) ; cs . close ( ) ; } } } 	0	['3', '1', '0', '0', '25', '3', '0', '0', '2', '2', '165', '0', '0', '0', '0.333333333', '0', '0', '54', '1', '0.6667', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . Set ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermDocs ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; public class TermQuery extends Query { private Term term ; private class TermWeight implements Weight { private Similarity similarity ; private float value ; private float idf ; private float queryNorm ; private float queryWeight ; public TermWeight ( Searcher searcher ) throws IOException { this . similarity = getSimilarity ( searcher ) ; idf = similarity . idf ( term , searcher ) ; } public String toString ( ) { return "weight(" + TermQuery . this + ")" ; } public Query getQuery ( ) { return TermQuery . this ; } public float getValue ( ) { return value ; } public float sumOfSquaredWeights ( ) { queryWeight = idf * getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float queryNorm ) { this . queryNorm = queryNorm ; queryWeight *= queryNorm ; value = queryWeight * idf ; } public Scorer scorer ( IndexReader reader ) throws IOException { TermDocs termDocs = reader . termDocs ( term ) ; if ( termDocs == null ) return null ; return new TermScorer ( this , termDocs , similarity , reader . norms ( term . field ( ) ) ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { ComplexExplanation result = new ComplexExplanation ( ) ; result . setDescription ( "weight(" + getQuery ( ) + " in " + doc + "), product of:" ) ; Explanation idfExpl = new Explanation ( idf , "idf(docFreq=" + reader . docFreq ( term ) + ", numDocs=" + reader . numDocs ( ) + ")" ) ; Explanation queryExpl = new Explanation ( ) ; queryExpl . setDescription ( "queryWeight(" + getQuery ( ) + "), product of:" ) ; Explanation boostExpl = new Explanation ( getBoost ( ) , "boost" ) ; if ( getBoost ( ) != 1.0f ) queryExpl . addDetail ( boostExpl ) ; queryExpl . addDetail ( idfExpl ) ; Explanation queryNormExpl = new Explanation ( queryNorm , "queryNorm" ) ; queryExpl . addDetail ( queryNormExpl ) ; queryExpl . setValue ( boostExpl . getValue ( ) * idfExpl . getValue ( ) * queryNormExpl . getValue ( ) ) ; result . addDetail ( queryExpl ) ; String field = term . field ( ) ; ComplexExplanation fieldExpl = new ComplexExplanation ( ) ; fieldExpl . setDescription ( "fieldWeight(" + term + " in " + doc + "), product of:" ) ; Explanation tfExpl = scorer ( reader ) . explain ( doc ) ; fieldExpl . addDetail ( tfExpl ) ; fieldExpl . addDetail ( idfExpl ) ; Explanation fieldNormExpl = new Explanation ( ) ; byte [ ] fieldNorms = reader . norms ( field ) ; float fieldNorm = fieldNorms != null ? Similarity . decodeNorm ( fieldNorms [ doc ] ) : 0.0f ; fieldNormExpl . setValue ( fieldNorm ) ; fieldNormExpl . setDescription ( "fieldNorm(field=" + field + ", doc=" + doc + ")" ) ; fieldExpl . addDetail ( fieldNormExpl ) ; fieldExpl . setMatch ( Boolean . valueOf ( tfExpl . isMatch ( ) ) ) ; fieldExpl . setValue ( tfExpl . getValue ( ) * idfExpl . getValue ( ) * fieldNormExpl . getValue ( ) ) ; result . addDetail ( fieldExpl ) ; result . setMatch ( fieldExpl . getMatch ( ) ) ; result . setValue ( queryExpl . getValue ( ) * fieldExpl . getValue ( ) ) ; if ( queryExpl . getValue ( ) == 1.0f ) return fieldExpl ; return result ; } } public TermQuery ( Term t ) { term = t ; } public Term getTerm ( ) { return term ; } protected Weight createWeight ( Searcher searcher ) throws IOException { return new TermWeight ( searcher ) ; } public void extractTerms ( Set terms ) { terms . add ( getTerm ( ) ) ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! term . field ( ) . equals ( field ) ) { buffer . append ( term . field ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( term . text ( ) ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof TermQuery ) ) return false ; TermQuery other = ( TermQuery ) o ; return ( this . getBoost ( ) == other . getBoost ( ) ) && this . term . equals ( other . term ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ term . hashCode ( ) ; } } 	1	['8', '2', '0', '16', '22', '0', '11', '6', '6', '0.142857143', '100', '1', '1', '0.631578947', '0.232142857', '2', '2', '11.375', '4', '1.375', '4']
package org . apache . lucene . index ; final class ByteSliceWriter { private byte [ ] slice ; private int upto ; private final ByteBlockPool pool ; int offset0 ; public ByteSliceWriter ( ByteBlockPool pool ) { this . pool = pool ; } public void init ( int address ) { slice = pool . buffers [ address > > DocumentsWriter . BYTE_BLOCK_SHIFT ] ; assert slice != null ; upto = address & DocumentsWriter . BYTE_BLOCK_MASK ; offset0 = address ; assert upto < slice . length ; } public void writeByte ( byte b ) { assert slice != null ; if ( slice [ upto ] != 0 ) { upto = pool . allocSlice ( slice , upto ) ; slice = pool . buffer ; offset0 = pool . byteOffset ; assert slice != null ; } slice [ upto ++ ] = b ; assert upto != slice . length ; } public void writeBytes ( final byte [ ] b , int offset , final int len ) { final int offsetEnd = offset + len ; while ( offset < offsetEnd ) { if ( slice [ upto ] != 0 ) { upto = pool . allocSlice ( slice , upto ) ; slice = pool . buffer ; offset0 = pool . byteOffset ; } slice [ upto ++ ] = b [ offset ++ ] ; assert upto != slice . length ; } } public int getAddress ( ) { return upto + ( offset0 & DocumentsWriter . BYTE_BLOCK_NOT_MASK ) ; } public void writeVInt ( int i ) { while ( ( i & ~ 0x7F ) != 0 ) { writeByte ( ( byte ) ( ( i & 0x7f ) | 0x80 ) ) ; i >>>= 7 ; } writeByte ( ( byte ) i ) ; } } 	0	['8', '1', '0', '1', '15', '4', '0', '1', '6', '0.547619048', '240', '0.5', '1', '0', '0.30952381', '0', '0', '28.25', '8', '2.75', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; import java . io . Reader ; public abstract class CharTokenizer extends Tokenizer { public CharTokenizer ( Reader input ) { super ( input ) ; } private int offset = 0 , bufferIndex = 0 , dataLen = 0 ; private static final int MAX_WORD_LEN = 255 ; private static final int IO_BUFFER_SIZE = 4096 ; private final char [ ] ioBuffer = new char [ IO_BUFFER_SIZE ] ; protected abstract boolean isTokenChar ( char c ) ; protected char normalize ( char c ) { return c ; } public final Token next ( final Token reusableToken ) throws IOException { assert reusableToken != null ; reusableToken . clear ( ) ; int length = 0 ; int start = bufferIndex ; char [ ] buffer = reusableToken . termBuffer ( ) ; while ( true ) { if ( bufferIndex >= dataLen ) { offset += dataLen ; dataLen = input . read ( ioBuffer ) ; if ( dataLen == - 1 ) { if ( length > 0 ) break ; else return null ; } bufferIndex = 0 ; } final char c = ioBuffer [ bufferIndex ++ ] ; if ( isTokenChar ( c ) ) { if ( length == 0 ) start = offset + bufferIndex - 1 ; else if ( length == buffer . length ) buffer = reusableToken . resizeTermBuffer ( 1 + length ) ; buffer [ length ++ ] = normalize ( c ) ; if ( length == MAX_WORD_LEN ) break ; } else if ( length > 0 ) break ; } reusableToken . setTermLength ( length ) ; reusableToken . setStartOffset ( start ) ; reusableToken . setEndOffset ( start + length ) ; return reusableToken ; } public void reset ( Reader input ) throws IOException { super . reset ( input ) ; bufferIndex = 0 ; offset = 0 ; dataLen = 0 ; } } 	1	['7', '3', '2', '4', '21', '13', '2', '2', '3', '0.875', '185', '0.75', '0', '0.583333333', '0.366666667', '1', '2', '24.28571429', '1', '0.7143', '4']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . BufferedIndexInput ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . Lock ; import java . util . HashMap ; import java . io . IOException ; class CompoundFileReader extends Directory { private int readBufferSize ; private static final class FileEntry { long offset ; long length ; } private Directory directory ; private String fileName ; private IndexInput stream ; private HashMap entries = new HashMap ( ) ; public CompoundFileReader ( Directory dir , String name ) throws IOException { this ( dir , name , BufferedIndexInput . BUFFER_SIZE ) ; } public CompoundFileReader ( Directory dir , String name , int readBufferSize ) throws IOException { directory = dir ; fileName = name ; this . readBufferSize = readBufferSize ; boolean success = false ; try { stream = dir . openInput ( name , readBufferSize ) ; int count = stream . readVInt ( ) ; FileEntry entry = null ; for ( int i = 0 ; i < count ; i ++ ) { long offset = stream . readLong ( ) ; String id = stream . readString ( ) ; if ( entry != null ) { entry . length = offset - entry . offset ; } entry = new FileEntry ( ) ; entry . offset = offset ; entries . put ( id , entry ) ; } if ( entry != null ) { entry . length = stream . length ( ) - entry . offset ; } success = true ; } finally { if ( ! success && ( stream != null ) ) { try { stream . close ( ) ; } catch ( IOException e ) { } } } } public Directory getDirectory ( ) { return directory ; } public String getName ( ) { return fileName ; } public synchronized void close ( ) throws IOException { if ( stream == null ) throw new IOException ( "Already closed" ) ; entries . clear ( ) ; stream . close ( ) ; stream = null ; } public synchronized IndexInput openInput ( String id ) throws IOException { return openInput ( id , readBufferSize ) ; } public synchronized IndexInput openInput ( String id , int readBufferSize ) throws IOException { if ( stream == null ) throw new IOException ( "Stream closed" ) ; FileEntry entry = ( FileEntry ) entries . get ( id ) ; if ( entry == null ) throw new IOException ( "No sub-file with id " + id + " found" ) ; return new CSIndexInput ( stream , entry . offset , entry . length , readBufferSize ) ; } public String [ ] list ( ) { String res [ ] = new String [ entries . size ( ) ] ; return ( String [ ] ) entries . keySet ( ) . toArray ( res ) ; } public boolean fileExists ( String name ) { return entries . containsKey ( name ) ; } public long fileModified ( String name ) throws IOException { return directory . fileModified ( fileName ) ; } public void touchFile ( String name ) throws IOException { directory . touchFile ( fileName ) ; } public void deleteFile ( String name ) { throw new UnsupportedOperationException ( ) ; } public void renameFile ( String from , String to ) { throw new UnsupportedOperationException ( ) ; } public long fileLength ( String name ) throws IOException { FileEntry e = ( FileEntry ) entries . get ( name ) ; if ( e == null ) throw new IOException ( "File " + name + " does not exist" ) ; return e . length ; } public IndexOutput createOutput ( String name ) { throw new UnsupportedOperationException ( ) ; } public Lock makeLock ( String name ) { throw new UnsupportedOperationException ( ) ; } static final class CSIndexInput extends BufferedIndexInput { IndexInput base ; long fileOffset ; long length ; CSIndexInput ( final IndexInput base , final long fileOffset , final long length ) { this ( base , fileOffset , length , BufferedIndexInput . BUFFER_SIZE ) ; } CSIndexInput ( final IndexInput base , final long fileOffset , final long length , int readBufferSize ) { super ( readBufferSize ) ; this . base = ( IndexInput ) base . clone ( ) ; this . fileOffset = fileOffset ; this . length = length ; } public Object clone ( ) { CSIndexInput clone = ( CSIndexInput ) super . clone ( ) ; clone . base = ( IndexInput ) base . clone ( ) ; clone . fileOffset = fileOffset ; clone . length = length ; return clone ; } protected void readInternal ( byte [ ] b , int offset , int len ) throws IOException { long start = getFilePointer ( ) ; if ( start + len > length ) throw new IOException ( "read past EOF" ) ; base . seek ( fileOffset + start ) ; base . readBytes ( b , offset , len , false ) ; } protected void seekInternal ( long pos ) { } public void close ( ) throws IOException { base . close ( ) ; } public long length ( ) { return length ; } } } 	0	['16', '2', '0', '9', '40', '70', '2', '7', '16', '0.72', '267', '1', '2', '0.575757576', '0.5', '1', '5', '15.375', '1', '0.875', '0']
package org . apache . lucene . index ; import java . util . Collection ; import java . util . List ; import java . util . ArrayList ; import java . io . IOException ; import org . apache . lucene . store . Directory ; public class SnapshotDeletionPolicy implements IndexDeletionPolicy { private IndexCommit lastCommit ; private IndexDeletionPolicy primary ; private String snapshot ; public SnapshotDeletionPolicy ( IndexDeletionPolicy primary ) { this . primary = primary ; } public synchronized void onInit ( List commits ) throws IOException { primary . onInit ( wrapCommits ( commits ) ) ; lastCommit = ( IndexCommit ) commits . get ( commits . size ( ) - 1 ) ; } public synchronized void onCommit ( List commits ) throws IOException { primary . onCommit ( wrapCommits ( commits ) ) ; lastCommit = ( IndexCommit ) commits . get ( commits . size ( ) - 1 ) ; } public synchronized IndexCommitPoint snapshot ( ) { if ( snapshot == null ) snapshot = lastCommit . getSegmentsFileName ( ) ; else throw new IllegalStateException ( "snapshot is already set; please call release() first" ) ; return lastCommit ; } public synchronized void release ( ) { if ( snapshot != null ) snapshot = null ; else throw new IllegalStateException ( "snapshot was not set; please call snapshot() first" ) ; } private class MyCommitPoint extends IndexCommit { IndexCommit cp ; MyCommitPoint ( IndexCommit cp ) { this . cp = cp ; } public String getSegmentsFileName ( ) { return cp . getSegmentsFileName ( ) ; } public Collection getFileNames ( ) throws IOException { return cp . getFileNames ( ) ; } public Directory getDirectory ( ) { return cp . getDirectory ( ) ; } public void delete ( ) { synchronized ( SnapshotDeletionPolicy . this ) { if ( snapshot == null || ! snapshot . equals ( getSegmentsFileName ( ) ) ) cp . delete ( ) ; } } public boolean isDeleted ( ) { return cp . isDeleted ( ) ; } public long getVersion ( ) { return cp . getVersion ( ) ; } public long getGeneration ( ) { return cp . getGeneration ( ) ; } } private List wrapCommits ( List commits ) { final int count = commits . size ( ) ; List myCommits = new ArrayList ( count ) ; for ( int i = 0 ; i < count ; i ++ ) myCommits . add ( new MyCommitPoint ( ( IndexCommit ) commits . get ( i ) ) ) ; return myCommits ; } } 	1	['7', '1', '0', '4', '17', '5', '1', '4', '5', '0.666666667', '109', '1', '2', '0', '0.392857143', '0', '0', '14.14285714', '2', '1.2857', '2']
package org . apache . lucene . index ; import java . util . Collection ; import java . util . Map ; import java . util . HashMap ; import java . util . Iterator ; import java . util . HashSet ; import java . util . Arrays ; import java . io . IOException ; import org . apache . lucene . util . ArrayUtil ; final class TermsHash extends InvertedDocConsumer { final TermsHashConsumer consumer ; final TermsHash nextTermsHash ; final int bytesPerPosting ; final int postingsFreeChunk ; final DocumentsWriter docWriter ; private TermsHash primaryTermsHash ; private RawPostingList [ ] postingsFreeList = new RawPostingList [ 1 ] ; private int postingsFreeCount ; private int postingsAllocCount ; boolean trackAllocations ; public TermsHash ( final DocumentsWriter docWriter , boolean trackAllocations , final TermsHashConsumer consumer , final TermsHash nextTermsHash ) { this . docWriter = docWriter ; this . consumer = consumer ; this . nextTermsHash = nextTermsHash ; this . trackAllocations = trackAllocations ; bytesPerPosting = consumer . bytesPerPosting ( ) + 4 * DocumentsWriter . POINTER_NUM_BYTE ; postingsFreeChunk = ( int ) ( DocumentsWriter . BYTE_BLOCK_SIZE / bytesPerPosting ) ; } InvertedDocConsumerPerThread addThread ( DocInverterPerThread docInverterPerThread ) { return new TermsHashPerThread ( docInverterPerThread , this , nextTermsHash , null ) ; } TermsHashPerThread addThread ( DocInverterPerThread docInverterPerThread , TermsHashPerThread primaryPerThread ) { return new TermsHashPerThread ( docInverterPerThread , this , nextTermsHash , primaryPerThread ) ; } void setFieldInfos ( FieldInfos fieldInfos ) { this . fieldInfos = fieldInfos ; consumer . setFieldInfos ( fieldInfos ) ; } synchronized public void abort ( ) { consumer . abort ( ) ; if ( nextTermsHash != null ) nextTermsHash . abort ( ) ; } void shrinkFreePostings ( Map threadsAndFields , DocumentsWriter . FlushState state ) { assert postingsFreeCount == postingsAllocCount : Thread . currentThread ( ) . getName ( ) + ": postingsFreeCount=" + postingsFreeCount + " postingsAllocCount=" + postingsAllocCount + " consumer=" + consumer ; final int newSize = ArrayUtil . getShrinkSize ( postingsFreeList . length , postingsAllocCount ) ; if ( newSize != postingsFreeList . length ) { RawPostingList [ ] newArray = new RawPostingList [ newSize ] ; System . arraycopy ( postingsFreeList , 0 , newArray , 0 , postingsFreeCount ) ; postingsFreeList = newArray ; } } synchronized void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException { consumer . closeDocStore ( state ) ; if ( nextTermsHash != null ) nextTermsHash . closeDocStore ( state ) ; } synchronized void flush ( Map threadsAndFields , final DocumentsWriter . FlushState state ) throws IOException { Map childThreadsAndFields = new HashMap ( ) ; Map nextThreadsAndFields ; if ( nextTermsHash != null ) nextThreadsAndFields = new HashMap ( ) ; else nextThreadsAndFields = null ; Iterator it = threadsAndFields . entrySet ( ) . iterator ( ) ; while ( it . hasNext ( ) ) { Map . Entry entry = ( Map . Entry ) it . next ( ) ; TermsHashPerThread perThread = ( TermsHashPerThread ) entry . getKey ( ) ; Collection fields = ( Collection ) entry . getValue ( ) ; Iterator fieldsIt = fields . iterator ( ) ; Collection childFields = new HashSet ( ) ; Collection nextChildFields ; if ( nextTermsHash != null ) nextChildFields = new HashSet ( ) ; else nextChildFields = null ; while ( fieldsIt . hasNext ( ) ) { TermsHashPerField perField = ( TermsHashPerField ) fieldsIt . next ( ) ; childFields . add ( perField . consumer ) ; if ( nextTermsHash != null ) nextChildFields . add ( perField . nextPerField ) ; } childThreadsAndFields . put ( perThread . consumer , childFields ) ; if ( nextTermsHash != null ) nextThreadsAndFields . put ( perThread . nextPerThread , nextChildFields ) ; } consumer . flush ( childThreadsAndFields , state ) ; shrinkFreePostings ( threadsAndFields , state ) ; if ( nextTermsHash != null ) nextTermsHash . flush ( nextThreadsAndFields , state ) ; } synchronized public boolean freeRAM ( ) { if ( ! trackAllocations ) return false ; boolean any ; final int numToFree ; if ( postingsFreeCount >= postingsFreeChunk ) numToFree = postingsFreeChunk ; else numToFree = postingsFreeCount ; any = numToFree > 0 ; if ( any ) { Arrays . fill ( postingsFreeList , postingsFreeCount - numToFree , postingsFreeCount , null ) ; postingsFreeCount -= numToFree ; postingsAllocCount -= numToFree ; docWriter . bytesAllocated ( - numToFree * bytesPerPosting ) ; any = true ; } if ( nextTermsHash != null ) any |= nextTermsHash . freeRAM ( ) ; return any ; } synchronized public void recyclePostings ( final RawPostingList [ ] postings , final int numPostings ) { assert postings . length >= numPostings ; assert postingsFreeCount + numPostings <= postingsFreeList . length ; System . arraycopy ( postings , 0 , postingsFreeList , postingsFreeCount , numPostings ) ; postingsFreeCount += numPostings ; } synchronized public void getPostings ( final RawPostingList [ ] postings ) { assert docWriter . writer . testPoint ( "TermsHash.getPostings start" ) ; assert postingsFreeCount <= postingsFreeList . length ; assert postingsFreeCount <= postingsAllocCount : "postingsFreeCount=" + postingsFreeCount + " postingsAllocCount=" + postingsAllocCount ; final int numToCopy ; if ( postingsFreeCount < postings . length ) numToCopy = postingsFreeCount ; else numToCopy = postings . length ; final int start = postingsFreeCount - numToCopy ; assert start >= 0 ; assert start + numToCopy <= postingsFreeList . length ; assert numToCopy <= postings . length ; System . arraycopy ( postingsFreeList , start , postings , 0 , numToCopy ) ; if ( numToCopy != postings . length ) { final int extra = postings . length - numToCopy ; final int newPostingsAllocCount = postingsAllocCount + extra ; consumer . createPostings ( postings , numToCopy , extra ) ; assert docWriter . writer . testPoint ( "TermsHash.getPostings after create" ) ; postingsAllocCount += extra ; if ( trackAllocations ) docWriter . bytesAllocated ( extra * bytesPerPosting ) ; if ( newPostingsAllocCount > postingsFreeList . length ) postingsFreeList = new RawPostingList [ ArrayUtil . getNextSize ( newPostingsAllocCount ) ] ; } postingsFreeCount -= numToCopy ; if ( trackAllocations ) docWriter . bytesUsed ( postings . length * bytesPerPosting ) ; } } 	0	['13', '2', '0', '14', '52', '0', '3', '14', '5', '0.763888889', '584', '0.333333333', '5', '0.352941176', '0.179487179', '0', '0', '43', '20', '3.3077', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class Directory { volatile boolean isOpen = true ; protected LockFactory lockFactory ; public abstract String [ ] list ( ) throws IOException ; public abstract boolean fileExists ( String name ) throws IOException ; public abstract long fileModified ( String name ) throws IOException ; public abstract void touchFile ( String name ) throws IOException ; public abstract void deleteFile ( String name ) throws IOException ; public abstract void renameFile ( String from , String to ) throws IOException ; public abstract long fileLength ( String name ) throws IOException ; public abstract IndexOutput createOutput ( String name ) throws IOException ; public void sync ( String name ) throws IOException { } public abstract IndexInput openInput ( String name ) throws IOException ; public IndexInput openInput ( String name , int bufferSize ) throws IOException { return openInput ( name ) ; } public Lock makeLock ( String name ) { return lockFactory . makeLock ( name ) ; } public void clearLock ( String name ) throws IOException { if ( lockFactory != null ) { lockFactory . clearLock ( name ) ; } } public abstract void close ( ) throws IOException ; public void setLockFactory ( LockFactory lockFactory ) { this . lockFactory = lockFactory ; lockFactory . setLockPrefix ( this . getLockID ( ) ) ; } public LockFactory getLockFactory ( ) { return this . lockFactory ; } public String getLockID ( ) { return this . toString ( ) ; } public static void copy ( Directory src , Directory dest , boolean closeDirSrc ) throws IOException { final String [ ] files = src . list ( ) ; if ( files == null ) throw new IOException ( "cannot read directory " + src + ": list() returned null" ) ; byte [ ] buf = new byte [ BufferedIndexOutput . BUFFER_SIZE ] ; for ( int i = 0 ; i < files . length ; i ++ ) { IndexOutput os = null ; IndexInput is = null ; try { os = dest . createOutput ( files [ i ] ) ; is = src . openInput ( files [ i ] ) ; long len = is . length ( ) ; long readCount = 0 ; while ( readCount < len ) { int toRead = readCount + BufferedIndexOutput . BUFFER_SIZE > len ? ( int ) ( len - readCount ) : BufferedIndexOutput . BUFFER_SIZE ; is . readBytes ( buf , 0 , toRead ) ; os . writeBytes ( buf , toRead ) ; readCount += toRead ; } } finally { try { if ( os != null ) os . close ( ) ; } finally { if ( is != null ) is . close ( ) ; } } } if ( closeDirSrc ) src . close ( ) ; } protected final void ensureOpen ( ) throws AlreadyClosedException { if ( ! isOpen ) throw new AlreadyClosedException ( "this Directory is closed" ) ; } } 	1	['20', '1', '3', '57', '36', '176', '52', '5', '19', '0.894736842', '193', '0.5', '1', '0', '0.291666667', '0', '0', '8.55', '1', '0.95', '2']
package org . apache . lucene . index ; import java . util . ArrayList ; import java . util . HashMap ; import java . util . List ; import java . util . Map ; public class PositionBasedTermVectorMapper extends TermVectorMapper { private Map fieldToTerms ; private String currentField ; private Map currentPositions ; private boolean storeOffsets ; public PositionBasedTermVectorMapper ( ) { super ( false , false ) ; } public PositionBasedTermVectorMapper ( boolean ignoringOffsets ) { super ( false , ignoringOffsets ) ; } public boolean isIgnoringPositions ( ) { return false ; } public void map ( String term , int frequency , TermVectorOffsetInfo [ ] offsets , int [ ] positions ) { for ( int i = 0 ; i < positions . length ; i ++ ) { Integer posVal = new Integer ( positions [ i ] ) ; TVPositionInfo pos = ( TVPositionInfo ) currentPositions . get ( posVal ) ; if ( pos == null ) { pos = new TVPositionInfo ( positions [ i ] , storeOffsets ) ; currentPositions . put ( posVal , pos ) ; } pos . addTerm ( term , offsets != null ? offsets [ i ] : null ) ; } } public void setExpectations ( String field , int numTerms , boolean storeOffsets , boolean storePositions ) { if ( storePositions == false ) { throw new RuntimeException ( "You must store positions in order to use this Mapper" ) ; } if ( storeOffsets == true ) { } fieldToTerms = new HashMap ( numTerms ) ; this . storeOffsets = storeOffsets ; currentField = field ; currentPositions = new HashMap ( ) ; fieldToTerms . put ( currentField , currentPositions ) ; } public Map getFieldToTerms ( ) { return fieldToTerms ; } public static class TVPositionInfo { private int position ; private List terms ; private List offsets ; public TVPositionInfo ( int position , boolean storeOffsets ) { this . position = position ; terms = new ArrayList ( ) ; if ( storeOffsets ) { offsets = new ArrayList ( ) ; } } void addTerm ( String term , TermVectorOffsetInfo info ) { terms . add ( term ) ; if ( offsets != null ) { offsets . add ( info ) ; } } public int getPosition ( ) { return position ; } public List getTerms ( ) { return terms ; } public List getOffsets ( ) { return offsets ; } } } 	0	['6', '2', '0', '3', '15', '11', '0', '3', '6', '0.85', '110', '1', '0', '0.555555556', '0.388888889', '0', '0', '16.66666667', '4', '1.5', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Set ; import org . apache . lucene . store . Directory ; public abstract class LogMergePolicy extends MergePolicy { public static final double LEVEL_LOG_SPAN = 0.75 ; public static final int DEFAULT_MERGE_FACTOR = 10 ; public static final int DEFAULT_MAX_MERGE_DOCS = Integer . MAX_VALUE ; private int mergeFactor = DEFAULT_MERGE_FACTOR ; long minMergeSize ; long maxMergeSize ; int maxMergeDocs = DEFAULT_MAX_MERGE_DOCS ; private boolean useCompoundFile = true ; private boolean useCompoundDocStore = true ; private IndexWriter writer ; private void message ( String message ) { if ( writer != null ) writer . message ( "LMP: " + message ) ; } public int getMergeFactor ( ) { return mergeFactor ; } public void setMergeFactor ( int mergeFactor ) { if ( mergeFactor < 2 ) throw new IllegalArgumentException ( "mergeFactor cannot be less than 2" ) ; this . mergeFactor = mergeFactor ; } public boolean useCompoundFile ( SegmentInfos infos , SegmentInfo info ) { return useCompoundFile ; } public void setUseCompoundFile ( boolean useCompoundFile ) { this . useCompoundFile = useCompoundFile ; } public boolean getUseCompoundFile ( ) { return useCompoundFile ; } public boolean useCompoundDocStore ( SegmentInfos infos ) { return useCompoundDocStore ; } public void setUseCompoundDocStore ( boolean useCompoundDocStore ) { this . useCompoundDocStore = useCompoundDocStore ; } public boolean getUseCompoundDocStore ( ) { return useCompoundDocStore ; } public void close ( ) { } abstract protected long size ( SegmentInfo info ) throws IOException ; private boolean isOptimized ( SegmentInfos infos , IndexWriter writer , int maxNumSegments , Set segmentsToOptimize ) throws IOException { final int numSegments = infos . size ( ) ; int numToOptimize = 0 ; SegmentInfo optimizeInfo = null ; for ( int i = 0 ; i < numSegments && numToOptimize <= maxNumSegments ; i ++ ) { final SegmentInfo info = infos . info ( i ) ; if ( segmentsToOptimize . contains ( info ) ) { numToOptimize ++ ; optimizeInfo = info ; } } return numToOptimize <= maxNumSegments && ( numToOptimize != 1 || isOptimized ( writer , optimizeInfo ) ) ; } private boolean isOptimized ( IndexWriter writer , SegmentInfo info ) throws IOException { return ! info . hasDeletions ( ) && ! info . hasSeparateNorms ( ) && info . dir == writer . getDirectory ( ) && info . getUseCompoundFile ( ) == useCompoundFile ; } public MergeSpecification findMergesForOptimize ( SegmentInfos infos , IndexWriter writer , int maxNumSegments , Set segmentsToOptimize ) throws IOException { MergeSpecification spec ; assert maxNumSegments > 0 ; if ( ! isOptimized ( infos , writer , maxNumSegments , segmentsToOptimize ) ) { int last = infos . size ( ) ; while ( last > 0 ) { final SegmentInfo info = infos . info ( -- last ) ; if ( segmentsToOptimize . contains ( info ) ) { last ++ ; break ; } } if ( last > 0 ) { spec = new MergeSpecification ( ) ; while ( last - maxNumSegments + 1 >= mergeFactor ) { spec . add ( new OneMerge ( infos . range ( last - mergeFactor , last ) , useCompoundFile ) ) ; last -= mergeFactor ; } if ( 0 == spec . merges . size ( ) ) { if ( maxNumSegments == 1 ) { if ( last > 1 || ! isOptimized ( writer , infos . info ( 0 ) ) ) spec . add ( new OneMerge ( infos . range ( 0 , last ) , useCompoundFile ) ) ; } else if ( last > maxNumSegments ) { final int finalMergeSize = last - maxNumSegments + 1 ; long bestSize = 0 ; int bestStart = 0 ; for ( int i = 0 ; i < last - finalMergeSize + 1 ; i ++ ) { long sumSize = 0 ; for ( int j = 0 ; j < finalMergeSize ; j ++ ) sumSize += size ( infos . info ( j + i ) ) ; if ( i == 0 || ( sumSize < 2 * size ( infos . info ( i - 1 ) ) && sumSize < bestSize ) ) { bestStart = i ; bestSize = sumSize ; } } spec . add ( new OneMerge ( infos . range ( bestStart , bestStart + finalMergeSize ) , useCompoundFile ) ) ; } } } else spec = null ; } else spec = null ; return spec ; } public MergeSpecification findMergesToExpungeDeletes ( SegmentInfos segmentInfos , IndexWriter writer ) throws CorruptIndexException , IOException { this . writer = writer ; final int numSegments = segmentInfos . size ( ) ; message ( "findMergesToExpungeDeletes: " + numSegments + " segments" ) ; MergeSpecification spec = new MergeSpecification ( ) ; int firstSegmentWithDeletions = - 1 ; for ( int i = 0 ; i < numSegments ; i ++ ) { final SegmentInfo info = segmentInfos . info ( i ) ; if ( info . hasDeletions ( ) ) { message ( "  segment " + info . name + " has deletions" ) ; if ( firstSegmentWithDeletions == - 1 ) firstSegmentWithDeletions = i ; else if ( i - firstSegmentWithDeletions == mergeFactor ) { message ( "  add merge " + firstSegmentWithDeletions + " to " + ( i - 1 ) + " inclusive" ) ; spec . add ( new OneMerge ( segmentInfos . range ( firstSegmentWithDeletions , i ) , useCompoundFile ) ) ; firstSegmentWithDeletions = i ; } } else if ( firstSegmentWithDeletions != - 1 ) { message ( "  add merge " + firstSegmentWithDeletions + " to " + ( i - 1 ) + " inclusive" ) ; spec . add ( new OneMerge ( segmentInfos . range ( firstSegmentWithDeletions , i ) , useCompoundFile ) ) ; firstSegmentWithDeletions = - 1 ; } } if ( firstSegmentWithDeletions != - 1 ) { message ( "  add merge " + firstSegmentWithDeletions + " to " + ( numSegments - 1 ) + " inclusive" ) ; spec . add ( new OneMerge ( segmentInfos . range ( firstSegmentWithDeletions , numSegments ) , useCompoundFile ) ) ; } return spec ; } public MergeSpecification findMerges ( SegmentInfos infos , IndexWriter writer ) throws IOException { final int numSegments = infos . size ( ) ; this . writer = writer ; message ( "findMerges: " + numSegments + " segments" ) ; float [ ] levels = new float [ numSegments ] ; final float norm = ( float ) Math . log ( mergeFactor ) ; final Directory directory = writer . getDirectory ( ) ; for ( int i = 0 ; i < numSegments ; i ++ ) { final SegmentInfo info = infos . info ( i ) ; long size = size ( info ) ; if ( size < 1 ) size = 1 ; levels [ i ] = ( float ) Math . log ( size ) / norm ; } final float levelFloor ; if ( minMergeSize <= 0 ) levelFloor = ( float ) 0.0 ; else levelFloor = ( float ) ( Math . log ( minMergeSize ) / norm ) ; MergeSpecification spec = null ; int start = 0 ; while ( start < numSegments ) { float maxLevel = levels [ start ] ; for ( int i = 1 + start ; i < numSegments ; i ++ ) { final float level = levels [ i ] ; if ( level > maxLevel ) maxLevel = level ; } float levelBottom ; if ( maxLevel < levelFloor ) levelBottom = - 1.0F ; else { levelBottom = ( float ) ( maxLevel - LEVEL_LOG_SPAN ) ; if ( levelBottom < levelFloor && maxLevel >= levelFloor ) levelBottom = levelFloor ; } int upto = numSegments - 1 ; while ( upto >= start ) { if ( levels [ upto ] >= levelBottom ) { break ; } upto -- ; } message ( "  level " + levelBottom + " to " + maxLevel + ": " + ( 1 + upto - start ) + " segments" ) ; int end = start + mergeFactor ; while ( end <= 1 + upto ) { boolean anyTooLarge = false ; for ( int i = start ; i < end ; i ++ ) { final SegmentInfo info = infos . info ( i ) ; anyTooLarge |= ( size ( info ) >= maxMergeSize || info . docCount >= maxMergeDocs ) ; } if ( ! anyTooLarge ) { if ( spec == null ) spec = new MergeSpecification ( ) ; message ( "    " + start + " to " + end + ": add this merge" ) ; spec . add ( new OneMerge ( infos . range ( start , end ) , useCompoundFile ) ) ; } else message ( "    " + start + " to " + end + ": contains segment over maxMergeSize or maxMergeDocs; skipping" ) ; start = end ; end = start + mergeFactor ; } start = 1 + upto ; } return spec ; } public void setMaxMergeDocs ( int maxMergeDocs ) { this . maxMergeDocs = maxMergeDocs ; } public int getMaxMergeDocs ( ) { return maxMergeDocs ; } } 	1	['21', '2', '2', '10', '47', '108', '3', '8', '15', '0.920833333', '817', '0.333333333', '1', '0.24', '0.26875', '0', '0', '37.33333333', '2', '1', '4']
package org . apache . lucene . index ; import org . apache . lucene . store . BufferedIndexInput ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import java . io . IOException ; import java . util . Arrays ; class TermVectorsReader implements Cloneable { static final int FORMAT_VERSION = 2 ; static final int FORMAT_VERSION2 = 3 ; static final int FORMAT_UTF8_LENGTH_IN_BYTES = 4 ; static final int FORMAT_CURRENT = FORMAT_UTF8_LENGTH_IN_BYTES ; static final int FORMAT_SIZE = 4 ; static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x1 ; static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x2 ; private FieldInfos fieldInfos ; private IndexInput tvx ; private IndexInput tvd ; private IndexInput tvf ; private int size ; private int numTotalDocs ; private int docStoreOffset ; private final int format ; TermVectorsReader ( Directory d , String segment , FieldInfos fieldInfos ) throws CorruptIndexException , IOException { this ( d , segment , fieldInfos , BufferedIndexInput . BUFFER_SIZE ) ; } TermVectorsReader ( Directory d , String segment , FieldInfos fieldInfos , int readBufferSize ) throws CorruptIndexException , IOException { this ( d , segment , fieldInfos , readBufferSize , - 1 , 0 ) ; } TermVectorsReader ( Directory d , String segment , FieldInfos fieldInfos , int readBufferSize , int docStoreOffset , int size ) throws CorruptIndexException , IOException { boolean success = false ; try { if ( d . fileExists ( segment + "." + IndexFileNames . VECTORS_INDEX_EXTENSION ) ) { tvx = d . openInput ( segment + "." + IndexFileNames . VECTORS_INDEX_EXTENSION , readBufferSize ) ; format = checkValidFormat ( tvx ) ; tvd = d . openInput ( segment + "." + IndexFileNames . VECTORS_DOCUMENTS_EXTENSION , readBufferSize ) ; final int tvdFormat = checkValidFormat ( tvd ) ; tvf = d . openInput ( segment + "." + IndexFileNames . VECTORS_FIELDS_EXTENSION , readBufferSize ) ; final int tvfFormat = checkValidFormat ( tvf ) ; assert format == tvdFormat ; assert format == tvfFormat ; if ( format >= FORMAT_VERSION2 ) { assert ( tvx . length ( ) - FORMAT_SIZE ) % 16 == 0 ; numTotalDocs = ( int ) ( tvx . length ( ) > > 4 ) ; } else { assert ( tvx . length ( ) - FORMAT_SIZE ) % 8 == 0 ; numTotalDocs = ( int ) ( tvx . length ( ) > > 3 ) ; } if ( - 1 == docStoreOffset ) { this . docStoreOffset = 0 ; this . size = numTotalDocs ; assert size == 0 || numTotalDocs == size ; } else { this . docStoreOffset = docStoreOffset ; this . size = size ; assert numTotalDocs >= size + docStoreOffset : "numTotalDocs=" + numTotalDocs + " size=" + size + " docStoreOffset=" + docStoreOffset ; } } else format = 0 ; this . fieldInfos = fieldInfos ; success = true ; } finally { if ( ! success ) { close ( ) ; } } } IndexInput getTvdStream ( ) { return tvd ; } IndexInput getTvfStream ( ) { return tvf ; } final private void seekTvx ( final int docNum ) throws IOException { if ( format < FORMAT_VERSION2 ) tvx . seek ( ( docNum + docStoreOffset ) * 8L + FORMAT_SIZE ) ; else tvx . seek ( ( docNum + docStoreOffset ) * 16L + FORMAT_SIZE ) ; } boolean canReadRawDocs ( ) { return format >= FORMAT_UTF8_LENGTH_IN_BYTES ; } final void rawDocs ( int [ ] tvdLengths , int [ ] tvfLengths , int startDocID , int numDocs ) throws IOException { if ( tvx == null ) { Arrays . fill ( tvdLengths , 0 ) ; Arrays . fill ( tvfLengths , 0 ) ; return ; } if ( format < FORMAT_VERSION2 ) throw new IllegalStateException ( "cannot read raw docs with older term vector formats" ) ; seekTvx ( startDocID ) ; long tvdPosition = tvx . readLong ( ) ; tvd . seek ( tvdPosition ) ; long tvfPosition = tvx . readLong ( ) ; tvf . seek ( tvfPosition ) ; long lastTvdPosition = tvdPosition ; long lastTvfPosition = tvfPosition ; int count = 0 ; while ( count < numDocs ) { final int docID = docStoreOffset + startDocID + count + 1 ; assert docID <= numTotalDocs ; if ( docID < numTotalDocs ) { tvdPosition = tvx . readLong ( ) ; tvfPosition = tvx . readLong ( ) ; } else { tvdPosition = tvd . length ( ) ; tvfPosition = tvf . length ( ) ; assert count == numDocs - 1 ; } tvdLengths [ count ] = ( int ) ( tvdPosition - lastTvdPosition ) ; tvfLengths [ count ] = ( int ) ( tvfPosition - lastTvfPosition ) ; count ++ ; lastTvdPosition = tvdPosition ; lastTvfPosition = tvfPosition ; } } private int checkValidFormat ( IndexInput in ) throws CorruptIndexException , IOException { int format = in . readInt ( ) ; if ( format > FORMAT_CURRENT ) { throw new CorruptIndexException ( "Incompatible format version: " + format + " expected " + FORMAT_CURRENT + " or less" ) ; } return format ; } void close ( ) throws IOException { IOException keep = null ; if ( tvx != null ) try { tvx . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tvd != null ) try { tvd . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tvf != null ) try { tvf . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( keep != null ) throw ( IOException ) keep . fillInStackTrace ( ) ; } int size ( ) { return size ; } public void get ( int docNum , String field , TermVectorMapper mapper ) throws IOException { if ( tvx != null ) { int fieldNumber = fieldInfos . fieldNumber ( field ) ; seekTvx ( docNum ) ; long tvdPosition = tvx . readLong ( ) ; tvd . seek ( tvdPosition ) ; int fieldCount = tvd . readVInt ( ) ; int number = 0 ; int found = - 1 ; for ( int i = 0 ; i < fieldCount ; i ++ ) { if ( format >= FORMAT_VERSION ) number = tvd . readVInt ( ) ; else number += tvd . readVInt ( ) ; if ( number == fieldNumber ) found = i ; } if ( found != - 1 ) { long position ; if ( format >= FORMAT_VERSION2 ) position = tvx . readLong ( ) ; else position = tvd . readVLong ( ) ; for ( int i = 1 ; i <= found ; i ++ ) position += tvd . readVLong ( ) ; mapper . setDocumentNumber ( docNum ) ; readTermVector ( field , position , mapper ) ; } else { } } else { } } TermFreqVector get ( int docNum , String field ) throws IOException { ParallelArrayTermVectorMapper mapper = new ParallelArrayTermVectorMapper ( ) ; get ( docNum , field , mapper ) ; return mapper . materializeVector ( ) ; } final private String [ ] readFields ( int fieldCount ) throws IOException { int number = 0 ; String [ ] fields = new String [ fieldCount ] ; for ( int i = 0 ; i < fieldCount ; i ++ ) { if ( format >= FORMAT_VERSION ) number = tvd . readVInt ( ) ; else number += tvd . readVInt ( ) ; fields [ i ] = fieldInfos . fieldName ( number ) ; } return fields ; } final private long [ ] readTvfPointers ( int fieldCount ) throws IOException { long position ; if ( format >= FORMAT_VERSION2 ) position = tvx . readLong ( ) ; else position = tvd . readVLong ( ) ; long [ ] tvfPointers = new long [ fieldCount ] ; tvfPointers [ 0 ] = position ; for ( int i = 1 ; i < fieldCount ; i ++ ) { position += tvd . readVLong ( ) ; tvfPointers [ i ] = position ; } return tvfPointers ; } TermFreqVector [ ] get ( int docNum ) throws IOException { TermFreqVector [ ] result = null ; if ( tvx != null ) { seekTvx ( docNum ) ; long tvdPosition = tvx . readLong ( ) ; tvd . seek ( tvdPosition ) ; int fieldCount = tvd . readVInt ( ) ; if ( fieldCount != 0 ) { final String [ ] fields = readFields ( fieldCount ) ; final long [ ] tvfPointers = readTvfPointers ( fieldCount ) ; result = readTermVectors ( docNum , fields , tvfPointers ) ; } } else { } return result ; } public void get ( int docNumber , TermVectorMapper mapper ) throws IOException { if ( tvx != null ) { seekTvx ( docNumber ) ; long tvdPosition = tvx . readLong ( ) ; tvd . seek ( tvdPosition ) ; int fieldCount = tvd . readVInt ( ) ; if ( fieldCount != 0 ) { final String [ ] fields = readFields ( fieldCount ) ; final long [ ] tvfPointers = readTvfPointers ( fieldCount ) ; mapper . setDocumentNumber ( docNumber ) ; readTermVectors ( fields , tvfPointers , mapper ) ; } } else { } } private SegmentTermVector [ ] readTermVectors ( int docNum , String fields [ ] , long tvfPointers [ ] ) throws IOException { SegmentTermVector res [ ] = new SegmentTermVector [ fields . length ] ; for ( int i = 0 ; i < fields . length ; i ++ ) { ParallelArrayTermVectorMapper mapper = new ParallelArrayTermVectorMapper ( ) ; mapper . setDocumentNumber ( docNum ) ; readTermVector ( fields [ i ] , tvfPointers [ i ] , mapper ) ; res [ i ] = ( SegmentTermVector ) mapper . materializeVector ( ) ; } return res ; } private void readTermVectors ( String fields [ ] , long tvfPointers [ ] , TermVectorMapper mapper ) throws IOException { for ( int i = 0 ; i < fields . length ; i ++ ) { readTermVector ( fields [ i ] , tvfPointers [ i ] , mapper ) ; } } private void readTermVector ( String field , long tvfPointer , TermVectorMapper mapper ) throws IOException { tvf . seek ( tvfPointer ) ; int numTerms = tvf . readVInt ( ) ; if ( numTerms == 0 ) return ; boolean storePositions ; boolean storeOffsets ; if ( format >= FORMAT_VERSION ) { byte bits = tvf . readByte ( ) ; storePositions = ( bits & STORE_POSITIONS_WITH_TERMVECTOR ) != 0 ; storeOffsets = ( bits & STORE_OFFSET_WITH_TERMVECTOR ) != 0 ; } else { tvf . readVInt ( ) ; storePositions = false ; storeOffsets = false ; } mapper . setExpectations ( field , numTerms , storeOffsets , storePositions ) ; int start = 0 ; int deltaLength = 0 ; int totalLength = 0 ; byte [ ] byteBuffer ; char [ ] charBuffer ; final boolean preUTF8 = format < FORMAT_UTF8_LENGTH_IN_BYTES ; if ( preUTF8 ) { charBuffer = new char [ 10 ] ; byteBuffer = null ; } else { charBuffer = null ; byteBuffer = new byte [ 20 ] ; } for ( int i = 0 ; i < numTerms ; i ++ ) { start = tvf . readVInt ( ) ; deltaLength = tvf . readVInt ( ) ; totalLength = start + deltaLength ; final String term ; if ( preUTF8 ) { if ( charBuffer . length < totalLength ) { char [ ] newCharBuffer = new char [ ( int ) ( 1.5 * totalLength ) ] ; System . arraycopy ( charBuffer , 0 , newCharBuffer , 0 , start ) ; charBuffer = newCharBuffer ; } tvf . readChars ( charBuffer , start , deltaLength ) ; term = new String ( charBuffer , 0 , totalLength ) ; } else { if ( byteBuffer . length < totalLength ) { byte [ ] newByteBuffer = new byte [ ( int ) ( 1.5 * totalLength ) ] ; System . arraycopy ( byteBuffer , 0 , newByteBuffer , 0 , start ) ; byteBuffer = newByteBuffer ; } tvf . readBytes ( byteBuffer , start , deltaLength ) ; term = new String ( byteBuffer , 0 , totalLength , "UTF-8" ) ; } int freq = tvf . readVInt ( ) ; int [ ] positions = null ; if ( storePositions ) { if ( mapper . isIgnoringPositions ( ) == false ) { positions = new int [ freq ] ; int prevPosition = 0 ; for ( int j = 0 ; j < freq ; j ++ ) { positions [ j ] = prevPosition + tvf . readVInt ( ) ; prevPosition = positions [ j ] ; } } else { for ( int j = 0 ; j < freq ; j ++ ) { tvf . readVInt ( ) ; } } } TermVectorOffsetInfo [ ] offsets = null ; if ( storeOffsets ) { if ( mapper . isIgnoringOffsets ( ) == false ) { offsets = new TermVectorOffsetInfo [ freq ] ; int prevOffset = 0 ; for ( int j = 0 ; j < freq ; j ++ ) { int startOffset = prevOffset + tvf . readVInt ( ) ; int endOffset = startOffset + tvf . readVInt ( ) ; offsets [ j ] = new TermVectorOffsetInfo ( startOffset , endOffset ) ; prevOffset = endOffset ; } } else { for ( int j = 0 ; j < freq ; j ++ ) { tvf . readVInt ( ) ; tvf . readVInt ( ) ; } } } mapper . map ( term , freq , offsets , positions ) ; } } protected Object clone ( ) throws CloneNotSupportedException { final TermVectorsReader clone = ( TermVectorsReader ) super . clone ( ) ; if ( tvx != null && tvd != null && tvf != null ) { clone . tvx = ( IndexInput ) tvx . clone ( ) ; clone . tvd = ( IndexInput ) tvd . clone ( ) ; clone . tvf = ( IndexInput ) tvf . clone ( ) ; } return clone ; } } class ParallelArrayTermVectorMapper extends TermVectorMapper { private String [ ] terms ; private int [ ] termFreqs ; private int positions [ ] [ ] ; private TermVectorOffsetInfo offsets [ ] [ ] ; private int currentPosition ; private boolean storingOffsets ; private boolean storingPositions ; private String field ; public void setExpectations ( String field , int numTerms , boolean storeOffsets , boolean storePositions ) { this . field = field ; terms = new String [ numTerms ] ; termFreqs = new int [ numTerms ] ; this . storingOffsets = storeOffsets ; this . storingPositions = storePositions ; if ( storePositions ) this . positions = new int [ numTerms ] [ ] ; if ( storeOffsets ) this . offsets = new TermVectorOffsetInfo [ numTerms ] [ ] ; } public void map ( String term , int frequency , TermVectorOffsetInfo [ ] offsets , int [ ] positions ) { terms [ currentPosition ] = term ; termFreqs [ currentPosition ] = frequency ; if ( storingOffsets ) { this . offsets [ currentPosition ] = offsets ; } if ( storingPositions ) { this . positions [ currentPosition ] = positions ; } currentPosition ++ ; } public TermFreqVector materializeVector ( ) { SegmentTermVector tv = null ; if ( field != null && terms != null ) { if ( storingPositions || storingOffsets ) { tv = new SegmentTermPositionVector ( field , terms , termFreqs , positions , offsets ) ; } else { tv = new SegmentTermVector ( field , terms , termFreqs ) ; } } return tv ; } } 	0	['23', '1', '0', '12', '66', '99', '3', '9', '2', '0.847593583', '1164', '0.470588235', '4', '0', '0.231404959', '0', '0', '48.86956522', '2', '0.8696', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class LockFactory { protected String lockPrefix = "" ; public void setLockPrefix ( String lockPrefix ) { this . lockPrefix = lockPrefix ; } public String getLockPrefix ( ) { return this . lockPrefix ; } public abstract Lock makeLock ( String lockName ) ; abstract public void clearLock ( String lockName ) throws IOException ; } 	1	['5', '1', '5', '10', '6', '4', '9', '1', '5', '0.5', '19', '1', '0', '0', '0.8', '0', '0', '2.6', '1', '0.8', '1']
package org . apache . lucene . index ; public class TermVectorEntry { private String field ; private String term ; private int frequency ; private TermVectorOffsetInfo [ ] offsets ; int [ ] positions ; public TermVectorEntry ( ) { } public TermVectorEntry ( String field , String term , int frequency , TermVectorOffsetInfo [ ] offsets , int [ ] positions ) { this . field = field ; this . term = term ; this . frequency = frequency ; this . offsets = offsets ; this . positions = positions ; } public String getField ( ) { return field ; } public int getFrequency ( ) { return frequency ; } public TermVectorOffsetInfo [ ] getOffsets ( ) { return offsets ; } public int [ ] getPositions ( ) { return positions ; } public String getTerm ( ) { return term ; } void setFrequency ( int frequency ) { this . frequency = frequency ; } void setOffsets ( TermVectorOffsetInfo [ ] offsets ) { this . offsets = offsets ; } void setPositions ( int [ ] positions ) { this . positions = positions ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( o == null || getClass ( ) != o . getClass ( ) ) return false ; TermVectorEntry that = ( TermVectorEntry ) o ; if ( term != null ? ! term . equals ( that . term ) : that . term != null ) return false ; return true ; } public int hashCode ( ) { return ( term != null ? term . hashCode ( ) : 0 ) ; } public String toString ( ) { return "TermVectorEntry{" + "field='" + field + '\'' + ", term='" + term + '\'' + ", frequency=" + frequency + '}' ; } } 	0	['13', '1', '0', '4', '22', '32', '3', '1', '10', '0.783333333', '135', '0.8', '1', '0', '0.269230769', '1', '1', '9', '7', '1.3846', '0']
package org . apache . lucene . search ; import java . io . IOException ; class NonMatchingScorer extends Scorer { public NonMatchingScorer ( ) { super ( null ) ; } public int doc ( ) { throw new UnsupportedOperationException ( ) ; } public boolean next ( ) throws IOException { return false ; } public float score ( ) { throw new UnsupportedOperationException ( ) ; } public boolean skipTo ( int target ) { return false ; } public Explanation explain ( int doc ) { Explanation e = new Explanation ( ) ; e . setDescription ( "No document matches." ) ; return e ; } } 	1	['6', '3', '0', '4', '10', '15', '1', '3', '6', '2', '31', '0', '0', '0.615384615', '0.666666667', '1', '3', '4.166666667', '1', '0.8333', '3']
package org . apache . lucene . analysis ; public class ISOLatin1AccentFilter extends TokenFilter { public ISOLatin1AccentFilter ( TokenStream input ) { super ( input ) ; } private char [ ] output = new char [ 256 ] ; private int outputPos ; public final Token next ( final Token reusableToken ) throws java . io . IOException { assert reusableToken != null ; Token nextToken = input . next ( reusableToken ) ; if ( nextToken != null ) { final char [ ] buffer = nextToken . termBuffer ( ) ; final int length = nextToken . termLength ( ) ; for ( int i = 0 ; i < length ; i ++ ) { final char c = buffer [ i ] ; if ( c >= '' && c <= '' ) { removeAccents ( buffer , length ) ; nextToken . setTermBuffer ( output , 0 , outputPos ) ; break ; } } return nextToken ; } else return null ; } public final void removeAccents ( char [ ] input , int length ) { final int maxSizeNeeded = 2 * length ; int size = output . length ; while ( size < maxSizeNeeded ) size *= 2 ; if ( size != output . length ) output = new char [ size ] ; outputPos = 0 ; int pos = 0 ; for ( int i = 0 ; i < length ; i ++ , pos ++ ) { final char c = input [ pos ] ; if ( c < '' || c > '' ) output [ outputPos ++ ] = c ; else { switch ( c ) { case '' : case '' : case '' : case '' : case '' : case '' : output [ outputPos ++ ] = 'A' ; break ; case '' : output [ outputPos ++ ] = 'A' ; output [ outputPos ++ ] = 'E' ; break ; case '' : output [ outputPos ++ ] = 'C' ; break ; case '' : case '' : case '' : case '' : output [ outputPos ++ ] = 'E' ; break ; case '' : case '' : case '' : case '' : output [ outputPos ++ ] = 'I' ; break ; case '' : output [ outputPos ++ ] = 'I' ; output [ outputPos ++ ] = 'J' ; break ; case '' : output [ outputPos ++ ] = 'D' ; break ; case '' : output [ outputPos ++ ] = 'N' ; break ; case '' : case '' : case '' : case '' : case '' : case '' : output [ outputPos ++ ] = 'O' ; break ; case '' : output [ outputPos ++ ] = 'O' ; output [ outputPos ++ ] = 'E' ; break ; case '' : output [ outputPos ++ ] = 'T' ; output [ outputPos ++ ] = 'H' ; break ; case '' : case '' : case '' : case '' : output [ outputPos ++ ] = 'U' ; break ; case '' : case '' : output [ outputPos ++ ] = 'Y' ; break ; case '' : case '' : case '' : case '' : case '' : case '' : output [ outputPos ++ ] = 'a' ; break ; case '' : output [ outputPos ++ ] = 'a' ; output [ outputPos ++ ] = 'e' ; break ; case '' : output [ outputPos ++ ] = 'c' ; break ; case '' : case '' : case '' : case '' : output [ outputPos ++ ] = 'e' ; break ; case '' : case '' : case '' : case '' : output [ outputPos ++ ] = 'i' ; break ; case '' : output [ outputPos ++ ] = 'i' ; output [ outputPos ++ ] = 'j' ; break ; case '' : output [ outputPos ++ ] = 'd' ; break ; case '' : output [ outputPos ++ ] = 'n' ; break ; case '' : case '' : case '' : case '' : case '' : case '' : output [ outputPos ++ ] = 'o' ; break ; case '' : output [ outputPos ++ ] = 'o' ; output [ outputPos ++ ] = 'e' ; break ; case '' : output [ outputPos ++ ] = 's' ; output [ outputPos ++ ] = 's' ; break ; case '' : output [ outputPos ++ ] = 't' ; output [ outputPos ++ ] = 'h' ; break ; case '' : case '' : case '' : case '' : output [ outputPos ++ ] = 'u' ; break ; case '' : case '' : output [ outputPos ++ ] = 'y' ; break ; case '' : output [ outputPos ++ ] = 'f' ; output [ outputPos ++ ] = 'f' ; break ; case '' : output [ outputPos ++ ] = 'f' ; output [ outputPos ++ ] = 'i' ; break ; case '' : output [ outputPos ++ ] = 'f' ; output [ outputPos ++ ] = 'l' ; break ; case '' : output [ outputPos ++ ] = 'f' ; output [ outputPos ++ ] = 't' ; break ; case '' : output [ outputPos ++ ] = 's' ; output [ outputPos ++ ] = 't' ; break ; default : output [ outputPos ++ ] = c ; break ; } } } } } 	0	['5', '3', '0', '3', '15', '2', '0', '3', '3', '0.75', '708', '0.5', '0', '0.7', '0.333333333', '1', '2', '139.8', '79', '16.2', '0']
package org . apache . lucene . search . spans ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . search . * ; import java . io . IOException ; import java . util . HashSet ; import java . util . Iterator ; import java . util . Set ; public class SpanWeight implements Weight { protected Similarity similarity ; protected float value ; protected float idf ; protected float queryNorm ; protected float queryWeight ; protected Set terms ; protected SpanQuery query ; public SpanWeight ( SpanQuery query , Searcher searcher ) throws IOException { this . similarity = query . getSimilarity ( searcher ) ; this . query = query ; terms = new HashSet ( ) ; query . extractTerms ( terms ) ; idf = this . query . getSimilarity ( searcher ) . idf ( terms , searcher ) ; } public Query getQuery ( ) { return query ; } public float getValue ( ) { return value ; } public float sumOfSquaredWeights ( ) throws IOException { queryWeight = idf * query . getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float queryNorm ) { this . queryNorm = queryNorm ; queryWeight *= queryNorm ; value = queryWeight * idf ; } public Scorer scorer ( IndexReader reader ) throws IOException { return new SpanScorer ( query . getSpans ( reader ) , this , similarity , reader . norms ( query . getField ( ) ) ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { ComplexExplanation result = new ComplexExplanation ( ) ; result . setDescription ( "weight(" + getQuery ( ) + " in " + doc + "), product of:" ) ; String field = ( ( SpanQuery ) getQuery ( ) ) . getField ( ) ; StringBuffer docFreqs = new StringBuffer ( ) ; Iterator i = terms . iterator ( ) ; while ( i . hasNext ( ) ) { Term term = ( Term ) i . next ( ) ; docFreqs . append ( term . text ( ) ) ; docFreqs . append ( "=" ) ; docFreqs . append ( reader . docFreq ( term ) ) ; if ( i . hasNext ( ) ) { docFreqs . append ( " " ) ; } } Explanation idfExpl = new Explanation ( idf , "idf(" + field + ": " + docFreqs + ")" ) ; Explanation queryExpl = new Explanation ( ) ; queryExpl . setDescription ( "queryWeight(" + getQuery ( ) + "), product of:" ) ; Explanation boostExpl = new Explanation ( getQuery ( ) . getBoost ( ) , "boost" ) ; if ( getQuery ( ) . getBoost ( ) != 1.0f ) queryExpl . addDetail ( boostExpl ) ; queryExpl . addDetail ( idfExpl ) ; Explanation queryNormExpl = new Explanation ( queryNorm , "queryNorm" ) ; queryExpl . addDetail ( queryNormExpl ) ; queryExpl . setValue ( boostExpl . getValue ( ) * idfExpl . getValue ( ) * queryNormExpl . getValue ( ) ) ; result . addDetail ( queryExpl ) ; ComplexExplanation fieldExpl = new ComplexExplanation ( ) ; fieldExpl . setDescription ( "fieldWeight(" + field + ":" + query . toString ( field ) + " in " + doc + "), product of:" ) ; Explanation tfExpl = scorer ( reader ) . explain ( doc ) ; fieldExpl . addDetail ( tfExpl ) ; fieldExpl . addDetail ( idfExpl ) ; Explanation fieldNormExpl = new Explanation ( ) ; byte [ ] fieldNorms = reader . norms ( field ) ; float fieldNorm = fieldNorms != null ? Similarity . decodeNorm ( fieldNorms [ doc ] ) : 0.0f ; fieldNormExpl . setValue ( fieldNorm ) ; fieldNormExpl . setDescription ( "fieldNorm(field=" + field + ", doc=" + doc + ")" ) ; fieldExpl . addDetail ( fieldNormExpl ) ; fieldExpl . setMatch ( Boolean . valueOf ( tfExpl . isMatch ( ) ) ) ; fieldExpl . setValue ( tfExpl . getValue ( ) * idfExpl . getValue ( ) * fieldNormExpl . getValue ( ) ) ; result . addDetail ( fieldExpl ) ; result . setMatch ( fieldExpl . getMatch ( ) ) ; result . setValue ( queryExpl . getValue ( ) * fieldExpl . getValue ( ) ) ; if ( queryExpl . getValue ( ) == 1.0f ) return fieldExpl ; return result ; } } 	1	['7', '1', '1', '13', '46', '0', '2', '12', '7', '0.69047619', '357', '1', '2', '0', '0.30952381', '0', '0', '49', '1', '0.8571', '4']
package org . apache . lucene . util . cache ; import java . util . LinkedHashMap ; import java . util . Map ; public class SimpleLRUCache extends SimpleMapCache { private final static float LOADFACTOR = 0.75f ; private int cacheSize ; public SimpleLRUCache ( int cacheSize ) { super ( null ) ; this . cacheSize = cacheSize ; int capacity = ( int ) Math . ceil ( cacheSize / LOADFACTOR ) + 1 ; super . map = new LinkedHashMap ( capacity , LOADFACTOR , true ) { protected boolean removeEldestEntry ( Map . Entry eldest ) { return size ( ) > SimpleLRUCache . this . cacheSize ; } } ; } } 	0	['2', '3', '0', '3', '5', '0', '2', '2', '1', '1', '33', '1', '0', '0.923076923', '0.5', '1', '4', '14.5', '1', '0.5', '0']
package org . apache . lucene . queryParser ; public interface CharStream { char readChar ( ) throws java . io . IOException ; int getColumn ( ) ; int getLine ( ) ; int getEndColumn ( ) ; int getEndLine ( ) ; int getBeginColumn ( ) ; int getBeginLine ( ) ; void backup ( int amount ) ; char BeginToken ( ) throws java . io . IOException ; String GetImage ( ) ; char [ ] GetSuffix ( int len ) ; void Done ( ) ; } 	1	['12', '1', '0', '3', '12', '66', '3', '0', '12', '2', '12', '0', '0', '0', '0.583333333', '0', '0', '0', '1', '1', '1']
package org . apache . lucene . store ; import java . util . ArrayList ; import java . io . Serializable ; class RAMFile implements Serializable { private static final long serialVersionUID = 1l ; private ArrayList buffers = new ArrayList ( ) ; long length ; RAMDirectory directory ; long sizeInBytes ; private long lastModified = System . currentTimeMillis ( ) ; RAMFile ( ) { } RAMFile ( RAMDirectory directory ) { this . directory = directory ; } synchronized long getLength ( ) { return length ; } synchronized void setLength ( long length ) { this . length = length ; } synchronized long getLastModified ( ) { return lastModified ; } synchronized void setLastModified ( long lastModified ) { this . lastModified = lastModified ; } final synchronized byte [ ] addBuffer ( int size ) { byte [ ] buffer = newBuffer ( size ) ; if ( directory != null ) synchronized ( directory ) { buffers . add ( buffer ) ; directory . sizeInBytes += size ; sizeInBytes += size ; } else buffers . add ( buffer ) ; return buffer ; } final synchronized byte [ ] getBuffer ( int index ) { return ( byte [ ] ) buffers . get ( index ) ; } final synchronized int numBuffers ( ) { return buffers . size ( ) ; } byte [ ] newBuffer ( int size ) { return new byte [ size ] ; } long getSizeInBytes ( ) { synchronized ( directory ) { return sizeInBytes ; } } } 	0	['11', '1', '0', '3', '17', '19', '3', '1', '0', '0.833333333', '133', '0.5', '1', '0', '0.386363636', '0', '0', '10.54545455', '2', '0.9091', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . TermPositions ; import java . io . IOException ; import java . util . HashMap ; final class SloppyPhraseScorer extends PhraseScorer { private int slop ; private PhrasePositions repeats [ ] ; private PhrasePositions tmpPos [ ] ; private boolean checkedRepeats ; SloppyPhraseScorer ( Weight weight , TermPositions [ ] tps , int [ ] offsets , Similarity similarity , int slop , byte [ ] norms ) { super ( weight , tps , offsets , similarity , norms ) ; this . slop = slop ; } protected final float phraseFreq ( ) throws IOException { int end = initPhrasePositions ( ) ; float freq = 0.0f ; boolean done = ( end < 0 ) ; while ( ! done ) { PhrasePositions pp = ( PhrasePositions ) pq . pop ( ) ; int start = pp . position ; int next = ( ( PhrasePositions ) pq . top ( ) ) . position ; boolean tpsDiffer = true ; for ( int pos = start ; pos <= next || ! tpsDiffer ; pos = pp . position ) { if ( pos <= next && tpsDiffer ) start = pos ; if ( ! pp . nextPosition ( ) ) { done = true ; break ; } PhrasePositions pp2 = null ; tpsDiffer = ! pp . repeats || ( pp2 = termPositionsDiffer ( pp ) ) == null ; if ( pp2 != null && pp2 != pp ) { pp = flip ( pp , pp2 ) ; } } int matchLength = end - start ; if ( matchLength <= slop ) freq += getSimilarity ( ) . sloppyFreq ( matchLength ) ; if ( pp . position > end ) end = pp . position ; pq . put ( pp ) ; } return freq ; } private PhrasePositions flip ( PhrasePositions pp , PhrasePositions pp2 ) { int n = 0 ; PhrasePositions pp3 ; while ( ( pp3 = ( PhrasePositions ) pq . pop ( ) ) != pp2 ) { tmpPos [ n ++ ] = pp3 ; } for ( n -- ; n >= 0 ; n -- ) { pq . insert ( tmpPos [ n ] ) ; } pq . put ( pp ) ; return pp2 ; } private int initPhrasePositions ( ) throws IOException { int end = 0 ; if ( checkedRepeats && repeats == null ) { pq . clear ( ) ; for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) { pp . firstPosition ( ) ; if ( pp . position > end ) end = pp . position ; pq . put ( pp ) ; } return end ; } for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) pp . firstPosition ( ) ; if ( ! checkedRepeats ) { checkedRepeats = true ; HashMap m = null ; for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) { int tpPos = pp . position + pp . offset ; for ( PhrasePositions pp2 = pp . next ; pp2 != null ; pp2 = pp2 . next ) { int tpPos2 = pp2 . position + pp2 . offset ; if ( tpPos2 == tpPos ) { if ( m == null ) m = new HashMap ( ) ; pp . repeats = true ; pp2 . repeats = true ; m . put ( pp , null ) ; m . put ( pp2 , null ) ; } } } if ( m != null ) repeats = ( PhrasePositions [ ] ) m . keySet ( ) . toArray ( new PhrasePositions [ 0 ] ) ; } if ( repeats != null ) { for ( int i = 0 ; i < repeats . length ; i ++ ) { PhrasePositions pp = repeats [ i ] ; PhrasePositions pp2 ; while ( ( pp2 = termPositionsDiffer ( pp ) ) != null ) { if ( ! pp2 . nextPosition ( ) ) return - 1 ; } } } pq . clear ( ) ; for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) { if ( pp . position > end ) end = pp . position ; pq . put ( pp ) ; } if ( repeats != null ) { tmpPos = new PhrasePositions [ pq . size ( ) ] ; } return end ; } private PhrasePositions termPositionsDiffer ( PhrasePositions pp ) { int tpPos = pp . position + pp . offset ; for ( int i = 0 ; i < repeats . length ; i ++ ) { PhrasePositions pp2 = repeats [ i ] ; if ( pp2 == pp ) continue ; int tpPos2 = pp2 . position + pp2 . offset ; if ( tpPos2 == tpPos ) return pp . offset > pp2 . offset ? pp : pp2 ; } return null ; } } 	1	['5', '4', '0', '8', '20', '0', '2', '6', '0', '0.625', '389', '1', '2', '0.833333333', '0.325', '1', '1', '76', '5', '2', '2']
package org . apache . lucene . store ; import java . io . IOException ; public class LockObtainFailedException extends IOException { public LockObtainFailedException ( String message ) { super ( message ) ; } } 	0	['1', '4', '0', '8', '2', '0', '8', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . index ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . IndexOutput ; import java . io . IOException ; import java . util . * ; final class FieldInfos { static final byte IS_INDEXED = 0x1 ; static final byte STORE_TERMVECTOR = 0x2 ; static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x4 ; static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x8 ; static final byte OMIT_NORMS = 0x10 ; static final byte STORE_PAYLOADS = 0x20 ; static final byte OMIT_TF = 0x40 ; private ArrayList byNumber = new ArrayList ( ) ; private HashMap byName = new HashMap ( ) ; FieldInfos ( ) { } FieldInfos ( Directory d , String name ) throws IOException { IndexInput input = d . openInput ( name ) ; try { read ( input ) ; } finally { input . close ( ) ; } } synchronized public Object clone ( ) { FieldInfos fis = new FieldInfos ( ) ; final int numField = byNumber . size ( ) ; for ( int i = 0 ; i < numField ; i ++ ) { FieldInfo fi = ( FieldInfo ) ( ( FieldInfo ) byNumber . get ( i ) ) . clone ( ) ; fis . byNumber . add ( fi ) ; fis . byName . put ( fi . name , fi ) ; } return fis ; } synchronized public void add ( Document doc ) { List fields = doc . getFields ( ) ; Iterator fieldIterator = fields . iterator ( ) ; while ( fieldIterator . hasNext ( ) ) { Fieldable field = ( Fieldable ) fieldIterator . next ( ) ; add ( field . name ( ) , field . isIndexed ( ) , field . isTermVectorStored ( ) , field . isStorePositionWithTermVector ( ) , field . isStoreOffsetWithTermVector ( ) , field . getOmitNorms ( ) ) ; } } boolean hasProx ( ) { final int numFields = byNumber . size ( ) ; for ( int i = 0 ; i < numFields ; i ++ ) if ( ! fieldInfo ( i ) . omitTf ) return true ; return false ; } synchronized public void addIndexed ( Collection names , boolean storeTermVectors , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector ) { Iterator i = names . iterator ( ) ; while ( i . hasNext ( ) ) { add ( ( String ) i . next ( ) , true , storeTermVectors , storePositionWithTermVector , storeOffsetWithTermVector ) ; } } synchronized public void add ( Collection names , boolean isIndexed ) { Iterator i = names . iterator ( ) ; while ( i . hasNext ( ) ) { add ( ( String ) i . next ( ) , isIndexed ) ; } } synchronized public void add ( String name , boolean isIndexed ) { add ( name , isIndexed , false , false , false , false ) ; } synchronized public void add ( String name , boolean isIndexed , boolean storeTermVector ) { add ( name , isIndexed , storeTermVector , false , false , false ) ; } synchronized public void add ( String name , boolean isIndexed , boolean storeTermVector , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector ) { add ( name , isIndexed , storeTermVector , storePositionWithTermVector , storeOffsetWithTermVector , false ) ; } synchronized public void add ( String name , boolean isIndexed , boolean storeTermVector , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector , boolean omitNorms ) { add ( name , isIndexed , storeTermVector , storePositionWithTermVector , storeOffsetWithTermVector , omitNorms , false , false ) ; } synchronized public FieldInfo add ( String name , boolean isIndexed , boolean storeTermVector , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector , boolean omitNorms , boolean storePayloads , boolean omitTf ) { FieldInfo fi = fieldInfo ( name ) ; if ( fi == null ) { return addInternal ( name , isIndexed , storeTermVector , storePositionWithTermVector , storeOffsetWithTermVector , omitNorms , storePayloads , omitTf ) ; } else { fi . update ( isIndexed , storeTermVector , storePositionWithTermVector , storeOffsetWithTermVector , omitNorms , storePayloads , omitTf ) ; } return fi ; } synchronized public FieldInfo add ( FieldInfo fieldInfo ) { FieldInfo fi = fieldInfo ( fieldInfo . name ) ; if ( fi == null ) { return addInternal ( fieldInfo . name , fieldInfo . isIndexed , fieldInfo . storeTermVector , fieldInfo . storePositionWithTermVector , fieldInfo . storeOffsetWithTermVector , fieldInfo . omitNorms , fieldInfo . storePayloads , fieldInfo . omitTf ) ; } else { fi . update ( fieldInfo ) ; } return fi ; } private FieldInfo addInternal ( String name , boolean isIndexed , boolean storeTermVector , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector , boolean omitNorms , boolean storePayloads , boolean omitTf ) { FieldInfo fi = new FieldInfo ( name , isIndexed , byNumber . size ( ) , storeTermVector , storePositionWithTermVector , storeOffsetWithTermVector , omitNorms , storePayloads , omitTf ) ; byNumber . add ( fi ) ; byName . put ( name , fi ) ; return fi ; } public int fieldNumber ( String fieldName ) { FieldInfo fi = fieldInfo ( fieldName ) ; return ( fi != null ) ? fi . number : - 1 ; } public FieldInfo fieldInfo ( String fieldName ) { return ( FieldInfo ) byName . get ( fieldName ) ; } public String fieldName ( int fieldNumber ) { FieldInfo fi = fieldInfo ( fieldNumber ) ; return ( fi != null ) ? fi . name : "" ; } public FieldInfo fieldInfo ( int fieldNumber ) { return ( fieldNumber >= 0 ) ? ( FieldInfo ) byNumber . get ( fieldNumber ) : null ; } public int size ( ) { return byNumber . size ( ) ; } public boolean hasVectors ( ) { boolean hasVectors = false ; for ( int i = 0 ; i < size ( ) ; i ++ ) { if ( fieldInfo ( i ) . storeTermVector ) { hasVectors = true ; break ; } } return hasVectors ; } public void write ( Directory d , String name ) throws IOException { IndexOutput output = d . createOutput ( name ) ; try { write ( output ) ; } finally { output . close ( ) ; } } public void write ( IndexOutput output ) throws IOException { output . writeVInt ( size ( ) ) ; for ( int i = 0 ; i < size ( ) ; i ++ ) { FieldInfo fi = fieldInfo ( i ) ; byte bits = 0x0 ; if ( fi . isIndexed ) bits |= IS_INDEXED ; if ( fi . storeTermVector ) bits |= STORE_TERMVECTOR ; if ( fi . storePositionWithTermVector ) bits |= STORE_POSITIONS_WITH_TERMVECTOR ; if ( fi . storeOffsetWithTermVector ) bits |= STORE_OFFSET_WITH_TERMVECTOR ; if ( fi . omitNorms ) bits |= OMIT_NORMS ; if ( fi . storePayloads ) bits |= STORE_PAYLOADS ; if ( fi . omitTf ) bits |= OMIT_TF ; output . writeString ( fi . name ) ; output . writeByte ( bits ) ; } } private void read ( IndexInput input ) throws IOException { int size = input . readVInt ( ) ; for ( int i = 0 ; i < size ; i ++ ) { String name = input . readString ( ) . intern ( ) ; byte bits = input . readByte ( ) ; boolean isIndexed = ( bits & IS_INDEXED ) != 0 ; boolean storeTermVector = ( bits & STORE_TERMVECTOR ) != 0 ; boolean storePositionsWithTermVector = ( bits & STORE_POSITIONS_WITH_TERMVECTOR ) != 0 ; boolean storeOffsetWithTermVector = ( bits & STORE_OFFSET_WITH_TERMVECTOR ) != 0 ; boolean omitNorms = ( bits & OMIT_NORMS ) != 0 ; boolean storePayloads = ( bits & STORE_PAYLOADS ) != 0 ; boolean omitTf = ( bits & OMIT_TF ) != 0 ; addInternal ( name , isIndexed , storeTermVector , storePositionsWithTermVector , storeOffsetWithTermVector , omitNorms , storePayloads , omitTf ) ; } } } 	1	['23', '1', '0', '31', '57', '203', '25', '6', '18', '0.934343434', '565', '0.222222222', '0', '0', '0.22173913', '0', '0', '23.17391304', '3', '1.4783', '6']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import org . apache . lucene . store . BufferedIndexInput ; import org . apache . lucene . store . IndexInput ; abstract class MultiLevelSkipListReader { private int maxNumberOfSkipLevels ; private int numberOfSkipLevels ; private int numberOfLevelsToBuffer = 1 ; private int docCount ; private boolean haveSkipped ; private IndexInput [ ] skipStream ; private long skipPointer [ ] ; private int skipInterval [ ] ; private int [ ] numSkipped ; private int [ ] skipDoc ; private int lastDoc ; private long [ ] childPointer ; private long lastChildPointer ; private boolean inputIsBuffered ; public MultiLevelSkipListReader ( IndexInput skipStream , int maxSkipLevels , int skipInterval ) { this . skipStream = new IndexInput [ maxSkipLevels ] ; this . skipPointer = new long [ maxSkipLevels ] ; this . childPointer = new long [ maxSkipLevels ] ; this . numSkipped = new int [ maxSkipLevels ] ; this . maxNumberOfSkipLevels = maxSkipLevels ; this . skipInterval = new int [ maxSkipLevels ] ; this . skipStream [ 0 ] = skipStream ; this . inputIsBuffered = ( skipStream instanceof BufferedIndexInput ) ; this . skipInterval [ 0 ] = skipInterval ; for ( int i = 1 ; i < maxSkipLevels ; i ++ ) { this . skipInterval [ i ] = this . skipInterval [ i - 1 ] * skipInterval ; } skipDoc = new int [ maxSkipLevels ] ; } int getDoc ( ) { return lastDoc ; } int skipTo ( int target ) throws IOException { if ( ! haveSkipped ) { loadSkipLevels ( ) ; haveSkipped = true ; } int level = 0 ; while ( level < numberOfSkipLevels - 1 && target > skipDoc [ level + 1 ] ) { level ++ ; } while ( level >= 0 ) { if ( target > skipDoc [ level ] ) { if ( ! loadNextSkip ( level ) ) { continue ; } } else { if ( level > 0 && lastChildPointer > skipStream [ level - 1 ] . getFilePointer ( ) ) { seekChild ( level - 1 ) ; } level -- ; } } return numSkipped [ 0 ] - skipInterval [ 0 ] - 1 ; } private boolean loadNextSkip ( int level ) throws IOException { setLastSkipData ( level ) ; numSkipped [ level ] += skipInterval [ level ] ; if ( numSkipped [ level ] > docCount ) { skipDoc [ level ] = Integer . MAX_VALUE ; if ( numberOfSkipLevels > level ) numberOfSkipLevels = level ; return false ; } skipDoc [ level ] += readSkipData ( level , skipStream [ level ] ) ; if ( level != 0 ) { childPointer [ level ] = skipStream [ level ] . readVLong ( ) + skipPointer [ level - 1 ] ; } return true ; } protected void seekChild ( int level ) throws IOException { skipStream [ level ] . seek ( lastChildPointer ) ; numSkipped [ level ] = numSkipped [ level + 1 ] - skipInterval [ level + 1 ] ; skipDoc [ level ] = lastDoc ; if ( level > 0 ) { childPointer [ level ] = skipStream [ level ] . readVLong ( ) + skipPointer [ level - 1 ] ; } } void close ( ) throws IOException { for ( int i = 1 ; i < skipStream . length ; i ++ ) { if ( skipStream [ i ] != null ) { skipStream [ i ] . close ( ) ; } } } void init ( long skipPointer , int df ) { this . skipPointer [ 0 ] = skipPointer ; this . docCount = df ; Arrays . fill ( skipDoc , 0 ) ; Arrays . fill ( numSkipped , 0 ) ; Arrays . fill ( childPointer , 0 ) ; haveSkipped = false ; for ( int i = 1 ; i < numberOfSkipLevels ; i ++ ) { skipStream [ i ] = null ; } } private void loadSkipLevels ( ) throws IOException { numberOfSkipLevels = docCount == 0 ? 0 : ( int ) Math . floor ( Math . log ( docCount ) / Math . log ( skipInterval [ 0 ] ) ) ; if ( numberOfSkipLevels > maxNumberOfSkipLevels ) { numberOfSkipLevels = maxNumberOfSkipLevels ; } skipStream [ 0 ] . seek ( skipPointer [ 0 ] ) ; int toBuffer = numberOfLevelsToBuffer ; for ( int i = numberOfSkipLevels - 1 ; i > 0 ; i -- ) { long length = skipStream [ 0 ] . readVLong ( ) ; skipPointer [ i ] = skipStream [ 0 ] . getFilePointer ( ) ; if ( toBuffer > 0 ) { skipStream [ i ] = new SkipBuffer ( skipStream [ 0 ] , ( int ) length ) ; toBuffer -- ; } else { skipStream [ i ] = ( IndexInput ) skipStream [ 0 ] . clone ( ) ; if ( inputIsBuffered && length < BufferedIndexInput . BUFFER_SIZE ) { ( ( BufferedIndexInput ) skipStream [ i ] ) . setBufferSize ( ( int ) length ) ; } skipStream [ 0 ] . seek ( skipStream [ 0 ] . getFilePointer ( ) + length ) ; } } skipPointer [ 0 ] = skipStream [ 0 ] . getFilePointer ( ) ; } protected abstract int readSkipData ( int level , IndexInput skipStream ) throws IOException ; protected void setLastSkipData ( int level ) { lastDoc = skipDoc [ level ] ; lastChildPointer = childPointer [ level ] ; } private final static class SkipBuffer extends IndexInput { private byte [ ] data ; private long pointer ; private int pos ; SkipBuffer ( IndexInput input , int length ) throws IOException { data = new byte [ length ] ; pointer = input . getFilePointer ( ) ; input . readBytes ( data , 0 , length ) ; } public void close ( ) throws IOException { data = null ; } public long getFilePointer ( ) { return pointer + pos ; } public long length ( ) { return data . length ; } public byte readByte ( ) throws IOException { return data [ pos ++ ] ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { System . arraycopy ( data , pos , b , offset , len ) ; pos += len ; } public void seek ( long pos ) throws IOException { this . pos = ( int ) ( pos - pointer ) ; } } } 	0	['10', '1', '1', '4', '22', '0', '1', '3', '1', '0.611111111', '481', '1', '1', '0', '0.5', '0', '0', '45.7', '2', '1', '0']
package org . apache . lucene . index ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . FieldSelector ; import org . apache . lucene . document . FieldSelectorResult ; import org . apache . lucene . document . Fieldable ; import java . io . IOException ; import java . util . * ; public class ParallelReader extends IndexReader { private List readers = new ArrayList ( ) ; private List decrefOnClose = new ArrayList ( ) ; boolean incRefReaders = false ; private SortedMap fieldToReader = new TreeMap ( ) ; private Map readerToFields = new HashMap ( ) ; private List storedFieldReaders = new ArrayList ( ) ; private int maxDoc ; private int numDocs ; private boolean hasDeletions ; public ParallelReader ( ) throws IOException { this ( true ) ; } public ParallelReader ( boolean closeSubReaders ) throws IOException { super ( ) ; this . incRefReaders = ! closeSubReaders ; } public void add ( IndexReader reader ) throws IOException { ensureOpen ( ) ; add ( reader , false ) ; } public void add ( IndexReader reader , boolean ignoreStoredFields ) throws IOException { ensureOpen ( ) ; if ( readers . size ( ) == 0 ) { this . maxDoc = reader . maxDoc ( ) ; this . numDocs = reader . numDocs ( ) ; this . hasDeletions = reader . hasDeletions ( ) ; } if ( reader . maxDoc ( ) != maxDoc ) throw new IllegalArgumentException ( "All readers must have same maxDoc: " + maxDoc + "!=" + reader . maxDoc ( ) ) ; if ( reader . numDocs ( ) != numDocs ) throw new IllegalArgumentException ( "All readers must have same numDocs: " + numDocs + "!=" + reader . numDocs ( ) ) ; Collection fields = reader . getFieldNames ( IndexReader . FieldOption . ALL ) ; readerToFields . put ( reader , fields ) ; Iterator i = fields . iterator ( ) ; while ( i . hasNext ( ) ) { String field = ( String ) i . next ( ) ; if ( fieldToReader . get ( field ) == null ) fieldToReader . put ( field , reader ) ; } if ( ! ignoreStoredFields ) storedFieldReaders . add ( reader ) ; readers . add ( reader ) ; if ( incRefReaders ) { reader . incRef ( ) ; } decrefOnClose . add ( Boolean . valueOf ( incRefReaders ) ) ; } public IndexReader reopen ( ) throws CorruptIndexException , IOException { ensureOpen ( ) ; boolean reopened = false ; List newReaders = new ArrayList ( ) ; List newDecrefOnClose = new ArrayList ( ) ; boolean success = false ; try { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader oldReader = ( IndexReader ) readers . get ( i ) ; IndexReader newReader = oldReader . reopen ( ) ; newReaders . add ( newReader ) ; if ( newReader != oldReader ) { reopened = true ; } } if ( reopened ) { ParallelReader pr = new ParallelReader ( ) ; for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader oldReader = ( IndexReader ) readers . get ( i ) ; IndexReader newReader = ( IndexReader ) newReaders . get ( i ) ; if ( newReader == oldReader ) { newDecrefOnClose . add ( Boolean . TRUE ) ; newReader . incRef ( ) ; } else { newDecrefOnClose . add ( Boolean . FALSE ) ; } pr . add ( newReader , ! storedFieldReaders . contains ( oldReader ) ) ; } pr . decrefOnClose = newDecrefOnClose ; pr . incRefReaders = incRefReaders ; success = true ; return pr ; } else { success = true ; return this ; } } finally { if ( ! success && reopened ) { for ( int i = 0 ; i < newReaders . size ( ) ; i ++ ) { IndexReader r = ( IndexReader ) newReaders . get ( i ) ; if ( r != null ) { try { if ( ( ( Boolean ) newDecrefOnClose . get ( i ) ) . booleanValue ( ) ) { r . decRef ( ) ; } else { r . close ( ) ; } } catch ( IOException ignore ) { } } } } } } public int numDocs ( ) { return numDocs ; } public int maxDoc ( ) { return maxDoc ; } public boolean hasDeletions ( ) { return hasDeletions ; } public boolean isDeleted ( int n ) { if ( readers . size ( ) > 0 ) return ( ( IndexReader ) readers . get ( 0 ) ) . isDeleted ( n ) ; return false ; } protected void doDelete ( int n ) throws CorruptIndexException , IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { ( ( IndexReader ) readers . get ( i ) ) . deleteDocument ( n ) ; } hasDeletions = true ; } protected void doUndeleteAll ( ) throws CorruptIndexException , IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { ( ( IndexReader ) readers . get ( i ) ) . undeleteAll ( ) ; } hasDeletions = false ; } public Document document ( int n , FieldSelector fieldSelector ) throws CorruptIndexException , IOException { ensureOpen ( ) ; Document result = new Document ( ) ; for ( int i = 0 ; i < storedFieldReaders . size ( ) ; i ++ ) { IndexReader reader = ( IndexReader ) storedFieldReaders . get ( i ) ; boolean include = ( fieldSelector == null ) ; if ( ! include ) { Iterator it = ( ( Collection ) readerToFields . get ( reader ) ) . iterator ( ) ; while ( it . hasNext ( ) ) if ( fieldSelector . accept ( ( String ) it . next ( ) ) != FieldSelectorResult . NO_LOAD ) { include = true ; break ; } } if ( include ) { Iterator fieldIterator = reader . document ( n , fieldSelector ) . getFields ( ) . iterator ( ) ; while ( fieldIterator . hasNext ( ) ) { result . add ( ( Fieldable ) fieldIterator . next ( ) ) ; } } } return result ; } public TermFreqVector [ ] getTermFreqVectors ( int n ) throws IOException { ensureOpen ( ) ; ArrayList results = new ArrayList ( ) ; Iterator i = fieldToReader . entrySet ( ) . iterator ( ) ; while ( i . hasNext ( ) ) { Map . Entry e = ( Map . Entry ) i . next ( ) ; String field = ( String ) e . getKey ( ) ; IndexReader reader = ( IndexReader ) e . getValue ( ) ; TermFreqVector vector = reader . getTermFreqVector ( n , field ) ; if ( vector != null ) results . add ( vector ) ; } return ( TermFreqVector [ ] ) results . toArray ( new TermFreqVector [ results . size ( ) ] ) ; } public TermFreqVector getTermFreqVector ( int n , String field ) throws IOException { ensureOpen ( ) ; IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; return reader == null ? null : reader . getTermFreqVector ( n , field ) ; } public void getTermFreqVector ( int docNumber , String field , TermVectorMapper mapper ) throws IOException { ensureOpen ( ) ; IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; if ( reader != null ) { reader . getTermFreqVector ( docNumber , field , mapper ) ; } } public void getTermFreqVector ( int docNumber , TermVectorMapper mapper ) throws IOException { ensureOpen ( ) ; ensureOpen ( ) ; Iterator i = fieldToReader . entrySet ( ) . iterator ( ) ; while ( i . hasNext ( ) ) { Map . Entry e = ( Map . Entry ) i . next ( ) ; String field = ( String ) e . getKey ( ) ; IndexReader reader = ( IndexReader ) e . getValue ( ) ; reader . getTermFreqVector ( docNumber , field , mapper ) ; } } public boolean hasNorms ( String field ) throws IOException { ensureOpen ( ) ; IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; return reader == null ? false : reader . hasNorms ( field ) ; } public byte [ ] norms ( String field ) throws IOException { ensureOpen ( ) ; IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; return reader == null ? null : reader . norms ( field ) ; } public void norms ( String field , byte [ ] result , int offset ) throws IOException { ensureOpen ( ) ; IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; if ( reader != null ) reader . norms ( field , result , offset ) ; } protected void doSetNorm ( int n , String field , byte value ) throws CorruptIndexException , IOException { IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; if ( reader != null ) reader . doSetNorm ( n , field , value ) ; } public TermEnum terms ( ) throws IOException { ensureOpen ( ) ; return new ParallelTermEnum ( ) ; } public TermEnum terms ( Term term ) throws IOException { ensureOpen ( ) ; return new ParallelTermEnum ( term ) ; } public int docFreq ( Term term ) throws IOException { ensureOpen ( ) ; IndexReader reader = ( ( IndexReader ) fieldToReader . get ( term . field ( ) ) ) ; return reader == null ? 0 : reader . docFreq ( term ) ; } public TermDocs termDocs ( Term term ) throws IOException { ensureOpen ( ) ; return new ParallelTermDocs ( term ) ; } public TermDocs termDocs ( ) throws IOException { ensureOpen ( ) ; return new ParallelTermDocs ( ) ; } public TermPositions termPositions ( Term term ) throws IOException { ensureOpen ( ) ; return new ParallelTermPositions ( term ) ; } public TermPositions termPositions ( ) throws IOException { ensureOpen ( ) ; return new ParallelTermPositions ( ) ; } public boolean isCurrent ( ) throws CorruptIndexException , IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { if ( ! ( ( IndexReader ) readers . get ( i ) ) . isCurrent ( ) ) { return false ; } } return true ; } public boolean isOptimized ( ) { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { if ( ! ( ( IndexReader ) readers . get ( i ) ) . isOptimized ( ) ) { return false ; } } return true ; } public long getVersion ( ) { throw new UnsupportedOperationException ( "ParallelReader does not support this method." ) ; } IndexReader [ ] getSubReaders ( ) { return ( IndexReader [ ] ) readers . toArray ( new IndexReader [ readers . size ( ) ] ) ; } protected void doCommit ( ) throws IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) ( ( IndexReader ) readers . get ( i ) ) . commit ( ) ; } protected synchronized void doClose ( ) throws IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { if ( ( ( Boolean ) decrefOnClose . get ( i ) ) . booleanValue ( ) ) { ( ( IndexReader ) readers . get ( i ) ) . decRef ( ) ; } else { ( ( IndexReader ) readers . get ( i ) ) . close ( ) ; } } } public Collection getFieldNames ( IndexReader . FieldOption fieldNames ) { ensureOpen ( ) ; Set fieldSet = new HashSet ( ) ; for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader reader = ( ( IndexReader ) readers . get ( i ) ) ; Collection names = reader . getFieldNames ( fieldNames ) ; fieldSet . addAll ( names ) ; } return fieldSet ; } private class ParallelTermEnum extends TermEnum { private String field ; private Iterator fieldIterator ; private TermEnum termEnum ; public ParallelTermEnum ( ) throws IOException { field = ( String ) fieldToReader . firstKey ( ) ; if ( field != null ) termEnum = ( ( IndexReader ) fieldToReader . get ( field ) ) . terms ( ) ; } public ParallelTermEnum ( Term term ) throws IOException { field = term . field ( ) ; IndexReader reader = ( ( IndexReader ) fieldToReader . get ( field ) ) ; if ( reader != null ) termEnum = reader . terms ( term ) ; } public boolean next ( ) throws IOException { if ( termEnum == null ) return false ; if ( termEnum . next ( ) && termEnum . term ( ) . field ( ) == field ) return true ; termEnum . close ( ) ; if ( fieldIterator == null ) { fieldIterator = fieldToReader . tailMap ( field ) . keySet ( ) . iterator ( ) ; fieldIterator . next ( ) ; } while ( fieldIterator . hasNext ( ) ) { field = ( String ) fieldIterator . next ( ) ; termEnum = ( ( IndexReader ) fieldToReader . get ( field ) ) . terms ( new Term ( field ) ) ; Term term = termEnum . term ( ) ; if ( term != null && term . field ( ) == field ) return true ; else termEnum . close ( ) ; } return false ; } public Term term ( ) { if ( termEnum == null ) return null ; return termEnum . term ( ) ; } public int docFreq ( ) { if ( termEnum == null ) return 0 ; return termEnum . docFreq ( ) ; } public void close ( ) throws IOException { if ( termEnum != null ) termEnum . close ( ) ; } } private class ParallelTermDocs implements TermDocs { protected TermDocs termDocs ; public ParallelTermDocs ( ) { } public ParallelTermDocs ( Term term ) throws IOException { seek ( term ) ; } public int doc ( ) { return termDocs . doc ( ) ; } public int freq ( ) { return termDocs . freq ( ) ; } public void seek ( Term term ) throws IOException { IndexReader reader = ( ( IndexReader ) fieldToReader . get ( term . field ( ) ) ) ; termDocs = reader != null ? reader . termDocs ( term ) : null ; } public void seek ( TermEnum termEnum ) throws IOException { seek ( termEnum . term ( ) ) ; } public boolean next ( ) throws IOException { if ( termDocs == null ) return false ; return termDocs . next ( ) ; } public int read ( final int [ ] docs , final int [ ] freqs ) throws IOException { if ( termDocs == null ) return 0 ; return termDocs . read ( docs , freqs ) ; } public boolean skipTo ( int target ) throws IOException { if ( termDocs == null ) return false ; return termDocs . skipTo ( target ) ; } public void close ( ) throws IOException { if ( termDocs != null ) termDocs . close ( ) ; } } private class ParallelTermPositions extends ParallelTermDocs implements TermPositions { public ParallelTermPositions ( ) { } public ParallelTermPositions ( Term term ) throws IOException { seek ( term ) ; } public void seek ( Term term ) throws IOException { IndexReader reader = ( ( IndexReader ) fieldToReader . get ( term . field ( ) ) ) ; termDocs = reader != null ? reader . termPositions ( term ) : null ; } public int nextPosition ( ) throws IOException { return ( ( TermPositions ) termDocs ) . nextPosition ( ) ; } public int getPayloadLength ( ) { return ( ( TermPositions ) termDocs ) . getPayloadLength ( ) ; } public byte [ ] getPayload ( byte [ ] data , int offset ) throws IOException { return ( ( TermPositions ) termDocs ) . getPayload ( data , offset ) ; } public boolean isPayloadAvailable ( ) { return ( ( TermPositions ) termDocs ) . isPayloadAvailable ( ) ; } } } 	1	['35', '2', '0', '16', '103', '317', '3', '16', '28', '0.81372549', '865', '0.888888889', '0', '0.688679245', '0.152380952', '1', '10', '23.45714286', '3', '1.0571', '5']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Map ; abstract class TermsHashConsumer { abstract int bytesPerPosting ( ) ; abstract void createPostings ( RawPostingList [ ] postings , int start , int count ) ; abstract TermsHashConsumerPerThread addThread ( TermsHashPerThread perThread ) ; abstract void flush ( Map threadsAndFields , final DocumentsWriter . FlushState state ) throws IOException ; abstract void abort ( ) ; abstract void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException ; FieldInfos fieldInfos ; void setFieldInfos ( FieldInfos fieldInfos ) { this . fieldInfos = fieldInfos ; } } 	0	['8', '1', '2', '9', '9', '28', '5', '5', '0', '1', '16', '0', '1', '0', '0.267857143', '0', '0', '0.875', '1', '0.875', '0']
package org . apache . lucene . util ; import java . io . IOException ; import org . apache . lucene . search . DocIdSetIterator ; public class OpenBitSetDISI extends OpenBitSet { public OpenBitSetDISI ( DocIdSetIterator disi , int maxSize ) throws IOException { super ( maxSize ) ; inPlaceOr ( disi ) ; } public OpenBitSetDISI ( int maxSize ) { super ( maxSize ) ; } public void inPlaceOr ( DocIdSetIterator disi ) throws IOException { while ( disi . next ( ) && ( disi . doc ( ) < size ( ) ) ) { fastSet ( disi . doc ( ) ) ; } } public void inPlaceAnd ( DocIdSetIterator disi ) throws IOException { int index = nextSetBit ( 0 ) ; int lastNotCleared = - 1 ; while ( ( index != - 1 ) && disi . skipTo ( index ) ) { while ( ( index != - 1 ) && ( index < disi . doc ( ) ) ) { fastClear ( index ) ; index = nextSetBit ( index + 1 ) ; } if ( index == disi . doc ( ) ) { lastNotCleared = index ; index ++ ; } assert ( index == - 1 ) || ( index > disi . doc ( ) ) ; } clear ( lastNotCleared + 1 , size ( ) ) ; } public void inPlaceNot ( DocIdSetIterator disi ) throws IOException { while ( disi . next ( ) && ( disi . doc ( ) < size ( ) ) ) { fastClear ( disi . doc ( ) ) ; } } public void inPlaceXor ( DocIdSetIterator disi ) throws IOException { while ( disi . next ( ) && ( disi . doc ( ) < size ( ) ) ) { fastFlip ( disi . doc ( ) ) ; } } } 	1	['8', '3', '0', '2', '23', '26', '0', '2', '6', '0.928571429', '155', '0', '0', '0.913793103', '0.5', '0', '0', '18.125', '1', '0.625', '2']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermEnum ; public abstract class FilteredTermEnum extends TermEnum { private Term currentTerm = null ; private TermEnum actualEnum = null ; public FilteredTermEnum ( ) { } protected abstract boolean termCompare ( Term term ) ; public abstract float difference ( ) ; protected abstract boolean endEnum ( ) ; protected void setEnum ( TermEnum actualEnum ) throws IOException { this . actualEnum = actualEnum ; Term term = actualEnum . term ( ) ; if ( term != null && termCompare ( term ) ) currentTerm = term ; else next ( ) ; } public int docFreq ( ) { if ( actualEnum == null ) return - 1 ; return actualEnum . docFreq ( ) ; } public boolean next ( ) throws IOException { if ( actualEnum == null ) return false ; currentTerm = null ; while ( currentTerm == null ) { if ( endEnum ( ) ) return false ; if ( actualEnum . next ( ) ) { Term term = actualEnum . term ( ) ; if ( termCompare ( term ) ) { currentTerm = term ; return true ; } } else return false ; } currentTerm = null ; return false ; } public Term term ( ) { return currentTerm ; } public void close ( ) throws IOException { actualEnum . close ( ) ; currentTerm = null ; actualEnum = null ; } } 	0	['9', '2', '2', '7', '14', '8', '5', '2', '6', '0.5', '103', '1', '2', '0.384615385', '0.407407407', '1', '2', '10.22222222', '2', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; import java . io . IOException ; import java . util . Set ; public class FilteredQuery extends Query { Query query ; Filter filter ; public FilteredQuery ( Query query , Filter filter ) { this . query = query ; this . filter = filter ; } protected Weight createWeight ( final Searcher searcher ) throws IOException { final Weight weight = query . createWeight ( searcher ) ; final Similarity similarity = query . getSimilarity ( searcher ) ; return new Weight ( ) { private float value ; public float getValue ( ) { return value ; } public float sumOfSquaredWeights ( ) throws IOException { return weight . sumOfSquaredWeights ( ) * getBoost ( ) * getBoost ( ) ; } public void normalize ( float v ) { weight . normalize ( v ) ; value = weight . getValue ( ) * getBoost ( ) ; } public Explanation explain ( IndexReader ir , int i ) throws IOException { Explanation inner = weight . explain ( ir , i ) ; if ( getBoost ( ) != 1 ) { Explanation preBoost = inner ; inner = new Explanation ( inner . getValue ( ) * getBoost ( ) , "product of:" ) ; inner . addDetail ( new Explanation ( getBoost ( ) , "boost" ) ) ; inner . addDetail ( preBoost ) ; } Filter f = FilteredQuery . this . filter ; DocIdSetIterator docIdSetIterator = f . getDocIdSet ( ir ) . iterator ( ) ; if ( docIdSetIterator . skipTo ( i ) && ( docIdSetIterator . doc ( ) == i ) ) { return inner ; } else { Explanation result = new Explanation ( 0.0f , "failure to match filter: " + f . toString ( ) ) ; result . addDetail ( inner ) ; return result ; } } public Query getQuery ( ) { return FilteredQuery . this ; } public Scorer scorer ( IndexReader indexReader ) throws IOException { final Scorer scorer = weight . scorer ( indexReader ) ; final DocIdSetIterator docIdSetIterator = filter . getDocIdSet ( indexReader ) . iterator ( ) ; return new Scorer ( similarity ) { private boolean advanceToCommon ( ) throws IOException { while ( scorer . doc ( ) != docIdSetIterator . doc ( ) ) { if ( scorer . doc ( ) < docIdSetIterator . doc ( ) ) { if ( ! scorer . skipTo ( docIdSetIterator . doc ( ) ) ) { return false ; } } else if ( ! docIdSetIterator . skipTo ( scorer . doc ( ) ) ) { return false ; } } return true ; } public boolean next ( ) throws IOException { return docIdSetIterator . next ( ) && scorer . next ( ) && advanceToCommon ( ) ; } public int doc ( ) { return scorer . doc ( ) ; } public boolean skipTo ( int i ) throws IOException { return docIdSetIterator . skipTo ( i ) && scorer . skipTo ( docIdSetIterator . doc ( ) ) && advanceToCommon ( ) ; } public float score ( ) throws IOException { return getBoost ( ) * scorer . score ( ) ; } public Explanation explain ( int i ) throws IOException { Explanation exp = scorer . explain ( i ) ; if ( docIdSetIterator . skipTo ( i ) && ( docIdSetIterator . doc ( ) == i ) ) { exp . setDescription ( "allowed by filter: " + exp . getDescription ( ) ) ; exp . setValue ( getBoost ( ) * exp . getValue ( ) ) ; } else { exp . setDescription ( "removed by filter: " + exp . getDescription ( ) ) ; exp . setValue ( 0.0f ) ; } return exp ; } } ; } } ; } public Query rewrite ( IndexReader reader ) throws IOException { Query rewritten = query . rewrite ( reader ) ; if ( rewritten != query ) { FilteredQuery clone = ( FilteredQuery ) this . clone ( ) ; clone . query = rewritten ; return clone ; } else { return this ; } } public Query getQuery ( ) { return query ; } public Filter getFilter ( ) { return filter ; } public void extractTerms ( Set terms ) { getQuery ( ) . extractTerms ( terms ) ; } public String toString ( String s ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "filtered(" ) ; buffer . append ( query . toString ( s ) ) ; buffer . append ( ")->" ) ; buffer . append ( filter ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( o instanceof FilteredQuery ) { FilteredQuery fq = ( FilteredQuery ) o ; return ( query . equals ( fq . query ) && filter . equals ( fq . filter ) && getBoost ( ) == fq . getBoost ( ) ) ; } return false ; } public int hashCode ( ) { return query . hashCode ( ) ^ filter . hashCode ( ) + Float . floatToRawIntBits ( getBoost ( ) ) ; } } 	1	['9', '2', '0', '10', '26', '0', '3', '8', '8', '0.3125', '143', '0', '2', '0.6', '0.222222222', '2', '4', '14.66666667', '5', '1.3333', '7']
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; import java . text . Collator ; import java . util . Locale ; class FieldDocSortedHitQueue extends PriorityQueue { volatile SortField [ ] fields ; volatile Collator [ ] collators ; FieldDocSortedHitQueue ( SortField [ ] fields , int size ) { this . fields = fields ; this . collators = hasCollators ( fields ) ; initialize ( size ) ; } synchronized void setFields ( SortField [ ] fields ) { if ( this . fields == null ) { this . fields = fields ; this . collators = hasCollators ( fields ) ; } } SortField [ ] getFields ( ) { return fields ; } private Collator [ ] hasCollators ( final SortField [ ] fields ) { if ( fields == null ) return null ; Collator [ ] ret = new Collator [ fields . length ] ; for ( int i = 0 ; i < fields . length ; ++ i ) { Locale locale = fields [ i ] . getLocale ( ) ; if ( locale != null ) ret [ i ] = Collator . getInstance ( locale ) ; } return ret ; } protected final boolean lessThan ( final Object a , final Object b ) { final FieldDoc docA = ( FieldDoc ) a ; final FieldDoc docB = ( FieldDoc ) b ; final int n = fields . length ; int c = 0 ; for ( int i = 0 ; i < n && c == 0 ; ++ i ) { final int type = fields [ i ] . getType ( ) ; switch ( type ) { case SortField . SCORE : { float r1 = ( ( Float ) docA . fields [ i ] ) . floatValue ( ) ; float r2 = ( ( Float ) docB . fields [ i ] ) . floatValue ( ) ; if ( r1 > r2 ) c = - 1 ; if ( r1 < r2 ) c = 1 ; break ; } case SortField . DOC : case SortField . INT : { int i1 = ( ( Integer ) docA . fields [ i ] ) . intValue ( ) ; int i2 = ( ( Integer ) docB . fields [ i ] ) . intValue ( ) ; if ( i1 < i2 ) c = - 1 ; if ( i1 > i2 ) c = 1 ; break ; } case SortField . LONG : { long l1 = ( ( Long ) docA . fields [ i ] ) . longValue ( ) ; long l2 = ( ( Long ) docB . fields [ i ] ) . longValue ( ) ; if ( l1 < l2 ) c = - 1 ; if ( l1 > l2 ) c = 1 ; break ; } case SortField . STRING : { String s1 = ( String ) docA . fields [ i ] ; String s2 = ( String ) docB . fields [ i ] ; if ( s1 == null ) c = ( s2 == null ) ? 0 : - 1 ; else if ( s2 == null ) c = 1 ; else if ( fields [ i ] . getLocale ( ) == null ) { c = s1 . compareTo ( s2 ) ; } else { c = collators [ i ] . compare ( s1 , s2 ) ; } break ; } case SortField . FLOAT : { float f1 = ( ( Float ) docA . fields [ i ] ) . floatValue ( ) ; float f2 = ( ( Float ) docB . fields [ i ] ) . floatValue ( ) ; if ( f1 < f2 ) c = - 1 ; if ( f1 > f2 ) c = 1 ; break ; } case SortField . DOUBLE : { double d1 = ( ( Double ) docA . fields [ i ] ) . doubleValue ( ) ; double d2 = ( ( Double ) docB . fields [ i ] ) . doubleValue ( ) ; if ( d1 < d2 ) c = - 1 ; if ( d1 > d2 ) c = 1 ; break ; } case SortField . BYTE : { int i1 = ( ( Byte ) docA . fields [ i ] ) . byteValue ( ) ; int i2 = ( ( Byte ) docB . fields [ i ] ) . byteValue ( ) ; if ( i1 < i2 ) c = - 1 ; if ( i1 > i2 ) c = 1 ; break ; } case SortField . SHORT : { int i1 = ( ( Short ) docA . fields [ i ] ) . shortValue ( ) ; int i2 = ( ( Short ) docB . fields [ i ] ) . shortValue ( ) ; if ( i1 < i2 ) c = - 1 ; if ( i1 > i2 ) c = 1 ; break ; } case SortField . CUSTOM : { c = docA . fields [ i ] . compareTo ( docB . fields [ i ] ) ; break ; } case SortField . AUTO : { throw new RuntimeException ( "FieldDocSortedHitQueue cannot use an AUTO SortField" ) ; } default : { throw new RuntimeException ( "invalid SortField type: " + type ) ; } } if ( fields [ i ] . getReverse ( ) ) { c = - c ; } } if ( c == 0 ) return docA . doc > docB . doc ; return c > 0 ; } } 	0	['5', '2', '0', '6', '25', '0', '3', '3', '0', '0.375', '378', '0', '1', '0.75', '0.5', '1', '3', '74.2', '26', '6.6', '0']
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . FieldCache ; import org . apache . lucene . search . function . DocValues ; import java . io . IOException ; public class FloatFieldSource extends FieldCacheSource { private FieldCache . FloatParser parser ; public FloatFieldSource ( String field ) { this ( field , null ) ; } public FloatFieldSource ( String field , FieldCache . FloatParser parser ) { super ( field ) ; this . parser = parser ; } public String description ( ) { return "float(" + super . description ( ) + ')' ; } public DocValues getCachedFieldValues ( FieldCache cache , String field , IndexReader reader ) throws IOException { final float [ ] arr = ( parser == null ) ? cache . getFloats ( reader , field ) : cache . getFloats ( reader , field , parser ) ; return new DocValues ( ) { public float floatVal ( int doc ) { return arr [ doc ] ; } public String toString ( int doc ) { return description ( ) + '=' + arr [ doc ] ; } Object getInnerArray ( ) { return arr ; } } ; } public boolean cachedFieldSourceEquals ( FieldCacheSource o ) { if ( o . getClass ( ) != FloatFieldSource . class ) { return false ; } FloatFieldSource other = ( FloatFieldSource ) o ; return this . parser == null ? other . parser == null : this . parser . getClass ( ) == other . parser . getClass ( ) ; } public int cachedFieldSourceHashCode ( ) { return parser == null ? Float . class . hashCode ( ) : parser . getClass ( ) . hashCode ( ) ; } } 	1	['7', '3', '0', '7', '21', '9', '2', '6', '6', '0.777777778', '120', '0.333333333', '1', '0.705882353', '0.333333333', '2', '3', '15.71428571', '6', '1.7143', '1']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . analysis . Token ; final class FreqProxTermsWriterPerField extends TermsHashConsumerPerField implements Comparable { final FreqProxTermsWriterPerThread perThread ; final TermsHashPerField termsHashPerField ; final FieldInfo fieldInfo ; final DocumentsWriter . DocState docState ; final DocInverter . FieldInvertState fieldState ; boolean omitTf ; public FreqProxTermsWriterPerField ( TermsHashPerField termsHashPerField , FreqProxTermsWriterPerThread perThread , FieldInfo fieldInfo ) { this . termsHashPerField = termsHashPerField ; this . perThread = perThread ; this . fieldInfo = fieldInfo ; docState = termsHashPerField . docState ; fieldState = termsHashPerField . fieldState ; omitTf = fieldInfo . omitTf ; } int getStreamCount ( ) { if ( fieldInfo . omitTf ) return 1 ; else return 2 ; } void finish ( ) { } boolean hasPayloads ; void skippingLongTerm ( Token t ) throws IOException { } public int compareTo ( Object other0 ) { FreqProxTermsWriterPerField other = ( FreqProxTermsWriterPerField ) other0 ; return fieldInfo . name . compareTo ( other . fieldInfo . name ) ; } void reset ( ) { omitTf = fieldInfo . omitTf ; } boolean start ( Fieldable [ ] fields , int count ) { for ( int i = 0 ; i < count ; i ++ ) if ( fields [ i ] . isIndexed ( ) ) return true ; return false ; } final void writeProx ( Token t , FreqProxTermsWriter . PostingList p , int proxCode ) { final Payload payload = t . getPayload ( ) ; if ( payload != null && payload . length > 0 ) { termsHashPerField . writeVInt ( 1 , ( proxCode << 1 ) | 1 ) ; termsHashPerField . writeVInt ( 1 , payload . length ) ; termsHashPerField . writeBytes ( 1 , payload . data , payload . offset , payload . length ) ; hasPayloads = true ; } else termsHashPerField . writeVInt ( 1 , proxCode << 1 ) ; p . lastPosition = fieldState . position ; } final void newTerm ( Token t , RawPostingList p0 ) { assert docState . testPoint ( "FreqProxTermsWriterPerField.newTerm start" ) ; FreqProxTermsWriter . PostingList p = ( FreqProxTermsWriter . PostingList ) p0 ; p . lastDocID = docState . docID ; if ( omitTf ) { p . lastDocCode = docState . docID ; } else { p . lastDocCode = docState . docID << 1 ; p . docFreq = 1 ; writeProx ( t , p , fieldState . position ) ; } } final void addTerm ( Token t , RawPostingList p0 ) { assert docState . testPoint ( "FreqProxTermsWriterPerField.addTerm start" ) ; FreqProxTermsWriter . PostingList p = ( FreqProxTermsWriter . PostingList ) p0 ; assert omitTf || p . docFreq > 0 ; if ( omitTf ) { if ( docState . docID != p . lastDocID ) { assert docState . docID > p . lastDocID ; termsHashPerField . writeVInt ( 0 , p . lastDocCode ) ; p . lastDocCode = docState . docID - p . lastDocID ; p . lastDocID = docState . docID ; } } else { if ( docState . docID != p . lastDocID ) { assert docState . docID > p . lastDocID ; if ( 1 == p . docFreq ) termsHashPerField . writeVInt ( 0 , p . lastDocCode | 1 ) ; else { termsHashPerField . writeVInt ( 0 , p . lastDocCode ) ; termsHashPerField . writeVInt ( 0 , p . docFreq ) ; } p . docFreq = 1 ; p . lastDocCode = ( docState . docID - p . lastDocID ) << 1 ; p . lastDocID = docState . docID ; writeProx ( t , p , fieldState . position ) ; } else { p . docFreq ++ ; writeProx ( t , p , fieldState . position - p . lastPosition ) ; } } } public void abort ( ) { } } 	0	['13', '2', '0', '13', '25', '46', '3', '11', '3', '0.833333333', '364', '0', '5', '0.352941176', '0.196969697', '0', '0', '26.30769231', '14', '2.4615', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . text . Collator ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermEnum ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . ToStringUtils ; public class RangeQuery extends Query { private Term lowerTerm ; private Term upperTerm ; private boolean inclusive ; private Collator collator ; public RangeQuery ( Term lowerTerm , Term upperTerm , boolean inclusive ) { if ( lowerTerm == null && upperTerm == null ) { throw new IllegalArgumentException ( "At least one term must be non-null" ) ; } if ( lowerTerm != null && upperTerm != null && lowerTerm . field ( ) != upperTerm . field ( ) ) { throw new IllegalArgumentException ( "Both terms must be for the same field" ) ; } if ( lowerTerm != null ) { this . lowerTerm = lowerTerm ; } else { this . lowerTerm = new Term ( upperTerm . field ( ) ) ; } this . upperTerm = upperTerm ; this . inclusive = inclusive ; } public RangeQuery ( Term lowerTerm , Term upperTerm , boolean inclusive , Collator collator ) { this ( lowerTerm , upperTerm , inclusive ) ; this . collator = collator ; } public Query rewrite ( IndexReader reader ) throws IOException { BooleanQuery query = new BooleanQuery ( true ) ; String testField = getField ( ) ; if ( collator != null ) { TermEnum enumerator = reader . terms ( new Term ( testField , "" ) ) ; String lowerTermText = lowerTerm != null ? lowerTerm . text ( ) : null ; String upperTermText = upperTerm != null ? upperTerm . text ( ) : null ; try { do { Term term = enumerator . term ( ) ; if ( term != null && term . field ( ) == testField ) { if ( ( lowerTermText == null || ( inclusive ? collator . compare ( term . text ( ) , lowerTermText ) >= 0 : collator . compare ( term . text ( ) , lowerTermText ) > 0 ) ) && ( upperTermText == null || ( inclusive ? collator . compare ( term . text ( ) , upperTermText ) <= 0 : collator . compare ( term . text ( ) , upperTermText ) < 0 ) ) ) { addTermToQuery ( term , query ) ; } } } while ( enumerator . next ( ) ) ; } finally { enumerator . close ( ) ; } } else { TermEnum enumerator = reader . terms ( lowerTerm ) ; try { boolean checkLower = false ; if ( ! inclusive ) checkLower = true ; do { Term term = enumerator . term ( ) ; if ( term != null && term . field ( ) == testField ) { if ( ! checkLower || term . text ( ) . compareTo ( lowerTerm . text ( ) ) > 0 ) { checkLower = false ; if ( upperTerm != null ) { int compare = upperTerm . text ( ) . compareTo ( term . text ( ) ) ; if ( ( compare < 0 ) || ( ! inclusive && compare == 0 ) ) break ; } addTermToQuery ( term , query ) ; } } else { break ; } } while ( enumerator . next ( ) ) ; } finally { enumerator . close ( ) ; } } return query ; } private void addTermToQuery ( Term term , BooleanQuery query ) { TermQuery tq = new TermQuery ( term ) ; tq . setBoost ( getBoost ( ) ) ; query . add ( tq , BooleanClause . Occur . SHOULD ) ; } public String getField ( ) { return ( lowerTerm != null ? lowerTerm . field ( ) : upperTerm . field ( ) ) ; } public Term getLowerTerm ( ) { return lowerTerm ; } public Term getUpperTerm ( ) { return upperTerm ; } public boolean isInclusive ( ) { return inclusive ; } public Collator getCollator ( ) { return collator ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! getField ( ) . equals ( field ) ) { buffer . append ( getField ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( inclusive ? "[" : "{" ) ; buffer . append ( lowerTerm != null ? lowerTerm . text ( ) : "null" ) ; buffer . append ( " TO " ) ; buffer . append ( upperTerm != null ? upperTerm . text ( ) : "null" ) ; buffer . append ( inclusive ? "]" : "}" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof RangeQuery ) ) return false ; final RangeQuery other = ( RangeQuery ) o ; if ( this . getBoost ( ) != other . getBoost ( ) ) return false ; if ( this . inclusive != other . inclusive ) return false ; if ( this . collator != null && ! this . collator . equals ( other . collator ) ) return false ; if ( this . lowerTerm != null ? ! this . lowerTerm . equals ( other . lowerTerm ) : other . lowerTerm != null ) return false ; if ( this . upperTerm != null ? ! this . upperTerm . equals ( other . upperTerm ) : other . upperTerm != null ) return false ; return true ; } public int hashCode ( ) { int h = Float . floatToIntBits ( getBoost ( ) ) ; h ^= lowerTerm != null ? lowerTerm . hashCode ( ) : 0 ; h ^= ( h << 25 ) | ( h > > > 8 ) ; h ^= upperTerm != null ? upperTerm . hashCode ( ) : 0 ; h ^= this . inclusive ? 0x2742E74A : 0 ; h ^= collator != null ? collator . hashCode ( ) : 0 ; return h ; } } 	1	['12', '2', '0', '9', '39', '0', '1', '8', '11', '0.454545455', '480', '1', '2', '0.545454545', '0.229166667', '2', '2', '38.66666667', '13', '2.6667', '4']
package org . apache . lucene . util . cache ; public abstract class Cache { static class SynchronizedCache extends Cache { Object mutex ; Cache cache ; SynchronizedCache ( Cache cache ) { this . cache = cache ; this . mutex = this ; } SynchronizedCache ( Cache cache , Object mutex ) { this . cache = cache ; this . mutex = mutex ; } public void put ( Object key , Object value ) { synchronized ( mutex ) { cache . put ( key , value ) ; } } public Object get ( Object key ) { synchronized ( mutex ) { return cache . get ( key ) ; } } public boolean containsKey ( Object key ) { synchronized ( mutex ) { return cache . containsKey ( key ) ; } } public void close ( ) { synchronized ( mutex ) { cache . close ( ) ; } } Cache getSynchronizedCache ( ) { return this ; } } public static Cache synchronizedCache ( Cache cache ) { return cache . getSynchronizedCache ( ) ; } Cache getSynchronizedCache ( ) { return new SynchronizedCache ( this ) ; } public abstract void put ( Object key , Object value ) ; public abstract Object get ( Object key ) ; public abstract boolean containsKey ( Object key ) ; public abstract void close ( ) ; } 	0	['7', '1', '2', '5', '9', '21', '5', '1', '6', '2', '18', '0', '0', '0', '0.476190476', '0', '0', '1.571428571', '1', '0.8571', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import java . io . IOException ; public final class FuzzyTermEnum extends FilteredTermEnum { private static final int TYPICAL_LONGEST_WORD_IN_INDEX = 19 ; private int [ ] [ ] d ; private float similarity ; private boolean endEnum = false ; private Term searchTerm = null ; private final String field ; private final String text ; private final String prefix ; private final float minimumSimilarity ; private final float scale_factor ; private final int [ ] maxDistances = new int [ TYPICAL_LONGEST_WORD_IN_INDEX ] ; public FuzzyTermEnum ( IndexReader reader , Term term ) throws IOException { this ( reader , term , FuzzyQuery . defaultMinSimilarity , FuzzyQuery . defaultPrefixLength ) ; } public FuzzyTermEnum ( IndexReader reader , Term term , float minSimilarity ) throws IOException { this ( reader , term , minSimilarity , FuzzyQuery . defaultPrefixLength ) ; } public FuzzyTermEnum ( IndexReader reader , Term term , final float minSimilarity , final int prefixLength ) throws IOException { super ( ) ; if ( minSimilarity >= 1.0f ) throw new IllegalArgumentException ( "minimumSimilarity cannot be greater than or equal to 1" ) ; else if ( minSimilarity < 0.0f ) throw new IllegalArgumentException ( "minimumSimilarity cannot be less than 0" ) ; if ( prefixLength < 0 ) throw new IllegalArgumentException ( "prefixLength cannot be less than 0" ) ; this . minimumSimilarity = minSimilarity ; this . scale_factor = 1.0f / ( 1.0f - minimumSimilarity ) ; this . searchTerm = term ; this . field = searchTerm . field ( ) ; final int fullSearchTermLength = searchTerm . text ( ) . length ( ) ; final int realPrefixLength = prefixLength > fullSearchTermLength ? fullSearchTermLength : prefixLength ; this . text = searchTerm . text ( ) . substring ( realPrefixLength ) ; this . prefix = searchTerm . text ( ) . substring ( 0 , realPrefixLength ) ; initializeMaxDistances ( ) ; this . d = initDistanceArray ( ) ; setEnum ( reader . terms ( new Term ( searchTerm . field ( ) , prefix ) ) ) ; } protected final boolean termCompare ( Term term ) { if ( field == term . field ( ) && term . text ( ) . startsWith ( prefix ) ) { final String target = term . text ( ) . substring ( prefix . length ( ) ) ; this . similarity = similarity ( target ) ; return ( similarity > minimumSimilarity ) ; } endEnum = true ; return false ; } public final float difference ( ) { return ( float ) ( ( similarity - minimumSimilarity ) * scale_factor ) ; } public final boolean endEnum ( ) { return endEnum ; } private static final int min ( int a , int b , int c ) { final int t = ( a < b ) ? a : b ; return ( t < c ) ? t : c ; } private final int [ ] [ ] initDistanceArray ( ) { return new int [ this . text . length ( ) + 1 ] [ TYPICAL_LONGEST_WORD_IN_INDEX ] ; } private synchronized final float similarity ( final String target ) { final int m = target . length ( ) ; final int n = text . length ( ) ; if ( n == 0 ) { return prefix . length ( ) == 0 ? 0.0f : 1.0f - ( ( float ) m / prefix . length ( ) ) ; } if ( m == 0 ) { return prefix . length ( ) == 0 ? 0.0f : 1.0f - ( ( float ) n / prefix . length ( ) ) ; } final int maxDistance = getMaxDistance ( m ) ; if ( maxDistance < Math . abs ( m - n ) ) { return 0.0f ; } if ( d [ 0 ] . length <= m ) { growDistanceArray ( m ) ; } for ( int i = 0 ; i <= n ; i ++ ) d [ i ] [ 0 ] = i ; for ( int j = 0 ; j <= m ; j ++ ) d [ 0 ] [ j ] = j ; for ( int i = 1 ; i <= n ; i ++ ) { int bestPossibleEditDistance = m ; final char s_i = text . charAt ( i - 1 ) ; for ( int j = 1 ; j <= m ; j ++ ) { if ( s_i != target . charAt ( j - 1 ) ) { d [ i ] [ j ] = min ( d [ i - 1 ] [ j ] , d [ i ] [ j - 1 ] , d [ i - 1 ] [ j - 1 ] ) + 1 ; } else { d [ i ] [ j ] = min ( d [ i - 1 ] [ j ] + 1 , d [ i ] [ j - 1 ] + 1 , d [ i - 1 ] [ j - 1 ] ) ; } bestPossibleEditDistance = Math . min ( bestPossibleEditDistance , d [ i ] [ j ] ) ; } if ( i > maxDistance && bestPossibleEditDistance > maxDistance ) { return 0.0f ; } } return 1.0f - ( ( float ) d [ n ] [ m ] / ( float ) ( prefix . length ( ) + Math . min ( n , m ) ) ) ; } private void growDistanceArray ( int m ) { for ( int i = 0 ; i < d . length ; i ++ ) { d [ i ] = new int [ m + 1 ] ; } } private final int getMaxDistance ( int m ) { return ( m < maxDistances . length ) ? maxDistances [ m ] : calculateMaxDistance ( m ) ; } private void initializeMaxDistances ( ) { for ( int i = 0 ; i < maxDistances . length ; i ++ ) { maxDistances [ i ] = calculateMaxDistance ( i ) ; } } private int calculateMaxDistance ( int m ) { return ( int ) ( ( 1 - minimumSimilarity ) * ( Math . min ( text . length ( ) , m ) + prefix . length ( ) ) ) ; } public void close ( ) throws IOException { super . close ( ) ; } } 	1	['14', '3', '0', '5', '29', '53', '1', '4', '6', '0.692307692', '514', '1', '1', '0.541666667', '0.333333333', '1', '4', '34.92857143', '14', '2.2857', '1']
package org . apache . lucene . analysis . standard ; import org . apache . lucene . analysis . TokenFilter ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . analysis . TokenStream ; public final class StandardFilter extends TokenFilter { public StandardFilter ( TokenStream in ) { super ( in ) ; } private static final String APOSTROPHE_TYPE = StandardTokenizerImpl . TOKEN_TYPES [ StandardTokenizerImpl . APOSTROPHE ] ; private static final String ACRONYM_TYPE = StandardTokenizerImpl . TOKEN_TYPES [ StandardTokenizerImpl . ACRONYM ] ; public final Token next ( final Token reusableToken ) throws java . io . IOException { assert reusableToken != null ; Token nextToken = input . next ( reusableToken ) ; if ( nextToken == null ) return null ; char [ ] buffer = nextToken . termBuffer ( ) ; final int bufferLength = nextToken . termLength ( ) ; final String type = nextToken . type ( ) ; if ( type == APOSTROPHE_TYPE && bufferLength >= 2 && buffer [ bufferLength - 2 ] == '\'' && ( buffer [ bufferLength - 1 ] == 's' || buffer [ bufferLength - 1 ] == 'S' ) ) { nextToken . setTermLength ( bufferLength - 2 ) ; } else if ( type == ACRONYM_TYPE ) { int upto = 0 ; for ( int i = 0 ; i < bufferLength ; i ++ ) { char c = buffer [ i ] ; if ( c != '.' ) buffer [ upto ++ ] = c ; } nextToken . setTermLength ( upto ) ; } return nextToken ; } } 	0	['4', '3', '0', '5', '15', '4', '1', '4', '2', '0.75', '133', '0.5', '0', '0.777777778', '0.416666667', '1', '2', '31.25', '1', '0.5', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class Lock { public static long LOCK_POLL_INTERVAL = 1000 ; public static final long LOCK_OBTAIN_WAIT_FOREVER = - 1 ; public abstract boolean obtain ( ) throws IOException ; protected Throwable failureReason ; public boolean obtain ( long lockWaitTimeout ) throws LockObtainFailedException , IOException { failureReason = null ; boolean locked = obtain ( ) ; if ( lockWaitTimeout < 0 && lockWaitTimeout != LOCK_OBTAIN_WAIT_FOREVER ) throw new IllegalArgumentException ( "lockWaitTimeout should be LOCK_OBTAIN_WAIT_FOREVER or a non-negative number (got " + lockWaitTimeout + ")" ) ; long maxSleepCount = lockWaitTimeout / LOCK_POLL_INTERVAL ; long sleepCount = 0 ; while ( ! locked ) { if ( lockWaitTimeout != LOCK_OBTAIN_WAIT_FOREVER && sleepCount ++ >= maxSleepCount ) { String reason = "Lock obtain timed out: " + this . toString ( ) ; if ( failureReason != null ) { reason += ": " + failureReason ; } LockObtainFailedException e = new LockObtainFailedException ( reason ) ; if ( failureReason != null ) { e . initCause ( failureReason ) ; } throw e ; } try { Thread . sleep ( LOCK_POLL_INTERVAL ) ; } catch ( InterruptedException e ) { throw new IOException ( e . toString ( ) ) ; } locked = obtain ( ) ; } return locked ; } public abstract void release ( ) throws IOException ; public abstract boolean isLocked ( ) ; public abstract static class With { private Lock lock ; private long lockWaitTimeout ; public With ( Lock lock , long lockWaitTimeout ) { this . lock = lock ; this . lockWaitTimeout = lockWaitTimeout ; } protected abstract Object doBody ( ) throws IOException ; public Object run ( ) throws LockObtainFailedException , IOException { boolean locked = false ; try { locked = lock . obtain ( lockWaitTimeout ) ; return doBody ( ) ; } finally { if ( locked ) lock . release ( ) ; } } } } 	1	['6', '1', '5', '19', '19', '13', '18', '1', '5', '1', '119', '0.333333333', '0', '0', '0.6', '0', '0', '18.33333333', '1', '0.6667', '1']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . util . PriorityQueue ; final class SegmentMergeQueue extends PriorityQueue { SegmentMergeQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object a , Object b ) { SegmentMergeInfo stiA = ( SegmentMergeInfo ) a ; SegmentMergeInfo stiB = ( SegmentMergeInfo ) b ; int comparison = stiA . term . compareTo ( stiB . term ) ; if ( comparison == 0 ) return stiA . base < stiB . base ; else return comparison < 0 ; } final void close ( ) throws IOException { while ( top ( ) != null ) ( ( SegmentMergeInfo ) pop ( ) ) . close ( ) ; } } 	0	['3', '2', '0', '5', '9', '3', '2', '3', '0', '2', '47', '0', '0', '0.857142857', '0.555555556', '1', '3', '14.66666667', '4', '1.6667', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . Collection ; import java . util . Set ; import java . util . ArrayList ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Query ; import org . apache . lucene . util . ToStringUtils ; public class SpanFirstQuery extends SpanQuery { private SpanQuery match ; private int end ; public SpanFirstQuery ( SpanQuery match , int end ) { this . match = match ; this . end = end ; } public SpanQuery getMatch ( ) { return match ; } public int getEnd ( ) { return end ; } public String getField ( ) { return match . getField ( ) ; } public Collection getTerms ( ) { return match . getTerms ( ) ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "spanFirst(" ) ; buffer . append ( match . toString ( field ) ) ; buffer . append ( ", " ) ; buffer . append ( end ) ; buffer . append ( ")" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public void extractTerms ( Set terms ) { match . extractTerms ( terms ) ; } public PayloadSpans getPayloadSpans ( IndexReader reader ) throws IOException { return ( PayloadSpans ) getSpans ( reader ) ; } public Spans getSpans ( final IndexReader reader ) throws IOException { return new PayloadSpans ( ) { private PayloadSpans spans = match . getPayloadSpans ( reader ) ; public boolean next ( ) throws IOException { while ( spans . next ( ) ) { if ( end ( ) <= end ) return true ; } return false ; } public boolean skipTo ( int target ) throws IOException { if ( ! spans . skipTo ( target ) ) return false ; return spans . end ( ) <= end || next ( ) ; } public int doc ( ) { return spans . doc ( ) ; } public int start ( ) { return spans . start ( ) ; } public int end ( ) { return spans . end ( ) ; } public Collection getPayload ( ) throws IOException { ArrayList result = null ; if ( spans . isPayloadAvailable ( ) ) { result = new ArrayList ( spans . getPayload ( ) ) ; } return result ; } public boolean isPayloadAvailable ( ) { return spans . isPayloadAvailable ( ) ; } public String toString ( ) { return "spans(" + SpanFirstQuery . this . toString ( ) + ")" ; } } ; } public Query rewrite ( IndexReader reader ) throws IOException { SpanFirstQuery clone = null ; SpanQuery rewritten = ( SpanQuery ) match . rewrite ( reader ) ; if ( rewritten != match ) { clone = ( SpanFirstQuery ) this . clone ( ) ; clone . match = rewritten ; } if ( clone != null ) { return clone ; } else { return this ; } } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof SpanFirstQuery ) ) return false ; SpanFirstQuery other = ( SpanFirstQuery ) o ; return this . end == other . end && this . match . equals ( other . match ) && this . getBoost ( ) == other . getBoost ( ) ; } public int hashCode ( ) { int h = match . hashCode ( ) ; h ^= ( h << 8 ) | ( h > > > 25 ) ; h ^= Float . floatToRawIntBits ( getBoost ( ) ) ^ end ; return h ; } } 	1	['14', '3', '0', '7', '31', '0', '1', '7', '12', '0.461538462', '182', '1', '1', '0.566666667', '0.196428571', '2', '2', '11.85714286', '6', '1.2857', '2']
package org . apache . lucene . index ; import java . io . Serializable ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . analysis . TokenStream ; import org . apache . lucene . util . ArrayUtil ; public class Payload implements Serializable , Cloneable { protected byte [ ] data ; protected int offset ; protected int length ; public Payload ( ) { } public Payload ( byte [ ] data ) { this ( data , 0 , data . length ) ; } public Payload ( byte [ ] data , int offset , int length ) { if ( offset < 0 || offset + length > data . length ) { throw new IllegalArgumentException ( ) ; } this . data = data ; this . offset = offset ; this . length = length ; } public void setData ( byte [ ] data ) { setData ( data , 0 , data . length ) ; } public void setData ( byte [ ] data , int offset , int length ) { this . data = data ; this . offset = offset ; this . length = length ; } public byte [ ] getData ( ) { return this . data ; } public int getOffset ( ) { return this . offset ; } public int length ( ) { return this . length ; } public byte byteAt ( int index ) { if ( 0 <= index && index < this . length ) { return this . data [ this . offset + index ] ; } throw new ArrayIndexOutOfBoundsException ( index ) ; } public byte [ ] toByteArray ( ) { byte [ ] retArray = new byte [ this . length ] ; System . arraycopy ( this . data , this . offset , retArray , 0 , this . length ) ; return retArray ; } public void copyTo ( byte [ ] target , int targetOffset ) { if ( this . length > target . length + targetOffset ) { throw new ArrayIndexOutOfBoundsException ( ) ; } System . arraycopy ( this . data , this . offset , target , targetOffset , this . length ) ; } public Object clone ( ) { try { Payload clone = ( Payload ) super . clone ( ) ; if ( offset == 0 && length == data . length ) { clone . data = ( byte [ ] ) data . clone ( ) ; } else { clone . data = this . toByteArray ( ) ; clone . offset = 0 ; } return clone ; } catch ( CloneNotSupportedException e ) { throw new RuntimeException ( e ) ; } } public boolean equals ( Object obj ) { if ( obj == this ) return true ; if ( obj instanceof Payload ) { Payload other = ( Payload ) obj ; if ( length == other . length ) { for ( int i = 0 ; i < length ; i ++ ) if ( data [ offset + i ] != other . data [ other . offset + i ] ) return false ; return true ; } else return false ; } else return false ; } public int hashCode ( ) { return ArrayUtil . hashCode ( data , offset , offset + length ) ; } } 	0	['14', '1', '0', '4', '22', '0', '3', '1', '14', '0.230769231', '227', '1', '0', '0', '0.428571429', '1', '1', '15', '6', '1.5', '0']
package org . apache . lucene . search ; public class TopDocs implements java . io . Serializable { public int totalHits ; public ScoreDoc [ ] scoreDocs ; private float maxScore ; public float getMaxScore ( ) { return maxScore ; } public void setMaxScore ( float maxScore ) { this . maxScore = maxScore ; } public TopDocs ( int totalHits , ScoreDoc [ ] scoreDocs , float maxScore ) { this . totalHits = totalHits ; this . scoreDocs = scoreDocs ; this . maxScore = maxScore ; } } 	1	['3', '1', '1', '14', '4', '0', '13', '1', '3', '0.666666667', '25', '0.333333333', '1', '0', '0.583333333', '0', '0', '6.333333333', '1', '0.6667', '1']
package org . apache . lucene . store ; import java . net . Socket ; import java . io . IOException ; import java . io . InputStream ; import java . io . OutputStream ; public class VerifyingLockFactory extends LockFactory { LockFactory lf ; byte id ; String host ; int port ; private class CheckedLock extends Lock { private Lock lock ; public CheckedLock ( Lock lock ) { this . lock = lock ; } private void verify ( byte message ) { try { Socket s = new Socket ( host , port ) ; OutputStream out = s . getOutputStream ( ) ; out . write ( id ) ; out . write ( message ) ; InputStream in = s . getInputStream ( ) ; int result = in . read ( ) ; in . close ( ) ; out . close ( ) ; s . close ( ) ; if ( result != 0 ) throw new RuntimeException ( "lock was double acquired" ) ; } catch ( Exception e ) { throw new RuntimeException ( e ) ; } } public synchronized boolean obtain ( long lockWaitTimeout ) throws LockObtainFailedException , IOException { boolean obtained = lock . obtain ( lockWaitTimeout ) ; if ( obtained ) verify ( ( byte ) 1 ) ; return obtained ; } public synchronized boolean obtain ( ) throws LockObtainFailedException , IOException { return lock . obtain ( ) ; } public synchronized boolean isLocked ( ) { return lock . isLocked ( ) ; } public synchronized void release ( ) throws IOException { if ( isLocked ( ) ) { verify ( ( byte ) 0 ) ; lock . release ( ) ; } } } public VerifyingLockFactory ( byte id , LockFactory lf , String host , int port ) throws IOException { this . id = id ; this . lf = lf ; this . host = host ; this . port = port ; } public synchronized Lock makeLock ( String lockName ) { return new CheckedLock ( lf . makeLock ( lockName ) ) ; } public synchronized void clearLock ( String lockName ) throws IOException { lf . clearLock ( lockName ) ; } } 	0	['3', '2', '0', '4', '7', '0', '2', '3', '3', '0.75', '36', '0', '1', '0.666666667', '0.6', '0', '0', '9.666666667', '1', '0.6667', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; public class TopFieldDocCollector extends TopDocCollector { private FieldDoc reusableFD ; public TopFieldDocCollector ( IndexReader reader , Sort sort , int numHits ) throws IOException { super ( new FieldSortedHitQueue ( reader , sort . fields , numHits ) ) ; } public void collect ( int doc , float score ) { if ( score > 0.0f ) { totalHits ++ ; if ( reusableFD == null ) reusableFD = new FieldDoc ( doc , score ) ; else { reusableFD . score = score ; reusableFD . doc = doc ; } reusableFD = ( FieldDoc ) hq . insertWithOverflow ( reusableFD ) ; } } public TopDocs topDocs ( ) { FieldSortedHitQueue fshq = ( FieldSortedHitQueue ) hq ; ScoreDoc [ ] scoreDocs = new ScoreDoc [ fshq . size ( ) ] ; for ( int i = fshq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = fshq . fillFields ( ( FieldDoc ) fshq . pop ( ) ) ; return new TopFieldDocs ( totalHits , scoreDocs , fshq . getFields ( ) , fshq . getMaxScore ( ) ) ; } } 	1	['3', '3', '0', '11', '13', '1', '1', '10', '3', '1', '88', '1', '1', '0.666666667', '0.533333333', '1', '3', '28', '3', '1.6667', '1']
package org . apache . lucene . index ; final class FreqProxTermsWriterPerThread extends TermsHashConsumerPerThread { final TermsHashPerThread termsHashPerThread ; final DocumentsWriter . DocState docState ; public FreqProxTermsWriterPerThread ( TermsHashPerThread perThread ) { docState = perThread . docState ; termsHashPerThread = perThread ; } public TermsHashConsumerPerField addField ( TermsHashPerField termsHashPerField , FieldInfo fieldInfo ) { return new FreqProxTermsWriterPerField ( termsHashPerField , this , fieldInfo ) ; } void startDocument ( ) { } DocumentsWriter . DocWriter finishDocument ( ) { return null ; } public void abort ( ) { } } 	0	['5', '2', '0', '10', '7', '10', '3', '8', '3', '1', '28', '0', '2', '0.5', '0.4', '0', '0', '4.2', '1', '0.8', '0']
package org . apache . lucene . search ; import java . util . BitSet ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . DocIdBitSet ; public abstract class Filter implements java . io . Serializable { public BitSet bits ( IndexReader reader ) throws IOException { return null ; } public DocIdSet getDocIdSet ( IndexReader reader ) throws IOException { return new DocIdBitSet ( bits ( reader ) ) ; } } 	1	['3', '1', '6', '28', '5', '3', '25', '3', '3', '2', '15', '0', '0', '0', '0.833333333', '0', '0', '4', '1', '0.6667', '1']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . util . UnicodeUtil ; final class TermsHashPerField extends InvertedDocConsumerPerField { final TermsHashConsumerPerField consumer ; final TermsHashPerField nextPerField ; final TermsHashPerThread perThread ; final DocumentsWriter . DocState docState ; final DocInverter . FieldInvertState fieldState ; final CharBlockPool charPool ; final IntBlockPool intPool ; final ByteBlockPool bytePool ; final int streamCount ; final int numPostingInt ; final FieldInfo fieldInfo ; boolean postingsCompacted ; int numPostings ; private int postingsHashSize = 4 ; private int postingsHashHalfSize = postingsHashSize / 2 ; private int postingsHashMask = postingsHashSize - 1 ; private RawPostingList [ ] postingsHash = new RawPostingList [ postingsHashSize ] ; private RawPostingList p ; public TermsHashPerField ( DocInverterPerField docInverterPerField , final TermsHashPerThread perThread , final TermsHashPerThread nextPerThread , final FieldInfo fieldInfo ) { this . perThread = perThread ; intPool = perThread . intPool ; charPool = perThread . charPool ; bytePool = perThread . bytePool ; docState = perThread . docState ; fieldState = docInverterPerField . fieldState ; this . consumer = perThread . consumer . addField ( this , fieldInfo ) ; streamCount = consumer . getStreamCount ( ) ; numPostingInt = 2 * streamCount ; this . fieldInfo = fieldInfo ; if ( nextPerThread != null ) nextPerField = ( TermsHashPerField ) nextPerThread . addField ( docInverterPerField , fieldInfo ) ; else nextPerField = null ; } void shrinkHash ( int targetSize ) { assert postingsCompacted || numPostings == 0 ; int newSize = postingsHash . length ; while ( newSize >= 8 && newSize / 4 > targetSize ) { newSize /= 2 ; } if ( newSize != postingsHash . length ) { postingsHash = new RawPostingList [ newSize ] ; postingsHashSize = newSize ; postingsHashHalfSize = newSize / 2 ; postingsHashMask = newSize - 1 ; } } public void reset ( ) { if ( ! postingsCompacted ) compactPostings ( ) ; assert numPostings <= postingsHash . length ; if ( numPostings > 0 ) { perThread . termsHash . recyclePostings ( postingsHash , numPostings ) ; Arrays . fill ( postingsHash , 0 , numPostings , null ) ; numPostings = 0 ; } postingsCompacted = false ; if ( nextPerField != null ) nextPerField . reset ( ) ; } synchronized public void abort ( ) { reset ( ) ; if ( nextPerField != null ) nextPerField . abort ( ) ; } public void initReader ( ByteSliceReader reader , RawPostingList p , int stream ) { assert stream < streamCount ; final int [ ] ints = intPool . buffers [ p . intStart > > DocumentsWriter . INT_BLOCK_SHIFT ] ; final int upto = p . intStart & DocumentsWriter . INT_BLOCK_MASK ; reader . init ( bytePool , p . byteStart + stream * ByteBlockPool . FIRST_LEVEL_SIZE , ints [ upto + stream ] ) ; } private synchronized void compactPostings ( ) { int upto = 0 ; for ( int i = 0 ; i < postingsHashSize ; i ++ ) { if ( postingsHash [ i ] != null ) { if ( upto < i ) { postingsHash [ upto ] = postingsHash [ i ] ; postingsHash [ i ] = null ; } upto ++ ; } } assert upto == numPostings ; postingsCompacted = true ; } public RawPostingList [ ] sortPostings ( ) { compactPostings ( ) ; quickSort ( postingsHash , 0 , numPostings - 1 ) ; return postingsHash ; } void quickSort ( RawPostingList [ ] postings , int lo , int hi ) { if ( lo >= hi ) return ; else if ( hi == 1 + lo ) { if ( comparePostings ( postings [ lo ] , postings [ hi ] ) > 0 ) { final RawPostingList tmp = postings [ lo ] ; postings [ lo ] = postings [ hi ] ; postings [ hi ] = tmp ; } return ; } int mid = ( lo + hi ) > > > 1 ; if ( comparePostings ( postings [ lo ] , postings [ mid ] ) > 0 ) { RawPostingList tmp = postings [ lo ] ; postings [ lo ] = postings [ mid ] ; postings [ mid ] = tmp ; } if ( comparePostings ( postings [ mid ] , postings [ hi ] ) > 0 ) { RawPostingList tmp = postings [ mid ] ; postings [ mid ] = postings [ hi ] ; postings [ hi ] = tmp ; if ( comparePostings ( postings [ lo ] , postings [ mid ] ) > 0 ) { RawPostingList tmp2 = postings [ lo ] ; postings [ lo ] = postings [ mid ] ; postings [ mid ] = tmp2 ; } } int left = lo + 1 ; int right = hi - 1 ; if ( left >= right ) return ; RawPostingList partition = postings [ mid ] ; for ( ; ; ) { while ( comparePostings ( postings [ right ] , partition ) > 0 ) -- right ; while ( left < right && comparePostings ( postings [ left ] , partition ) <= 0 ) ++ left ; if ( left < right ) { RawPostingList tmp = postings [ left ] ; postings [ left ] = postings [ right ] ; postings [ right ] = tmp ; -- right ; } else { break ; } } quickSort ( postings , lo , left ) ; quickSort ( postings , left + 1 , hi ) ; } int comparePostings ( RawPostingList p1 , RawPostingList p2 ) { if ( p1 == p2 ) return 0 ; final char [ ] text1 = charPool . buffers [ p1 . textStart > > DocumentsWriter . CHAR_BLOCK_SHIFT ] ; int pos1 = p1 . textStart & DocumentsWriter . CHAR_BLOCK_MASK ; final char [ ] text2 = charPool . buffers [ p2 . textStart > > DocumentsWriter . CHAR_BLOCK_SHIFT ] ; int pos2 = p2 . textStart & DocumentsWriter . CHAR_BLOCK_MASK ; assert text1 != text2 || pos1 != pos2 ; while ( true ) { final char c1 = text1 [ pos1 ++ ] ; final char c2 = text2 [ pos2 ++ ] ; if ( c1 != c2 ) { if ( 0xffff == c2 ) return 1 ; else if ( 0xffff == c1 ) return - 1 ; else return c1 - c2 ; } else assert c1 != 0xffff ; } } private boolean postingEquals ( final char [ ] tokenText , final int tokenTextLen ) { final char [ ] text = perThread . charPool . buffers [ p . textStart > > DocumentsWriter . CHAR_BLOCK_SHIFT ] ; assert text != null ; int pos = p . textStart & DocumentsWriter . CHAR_BLOCK_MASK ; int tokenPos = 0 ; for ( ; tokenPos < tokenTextLen ; pos ++ , tokenPos ++ ) if ( tokenText [ tokenPos ] != text [ pos ] ) return false ; return 0xffff == text [ pos ] ; } private boolean doCall ; private boolean doNextCall ; boolean start ( Fieldable [ ] fields , int count ) throws IOException { doCall = consumer . start ( fields , count ) ; if ( nextPerField != null ) doNextCall = nextPerField . start ( fields , count ) ; return doCall || doNextCall ; } public void add ( Token token , int textStart ) throws IOException { int code = textStart ; int hashPos = code & postingsHashMask ; assert ! postingsCompacted ; p = postingsHash [ hashPos ] ; if ( p != null && p . textStart != textStart ) { final int inc = ( ( code > > 8 ) + code ) | 1 ; do { code += inc ; hashPos = code & postingsHashMask ; p = postingsHash [ hashPos ] ; } while ( p != null && p . textStart != textStart ) ; } if ( p == null ) { if ( 0 == perThread . freePostingsCount ) perThread . morePostings ( ) ; p = perThread . freePostings [ -- perThread . freePostingsCount ] ; assert p != null ; p . textStart = textStart ; assert postingsHash [ hashPos ] == null ; postingsHash [ hashPos ] = p ; numPostings ++ ; if ( numPostings == postingsHashHalfSize ) rehashPostings ( 2 * postingsHashSize ) ; if ( numPostingInt + intPool . intUpto > DocumentsWriter . INT_BLOCK_SIZE ) intPool . nextBuffer ( ) ; if ( DocumentsWriter . BYTE_BLOCK_SIZE - bytePool . byteUpto < numPostingInt * ByteBlockPool . FIRST_LEVEL_SIZE ) bytePool . nextBuffer ( ) ; intUptos = intPool . buffer ; intUptoStart = intPool . intUpto ; intPool . intUpto += streamCount ; p . intStart = intUptoStart + intPool . intOffset ; for ( int i = 0 ; i < streamCount ; i ++ ) { final int upto = bytePool . newSlice ( ByteBlockPool . FIRST_LEVEL_SIZE ) ; intUptos [ intUptoStart + i ] = upto + bytePool . byteOffset ; } p . byteStart = intUptos [ intUptoStart ] ; consumer . newTerm ( token , p ) ; } else { intUptos = intPool . buffers [ p . intStart > > DocumentsWriter . INT_BLOCK_SHIFT ] ; intUptoStart = p . intStart & DocumentsWriter . INT_BLOCK_MASK ; consumer . addTerm ( token , p ) ; } } void add ( Token token ) throws IOException { assert ! postingsCompacted ; final char [ ] tokenText = token . termBuffer ( ) ; final int tokenTextLen = token . termLength ( ) ; int downto = tokenTextLen ; int code = 0 ; while ( downto > 0 ) { char ch = tokenText [ -- downto ] ; if ( ch >= UnicodeUtil . UNI_SUR_LOW_START && ch <= UnicodeUtil . UNI_SUR_LOW_END ) { if ( 0 == downto ) { ch = tokenText [ downto ] = UnicodeUtil . UNI_REPLACEMENT_CHAR ; } else { final char ch2 = tokenText [ downto - 1 ] ; if ( ch2 >= UnicodeUtil . UNI_SUR_HIGH_START && ch2 <= UnicodeUtil . UNI_SUR_HIGH_END ) { code = ( ( code * 31 ) + ch ) * 31 + ch2 ; downto -- ; continue ; } else { ch = tokenText [ downto ] = UnicodeUtil . UNI_REPLACEMENT_CHAR ; } } } else if ( ch >= UnicodeUtil . UNI_SUR_HIGH_START && ch <= UnicodeUtil . UNI_SUR_HIGH_END ) ch = tokenText [ downto ] = UnicodeUtil . UNI_REPLACEMENT_CHAR ; code = ( code * 31 ) + ch ; } int hashPos = code & postingsHashMask ; p = postingsHash [ hashPos ] ; if ( p != null && ! postingEquals ( tokenText , tokenTextLen ) ) { final int inc = ( ( code > > 8 ) + code ) | 1 ; do { code += inc ; hashPos = code & postingsHashMask ; p = postingsHash [ hashPos ] ; } while ( p != null && ! postingEquals ( tokenText , tokenTextLen ) ) ; } if ( p == null ) { final int textLen1 = 1 + tokenTextLen ; if ( textLen1 + charPool . charUpto > DocumentsWriter . CHAR_BLOCK_SIZE ) { if ( textLen1 > DocumentsWriter . CHAR_BLOCK_SIZE ) { if ( docState . maxTermPrefix == null ) docState . maxTermPrefix = new String ( tokenText , 0 , 30 ) ; consumer . skippingLongTerm ( token ) ; return ; } charPool . nextBuffer ( ) ; } if ( 0 == perThread . freePostingsCount ) perThread . morePostings ( ) ; p = perThread . freePostings [ -- perThread . freePostingsCount ] ; assert p != null ; final char [ ] text = charPool . buffer ; final int textUpto = charPool . charUpto ; p . textStart = textUpto + charPool . charOffset ; charPool . charUpto += textLen1 ; System . arraycopy ( tokenText , 0 , text , textUpto , tokenTextLen ) ; text [ textUpto + tokenTextLen ] = 0xffff ; assert postingsHash [ hashPos ] == null ; postingsHash [ hashPos ] = p ; numPostings ++ ; if ( numPostings == postingsHashHalfSize ) rehashPostings ( 2 * postingsHashSize ) ; if ( numPostingInt + intPool . intUpto > DocumentsWriter . INT_BLOCK_SIZE ) intPool . nextBuffer ( ) ; if ( DocumentsWriter . BYTE_BLOCK_SIZE - bytePool . byteUpto < numPostingInt * ByteBlockPool . FIRST_LEVEL_SIZE ) bytePool . nextBuffer ( ) ; intUptos = intPool . buffer ; intUptoStart = intPool . intUpto ; intPool . intUpto += streamCount ; p . intStart = intUptoStart + intPool . intOffset ; for ( int i = 0 ; i < streamCount ; i ++ ) { final int upto = bytePool . newSlice ( ByteBlockPool . FIRST_LEVEL_SIZE ) ; intUptos [ intUptoStart + i ] = upto + bytePool . byteOffset ; } p . byteStart = intUptos [ intUptoStart ] ; consumer . newTerm ( token , p ) ; } else { intUptos = intPool . buffers [ p . intStart > > DocumentsWriter . INT_BLOCK_SHIFT ] ; intUptoStart = p . intStart & DocumentsWriter . INT_BLOCK_MASK ; consumer . addTerm ( token , p ) ; } if ( doNextCall ) nextPerField . add ( token , p . textStart ) ; } int [ ] intUptos ; int intUptoStart ; void writeByte ( int stream , byte b ) { int upto = intUptos [ intUptoStart + stream ] ; byte [ ] bytes = bytePool . buffers [ upto > > DocumentsWriter . BYTE_BLOCK_SHIFT ] ; assert bytes != null ; int offset = upto & DocumentsWriter . BYTE_BLOCK_MASK ; if ( bytes [ offset ] != 0 ) { offset = bytePool . allocSlice ( bytes , offset ) ; bytes = bytePool . buffer ; intUptos [ intUptoStart + stream ] = offset + bytePool . byteOffset ; } bytes [ offset ] = b ; ( intUptos [ intUptoStart + stream ] ) ++ ; } public void writeBytes ( int stream , byte [ ] b , int offset , int len ) { final int end = offset + len ; for ( int i = offset ; i < end ; i ++ ) writeByte ( stream , b [ i ] ) ; } void writeVInt ( int stream , int i ) { assert stream < streamCount ; while ( ( i & ~ 0x7F ) != 0 ) { writeByte ( stream , ( byte ) ( ( i & 0x7f ) | 0x80 ) ) ; i >>>= 7 ; } writeByte ( stream , ( byte ) i ) ; } void finish ( ) throws IOException { consumer . finish ( ) ; if ( nextPerField != null ) nextPerField . finish ( ) ; } void rehashPostings ( final int newSize ) { final int newMask = newSize - 1 ; RawPostingList [ ] newHash = new RawPostingList [ newSize ] ; for ( int i = 0 ; i < postingsHashSize ; i ++ ) { RawPostingList p0 = postingsHash [ i ] ; if ( p0 != null ) { int code ; if ( perThread . primary ) { final int start = p0 . textStart & DocumentsWriter . CHAR_BLOCK_MASK ; final char [ ] text = charPool . buffers [ p0 . textStart > > DocumentsWriter . CHAR_BLOCK_SHIFT ] ; int pos = start ; while ( text [ pos ] != 0xffff ) pos ++ ; code = 0 ; while ( pos > start ) code = ( code * 31 ) + text [ -- pos ] ; } else code = p0 . textStart ; int hashPos = code & newMask ; assert hashPos >= 0 ; if ( newHash [ hashPos ] != null ) { final int inc = ( ( code > > 8 ) + code ) | 1 ; do { code += inc ; hashPos = code & newMask ; } while ( newHash [ hashPos ] != null ) ; } newHash [ hashPos ] = p0 ; } } postingsHashMask = newMask ; postingsHash = newHash ; postingsHashSize = newSize ; postingsHashHalfSize = newSize > > 1 ; } } 	0	['20', '2', '0', '23', '47', '0', '10', '16', '7', '0.785087719', '1605', '0.291666667', '11', '0.181818182', '0.157894737', '0', '0', '78.05', '12', '3.75', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; public interface FieldCache { public static final int STRING_INDEX = - 1 ; public static class StringIndex { public final String [ ] lookup ; public final int [ ] order ; public StringIndex ( int [ ] values , String [ ] lookup ) { this . order = values ; this . lookup = lookup ; } } public interface ByteParser { public byte parseByte ( String string ) ; } public interface ShortParser { public short parseShort ( String string ) ; } public interface IntParser { public int parseInt ( String string ) ; } public interface FloatParser { public float parseFloat ( String string ) ; } public static FieldCache DEFAULT = new FieldCacheImpl ( ) ; public byte [ ] getBytes ( IndexReader reader , String field ) throws IOException ; public byte [ ] getBytes ( IndexReader reader , String field , ByteParser parser ) throws IOException ; public short [ ] getShorts ( IndexReader reader , String field ) throws IOException ; public short [ ] getShorts ( IndexReader reader , String field , ShortParser parser ) throws IOException ; public int [ ] getInts ( IndexReader reader , String field ) throws IOException ; public int [ ] getInts ( IndexReader reader , String field , IntParser parser ) throws IOException ; public float [ ] getFloats ( IndexReader reader , String field ) throws IOException ; public float [ ] getFloats ( IndexReader reader , String field , FloatParser parser ) throws IOException ; public String [ ] getStrings ( IndexReader reader , String field ) throws IOException ; public StringIndex getStringIndex ( IndexReader reader , String field ) throws IOException ; public Object getAuto ( IndexReader reader , String field ) throws IOException ; public Comparable [ ] getCustom ( IndexReader reader , String field , SortComparator comparator ) throws IOException ; } 	1	['13', '1', '0', '17', '14', '78', '11', '8', '12', '1.041666667', '20', '0', '1', '0', '0.427083333', '0', '0', '0.384615385', '1', '0.9231', '6']
package org . apache . lucene . index ; import java . io . IOException ; public class StaleReaderException extends IOException { public StaleReaderException ( String message ) { super ( message ) ; } } 	0	['1', '4', '0', '3', '2', '0', '3', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class IndexInput implements Cloneable { private byte [ ] bytes ; private char [ ] chars ; private boolean preUTF8Strings ; public abstract byte readByte ( ) throws IOException ; public abstract void readBytes ( byte [ ] b , int offset , int len ) throws IOException ; public void readBytes ( byte [ ] b , int offset , int len , boolean useBuffer ) throws IOException { readBytes ( b , offset , len ) ; } public int readInt ( ) throws IOException { return ( ( readByte ( ) & 0xFF ) << 24 ) | ( ( readByte ( ) & 0xFF ) << 16 ) | ( ( readByte ( ) & 0xFF ) << 8 ) | ( readByte ( ) & 0xFF ) ; } public int readVInt ( ) throws IOException { byte b = readByte ( ) ; int i = b & 0x7F ; for ( int shift = 7 ; ( b & 0x80 ) != 0 ; shift += 7 ) { b = readByte ( ) ; i |= ( b & 0x7F ) << shift ; } return i ; } public long readLong ( ) throws IOException { return ( ( ( long ) readInt ( ) ) << 32 ) | ( readInt ( ) & 0xFFFFFFFFL ) ; } public long readVLong ( ) throws IOException { byte b = readByte ( ) ; long i = b & 0x7F ; for ( int shift = 7 ; ( b & 0x80 ) != 0 ; shift += 7 ) { b = readByte ( ) ; i |= ( b & 0x7FL ) << shift ; } return i ; } public void setModifiedUTF8StringsMode ( ) { preUTF8Strings = true ; } public String readString ( ) throws IOException { if ( preUTF8Strings ) return readModifiedUTF8String ( ) ; int length = readVInt ( ) ; if ( bytes == null || length > bytes . length ) bytes = new byte [ ( int ) ( length * 1.25 ) ] ; readBytes ( bytes , 0 , length ) ; return new String ( bytes , 0 , length , "UTF-8" ) ; } private String readModifiedUTF8String ( ) throws IOException { int length = readVInt ( ) ; if ( chars == null || length > chars . length ) chars = new char [ length ] ; readChars ( chars , 0 , length ) ; return new String ( chars , 0 , length ) ; } public void readChars ( char [ ] buffer , int start , int length ) throws IOException { final int end = start + length ; for ( int i = start ; i < end ; i ++ ) { byte b = readByte ( ) ; if ( ( b & 0x80 ) == 0 ) buffer [ i ] = ( char ) ( b & 0x7F ) ; else if ( ( b & 0xE0 ) != 0xE0 ) { buffer [ i ] = ( char ) ( ( ( b & 0x1F ) << 6 ) | ( readByte ( ) & 0x3F ) ) ; } else buffer [ i ] = ( char ) ( ( ( b & 0x0F ) << 12 ) | ( ( readByte ( ) & 0x3F ) << 6 ) | ( readByte ( ) & 0x3F ) ) ; } } public void skipChars ( int length ) throws IOException { for ( int i = 0 ; i < length ; i ++ ) { byte b = readByte ( ) ; if ( ( b & 0x80 ) == 0 ) { } else if ( ( b & 0xE0 ) != 0xE0 ) { readByte ( ) ; } else { readByte ( ) ; readByte ( ) ; } } } public abstract void close ( ) throws IOException ; public abstract long getFilePointer ( ) ; public abstract void seek ( long pos ) throws IOException ; public abstract long length ( ) ; public Object clone ( ) { IndexInput clone = null ; try { clone = ( IndexInput ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . bytes = null ; clone . chars = null ; return clone ; } } 	1	['18', '1', '7', '40', '22', '147', '40', '0', '17', '0.921568627', '314', '1', '0', '0', '0.25', '0', '0', '16.27777778', '1', '0.9444', '2']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Collection ; import java . util . Iterator ; import java . util . HashMap ; import java . util . Map ; import java . util . List ; import java . util . ArrayList ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . search . Similarity ; final class NormsWriter extends InvertedDocEndConsumer { private static final byte defaultNorm = Similarity . encodeNorm ( 1.0f ) ; private FieldInfos fieldInfos ; public InvertedDocEndConsumerPerThread addThread ( DocInverterPerThread docInverterPerThread ) { return new NormsWriterPerThread ( docInverterPerThread , this ) ; } public void abort ( ) { } void files ( Collection files ) { } void setFieldInfos ( FieldInfos fieldInfos ) { this . fieldInfos = fieldInfos ; } public void flush ( Map threadsAndFields , DocumentsWriter . FlushState state ) throws IOException { final Map byField = new HashMap ( ) ; final Iterator it = threadsAndFields . entrySet ( ) . iterator ( ) ; while ( it . hasNext ( ) ) { Map . Entry entry = ( Map . Entry ) it . next ( ) ; Collection fields = ( Collection ) entry . getValue ( ) ; Iterator fieldsIt = fields . iterator ( ) ; while ( fieldsIt . hasNext ( ) ) { NormsWriterPerField perField = ( NormsWriterPerField ) fieldsIt . next ( ) ; if ( perField . upto > 0 ) { List l = ( List ) byField . get ( perField . fieldInfo ) ; if ( l == null ) { l = new ArrayList ( ) ; byField . put ( perField . fieldInfo , l ) ; } l . add ( perField ) ; } else fieldsIt . remove ( ) ; } } final String normsFileName = state . segmentName + "." + IndexFileNames . NORMS_EXTENSION ; state . flushedFiles . add ( normsFileName ) ; IndexOutput normsOut = state . directory . createOutput ( normsFileName ) ; try { normsOut . writeBytes ( SegmentMerger . NORMS_HEADER , 0 , SegmentMerger . NORMS_HEADER . length ) ; final int numField = fieldInfos . size ( ) ; int normCount = 0 ; for ( int fieldNumber = 0 ; fieldNumber < numField ; fieldNumber ++ ) { final FieldInfo fieldInfo = fieldInfos . fieldInfo ( fieldNumber ) ; List toMerge = ( List ) byField . get ( fieldInfo ) ; int upto = 0 ; if ( toMerge != null ) { final int numFields = toMerge . size ( ) ; normCount ++ ; final NormsWriterPerField [ ] fields = new NormsWriterPerField [ numFields ] ; int [ ] uptos = new int [ numFields ] ; for ( int j = 0 ; j < numFields ; j ++ ) fields [ j ] = ( NormsWriterPerField ) toMerge . get ( j ) ; int numLeft = numFields ; while ( numLeft > 0 ) { assert uptos [ 0 ] < fields [ 0 ] . docIDs . length : " uptos[0]=" + uptos [ 0 ] + " len=" + ( fields [ 0 ] . docIDs . length ) ; int minLoc = 0 ; int minDocID = fields [ 0 ] . docIDs [ uptos [ 0 ] ] ; for ( int j = 1 ; j < numLeft ; j ++ ) { final int docID = fields [ j ] . docIDs [ uptos [ j ] ] ; if ( docID < minDocID ) { minDocID = docID ; minLoc = j ; } } assert minDocID < state . numDocsInRAM ; for ( ; upto < minDocID ; upto ++ ) normsOut . writeByte ( defaultNorm ) ; normsOut . writeByte ( fields [ minLoc ] . norms [ uptos [ minLoc ] ] ) ; ( uptos [ minLoc ] ) ++ ; upto ++ ; if ( uptos [ minLoc ] == fields [ minLoc ] . upto ) { fields [ minLoc ] . reset ( ) ; if ( minLoc != numLeft - 1 ) { fields [ minLoc ] = fields [ numLeft - 1 ] ; uptos [ minLoc ] = uptos [ numLeft - 1 ] ; } numLeft -- ; } } for ( ; upto < state . numDocsInRAM ; upto ++ ) normsOut . writeByte ( defaultNorm ) ; } else if ( fieldInfo . isIndexed && ! fieldInfo . omitNorms ) { normCount ++ ; for ( ; upto < state . numDocsInRAM ; upto ++ ) normsOut . writeByte ( defaultNorm ) ; } assert 4 + normCount * state . numDocsInRAM == normsOut . getFilePointer ( ) : ".nrm file size mismatch: expected=" + ( 4 + normCount * state . numDocsInRAM ) + " actual=" + normsOut . getFilePointer ( ) ; } } finally { normsOut . close ( ) ; } } void closeDocStore ( DocumentsWriter . FlushState state ) { } } 	0	['9', '2', '0', '13', '46', '32', '2', '12', '3', '0.90625', '409', '0.5', '1', '0.416666667', '0.25', '0', '0', '44', '1', '0.7778', '0']
package org . apache . lucene . search . function ; import org . apache . lucene . search . Explanation ; public abstract class DocValues { public abstract float floatVal ( int doc ) ; public int intVal ( int doc ) { return ( int ) floatVal ( doc ) ; } public long longVal ( int doc ) { return ( long ) floatVal ( doc ) ; } public double doubleVal ( int doc ) { return ( double ) floatVal ( doc ) ; } public String strVal ( int doc ) { return Float . toString ( floatVal ( doc ) ) ; } public abstract String toString ( int doc ) ; public Explanation explain ( int doc ) { return new Explanation ( floatVal ( doc ) , toString ( doc ) ) ; } Object getInnerArray ( ) { throw new UnsupportedOperationException ( "this optional method is for test purposes only" ) ; } private float minVal ; private float maxVal ; private float avgVal ; private boolean computed = false ; private void compute ( ) { if ( computed ) { return ; } minVal = Float . MAX_VALUE ; maxVal = 0 ; float sum = 0 ; int n = 0 ; while ( true ) { float val ; try { val = floatVal ( n ) ; } catch ( ArrayIndexOutOfBoundsException e ) { break ; } sum += val ; minVal = Math . min ( minVal , val ) ; maxVal = Math . max ( maxVal , val ) ; } avgVal = sum / n ; computed = true ; } public float getMinValue ( ) { compute ( ) ; return minVal ; } public float getMaxValue ( ) { compute ( ) ; return maxVal ; } public float getAverageValue ( ) { compute ( ) ; return avgVal ; } } 	1	['13', '1', '6', '16', '19', '70', '15', '1', '11', '0.729166667', '121', '1', '0', '0', '0.769230769', '0', '0', '8', '2', '1', '2']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import org . apache . lucene . store . IndexOutput ; class DefaultSkipListWriter extends MultiLevelSkipListWriter { private int [ ] lastSkipDoc ; private int [ ] lastSkipPayloadLength ; private long [ ] lastSkipFreqPointer ; private long [ ] lastSkipProxPointer ; private IndexOutput freqOutput ; private IndexOutput proxOutput ; private int curDoc ; private boolean curStorePayloads ; private int curPayloadLength ; private long curFreqPointer ; private long curProxPointer ; DefaultSkipListWriter ( int skipInterval , int numberOfSkipLevels , int docCount , IndexOutput freqOutput , IndexOutput proxOutput ) { super ( skipInterval , numberOfSkipLevels , docCount ) ; this . freqOutput = freqOutput ; this . proxOutput = proxOutput ; lastSkipDoc = new int [ numberOfSkipLevels ] ; lastSkipPayloadLength = new int [ numberOfSkipLevels ] ; lastSkipFreqPointer = new long [ numberOfSkipLevels ] ; lastSkipProxPointer = new long [ numberOfSkipLevels ] ; } void setSkipData ( int doc , boolean storePayloads , int payloadLength ) { this . curDoc = doc ; this . curStorePayloads = storePayloads ; this . curPayloadLength = payloadLength ; this . curFreqPointer = freqOutput . getFilePointer ( ) ; if ( proxOutput != null ) this . curProxPointer = proxOutput . getFilePointer ( ) ; } protected void resetSkip ( ) { super . resetSkip ( ) ; Arrays . fill ( lastSkipDoc , 0 ) ; Arrays . fill ( lastSkipPayloadLength , - 1 ) ; Arrays . fill ( lastSkipFreqPointer , freqOutput . getFilePointer ( ) ) ; if ( proxOutput != null ) Arrays . fill ( lastSkipProxPointer , proxOutput . getFilePointer ( ) ) ; } protected void writeSkipData ( int level , IndexOutput skipBuffer ) throws IOException { if ( curStorePayloads ) { int delta = curDoc - lastSkipDoc [ level ] ; if ( curPayloadLength == lastSkipPayloadLength [ level ] ) { skipBuffer . writeVInt ( delta * 2 ) ; } else { skipBuffer . writeVInt ( delta * 2 + 1 ) ; skipBuffer . writeVInt ( curPayloadLength ) ; lastSkipPayloadLength [ level ] = curPayloadLength ; } } else { skipBuffer . writeVInt ( curDoc - lastSkipDoc [ level ] ) ; } skipBuffer . writeVInt ( ( int ) ( curFreqPointer - lastSkipFreqPointer [ level ] ) ) ; skipBuffer . writeVInt ( ( int ) ( curProxPointer - lastSkipProxPointer [ level ] ) ) ; lastSkipDoc [ level ] = curDoc ; lastSkipFreqPointer [ level ] = curFreqPointer ; lastSkipProxPointer [ level ] = curProxPointer ; } } 	0	['4', '2', '0', '4', '10', '0', '2', '2', '0', '0.484848485', '182', '1', '2', '0.625', '0.625', '1', '1', '41.75', '2', '1.25', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . * ; final class ExactPhraseScorer extends PhraseScorer { ExactPhraseScorer ( Weight weight , TermPositions [ ] tps , int [ ] offsets , Similarity similarity , byte [ ] norms ) { super ( weight , tps , offsets , similarity , norms ) ; } protected final float phraseFreq ( ) throws IOException { pq . clear ( ) ; for ( PhrasePositions pp = first ; pp != null ; pp = pp . next ) { pp . firstPosition ( ) ; pq . put ( pp ) ; } pqToList ( ) ; int freq = 0 ; do { while ( first . position < last . position ) { do { if ( ! first . nextPosition ( ) ) return ( float ) freq ; } while ( first . position < last . position ) ; firstToLast ( ) ; } freq ++ ; } while ( last . nextPosition ( ) ) ; return ( float ) freq ; } } 	1	['2', '4', '0', '8', '9', '1', '2', '6', '0', '2', '64', '0', '0', '0.952380952', '0.583333333', '1', '1', '31', '1', '0.5', '2']
package org . apache . lucene . util . cache ; import java . util . HashMap ; import java . util . Map ; import java . util . Set ; public class SimpleMapCache extends Cache { Map map ; public SimpleMapCache ( ) { this ( new HashMap ( ) ) ; } public SimpleMapCache ( Map map ) { this . map = map ; } public Object get ( Object key ) { return map . get ( key ) ; } public void put ( Object key , Object value ) { map . put ( key , value ) ; } public void close ( ) { } public boolean containsKey ( Object key ) { return map . containsKey ( key ) ; } public Set keySet ( ) { return map . keySet ( ) ; } Cache getSynchronizedCache ( ) { return new SynchronizedSimpleMapCache ( this ) ; } private static class SynchronizedSimpleMapCache extends SimpleMapCache { Object mutex ; SimpleMapCache cache ; SynchronizedSimpleMapCache ( SimpleMapCache cache ) { this . cache = cache ; this . mutex = this ; } public void put ( Object key , Object value ) { synchronized ( mutex ) { cache . put ( key , value ) ; } } public Object get ( Object key ) { synchronized ( mutex ) { return cache . get ( key ) ; } } public boolean containsKey ( Object key ) { synchronized ( mutex ) { return cache . containsKey ( key ) ; } } public void close ( ) { synchronized ( mutex ) { cache . close ( ) ; } } public Set keySet ( ) { synchronized ( mutex ) { return cache . keySet ( ) ; } } Cache getSynchronizedCache ( ) { return this ; } } } 	0	['8', '2', '2', '3', '15', '8', '2', '2', '7', '0.285714286', '48', '0', '0', '0.5', '0.5', '1', '1', '4.875', '1', '0.75', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermDocs ; import org . apache . lucene . index . TermEnum ; import java . io . IOException ; import java . util . HashMap ; import java . util . Locale ; import java . util . Map ; import java . util . WeakHashMap ; class FieldCacheImpl implements FieldCache { abstract static class Cache { private final Map readerCache = new WeakHashMap ( ) ; protected abstract Object createValue ( IndexReader reader , Object key ) throws IOException ; public Object get ( IndexReader reader , Object key ) throws IOException { Map innerCache ; Object value ; synchronized ( readerCache ) { innerCache = ( Map ) readerCache . get ( reader ) ; if ( innerCache == null ) { innerCache = new HashMap ( ) ; readerCache . put ( reader , innerCache ) ; value = null ; } else { value = innerCache . get ( key ) ; } if ( value == null ) { value = new CreationPlaceholder ( ) ; innerCache . put ( key , value ) ; } } if ( value instanceof CreationPlaceholder ) { synchronized ( value ) { CreationPlaceholder progress = ( CreationPlaceholder ) value ; if ( progress . value == null ) { progress . value = createValue ( reader , key ) ; synchronized ( readerCache ) { innerCache . put ( key , progress . value ) ; } } return progress . value ; } } return value ; } } static final class CreationPlaceholder { Object value ; } static class Entry { final String field ; final int type ; final Object custom ; final Locale locale ; Entry ( String field , int type , Locale locale ) { this . field = field . intern ( ) ; this . type = type ; this . custom = null ; this . locale = locale ; } Entry ( String field , Object custom ) { this . field = field . intern ( ) ; this . type = SortField . CUSTOM ; this . custom = custom ; this . locale = null ; } public boolean equals ( Object o ) { if ( o instanceof Entry ) { Entry other = ( Entry ) o ; if ( other . field == field && other . type == type ) { if ( other . locale == null ? locale == null : other . locale . equals ( locale ) ) { if ( other . custom == null ) { if ( custom == null ) return true ; } else if ( other . custom . equals ( custom ) ) { return true ; } } } } return false ; } public int hashCode ( ) { return field . hashCode ( ) ^ type ^ ( custom == null ? 0 : custom . hashCode ( ) ) ^ ( locale == null ? 0 : locale . hashCode ( ) ) ; } } private static final ByteParser BYTE_PARSER = new ByteParser ( ) { public byte parseByte ( String value ) { return Byte . parseByte ( value ) ; } } ; private static final ShortParser SHORT_PARSER = new ShortParser ( ) { public short parseShort ( String value ) { return Short . parseShort ( value ) ; } } ; private static final IntParser INT_PARSER = new IntParser ( ) { public int parseInt ( String value ) { return Integer . parseInt ( value ) ; } } ; private static final FloatParser FLOAT_PARSER = new FloatParser ( ) { public float parseFloat ( String value ) { return Float . parseFloat ( value ) ; } } ; public byte [ ] getBytes ( IndexReader reader , String field ) throws IOException { return getBytes ( reader , field , BYTE_PARSER ) ; } public byte [ ] getBytes ( IndexReader reader , String field , ByteParser parser ) throws IOException { return ( byte [ ] ) bytesCache . get ( reader , new Entry ( field , parser ) ) ; } Cache bytesCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object entryKey ) throws IOException { Entry entry = ( Entry ) entryKey ; String field = entry . field ; ByteParser parser = ( ByteParser ) entry . custom ; final byte [ ] retArray = new byte [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; byte termval = parser . parseByte ( term . text ( ) ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } return retArray ; } } ; public short [ ] getShorts ( IndexReader reader , String field ) throws IOException { return getShorts ( reader , field , SHORT_PARSER ) ; } public short [ ] getShorts ( IndexReader reader , String field , ShortParser parser ) throws IOException { return ( short [ ] ) shortsCache . get ( reader , new Entry ( field , parser ) ) ; } Cache shortsCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object entryKey ) throws IOException { Entry entry = ( Entry ) entryKey ; String field = entry . field ; ShortParser parser = ( ShortParser ) entry . custom ; final short [ ] retArray = new short [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; short termval = parser . parseShort ( term . text ( ) ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } return retArray ; } } ; public int [ ] getInts ( IndexReader reader , String field ) throws IOException { return getInts ( reader , field , INT_PARSER ) ; } public int [ ] getInts ( IndexReader reader , String field , IntParser parser ) throws IOException { return ( int [ ] ) intsCache . get ( reader , new Entry ( field , parser ) ) ; } Cache intsCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object entryKey ) throws IOException { Entry entry = ( Entry ) entryKey ; String field = entry . field ; IntParser parser = ( IntParser ) entry . custom ; final int [ ] retArray = new int [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; int termval = parser . parseInt ( term . text ( ) ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } return retArray ; } } ; public float [ ] getFloats ( IndexReader reader , String field ) throws IOException { return getFloats ( reader , field , FLOAT_PARSER ) ; } public float [ ] getFloats ( IndexReader reader , String field , FloatParser parser ) throws IOException { return ( float [ ] ) floatsCache . get ( reader , new Entry ( field , parser ) ) ; } Cache floatsCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object entryKey ) throws IOException { Entry entry = ( Entry ) entryKey ; String field = entry . field ; FloatParser parser = ( FloatParser ) entry . custom ; final float [ ] retArray = new float [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; float termval = parser . parseFloat ( term . text ( ) ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } return retArray ; } } ; public String [ ] getStrings ( IndexReader reader , String field ) throws IOException { return ( String [ ] ) stringsCache . get ( reader , field ) ; } Cache stringsCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object fieldKey ) throws IOException { String field = ( ( String ) fieldKey ) . intern ( ) ; final String [ ] retArray = new String [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; String termval = term . text ( ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } return retArray ; } } ; public StringIndex getStringIndex ( IndexReader reader , String field ) throws IOException { return ( StringIndex ) stringsIndexCache . get ( reader , field ) ; } Cache stringsIndexCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object fieldKey ) throws IOException { String field = ( ( String ) fieldKey ) . intern ( ) ; final int [ ] retArray = new int [ reader . maxDoc ( ) ] ; String [ ] mterms = new String [ reader . maxDoc ( ) + 1 ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field ) ) ; int t = 0 ; mterms [ t ++ ] = null ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; if ( t >= mterms . length ) throw new RuntimeException ( "there are more terms than " + "documents in field \"" + field + "\", but it's impossible to sort on " + "tokenized fields" ) ; mterms [ t ] = term . text ( ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = t ; } t ++ ; } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } if ( t == 0 ) { mterms = new String [ 1 ] ; } else if ( t < mterms . length ) { String [ ] terms = new String [ t ] ; System . arraycopy ( mterms , 0 , terms , 0 , t ) ; mterms = terms ; } StringIndex value = new StringIndex ( retArray , mterms ) ; return value ; } } ; public Object getAuto ( IndexReader reader , String field ) throws IOException { return autoCache . get ( reader , field ) ; } Cache autoCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object fieldKey ) throws IOException { String field = ( ( String ) fieldKey ) . intern ( ) ; TermEnum enumerator = reader . terms ( new Term ( field ) ) ; try { Term term = enumerator . term ( ) ; if ( term == null ) { throw new RuntimeException ( "no terms in field " + field + " - cannot determine sort type" ) ; } Object ret = null ; if ( term . field ( ) == field ) { String termtext = term . text ( ) . trim ( ) ; try { Integer . parseInt ( termtext ) ; ret = getInts ( reader , field ) ; } catch ( NumberFormatException nfe1 ) { try { Float . parseFloat ( termtext ) ; ret = getFloats ( reader , field ) ; } catch ( NumberFormatException nfe3 ) { ret = getStringIndex ( reader , field ) ; } } } else { throw new RuntimeException ( "field \"" + field + "\" does not appear to be indexed" ) ; } return ret ; } finally { enumerator . close ( ) ; } } } ; public Comparable [ ] getCustom ( IndexReader reader , String field , SortComparator comparator ) throws IOException { return ( Comparable [ ] ) customCache . get ( reader , new Entry ( field , comparator ) ) ; } Cache customCache = new Cache ( ) { protected Object createValue ( IndexReader reader , Object entryKey ) throws IOException { Entry entry = ( Entry ) entryKey ; String field = entry . field ; SortComparator comparator = ( SortComparator ) entry . custom ; final Comparable [ ] retArray = new Comparable [ reader . maxDoc ( ) ] ; TermDocs termDocs = reader . termDocs ( ) ; TermEnum termEnum = reader . terms ( new Term ( field ) ) ; try { do { Term term = termEnum . term ( ) ; if ( term == null || term . field ( ) != field ) break ; Comparable termval = comparator . getComparable ( term . text ( ) ) ; termDocs . seek ( termEnum ) ; while ( termDocs . next ( ) ) { retArray [ termDocs . doc ( ) ] = termval ; } } while ( termEnum . next ( ) ) ; } finally { termDocs . close ( ) ; termEnum . close ( ) ; } return retArray ; } } ; } 	1	['14', '1', '1', '23', '29', '67', '10', '22', '12', '0.897435897', '199', '0.333333333', '12', '0', '0.403846154', '0', '0', '12.35714286', '1', '0.8571', '4']
package org . apache . lucene . search ; import org . apache . lucene . search . Filter ; import org . apache . lucene . util . OpenBitSet ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . TermEnum ; import org . apache . lucene . index . TermDocs ; import java . util . BitSet ; import java . io . IOException ; public class PrefixFilter extends Filter { protected final Term prefix ; public PrefixFilter ( Term prefix ) { this . prefix = prefix ; } public Term getPrefix ( ) { return prefix ; } public BitSet bits ( IndexReader reader ) throws IOException { final BitSet bitSet = new BitSet ( reader . maxDoc ( ) ) ; new PrefixGenerator ( prefix ) { public void handleDoc ( int doc ) { bitSet . set ( doc ) ; } } . generate ( reader ) ; return bitSet ; } public DocIdSet getDocIdSet ( IndexReader reader ) throws IOException { final OpenBitSet bitSet = new OpenBitSet ( reader . maxDoc ( ) ) ; new PrefixGenerator ( prefix ) { public void handleDoc ( int doc ) { bitSet . set ( doc ) ; } } . generate ( reader ) ; return bitSet ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "PrefixFilter(" ) ; buffer . append ( prefix . toString ( ) ) ; buffer . append ( ")" ) ; return buffer . toString ( ) ; } } interface IdGenerator { public void generate ( IndexReader reader ) throws IOException ; public void handleDoc ( int doc ) ; } abstract class PrefixGenerator implements IdGenerator { protected final Term prefix ; PrefixGenerator ( Term prefix ) { this . prefix = prefix ; } public void generate ( IndexReader reader ) throws IOException { TermEnum enumerator = reader . terms ( prefix ) ; TermDocs termDocs = reader . termDocs ( ) ; try { String prefixText = prefix . text ( ) ; String prefixField = prefix . field ( ) ; do { Term term = enumerator . term ( ) ; if ( term != null && term . text ( ) . startsWith ( prefixText ) && term . field ( ) == prefixField ) { termDocs . seek ( term ) ; while ( termDocs . next ( ) ) { handleDoc ( termDocs . doc ( ) ) ; } } else { break ; } } while ( enumerator . next ( ) ) ; } finally { termDocs . close ( ) ; enumerator . close ( ) ; } } } 	0	['5', '2', '0', '7', '17', '0', '2', '7', '5', '0', '71', '1', '1', '0.333333333', '0.533333333', '1', '1', '13', '1', '0.8', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . util . BitVector ; import java . io . IOException ; import java . util . List ; import java . util . ArrayList ; final class SegmentInfo { static final int NO = - 1 ; static final int YES = 1 ; static final int CHECK_DIR = 0 ; static final int WITHOUT_GEN = 0 ; public String name ; public int docCount ; public Directory dir ; private boolean preLockless ; private long delGen ; private long [ ] normGen ; private byte isCompoundFile ; private boolean hasSingleNormFile ; private List files ; long sizeInBytes = - 1 ; private int docStoreOffset ; private String docStoreSegment ; private boolean docStoreIsCompoundFile ; private int delCount ; private boolean hasProx ; public SegmentInfo ( String name , int docCount , Directory dir ) { this . name = name ; this . docCount = docCount ; this . dir = dir ; delGen = NO ; isCompoundFile = CHECK_DIR ; preLockless = true ; hasSingleNormFile = false ; docStoreOffset = - 1 ; docStoreSegment = name ; docStoreIsCompoundFile = false ; delCount = 0 ; hasProx = true ; } public SegmentInfo ( String name , int docCount , Directory dir , boolean isCompoundFile , boolean hasSingleNormFile ) { this ( name , docCount , dir , isCompoundFile , hasSingleNormFile , - 1 , null , false , true ) ; } public SegmentInfo ( String name , int docCount , Directory dir , boolean isCompoundFile , boolean hasSingleNormFile , int docStoreOffset , String docStoreSegment , boolean docStoreIsCompoundFile , boolean hasProx ) { this ( name , docCount , dir ) ; this . isCompoundFile = ( byte ) ( isCompoundFile ? YES : NO ) ; this . hasSingleNormFile = hasSingleNormFile ; preLockless = false ; this . docStoreOffset = docStoreOffset ; this . docStoreSegment = docStoreSegment ; this . docStoreIsCompoundFile = docStoreIsCompoundFile ; this . hasProx = hasProx ; delCount = 0 ; assert docStoreOffset == - 1 || docStoreSegment != null : "dso=" + docStoreOffset + " dss=" + docStoreSegment + " docCount=" + docCount ; } void reset ( SegmentInfo src ) { clearFiles ( ) ; name = src . name ; docCount = src . docCount ; dir = src . dir ; preLockless = src . preLockless ; delGen = src . delGen ; docStoreOffset = src . docStoreOffset ; docStoreIsCompoundFile = src . docStoreIsCompoundFile ; if ( src . normGen == null ) { normGen = null ; } else { normGen = new long [ src . normGen . length ] ; System . arraycopy ( src . normGen , 0 , normGen , 0 , src . normGen . length ) ; } isCompoundFile = src . isCompoundFile ; hasSingleNormFile = src . hasSingleNormFile ; delCount = src . delCount ; } SegmentInfo ( Directory dir , int format , IndexInput input ) throws IOException { this . dir = dir ; name = input . readString ( ) ; docCount = input . readInt ( ) ; if ( format <= SegmentInfos . FORMAT_LOCKLESS ) { delGen = input . readLong ( ) ; if ( format <= SegmentInfos . FORMAT_SHARED_DOC_STORE ) { docStoreOffset = input . readInt ( ) ; if ( docStoreOffset != - 1 ) { docStoreSegment = input . readString ( ) ; docStoreIsCompoundFile = ( 1 == input . readByte ( ) ) ; } else { docStoreSegment = name ; docStoreIsCompoundFile = false ; } } else { docStoreOffset = - 1 ; docStoreSegment = name ; docStoreIsCompoundFile = false ; } if ( format <= SegmentInfos . FORMAT_SINGLE_NORM_FILE ) { hasSingleNormFile = ( 1 == input . readByte ( ) ) ; } else { hasSingleNormFile = false ; } int numNormGen = input . readInt ( ) ; if ( numNormGen == NO ) { normGen = null ; } else { normGen = new long [ numNormGen ] ; for ( int j = 0 ; j < numNormGen ; j ++ ) { normGen [ j ] = input . readLong ( ) ; } } isCompoundFile = input . readByte ( ) ; preLockless = ( isCompoundFile == CHECK_DIR ) ; if ( format <= SegmentInfos . FORMAT_DEL_COUNT ) { delCount = input . readInt ( ) ; assert delCount <= docCount ; } else delCount = - 1 ; if ( format <= SegmentInfos . FORMAT_HAS_PROX ) hasProx = input . readByte ( ) == 1 ; else hasProx = true ; } else { delGen = CHECK_DIR ; normGen = null ; isCompoundFile = CHECK_DIR ; preLockless = true ; hasSingleNormFile = false ; docStoreOffset = - 1 ; docStoreIsCompoundFile = false ; docStoreSegment = null ; delCount = - 1 ; hasProx = true ; } } void setNumFields ( int numFields ) { if ( normGen == null ) { normGen = new long [ numFields ] ; if ( preLockless ) { } else { for ( int i = 0 ; i < numFields ; i ++ ) { normGen [ i ] = NO ; } } } } long sizeInBytes ( ) throws IOException { if ( sizeInBytes == - 1 ) { List files = files ( ) ; final int size = files . size ( ) ; sizeInBytes = 0 ; for ( int i = 0 ; i < size ; i ++ ) { final String fileName = ( String ) files . get ( i ) ; if ( docStoreOffset == - 1 || ! IndexFileNames . isDocStoreFile ( fileName ) ) sizeInBytes += dir . fileLength ( fileName ) ; } } return sizeInBytes ; } boolean hasDeletions ( ) throws IOException { if ( delGen == NO ) { return false ; } else if ( delGen >= YES ) { return true ; } else { return dir . fileExists ( getDelFileName ( ) ) ; } } void advanceDelGen ( ) { if ( delGen == NO ) { delGen = YES ; } else { delGen ++ ; } clearFiles ( ) ; } void clearDelGen ( ) { delGen = NO ; clearFiles ( ) ; } public Object clone ( ) { SegmentInfo si = new SegmentInfo ( name , docCount , dir ) ; si . isCompoundFile = isCompoundFile ; si . delGen = delGen ; si . delCount = delCount ; si . preLockless = preLockless ; si . hasSingleNormFile = hasSingleNormFile ; if ( normGen != null ) { si . normGen = ( long [ ] ) normGen . clone ( ) ; } si . docStoreOffset = docStoreOffset ; si . docStoreSegment = docStoreSegment ; si . docStoreIsCompoundFile = docStoreIsCompoundFile ; return si ; } String getDelFileName ( ) { if ( delGen == NO ) { return null ; } else { return IndexFileNames . fileNameFromGeneration ( name , "." + IndexFileNames . DELETES_EXTENSION , delGen ) ; } } boolean hasSeparateNorms ( int fieldNumber ) throws IOException { if ( ( normGen == null && preLockless ) || ( normGen != null && normGen [ fieldNumber ] == CHECK_DIR ) ) { String fileName = name + ".s" + fieldNumber ; return dir . fileExists ( fileName ) ; } else if ( normGen == null || normGen [ fieldNumber ] == NO ) { return false ; } else { return true ; } } boolean hasSeparateNorms ( ) throws IOException { if ( normGen == null ) { if ( ! preLockless ) { return false ; } else { String [ ] result = dir . list ( ) ; if ( result == null ) throw new IOException ( "cannot read directory " + dir + ": list() returned null" ) ; String pattern ; pattern = name + ".s" ; int patternLength = pattern . length ( ) ; for ( int i = 0 ; i < result . length ; i ++ ) { if ( result [ i ] . startsWith ( pattern ) && Character . isDigit ( result [ i ] . charAt ( patternLength ) ) ) return true ; } return false ; } } else { for ( int i = 0 ; i < normGen . length ; i ++ ) { if ( normGen [ i ] >= YES ) { return true ; } } for ( int i = 0 ; i < normGen . length ; i ++ ) { if ( normGen [ i ] == CHECK_DIR ) { if ( hasSeparateNorms ( i ) ) { return true ; } } } } return false ; } void advanceNormGen ( int fieldIndex ) { if ( normGen [ fieldIndex ] == NO ) { normGen [ fieldIndex ] = YES ; } else { normGen [ fieldIndex ] ++ ; } clearFiles ( ) ; } String getNormFileName ( int number ) throws IOException { String prefix ; long gen ; if ( normGen == null ) { gen = CHECK_DIR ; } else { gen = normGen [ number ] ; } if ( hasSeparateNorms ( number ) ) { prefix = ".s" ; return IndexFileNames . fileNameFromGeneration ( name , prefix + number , gen ) ; } if ( hasSingleNormFile ) { prefix = "." + IndexFileNames . NORMS_EXTENSION ; return IndexFileNames . fileNameFromGeneration ( name , prefix , WITHOUT_GEN ) ; } prefix = ".f" ; return IndexFileNames . fileNameFromGeneration ( name , prefix + number , WITHOUT_GEN ) ; } void setUseCompoundFile ( boolean isCompoundFile ) { if ( isCompoundFile ) { this . isCompoundFile = YES ; } else { this . isCompoundFile = NO ; } clearFiles ( ) ; } boolean getUseCompoundFile ( ) throws IOException { if ( isCompoundFile == NO ) { return false ; } else if ( isCompoundFile == YES ) { return true ; } else { return dir . fileExists ( name + "." + IndexFileNames . COMPOUND_FILE_EXTENSION ) ; } } int getDelCount ( ) throws IOException { if ( delCount == - 1 ) { if ( hasDeletions ( ) ) { final String delFileName = getDelFileName ( ) ; delCount = new BitVector ( dir , delFileName ) . count ( ) ; } else delCount = 0 ; } assert delCount <= docCount ; return delCount ; } void setDelCount ( int delCount ) { this . delCount = delCount ; assert delCount <= docCount ; } int getDocStoreOffset ( ) { return docStoreOffset ; } boolean getDocStoreIsCompoundFile ( ) { return docStoreIsCompoundFile ; } void setDocStoreIsCompoundFile ( boolean v ) { docStoreIsCompoundFile = v ; clearFiles ( ) ; } String getDocStoreSegment ( ) { return docStoreSegment ; } void setDocStoreOffset ( int offset ) { docStoreOffset = offset ; clearFiles ( ) ; } void write ( IndexOutput output ) throws IOException { output . writeString ( name ) ; output . writeInt ( docCount ) ; output . writeLong ( delGen ) ; output . writeInt ( docStoreOffset ) ; if ( docStoreOffset != - 1 ) { output . writeString ( docStoreSegment ) ; output . writeByte ( ( byte ) ( docStoreIsCompoundFile ? 1 : 0 ) ) ; } output . writeByte ( ( byte ) ( hasSingleNormFile ? 1 : 0 ) ) ; if ( normGen == null ) { output . writeInt ( NO ) ; } else { output . writeInt ( normGen . length ) ; for ( int j = 0 ; j < normGen . length ; j ++ ) { output . writeLong ( normGen [ j ] ) ; } } output . writeByte ( isCompoundFile ) ; output . writeInt ( delCount ) ; output . writeByte ( ( byte ) ( hasProx ? 1 : 0 ) ) ; } void setHasProx ( boolean hasProx ) { this . hasProx = hasProx ; clearFiles ( ) ; } boolean getHasProx ( ) { return hasProx ; } private void addIfExists ( List files , String fileName ) throws IOException { if ( dir . fileExists ( fileName ) ) files . add ( fileName ) ; } public List files ( ) throws IOException { if ( files != null ) { return files ; } files = new ArrayList ( ) ; boolean useCompoundFile = getUseCompoundFile ( ) ; if ( useCompoundFile ) { files . add ( name + "." + IndexFileNames . COMPOUND_FILE_EXTENSION ) ; } else { final String [ ] exts = IndexFileNames . NON_STORE_INDEX_EXTENSIONS ; for ( int i = 0 ; i < exts . length ; i ++ ) addIfExists ( files , name + "." + exts [ i ] ) ; } if ( docStoreOffset != - 1 ) { assert docStoreSegment != null ; if ( docStoreIsCompoundFile ) { files . add ( docStoreSegment + "." + IndexFileNames . COMPOUND_FILE_STORE_EXTENSION ) ; } else { final String [ ] exts = IndexFileNames . STORE_INDEX_EXTENSIONS ; for ( int i = 0 ; i < exts . length ; i ++ ) addIfExists ( files , docStoreSegment + "." + exts [ i ] ) ; } } else if ( ! useCompoundFile ) { final String [ ] exts = IndexFileNames . STORE_INDEX_EXTENSIONS ; for ( int i = 0 ; i < exts . length ; i ++ ) addIfExists ( files , name + "." + exts [ i ] ) ; } String delFileName = IndexFileNames . fileNameFromGeneration ( name , "." + IndexFileNames . DELETES_EXTENSION , delGen ) ; if ( delFileName != null && ( delGen >= YES || dir . fileExists ( delFileName ) ) ) { files . add ( delFileName ) ; } if ( normGen != null ) { for ( int i = 0 ; i < normGen . length ; i ++ ) { long gen = normGen [ i ] ; if ( gen >= YES ) { files . add ( IndexFileNames . fileNameFromGeneration ( name , "." + IndexFileNames . SEPARATE_NORMS_EXTENSION + i , gen ) ) ; } else if ( NO == gen ) { if ( ! hasSingleNormFile && ! useCompoundFile ) { String fileName = name + "." + IndexFileNames . PLAIN_NORMS_EXTENSION + i ; if ( dir . fileExists ( fileName ) ) { files . add ( fileName ) ; } } } else if ( CHECK_DIR == gen ) { String fileName = null ; if ( useCompoundFile ) { fileName = name + "." + IndexFileNames . SEPARATE_NORMS_EXTENSION + i ; } else if ( ! hasSingleNormFile ) { fileName = name + "." + IndexFileNames . PLAIN_NORMS_EXTENSION + i ; } if ( fileName != null && dir . fileExists ( fileName ) ) { files . add ( fileName ) ; } } } } else if ( preLockless || ( ! hasSingleNormFile && ! useCompoundFile ) ) { String prefix ; if ( useCompoundFile ) prefix = name + "." + IndexFileNames . SEPARATE_NORMS_EXTENSION ; else prefix = name + "." + IndexFileNames . PLAIN_NORMS_EXTENSION ; int prefixLength = prefix . length ( ) ; String [ ] allFiles = dir . list ( ) ; if ( allFiles == null ) throw new IOException ( "cannot read directory " + dir + ": list() returned null" ) ; for ( int i = 0 ; i < allFiles . length ; i ++ ) { String fileName = allFiles [ i ] ; if ( fileName . length ( ) > prefixLength && Character . isDigit ( fileName . charAt ( prefixLength ) ) && fileName . startsWith ( prefix ) ) { files . add ( fileName ) ; } } } return files ; } private void clearFiles ( ) { files = null ; sizeInBytes = - 1 ; } public String segString ( Directory dir ) { String cfs ; try { if ( getUseCompoundFile ( ) ) cfs = "c" ; else cfs = "C" ; } catch ( IOException ioe ) { cfs = "?" ; } String docStore ; if ( docStoreOffset != - 1 ) docStore = "->" + docStoreSegment ; else docStore = "" ; return name + ":" + cfs + ( this . dir == dir ? "" : "x" ) + docCount + docStore ; } public boolean equals ( Object obj ) { SegmentInfo other ; try { other = ( SegmentInfo ) obj ; } catch ( ClassCastException cce ) { return false ; } return other . dir == dir && other . name . equals ( name ) ; } public int hashCode ( ) { return dir . hashCode ( ) + name . hashCode ( ) ; } } 	1	['36', '1', '0', '23', '77', '92', '18', '5', '8', '0.776870748', '1568', '0.523809524', '1', '0', '0.182857143', '1', '1', '41.97222222', '4', '1.3056', '2']
package org . apache . lucene . index ; import java . util . * ; public class SortedTermVectorMapper extends TermVectorMapper { private SortedSet currentSet ; private Map termToTVE = new HashMap ( ) ; private boolean storeOffsets ; private boolean storePositions ; public static final String ALL = "_ALL_" ; public SortedTermVectorMapper ( Comparator comparator ) { this ( false , false , comparator ) ; } public SortedTermVectorMapper ( boolean ignoringPositions , boolean ignoringOffsets , Comparator comparator ) { super ( ignoringPositions , ignoringOffsets ) ; currentSet = new TreeSet ( comparator ) ; } public void map ( String term , int frequency , TermVectorOffsetInfo [ ] offsets , int [ ] positions ) { TermVectorEntry entry = ( TermVectorEntry ) termToTVE . get ( term ) ; if ( entry == null ) { entry = new TermVectorEntry ( ALL , term , frequency , storeOffsets == true ? offsets : null , storePositions == true ? positions : null ) ; termToTVE . put ( term , entry ) ; currentSet . add ( entry ) ; } else { entry . setFrequency ( entry . getFrequency ( ) + frequency ) ; if ( storeOffsets ) { TermVectorOffsetInfo [ ] existingOffsets = entry . getOffsets ( ) ; if ( existingOffsets != null && offsets != null && offsets . length > 0 ) { TermVectorOffsetInfo [ ] newOffsets = new TermVectorOffsetInfo [ existingOffsets . length + offsets . length ] ; System . arraycopy ( existingOffsets , 0 , newOffsets , 0 , existingOffsets . length ) ; System . arraycopy ( offsets , 0 , newOffsets , existingOffsets . length , offsets . length ) ; entry . setOffsets ( newOffsets ) ; } else if ( existingOffsets == null && offsets != null && offsets . length > 0 ) { entry . setOffsets ( offsets ) ; } } if ( storePositions ) { int [ ] existingPositions = entry . getPositions ( ) ; if ( existingPositions != null && positions != null && positions . length > 0 ) { int [ ] newPositions = new int [ existingPositions . length + positions . length ] ; System . arraycopy ( existingPositions , 0 , newPositions , 0 , existingPositions . length ) ; System . arraycopy ( positions , 0 , newPositions , existingPositions . length , positions . length ) ; entry . setPositions ( newPositions ) ; } else if ( existingPositions == null && positions != null && positions . length > 0 ) { entry . setPositions ( positions ) ; } } } } public void setExpectations ( String field , int numTerms , boolean storeOffsets , boolean storePositions ) { this . storeOffsets = storeOffsets ; this . storePositions = storePositions ; } public SortedSet getTermVectorEntrySet ( ) { return currentSet ; } } 	0	['5', '2', '0', '3', '19', '2', '0', '3', '5', '0.7', '188', '0.8', '0', '0.625', '0.428571429', '0', '0', '35.6', '18', '4', '0']
package org . apache . lucene . index ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . document . Document ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . FSDirectory ; import org . apache . lucene . store . LockObtainFailedException ; import java . io . File ; import java . io . IOException ; import java . io . PrintStream ; public class IndexModifier { protected IndexWriter indexWriter = null ; protected IndexReader indexReader = null ; protected Directory directory = null ; protected Analyzer analyzer = null ; protected boolean open = false ; protected PrintStream infoStream = null ; protected boolean useCompoundFile = true ; protected int maxBufferedDocs = IndexWriter . DEFAULT_MAX_BUFFERED_DOCS ; protected int maxFieldLength = IndexWriter . DEFAULT_MAX_FIELD_LENGTH ; protected int mergeFactor = IndexWriter . DEFAULT_MERGE_FACTOR ; public IndexModifier ( Directory directory , Analyzer analyzer , boolean create ) throws CorruptIndexException , LockObtainFailedException , IOException { init ( directory , analyzer , create ) ; } public IndexModifier ( String dirName , Analyzer analyzer , boolean create ) throws CorruptIndexException , LockObtainFailedException , IOException { Directory dir = FSDirectory . getDirectory ( dirName ) ; init ( dir , analyzer , create ) ; } public IndexModifier ( File file , Analyzer analyzer , boolean create ) throws CorruptIndexException , LockObtainFailedException , IOException { Directory dir = FSDirectory . getDirectory ( file ) ; init ( dir , analyzer , create ) ; } protected void init ( Directory directory , Analyzer analyzer , boolean create ) throws CorruptIndexException , LockObtainFailedException , IOException { this . directory = directory ; synchronized ( this . directory ) { this . analyzer = analyzer ; indexWriter = new IndexWriter ( directory , analyzer , create , IndexWriter . MaxFieldLength . LIMITED ) ; open = true ; } } protected void assureOpen ( ) { if ( ! open ) { throw new IllegalStateException ( "Index is closed" ) ; } } protected void createIndexWriter ( ) throws CorruptIndexException , LockObtainFailedException , IOException { if ( indexWriter == null ) { if ( indexReader != null ) { indexReader . close ( ) ; indexReader = null ; } indexWriter = new IndexWriter ( directory , analyzer , false , new IndexWriter . MaxFieldLength ( maxFieldLength ) ) ; indexWriter . setMergeScheduler ( new SerialMergeScheduler ( ) ) ; indexWriter . setInfoStream ( infoStream ) ; indexWriter . setUseCompoundFile ( useCompoundFile ) ; if ( maxBufferedDocs != IndexWriter . DISABLE_AUTO_FLUSH ) indexWriter . setMaxBufferedDocs ( maxBufferedDocs ) ; indexWriter . setMergeFactor ( mergeFactor ) ; } } protected void createIndexReader ( ) throws CorruptIndexException , IOException { if ( indexReader == null ) { if ( indexWriter != null ) { indexWriter . close ( ) ; indexWriter = null ; } indexReader = IndexReader . open ( directory ) ; } } public void flush ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . close ( ) ; indexWriter = null ; createIndexWriter ( ) ; } else { indexReader . close ( ) ; indexReader = null ; createIndexReader ( ) ; } } } public void addDocument ( Document doc , Analyzer docAnalyzer ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; if ( docAnalyzer != null ) indexWriter . addDocument ( doc , docAnalyzer ) ; else indexWriter . addDocument ( doc ) ; } } public void addDocument ( Document doc ) throws CorruptIndexException , LockObtainFailedException , IOException { addDocument ( doc , null ) ; } public int deleteDocuments ( Term term ) throws StaleReaderException , CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexReader ( ) ; return indexReader . deleteDocuments ( term ) ; } } public void deleteDocument ( int docNum ) throws StaleReaderException , CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexReader ( ) ; indexReader . deleteDocument ( docNum ) ; } } public int docCount ( ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { return indexWriter . docCount ( ) ; } else { return indexReader . numDocs ( ) ; } } } public void optimize ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; indexWriter . optimize ( ) ; } } public void setInfoStream ( PrintStream infoStream ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . setInfoStream ( infoStream ) ; } this . infoStream = infoStream ; } } public PrintStream getInfoStream ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; return indexWriter . getInfoStream ( ) ; } } public void setUseCompoundFile ( boolean useCompoundFile ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . setUseCompoundFile ( useCompoundFile ) ; } this . useCompoundFile = useCompoundFile ; } } public boolean getUseCompoundFile ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; return indexWriter . getUseCompoundFile ( ) ; } } public void setMaxFieldLength ( int maxFieldLength ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . setMaxFieldLength ( maxFieldLength ) ; } this . maxFieldLength = maxFieldLength ; } } public int getMaxFieldLength ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; return indexWriter . getMaxFieldLength ( ) ; } } public void setMaxBufferedDocs ( int maxBufferedDocs ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . setMaxBufferedDocs ( maxBufferedDocs ) ; } this . maxBufferedDocs = maxBufferedDocs ; } } public int getMaxBufferedDocs ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; return indexWriter . getMaxBufferedDocs ( ) ; } } public void setMergeFactor ( int mergeFactor ) { synchronized ( directory ) { assureOpen ( ) ; if ( indexWriter != null ) { indexWriter . setMergeFactor ( mergeFactor ) ; } this . mergeFactor = mergeFactor ; } } public int getMergeFactor ( ) throws CorruptIndexException , LockObtainFailedException , IOException { synchronized ( directory ) { assureOpen ( ) ; createIndexWriter ( ) ; return indexWriter . getMergeFactor ( ) ; } } public void close ( ) throws CorruptIndexException , IOException { synchronized ( directory ) { if ( ! open ) throw new IllegalStateException ( "Index is closed already" ) ; if ( indexWriter != null ) { indexWriter . close ( ) ; indexWriter = null ; } else if ( indexReader != null ) { indexReader . close ( ) ; indexReader = null ; } open = false ; } } public String toString ( ) { return "Index@" + directory ; } } 	1	['26', '1', '0', '13', '58', '0', '0', '13', '22', '0.344', '719', '1', '4', '0', '0.184615385', '0', '0', '26.26923077', '2', '1.1538', '2']
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . FieldCache ; import java . io . IOException ; public class OrdFieldSource extends ValueSource { protected String field ; public OrdFieldSource ( String field ) { this . field = field ; } public String description ( ) { return "ord(" + field + ')' ; } public DocValues getValues ( IndexReader reader ) throws IOException { final int [ ] arr = FieldCache . DEFAULT . getStringIndex ( reader , field ) . order ; return new DocValues ( ) { public float floatVal ( int doc ) { return ( float ) arr [ doc ] ; } public String strVal ( int doc ) { return Integer . toString ( arr [ doc ] ) ; } public String toString ( int doc ) { return description ( ) + '=' + intVal ( doc ) ; } Object getInnerArray ( ) { return arr ; } } ; } public boolean equals ( Object o ) { if ( o . getClass ( ) != OrdFieldSource . class ) return false ; OrdFieldSource other = ( OrdFieldSource ) o ; return this . field . equals ( other . field ) ; } private static final int hcode = OrdFieldSource . class . hashCode ( ) ; public int hashCode ( ) { return hcode + field . hashCode ( ) ; } } 	0	['7', '2', '0', '6', '21', '0', '1', '6', '5', '0.666666667', '90', '0.666666667', '0', '0.5', '0.375', '2', '2', '11.42857143', '3', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . FieldSelector ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . CorruptIndexException ; import java . io . IOException ; public interface Searchable extends java . rmi . Remote { void search ( Weight weight , Filter filter , HitCollector results ) throws IOException ; void close ( ) throws IOException ; int docFreq ( Term term ) throws IOException ; int [ ] docFreqs ( Term [ ] terms ) throws IOException ; int maxDoc ( ) throws IOException ; TopDocs search ( Weight weight , Filter filter , int n ) throws IOException ; Document doc ( int i ) throws CorruptIndexException , IOException ; Document doc ( int n , FieldSelector fieldSelector ) throws CorruptIndexException , IOException ; Query rewrite ( Query query ) throws IOException ; Explanation explain ( Weight weight , int doc ) throws IOException ; TopFieldDocs search ( Weight weight , Filter filter , int n , Sort sort ) throws IOException ; } 	1	['11', '1', '0', '18', '11', '55', '6', '12', '11', '2', '11', '0', '0', '0', '0.263636364', '0', '0', '0', '1', '1', '5']
package org . apache . lucene . document ; import java . util . HashMap ; import java . util . List ; import java . util . Map ; public class MapFieldSelector implements FieldSelector { Map fieldSelections ; public MapFieldSelector ( Map fieldSelections ) { this . fieldSelections = fieldSelections ; } public MapFieldSelector ( List fields ) { fieldSelections = new HashMap ( fields . size ( ) * 5 / 3 ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) fieldSelections . put ( fields . get ( i ) , FieldSelectorResult . LOAD ) ; } public MapFieldSelector ( String [ ] fields ) { fieldSelections = new HashMap ( fields . length * 5 / 3 ) ; for ( int i = 0 ; i < fields . length ; i ++ ) fieldSelections . put ( fields [ i ] , FieldSelectorResult . LOAD ) ; } public FieldSelectorResult accept ( String field ) { FieldSelectorResult selection = ( FieldSelectorResult ) fieldSelections . get ( field ) ; return selection != null ? selection : FieldSelectorResult . NO_LOAD ; } } 	0	['4', '1', '0', '2', '10', '0', '0', '2', '4', '0', '83', '0', '0', '0', '0.4', '0', '0', '19.5', '2', '0.5', '0']
package org . apache . lucene . store ; import java . io . IOException ; import org . apache . lucene . util . UnicodeUtil ; public abstract class IndexOutput { private UnicodeUtil . UTF8Result utf8Result = new UnicodeUtil . UTF8Result ( ) ; public abstract void writeByte ( byte b ) throws IOException ; public void writeBytes ( byte [ ] b , int length ) throws IOException { writeBytes ( b , 0 , length ) ; } public abstract void writeBytes ( byte [ ] b , int offset , int length ) throws IOException ; public void writeInt ( int i ) throws IOException { writeByte ( ( byte ) ( i > > 24 ) ) ; writeByte ( ( byte ) ( i > > 16 ) ) ; writeByte ( ( byte ) ( i > > 8 ) ) ; writeByte ( ( byte ) i ) ; } public void writeVInt ( int i ) throws IOException { while ( ( i & ~ 0x7F ) != 0 ) { writeByte ( ( byte ) ( ( i & 0x7f ) | 0x80 ) ) ; i >>>= 7 ; } writeByte ( ( byte ) i ) ; } public void writeLong ( long i ) throws IOException { writeInt ( ( int ) ( i > > 32 ) ) ; writeInt ( ( int ) i ) ; } public void writeVLong ( long i ) throws IOException { while ( ( i & ~ 0x7F ) != 0 ) { writeByte ( ( byte ) ( ( i & 0x7f ) | 0x80 ) ) ; i >>>= 7 ; } writeByte ( ( byte ) i ) ; } public void writeString ( String s ) throws IOException { UnicodeUtil . UTF16toUTF8 ( s , 0 , s . length ( ) , utf8Result ) ; writeVInt ( utf8Result . length ) ; writeBytes ( utf8Result . result , 0 , utf8Result . length ) ; } public void writeChars ( String s , int start , int length ) throws IOException { final int end = start + length ; for ( int i = start ; i < end ; i ++ ) { final int code = ( int ) s . charAt ( i ) ; if ( code >= 0x01 && code <= 0x7F ) writeByte ( ( byte ) code ) ; else if ( ( ( code >= 0x80 ) && ( code <= 0x7FF ) ) || code == 0 ) { writeByte ( ( byte ) ( 0xC0 | ( code > > 6 ) ) ) ; writeByte ( ( byte ) ( 0x80 | ( code & 0x3F ) ) ) ; } else { writeByte ( ( byte ) ( 0xE0 | ( code > > > 12 ) ) ) ; writeByte ( ( byte ) ( 0x80 | ( ( code > > 6 ) & 0x3F ) ) ) ; writeByte ( ( byte ) ( 0x80 | ( code & 0x3F ) ) ) ; } } } public void writeChars ( char [ ] s , int start , int length ) throws IOException { final int end = start + length ; for ( int i = start ; i < end ; i ++ ) { final int code = ( int ) s [ i ] ; if ( code >= 0x01 && code <= 0x7F ) writeByte ( ( byte ) code ) ; else if ( ( ( code >= 0x80 ) && ( code <= 0x7FF ) ) || code == 0 ) { writeByte ( ( byte ) ( 0xC0 | ( code > > 6 ) ) ) ; writeByte ( ( byte ) ( 0x80 | ( code & 0x3F ) ) ) ; } else { writeByte ( ( byte ) ( 0xE0 | ( code > > > 12 ) ) ) ; writeByte ( ( byte ) ( 0x80 | ( ( code > > 6 ) & 0x3F ) ) ) ; writeByte ( ( byte ) ( 0x80 | ( code & 0x3F ) ) ) ; } } } private static int COPY_BUFFER_SIZE = 16384 ; private byte [ ] copyBuffer ; public void copyBytes ( IndexInput input , long numBytes ) throws IOException { long left = numBytes ; if ( copyBuffer == null ) copyBuffer = new byte [ COPY_BUFFER_SIZE ] ; while ( left > 0 ) { final int toCopy ; if ( left > COPY_BUFFER_SIZE ) toCopy = COPY_BUFFER_SIZE ; else toCopy = ( int ) left ; input . readBytes ( copyBuffer , 0 , toCopy ) ; writeBytes ( copyBuffer , 0 , toCopy ) ; left -= toCopy ; } } public abstract void flush ( ) throws IOException ; public abstract void close ( ) throws IOException ; public abstract long getFilePointer ( ) ; public abstract void seek ( long pos ) throws IOException ; public abstract long length ( ) throws IOException ; public void setLength ( long length ) throws IOException { } ; } 	1	['19', '1', '3', '29', '25', '167', '26', '3', '18', '0.962962963', '343', '1', '1', '0', '0.25', '0', '0', '16.89473684', '1', '0.8947', '2']
package org . apache . lucene . analysis ; import java . io . * ; class PorterStemmer { private char [ ] b ; private int i , j , k , k0 ; private boolean dirty = false ; private static final int INC = 50 ; private static final int EXTRA = 1 ; public PorterStemmer ( ) { b = new char [ INC ] ; i = 0 ; } public void reset ( ) { i = 0 ; dirty = false ; } public void add ( char ch ) { if ( b . length <= i + EXTRA ) { char [ ] new_b = new char [ b . length + INC ] ; System . arraycopy ( b , 0 , new_b , 0 , b . length ) ; b = new_b ; } b [ i ++ ] = ch ; } public String toString ( ) { return new String ( b , 0 , i ) ; } public int getResultLength ( ) { return i ; } public char [ ] getResultBuffer ( ) { return b ; } private final boolean cons ( int i ) { switch ( b [ i ] ) { case 'a' : case 'e' : case 'i' : case 'o' : case 'u' : return false ; case 'y' : return ( i == k0 ) ? true : ! cons ( i - 1 ) ; default : return true ; } } private final int m ( ) { int n = 0 ; int i = k0 ; while ( true ) { if ( i > j ) return n ; if ( ! cons ( i ) ) break ; i ++ ; } i ++ ; while ( true ) { while ( true ) { if ( i > j ) return n ; if ( cons ( i ) ) break ; i ++ ; } i ++ ; n ++ ; while ( true ) { if ( i > j ) return n ; if ( ! cons ( i ) ) break ; i ++ ; } i ++ ; } } private final boolean vowelinstem ( ) { int i ; for ( i = k0 ; i <= j ; i ++ ) if ( ! cons ( i ) ) return true ; return false ; } private final boolean doublec ( int j ) { if ( j < k0 + 1 ) return false ; if ( b [ j ] != b [ j - 1 ] ) return false ; return cons ( j ) ; } private final boolean cvc ( int i ) { if ( i < k0 + 2 || ! cons ( i ) || cons ( i - 1 ) || ! cons ( i - 2 ) ) return false ; else { int ch = b [ i ] ; if ( ch == 'w' || ch == 'x' || ch == 'y' ) return false ; } return true ; } private final boolean ends ( String s ) { int l = s . length ( ) ; int o = k - l + 1 ; if ( o < k0 ) return false ; for ( int i = 0 ; i < l ; i ++ ) if ( b [ o + i ] != s . charAt ( i ) ) return false ; j = k - l ; return true ; } void setto ( String s ) { int l = s . length ( ) ; int o = j + 1 ; for ( int i = 0 ; i < l ; i ++ ) b [ o + i ] = s . charAt ( i ) ; k = j + l ; dirty = true ; } void r ( String s ) { if ( m ( ) > 0 ) setto ( s ) ; } private final void step1 ( ) { if ( b [ k ] == 's' ) { if ( ends ( "sses" ) ) k -= 2 ; else if ( ends ( "ies" ) ) setto ( "i" ) ; else if ( b [ k - 1 ] != 's' ) k -- ; } if ( ends ( "eed" ) ) { if ( m ( ) > 0 ) k -- ; } else if ( ( ends ( "ed" ) || ends ( "ing" ) ) && vowelinstem ( ) ) { k = j ; if ( ends ( "at" ) ) setto ( "ate" ) ; else if ( ends ( "bl" ) ) setto ( "ble" ) ; else if ( ends ( "iz" ) ) setto ( "ize" ) ; else if ( doublec ( k ) ) { int ch = b [ k -- ] ; if ( ch == 'l' || ch == 's' || ch == 'z' ) k ++ ; } else if ( m ( ) == 1 && cvc ( k ) ) setto ( "e" ) ; } } private final void step2 ( ) { if ( ends ( "y" ) && vowelinstem ( ) ) { b [ k ] = 'i' ; dirty = true ; } } private final void step3 ( ) { if ( k == k0 ) return ; switch ( b [ k - 1 ] ) { case 'a' : if ( ends ( "ational" ) ) { r ( "ate" ) ; break ; } if ( ends ( "tional" ) ) { r ( "tion" ) ; break ; } break ; case 'c' : if ( ends ( "enci" ) ) { r ( "ence" ) ; break ; } if ( ends ( "anci" ) ) { r ( "ance" ) ; break ; } break ; case 'e' : if ( ends ( "izer" ) ) { r ( "ize" ) ; break ; } break ; case 'l' : if ( ends ( "bli" ) ) { r ( "ble" ) ; break ; } if ( ends ( "alli" ) ) { r ( "al" ) ; break ; } if ( ends ( "entli" ) ) { r ( "ent" ) ; break ; } if ( ends ( "eli" ) ) { r ( "e" ) ; break ; } if ( ends ( "ousli" ) ) { r ( "ous" ) ; break ; } break ; case 'o' : if ( ends ( "ization" ) ) { r ( "ize" ) ; break ; } if ( ends ( "ation" ) ) { r ( "ate" ) ; break ; } if ( ends ( "ator" ) ) { r ( "ate" ) ; break ; } break ; case 's' : if ( ends ( "alism" ) ) { r ( "al" ) ; break ; } if ( ends ( "iveness" ) ) { r ( "ive" ) ; break ; } if ( ends ( "fulness" ) ) { r ( "ful" ) ; break ; } if ( ends ( "ousness" ) ) { r ( "ous" ) ; break ; } break ; case 't' : if ( ends ( "aliti" ) ) { r ( "al" ) ; break ; } if ( ends ( "iviti" ) ) { r ( "ive" ) ; break ; } if ( ends ( "biliti" ) ) { r ( "ble" ) ; break ; } break ; case 'g' : if ( ends ( "logi" ) ) { r ( "log" ) ; break ; } } } private final void step4 ( ) { switch ( b [ k ] ) { case 'e' : if ( ends ( "icate" ) ) { r ( "ic" ) ; break ; } if ( ends ( "ative" ) ) { r ( "" ) ; break ; } if ( ends ( "alize" ) ) { r ( "al" ) ; break ; } break ; case 'i' : if ( ends ( "iciti" ) ) { r ( "ic" ) ; break ; } break ; case 'l' : if ( ends ( "ical" ) ) { r ( "ic" ) ; break ; } if ( ends ( "ful" ) ) { r ( "" ) ; break ; } break ; case 's' : if ( ends ( "ness" ) ) { r ( "" ) ; break ; } break ; } } private final void step5 ( ) { if ( k == k0 ) return ; switch ( b [ k - 1 ] ) { case 'a' : if ( ends ( "al" ) ) break ; return ; case 'c' : if ( ends ( "ance" ) ) break ; if ( ends ( "ence" ) ) break ; return ; case 'e' : if ( ends ( "er" ) ) break ; return ; case 'i' : if ( ends ( "ic" ) ) break ; return ; case 'l' : if ( ends ( "able" ) ) break ; if ( ends ( "ible" ) ) break ; return ; case 'n' : if ( ends ( "ant" ) ) break ; if ( ends ( "ement" ) ) break ; if ( ends ( "ment" ) ) break ; if ( ends ( "ent" ) ) break ; return ; case 'o' : if ( ends ( "ion" ) && j >= 0 && ( b [ j ] == 's' || b [ j ] == 't' ) ) break ; if ( ends ( "ou" ) ) break ; return ; case 's' : if ( ends ( "ism" ) ) break ; return ; case 't' : if ( ends ( "ate" ) ) break ; if ( ends ( "iti" ) ) break ; return ; case 'u' : if ( ends ( "ous" ) ) break ; return ; case 'v' : if ( ends ( "ive" ) ) break ; return ; case 'z' : if ( ends ( "ize" ) ) break ; return ; default : return ; } if ( m ( ) > 1 ) k = j ; } private final void step6 ( ) { j = k ; if ( b [ k ] == 'e' ) { int a = m ( ) ; if ( a > 1 || a == 1 && ! cvc ( k - 1 ) ) k -- ; } if ( b [ k ] == 'l' && doublec ( k ) && m ( ) > 1 ) k -- ; } public String stem ( String s ) { if ( stem ( s . toCharArray ( ) , s . length ( ) ) ) return toString ( ) ; else return s ; } public boolean stem ( char [ ] word ) { return stem ( word , word . length ) ; } public boolean stem ( char [ ] wordBuffer , int offset , int wordLen ) { reset ( ) ; if ( b . length < wordLen ) { char [ ] new_b = new char [ wordLen + EXTRA ] ; b = new_b ; } System . arraycopy ( wordBuffer , offset , b , 0 , wordLen ) ; i = wordLen ; return stem ( 0 ) ; } public boolean stem ( char [ ] word , int wordLen ) { return stem ( word , 0 , wordLen ) ; } public boolean stem ( ) { return stem ( 0 ) ; } public boolean stem ( int i0 ) { k = i - 1 ; k0 = i0 ; if ( k > k0 + 1 ) { step1 ( ) ; step2 ( ) ; step3 ( ) ; step4 ( ) ; step5 ( ) ; step6 ( ) ; } if ( i != k + 1 ) dirty = true ; i = k + 1 ; return dirty ; } public static void main ( String [ ] args ) { PorterStemmer s = new PorterStemmer ( ) ; for ( int i = 0 ; i < args . length ; i ++ ) { try { InputStream in = new FileInputStream ( args [ i ] ) ; byte [ ] buffer = new byte [ 1024 ] ; int bufferLen , offset , ch ; bufferLen = in . read ( buffer ) ; offset = 0 ; s . reset ( ) ; while ( true ) { if ( offset < bufferLen ) ch = buffer [ offset ++ ] ; else { bufferLen = in . read ( buffer ) ; offset = 0 ; if ( bufferLen < 0 ) ch = - 1 ; else ch = buffer [ offset ++ ] ; } if ( Character . isLetter ( ( char ) ch ) ) { s . add ( Character . toLowerCase ( ( char ) ch ) ) ; } else { s . stem ( ) ; System . out . print ( s . toString ( ) ) ; s . reset ( ) ; if ( ch < 0 ) break ; else { System . out . print ( ( char ) ch ) ; } } } in . close ( ) ; } catch ( IOException e ) { System . out . println ( "error reading " + args [ i ] ) ; } } } } 	0	['27', '1', '0', '1', '44', '13', '1', '0', '13', '0.600961538', '1158', '1', '0', '0', '0.25308642', '0', '0', '41.59259259', '26', '5.6667', '0']
package org . apache . lucene . search . function ; import java . io . IOException ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . ComplexExplanation ; import org . apache . lucene . search . Explanation ; import org . apache . lucene . search . Query ; import org . apache . lucene . search . Scorer ; import org . apache . lucene . search . Searcher ; import org . apache . lucene . search . Similarity ; import org . apache . lucene . search . Weight ; import org . apache . lucene . util . ToStringUtils ; public class CustomScoreQuery extends Query { private Query subQuery ; private ValueSourceQuery [ ] valSrcQueries ; private boolean strict = false ; public CustomScoreQuery ( Query subQuery ) { this ( subQuery , new ValueSourceQuery [ 0 ] ) ; } public CustomScoreQuery ( Query subQuery , ValueSourceQuery valSrcQuery ) { this ( subQuery , valSrcQuery != null ? new ValueSourceQuery [ ] { valSrcQuery } : new ValueSourceQuery [ 0 ] ) ; } public CustomScoreQuery ( Query subQuery , ValueSourceQuery valSrcQueries [ ] ) { super ( ) ; this . subQuery = subQuery ; this . valSrcQueries = valSrcQueries != null ? valSrcQueries : new ValueSourceQuery [ 0 ] ; if ( subQuery == null ) throw new IllegalArgumentException ( "<subquery> must not be null!" ) ; } public Query rewrite ( IndexReader reader ) throws IOException { subQuery = subQuery . rewrite ( reader ) ; for ( int i = 0 ; i < valSrcQueries . length ; i ++ ) { valSrcQueries [ i ] = ( ValueSourceQuery ) valSrcQueries [ i ] . rewrite ( reader ) ; } return this ; } public void extractTerms ( Set terms ) { subQuery . extractTerms ( terms ) ; for ( int i = 0 ; i < valSrcQueries . length ; i ++ ) { valSrcQueries [ i ] . extractTerms ( terms ) ; } } public Object clone ( ) { CustomScoreQuery clone = ( CustomScoreQuery ) super . clone ( ) ; clone . subQuery = ( Query ) subQuery . clone ( ) ; clone . valSrcQueries = new ValueSourceQuery [ valSrcQueries . length ] ; for ( int i = 0 ; i < valSrcQueries . length ; i ++ ) { clone . valSrcQueries [ i ] = ( ValueSourceQuery ) valSrcQueries [ i ] . clone ( ) ; } return clone ; } public String toString ( String field ) { StringBuffer sb = new StringBuffer ( name ( ) ) . append ( "(" ) ; sb . append ( subQuery . toString ( field ) ) ; for ( int i = 0 ; i < valSrcQueries . length ; i ++ ) { sb . append ( ", " ) . append ( valSrcQueries [ i ] . toString ( field ) ) ; } sb . append ( ")" ) ; sb . append ( strict ? " STRICT" : "" ) ; return sb . toString ( ) + ToStringUtils . boost ( getBoost ( ) ) ; } public boolean equals ( Object o ) { if ( getClass ( ) != o . getClass ( ) ) { return false ; } CustomScoreQuery other = ( CustomScoreQuery ) o ; if ( this . getBoost ( ) != other . getBoost ( ) || ! this . subQuery . equals ( other . subQuery ) || this . valSrcQueries . length != other . valSrcQueries . length ) { return false ; } for ( int i = 0 ; i < valSrcQueries . length ; i ++ ) { if ( ! valSrcQueries [ i ] . equals ( other . valSrcQueries [ i ] ) ) { return false ; } } return true ; } public int hashCode ( ) { int valSrcHash = 0 ; for ( int i = 0 ; i < valSrcQueries . length ; i ++ ) { valSrcHash += valSrcQueries [ i ] . hashCode ( ) ; } return ( getClass ( ) . hashCode ( ) + subQuery . hashCode ( ) + valSrcHash ) ^ Float . floatToIntBits ( getBoost ( ) ) ; } public float customScore ( int doc , float subQueryScore , float valSrcScores [ ] ) { if ( valSrcScores . length == 1 ) { return customScore ( doc , subQueryScore , valSrcScores [ 0 ] ) ; } if ( valSrcScores . length == 0 ) { return customScore ( doc , subQueryScore , 1 ) ; } float score = subQueryScore ; for ( int i = 0 ; i < valSrcScores . length ; i ++ ) { score *= valSrcScores [ i ] ; } return score ; } public float customScore ( int doc , float subQueryScore , float valSrcScore ) { return subQueryScore * valSrcScore ; } public Explanation customExplain ( int doc , Explanation subQueryExpl , Explanation valSrcExpls [ ] ) { if ( valSrcExpls . length == 1 ) { return customExplain ( doc , subQueryExpl , valSrcExpls [ 0 ] ) ; } if ( valSrcExpls . length == 0 ) { return subQueryExpl ; } float valSrcScore = 1 ; for ( int i = 0 ; i < valSrcExpls . length ; i ++ ) { valSrcScore *= valSrcExpls [ i ] . getValue ( ) ; } Explanation exp = new Explanation ( valSrcScore * subQueryExpl . getValue ( ) , "custom score: product of:" ) ; exp . addDetail ( subQueryExpl ) ; for ( int i = 0 ; i < valSrcExpls . length ; i ++ ) { exp . addDetail ( valSrcExpls [ i ] ) ; } return exp ; } public Explanation customExplain ( int doc , Explanation subQueryExpl , Explanation valSrcExpl ) { float valSrcScore = 1 ; if ( valSrcExpl != null ) { valSrcScore *= valSrcExpl . getValue ( ) ; } Explanation exp = new Explanation ( valSrcScore * subQueryExpl . getValue ( ) , "custom score: product of:" ) ; exp . addDetail ( subQueryExpl ) ; exp . addDetail ( valSrcExpl ) ; return exp ; } private class CustomWeight implements Weight { Similarity similarity ; Weight subQueryWeight ; Weight [ ] valSrcWeights ; boolean qStrict ; public CustomWeight ( Searcher searcher ) throws IOException { this . similarity = getSimilarity ( searcher ) ; this . subQueryWeight = subQuery . weight ( searcher ) ; this . subQueryWeight = subQuery . weight ( searcher ) ; this . valSrcWeights = new Weight [ valSrcQueries . length ] ; for ( int i = 0 ; i < valSrcQueries . length ; i ++ ) { this . valSrcWeights [ i ] = valSrcQueries [ i ] . createWeight ( searcher ) ; } this . qStrict = strict ; } public Query getQuery ( ) { return CustomScoreQuery . this ; } public float getValue ( ) { return getBoost ( ) ; } public float sumOfSquaredWeights ( ) throws IOException { float sum = subQueryWeight . sumOfSquaredWeights ( ) ; for ( int i = 0 ; i < valSrcWeights . length ; i ++ ) { if ( qStrict ) { valSrcWeights [ i ] . sumOfSquaredWeights ( ) ; } else { sum += valSrcWeights [ i ] . sumOfSquaredWeights ( ) ; } } sum *= getBoost ( ) * getBoost ( ) ; return sum ; } public void normalize ( float norm ) { norm *= getBoost ( ) ; subQueryWeight . normalize ( norm ) ; for ( int i = 0 ; i < valSrcWeights . length ; i ++ ) { if ( qStrict ) { valSrcWeights [ i ] . normalize ( 1 ) ; } else { valSrcWeights [ i ] . normalize ( norm ) ; } } } public Scorer scorer ( IndexReader reader ) throws IOException { Scorer subQueryScorer = subQueryWeight . scorer ( reader ) ; Scorer [ ] valSrcScorers = new Scorer [ valSrcWeights . length ] ; for ( int i = 0 ; i < valSrcScorers . length ; i ++ ) { valSrcScorers [ i ] = valSrcWeights [ i ] . scorer ( reader ) ; } return new CustomScorer ( similarity , reader , this , subQueryScorer , valSrcScorers ) ; } public Explanation explain ( IndexReader reader , int doc ) throws IOException { return scorer ( reader ) . explain ( doc ) ; } } private class CustomScorer extends Scorer { private final CustomWeight weight ; private final float qWeight ; private Scorer subQueryScorer ; private Scorer [ ] valSrcScorers ; private IndexReader reader ; private float vScores [ ] ; private CustomScorer ( Similarity similarity , IndexReader reader , CustomWeight w , Scorer subQueryScorer , Scorer [ ] valSrcScorers ) throws IOException { super ( similarity ) ; this . weight = w ; this . qWeight = w . getValue ( ) ; this . subQueryScorer = subQueryScorer ; this . valSrcScorers = valSrcScorers ; this . reader = reader ; this . vScores = new float [ valSrcScorers . length ] ; } public boolean next ( ) throws IOException { boolean hasNext = subQueryScorer . next ( ) ; if ( hasNext ) { for ( int i = 0 ; i < valSrcScorers . length ; i ++ ) { valSrcScorers [ i ] . skipTo ( subQueryScorer . doc ( ) ) ; } } return hasNext ; } public int doc ( ) { return subQueryScorer . doc ( ) ; } public float score ( ) throws IOException { for ( int i = 0 ; i < valSrcScorers . length ; i ++ ) { vScores [ i ] = valSrcScorers [ i ] . score ( ) ; } return qWeight * customScore ( subQueryScorer . doc ( ) , subQueryScorer . score ( ) , vScores ) ; } public boolean skipTo ( int target ) throws IOException { boolean hasNext = subQueryScorer . skipTo ( target ) ; if ( hasNext ) { for ( int i = 0 ; i < valSrcScorers . length ; i ++ ) { valSrcScorers [ i ] . skipTo ( subQueryScorer . doc ( ) ) ; } } return hasNext ; } public Explanation explain ( int doc ) throws IOException { Explanation subQueryExpl = weight . subQueryWeight . explain ( reader , doc ) ; if ( ! subQueryExpl . isMatch ( ) ) { return subQueryExpl ; } Explanation [ ] valSrcExpls = new Explanation [ valSrcScorers . length ] ; for ( int i = 0 ; i < valSrcScorers . length ; i ++ ) { valSrcExpls [ i ] = valSrcScorers [ i ] . explain ( doc ) ; } Explanation customExp = customExplain ( doc , subQueryExpl , valSrcExpls ) ; float sc = qWeight * customExp . getValue ( ) ; Explanation res = new ComplexExplanation ( true , sc , CustomScoreQuery . this . toString ( ) + ", product of:" ) ; res . addDetail ( customExp ) ; res . addDetail ( new Explanation ( qWeight , "queryBoost" ) ) ; return res ; } } protected Weight createWeight ( Searcher searcher ) throws IOException { return new CustomWeight ( searcher ) ; } public boolean isStrict ( ) { return strict ; } public void setStrict ( boolean strict ) { this . strict = strict ; } public String name ( ) { return "custom" ; } } 	1	['20', '2', '0', '9', '46', '102', '2', '8', '16', '0.578947368', '454', '1', '2', '0.413793103', '0.128125', '2', '4', '21.55', '7', '1.8', '6']
package org . apache . lucene . index ; import java . io . IOException ; public interface TermPositions extends TermDocs { int nextPosition ( ) throws IOException ; int getPayloadLength ( ) ; byte [ ] getPayload ( byte [ ] data , int offset ) throws IOException ; public boolean isPayloadAvailable ( ) ; } 	0	['4', '1', '0', '25', '4', '6', '24', '1', '4', '2', '4', '0', '0', '0', '0.5', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . analysis ; import java . io . BufferedReader ; import java . io . File ; import java . io . FileReader ; import java . io . IOException ; import java . io . Reader ; import java . util . HashMap ; import java . util . HashSet ; public class WordlistLoader { public static HashSet getWordSet ( File wordfile ) throws IOException { HashSet result = new HashSet ( ) ; FileReader reader = null ; try { reader = new FileReader ( wordfile ) ; result = getWordSet ( reader ) ; } finally { if ( reader != null ) reader . close ( ) ; } return result ; } public static HashSet getWordSet ( Reader reader ) throws IOException { HashSet result = new HashSet ( ) ; BufferedReader br = null ; try { if ( reader instanceof BufferedReader ) { br = ( BufferedReader ) reader ; } else { br = new BufferedReader ( reader ) ; } String word = null ; while ( ( word = br . readLine ( ) ) != null ) { result . add ( word . trim ( ) ) ; } } finally { if ( br != null ) br . close ( ) ; } return result ; } public static HashMap getStemDict ( File wordstemfile ) throws IOException { if ( wordstemfile == null ) throw new NullPointerException ( "wordstemfile may not be null" ) ; HashMap result = new HashMap ( ) ; BufferedReader br = null ; FileReader fr = null ; try { fr = new FileReader ( wordstemfile ) ; br = new BufferedReader ( fr ) ; String line ; while ( ( line = br . readLine ( ) ) != null ) { String [ ] wordstem = line . split ( "\t" , 2 ) ; result . put ( wordstem [ 0 ] , wordstem [ 1 ] ) ; } } finally { if ( fr != null ) fr . close ( ) ; if ( br != null ) br . close ( ) ; } return result ; } } 	1	['4', '1', '0', '2', '17', '6', '2', '0', '4', '2', '147', '0', '0', '0', '0.333333333', '0', '0', '35.75', '1', '0.75', '1']
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . FieldCache ; import java . io . IOException ; public class ReverseOrdFieldSource extends ValueSource { public String field ; public ReverseOrdFieldSource ( String field ) { this . field = field ; } public String description ( ) { return "rord(" + field + ')' ; } public DocValues getValues ( IndexReader reader ) throws IOException { final FieldCache . StringIndex sindex = FieldCache . DEFAULT . getStringIndex ( reader , field ) ; final int arr [ ] = sindex . order ; final int end = sindex . lookup . length ; return new DocValues ( ) { public float floatVal ( int doc ) { return ( float ) ( end - arr [ doc ] ) ; } public int intVal ( int doc ) { return end - arr [ doc ] ; } public String strVal ( int doc ) { return Integer . toString ( intVal ( doc ) ) ; } public String toString ( int doc ) { return description ( ) + '=' + strVal ( doc ) ; } Object getInnerArray ( ) { return arr ; } } ; } public boolean equals ( Object o ) { if ( o . getClass ( ) != ReverseOrdFieldSource . class ) return false ; ReverseOrdFieldSource other = ( ReverseOrdFieldSource ) o ; return this . field . equals ( other . field ) ; } private static final int hcode = ReverseOrdFieldSource . class . hashCode ( ) ; public int hashCode ( ) { return hcode + field . hashCode ( ) ; } } 	0	['7', '2', '0', '6', '21', '0', '1', '6', '5', '0.666666667', '97', '0.333333333', '0', '0.5', '0.375', '2', '2', '12.42857143', '3', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; final class HitQueue extends PriorityQueue { HitQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object a , Object b ) { ScoreDoc hitA = ( ScoreDoc ) a ; ScoreDoc hitB = ( ScoreDoc ) b ; if ( hitA . score == hitB . score ) return hitA . doc > hitB . doc ; else return hitA . score < hitB . score ; } } 	1	['2', '2', '0', '6', '4', '1', '4', '2', '0', '2', '39', '0', '0', '0.923076923', '0.666666667', '1', '3', '18.5', '4', '2', '1']
package org . apache . lucene . analysis ; import java . io . IOException ; public final class PorterStemFilter extends TokenFilter { private PorterStemmer stemmer ; public PorterStemFilter ( TokenStream in ) { super ( in ) ; stemmer = new PorterStemmer ( ) ; } public final Token next ( final Token reusableToken ) throws IOException { assert reusableToken != null ; Token nextToken = input . next ( reusableToken ) ; if ( nextToken == null ) return null ; if ( stemmer . stem ( nextToken . termBuffer ( ) , 0 , nextToken . termLength ( ) ) ) nextToken . setTermBuffer ( stemmer . getResultBuffer ( ) , 0 , stemmer . getResultLength ( ) ) ; return nextToken ; } } 	0	['4', '3', '0', '4', '18', '2', '0', '4', '2', '0.777777778', '78', '0.333333333', '1', '0.777777778', '0.416666667', '1', '2', '17.75', '1', '0.5', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; import java . io . Reader ; public class KeywordTokenizer extends Tokenizer { private static final int DEFAULT_BUFFER_SIZE = 256 ; private boolean done ; public KeywordTokenizer ( Reader input ) { this ( input , DEFAULT_BUFFER_SIZE ) ; } public KeywordTokenizer ( Reader input , int bufferSize ) { super ( input ) ; this . done = false ; } public Token next ( final Token reusableToken ) throws IOException { assert reusableToken != null ; if ( ! done ) { done = true ; int upto = 0 ; reusableToken . clear ( ) ; char [ ] buffer = reusableToken . termBuffer ( ) ; while ( true ) { final int length = input . read ( buffer , upto , buffer . length - upto ) ; if ( length == - 1 ) break ; upto += length ; if ( upto == buffer . length ) buffer = reusableToken . resizeTermBuffer ( 1 + buffer . length ) ; } reusableToken . setTermLength ( upto ) ; return reusableToken ; } return null ; } public void reset ( Reader input ) throws IOException { super . reset ( input ) ; this . done = false ; } } 	1	['6', '3', '0', '3', '18', '7', '1', '2', '4', '0.85', '112', '0.5', '0', '0.7', '0.4', '1', '2', '17', '1', '0.5', '5']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import org . apache . lucene . store . IndexInput ; class DefaultSkipListReader extends MultiLevelSkipListReader { private boolean currentFieldStoresPayloads ; private long freqPointer [ ] ; private long proxPointer [ ] ; private int payloadLength [ ] ; private long lastFreqPointer ; private long lastProxPointer ; private int lastPayloadLength ; DefaultSkipListReader ( IndexInput skipStream , int maxSkipLevels , int skipInterval ) { super ( skipStream , maxSkipLevels , skipInterval ) ; freqPointer = new long [ maxSkipLevels ] ; proxPointer = new long [ maxSkipLevels ] ; payloadLength = new int [ maxSkipLevels ] ; } void init ( long skipPointer , long freqBasePointer , long proxBasePointer , int df , boolean storesPayloads ) { super . init ( skipPointer , df ) ; this . currentFieldStoresPayloads = storesPayloads ; lastFreqPointer = freqBasePointer ; lastProxPointer = proxBasePointer ; Arrays . fill ( freqPointer , freqBasePointer ) ; Arrays . fill ( proxPointer , proxBasePointer ) ; Arrays . fill ( payloadLength , 0 ) ; } long getFreqPointer ( ) { return lastFreqPointer ; } long getProxPointer ( ) { return lastProxPointer ; } int getPayloadLength ( ) { return lastPayloadLength ; } protected void seekChild ( int level ) throws IOException { super . seekChild ( level ) ; freqPointer [ level ] = lastFreqPointer ; proxPointer [ level ] = lastProxPointer ; payloadLength [ level ] = lastPayloadLength ; } protected void setLastSkipData ( int level ) { super . setLastSkipData ( level ) ; lastFreqPointer = freqPointer [ level ] ; lastProxPointer = proxPointer [ level ] ; lastPayloadLength = payloadLength [ level ] ; } protected int readSkipData ( int level , IndexInput skipStream ) throws IOException { int delta ; if ( currentFieldStoresPayloads ) { delta = skipStream . readVInt ( ) ; if ( ( delta & 1 ) != 0 ) { payloadLength [ level ] = skipStream . readVInt ( ) ; } delta >>>= 1 ; } else { delta = skipStream . readVInt ( ) ; } freqPointer [ level ] += skipStream . readVInt ( ) ; proxPointer [ level ] += skipStream . readVInt ( ) ; return delta ; } } 	0	['8', '2', '0', '3', '15', '0', '1', '2', '0', '0.571428571', '158', '1', '0', '0.5625', '0.425', '1', '3', '17.875', '1', '0.875', '0']
package org . apache . lucene . index ; public interface TermFreqVector { public String getField ( ) ; public int size ( ) ; public String [ ] getTerms ( ) ; public int [ ] getTermFrequencies ( ) ; public int indexOf ( String term ) ; public int [ ] indexesOf ( String [ ] terms , int start , int len ) ; } 	1	['6', '1', '0', '14', '6', '15', '14', '0', '6', '2', '6', '0', '0', '0', '0.375', '0', '0', '0', '1', '1', '1']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexInput ; final class SegmentTermEnum extends TermEnum implements Cloneable { private IndexInput input ; FieldInfos fieldInfos ; long size ; long position = - 1 ; private TermBuffer termBuffer = new TermBuffer ( ) ; private TermBuffer prevBuffer = new TermBuffer ( ) ; private TermBuffer scanBuffer = new TermBuffer ( ) ; private TermInfo termInfo = new TermInfo ( ) ; private int format ; private boolean isIndex = false ; long indexPointer = 0 ; int indexInterval ; int skipInterval ; int maxSkipLevels ; private int formatM1SkipInterval ; SegmentTermEnum ( IndexInput i , FieldInfos fis , boolean isi ) throws CorruptIndexException , IOException { input = i ; fieldInfos = fis ; isIndex = isi ; maxSkipLevels = 1 ; int firstInt = input . readInt ( ) ; if ( firstInt >= 0 ) { format = 0 ; size = firstInt ; indexInterval = 128 ; skipInterval = Integer . MAX_VALUE ; } else { format = firstInt ; if ( format < TermInfosWriter . FORMAT_CURRENT ) throw new CorruptIndexException ( "Unknown format version:" + format + " expected " + TermInfosWriter . FORMAT_CURRENT + " or higher" ) ; size = input . readLong ( ) ; if ( format == - 1 ) { if ( ! isIndex ) { indexInterval = input . readInt ( ) ; formatM1SkipInterval = input . readInt ( ) ; } skipInterval = Integer . MAX_VALUE ; } else { indexInterval = input . readInt ( ) ; skipInterval = input . readInt ( ) ; if ( format <= TermInfosWriter . FORMAT ) { maxSkipLevels = input . readInt ( ) ; } } } if ( format > TermInfosWriter . FORMAT_VERSION_UTF8_LENGTH_IN_BYTES ) { termBuffer . setPreUTF8Strings ( ) ; scanBuffer . setPreUTF8Strings ( ) ; prevBuffer . setPreUTF8Strings ( ) ; } } protected Object clone ( ) { SegmentTermEnum clone = null ; try { clone = ( SegmentTermEnum ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . input = ( IndexInput ) input . clone ( ) ; clone . termInfo = new TermInfo ( termInfo ) ; clone . termBuffer = ( TermBuffer ) termBuffer . clone ( ) ; clone . prevBuffer = ( TermBuffer ) prevBuffer . clone ( ) ; clone . scanBuffer = new TermBuffer ( ) ; return clone ; } final void seek ( long pointer , int p , Term t , TermInfo ti ) throws IOException { input . seek ( pointer ) ; position = p ; termBuffer . set ( t ) ; prevBuffer . reset ( ) ; termInfo . set ( ti ) ; } public final boolean next ( ) throws IOException { if ( position ++ >= size - 1 ) { prevBuffer . set ( termBuffer ) ; termBuffer . reset ( ) ; return false ; } prevBuffer . set ( termBuffer ) ; termBuffer . read ( input , fieldInfos ) ; termInfo . docFreq = input . readVInt ( ) ; termInfo . freqPointer += input . readVLong ( ) ; termInfo . proxPointer += input . readVLong ( ) ; if ( format == - 1 ) { if ( ! isIndex ) { if ( termInfo . docFreq > formatM1SkipInterval ) { termInfo . skipOffset = input . readVInt ( ) ; } } } else { if ( termInfo . docFreq >= skipInterval ) termInfo . skipOffset = input . readVInt ( ) ; } if ( isIndex ) indexPointer += input . readVLong ( ) ; return true ; } final int scanTo ( Term term ) throws IOException { scanBuffer . set ( term ) ; int count = 0 ; while ( scanBuffer . compareTo ( termBuffer ) > 0 && next ( ) ) { count ++ ; } return count ; } public final Term term ( ) { return termBuffer . toTerm ( ) ; } final Term prev ( ) { return prevBuffer . toTerm ( ) ; } final TermInfo termInfo ( ) { return new TermInfo ( termInfo ) ; } final void termInfo ( TermInfo ti ) { ti . set ( termInfo ) ; } public final int docFreq ( ) { return termInfo . docFreq ; } final long freqPointer ( ) { return termInfo . freqPointer ; } final long proxPointer ( ) { return termInfo . proxPointer ; } public final void close ( ) throws IOException { input . close ( ) ; } } 	0	['13', '2', '0', '11', '39', '0', '4', '7', '4', '0.761111111', '394', '0.533333333', '6', '0.294117647', '0.211538462', '1', '2', '28.15384615', '1', '0.9231', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . ChecksumIndexOutput ; import org . apache . lucene . store . ChecksumIndexInput ; import java . io . File ; import java . io . FileNotFoundException ; import java . io . IOException ; import java . io . PrintStream ; import java . util . Vector ; final class SegmentInfos extends Vector { public static final int FORMAT = - 1 ; public static final int FORMAT_LOCKLESS = - 2 ; public static final int FORMAT_SINGLE_NORM_FILE = - 3 ; public static final int FORMAT_SHARED_DOC_STORE = - 4 ; public static final int FORMAT_CHECKSUM = - 5 ; public static final int FORMAT_DEL_COUNT = - 6 ; public static final int FORMAT_HAS_PROX = - 7 ; static final int CURRENT_FORMAT = FORMAT_HAS_PROX ; public int counter = 0 ; private long version = System . currentTimeMillis ( ) ; private long generation = 0 ; private long lastGeneration = 0 ; private static PrintStream infoStream ; public final SegmentInfo info ( int i ) { return ( SegmentInfo ) get ( i ) ; } public static long getCurrentSegmentGeneration ( String [ ] files ) { if ( files == null ) { return - 1 ; } long max = - 1 ; for ( int i = 0 ; i < files . length ; i ++ ) { String file = files [ i ] ; if ( file . startsWith ( IndexFileNames . SEGMENTS ) && ! file . equals ( IndexFileNames . SEGMENTS_GEN ) ) { long gen = generationFromSegmentsFileName ( file ) ; if ( gen > max ) { max = gen ; } } } return max ; } public static long getCurrentSegmentGeneration ( Directory directory ) throws IOException { String [ ] files = directory . list ( ) ; if ( files == null ) throw new IOException ( "cannot read directory " + directory + ": list() returned null" ) ; return getCurrentSegmentGeneration ( files ) ; } public static String getCurrentSegmentFileName ( String [ ] files ) throws IOException { return IndexFileNames . fileNameFromGeneration ( IndexFileNames . SEGMENTS , "" , getCurrentSegmentGeneration ( files ) ) ; } public static String getCurrentSegmentFileName ( Directory directory ) throws IOException { return IndexFileNames . fileNameFromGeneration ( IndexFileNames . SEGMENTS , "" , getCurrentSegmentGeneration ( directory ) ) ; } public String getCurrentSegmentFileName ( ) { return IndexFileNames . fileNameFromGeneration ( IndexFileNames . SEGMENTS , "" , lastGeneration ) ; } public static long generationFromSegmentsFileName ( String fileName ) { if ( fileName . equals ( IndexFileNames . SEGMENTS ) ) { return 0 ; } else if ( fileName . startsWith ( IndexFileNames . SEGMENTS ) ) { return Long . parseLong ( fileName . substring ( 1 + IndexFileNames . SEGMENTS . length ( ) ) , Character . MAX_RADIX ) ; } else { throw new IllegalArgumentException ( "fileName \"" + fileName + "\" is not a segments file" ) ; } } public String getNextSegmentFileName ( ) { long nextGeneration ; if ( generation == - 1 ) { nextGeneration = 1 ; } else { nextGeneration = generation + 1 ; } return IndexFileNames . fileNameFromGeneration ( IndexFileNames . SEGMENTS , "" , nextGeneration ) ; } public final void read ( Directory directory , String segmentFileName ) throws CorruptIndexException , IOException { boolean success = false ; clear ( ) ; ChecksumIndexInput input = new ChecksumIndexInput ( directory . openInput ( segmentFileName ) ) ; generation = generationFromSegmentsFileName ( segmentFileName ) ; lastGeneration = generation ; try { int format = input . readInt ( ) ; if ( format < 0 ) { if ( format < CURRENT_FORMAT ) throw new CorruptIndexException ( "Unknown format version: " + format ) ; version = input . readLong ( ) ; counter = input . readInt ( ) ; } else { counter = format ; } for ( int i = input . readInt ( ) ; i > 0 ; i -- ) { add ( new SegmentInfo ( directory , format , input ) ) ; } if ( format >= 0 ) { if ( input . getFilePointer ( ) >= input . length ( ) ) version = System . currentTimeMillis ( ) ; else version = input . readLong ( ) ; } if ( format <= FORMAT_CHECKSUM ) { final long checksumNow = input . getChecksum ( ) ; final long checksumThen = input . readLong ( ) ; if ( checksumNow != checksumThen ) throw new CorruptIndexException ( "checksum mismatch in segments file" ) ; } success = true ; } finally { input . close ( ) ; if ( ! success ) { clear ( ) ; } } } public final void read ( Directory directory ) throws CorruptIndexException , IOException { generation = lastGeneration = - 1 ; new FindSegmentsFile ( directory ) { protected Object doBody ( String segmentFileName ) throws CorruptIndexException , IOException { read ( directory , segmentFileName ) ; return null ; } } . run ( ) ; } ChecksumIndexOutput pendingOutput ; private final void write ( Directory directory ) throws IOException { String segmentFileName = getNextSegmentFileName ( ) ; if ( generation == - 1 ) { generation = 1 ; } else { generation ++ ; } ChecksumIndexOutput output = new ChecksumIndexOutput ( directory . createOutput ( segmentFileName ) ) ; boolean success = false ; try { output . writeInt ( CURRENT_FORMAT ) ; output . writeLong ( ++ version ) ; output . writeInt ( counter ) ; output . writeInt ( size ( ) ) ; for ( int i = 0 ; i < size ( ) ; i ++ ) { info ( i ) . write ( output ) ; } output . prepareCommit ( ) ; success = true ; pendingOutput = output ; } finally { if ( ! success ) { try { output . close ( ) ; } catch ( Throwable t ) { } try { directory . deleteFile ( segmentFileName ) ; } catch ( Throwable t ) { } } } } public Object clone ( ) { SegmentInfos sis = ( SegmentInfos ) super . clone ( ) ; for ( int i = 0 ; i < sis . size ( ) ; i ++ ) { sis . set ( i , sis . info ( i ) . clone ( ) ) ; } return sis ; } public long getVersion ( ) { return version ; } public long getGeneration ( ) { return generation ; } public long getLastGeneration ( ) { return lastGeneration ; } public static long readCurrentVersion ( Directory directory ) throws CorruptIndexException , IOException { return ( ( Long ) new FindSegmentsFile ( directory ) { protected Object doBody ( String segmentFileName ) throws CorruptIndexException , IOException { IndexInput input = directory . openInput ( segmentFileName ) ; int format = 0 ; long version = 0 ; try { format = input . readInt ( ) ; if ( format < 0 ) { if ( format < CURRENT_FORMAT ) throw new CorruptIndexException ( "Unknown format version: " + format ) ; version = input . readLong ( ) ; } } finally { input . close ( ) ; } if ( format < 0 ) return new Long ( version ) ; SegmentInfos sis = new SegmentInfos ( ) ; sis . read ( directory , segmentFileName ) ; return new Long ( sis . getVersion ( ) ) ; } } . run ( ) ) . longValue ( ) ; } public static void setInfoStream ( PrintStream infoStream ) { SegmentInfos . infoStream = infoStream ; } private static int defaultGenFileRetryCount = 10 ; private static int defaultGenFileRetryPauseMsec = 50 ; private static int defaultGenLookaheadCount = 10 ; public static void setDefaultGenFileRetryCount ( int count ) { defaultGenFileRetryCount = count ; } public static int getDefaultGenFileRetryCount ( ) { return defaultGenFileRetryCount ; } public static void setDefaultGenFileRetryPauseMsec ( int msec ) { defaultGenFileRetryPauseMsec = msec ; } public static int getDefaultGenFileRetryPauseMsec ( ) { return defaultGenFileRetryPauseMsec ; } public static void setDefaultGenLookaheadCount ( int count ) { defaultGenLookaheadCount = count ; } public static int getDefaultGenLookahedCount ( ) { return defaultGenLookaheadCount ; } public static PrintStream getInfoStream ( ) { return infoStream ; } private static void message ( String message ) { if ( infoStream != null ) { infoStream . println ( "SIS [" + Thread . currentThread ( ) . getName ( ) + "]: " + message ) ; } } public abstract static class FindSegmentsFile { File fileDirectory ; Directory directory ; public FindSegmentsFile ( File directory ) { this . fileDirectory = directory ; } public FindSegmentsFile ( Directory directory ) { this . directory = directory ; } public Object run ( ) throws CorruptIndexException , IOException { String segmentFileName = null ; long lastGen = - 1 ; long gen = 0 ; int genLookaheadCount = 0 ; IOException exc = null ; boolean retry = false ; int method = 0 ; while ( true ) { if ( 0 == method ) { String [ ] files = null ; long genA = - 1 ; if ( directory != null ) files = directory . list ( ) ; else files = fileDirectory . list ( ) ; if ( files != null ) genA = getCurrentSegmentGeneration ( files ) ; message ( "directory listing genA=" + genA ) ; long genB = - 1 ; if ( directory != null ) { for ( int i = 0 ; i < defaultGenFileRetryCount ; i ++ ) { IndexInput genInput = null ; try { genInput = directory . openInput ( IndexFileNames . SEGMENTS_GEN ) ; } catch ( FileNotFoundException e ) { message ( "segments.gen open: FileNotFoundException " + e ) ; break ; } catch ( IOException e ) { message ( "segments.gen open: IOException " + e ) ; } if ( genInput != null ) { try { int version = genInput . readInt ( ) ; if ( version == FORMAT_LOCKLESS ) { long gen0 = genInput . readLong ( ) ; long gen1 = genInput . readLong ( ) ; message ( "fallback check: " + gen0 + "; " + gen1 ) ; if ( gen0 == gen1 ) { genB = gen0 ; break ; } } } catch ( IOException err2 ) { } finally { genInput . close ( ) ; } } try { Thread . sleep ( defaultGenFileRetryPauseMsec ) ; } catch ( InterruptedException e ) { } } } message ( IndexFileNames . SEGMENTS_GEN + " check: genB=" + genB ) ; if ( genA > genB ) gen = genA ; else gen = genB ; if ( gen == - 1 ) { String s ; if ( files != null ) { s = "" ; for ( int i = 0 ; i < files . length ; i ++ ) s += " " + files [ i ] ; } else s = " null" ; throw new FileNotFoundException ( "no segments* file found in " + directory + ": files:" + s ) ; } } if ( 1 == method || ( 0 == method && lastGen == gen && retry ) ) { method = 1 ; if ( genLookaheadCount < defaultGenLookaheadCount ) { gen ++ ; genLookaheadCount ++ ; message ( "look ahead increment gen to " + gen ) ; } } if ( lastGen == gen ) { if ( retry ) { throw exc ; } else { retry = true ; } } else if ( 0 == method ) { retry = false ; } lastGen = gen ; segmentFileName = IndexFileNames . fileNameFromGeneration ( IndexFileNames . SEGMENTS , "" , gen ) ; try { Object v = doBody ( segmentFileName ) ; if ( exc != null ) { message ( "success on " + segmentFileName ) ; } return v ; } catch ( IOException err ) { if ( exc == null ) { exc = err ; } message ( "primary Exception on '" + segmentFileName + "': " + err + "'; will retry: retry=" + retry + "; gen = " + gen ) ; if ( ! retry && gen > 1 ) { String prevSegmentFileName = IndexFileNames . fileNameFromGeneration ( IndexFileNames . SEGMENTS , "" , gen - 1 ) ; final boolean prevExists ; if ( directory != null ) prevExists = directory . fileExists ( prevSegmentFileName ) ; else prevExists = new File ( fileDirectory , prevSegmentFileName ) . exists ( ) ; if ( prevExists ) { message ( "fallback to prior segment file '" + prevSegmentFileName + "'" ) ; try { Object v = doBody ( prevSegmentFileName ) ; if ( exc != null ) { message ( "success on fallback " + prevSegmentFileName ) ; } return v ; } catch ( IOException err2 ) { message ( "secondary Exception on '" + prevSegmentFileName + "': " + err2 + "'; will retry" ) ; } } } } } } protected abstract Object doBody ( String segmentFileName ) throws CorruptIndexException , IOException ; } public SegmentInfos range ( int first , int last ) { SegmentInfos infos = new SegmentInfos ( ) ; infos . addAll ( super . subList ( first , last ) ) ; return infos ; } void updateGeneration ( SegmentInfos other ) { lastGeneration = other . lastGeneration ; generation = other . generation ; version = other . version ; } public final void rollbackCommit ( Directory dir ) throws IOException { if ( pendingOutput != null ) { try { pendingOutput . close ( ) ; } catch ( Throwable t ) { } try { final String segmentFileName = IndexFileNames . fileNameFromGeneration ( IndexFileNames . SEGMENTS , "" , generation ) ; dir . deleteFile ( segmentFileName ) ; } catch ( Throwable t ) { } pendingOutput = null ; } } public final void prepareCommit ( Directory dir ) throws IOException { if ( pendingOutput != null ) throw new IllegalStateException ( "prepareCommit was already called" ) ; write ( dir ) ; } public final void finishCommit ( Directory dir ) throws IOException { if ( pendingOutput == null ) throw new IllegalStateException ( "prepareCommit was not called" ) ; boolean success = false ; try { pendingOutput . finishCommit ( ) ; pendingOutput . close ( ) ; pendingOutput = null ; success = true ; } finally { if ( ! success ) rollbackCommit ( dir ) ; } final String fileName = IndexFileNames . fileNameFromGeneration ( IndexFileNames . SEGMENTS , "" , generation ) ; success = false ; try { dir . sync ( fileName ) ; success = true ; } finally { if ( ! success ) { try { dir . deleteFile ( fileName ) ; } catch ( Throwable t ) { } } } lastGeneration = generation ; try { IndexOutput genOutput = dir . createOutput ( IndexFileNames . SEGMENTS_GEN ) ; try { genOutput . writeInt ( FORMAT_LOCKLESS ) ; genOutput . writeLong ( generation ) ; genOutput . writeLong ( generation ) ; } finally { genOutput . close ( ) ; } } catch ( Throwable t ) { } } public final void commit ( Directory dir ) throws IOException { prepareCommit ( dir ) ; finishCommit ( dir ) ; } synchronized String segString ( Directory directory ) { StringBuffer buffer = new StringBuffer ( ) ; final int count = size ( ) ; for ( int i = 0 ; i < count ; i ++ ) { if ( i > 0 ) { buffer . append ( ' ' ) ; } final SegmentInfo info = info ( i ) ; buffer . append ( info . segString ( directory ) ) ; if ( info . dir != directory ) buffer . append ( "**" ) ; } return buffer . toString ( ) ; } } 	1	['38', '4', '0', '30', '97', '553', '22', '10', '28', '0.93799682', '729', '0.411764706', '1', '0.686956522', '0.162162162', '1', '2', '17.73684211', '6', '1.2895', '11']
package org . apache . lucene . index ; import java . util . Map ; import java . io . IOException ; abstract class InvertedDocEndConsumer { abstract InvertedDocEndConsumerPerThread addThread ( DocInverterPerThread docInverterPerThread ) ; abstract void flush ( Map threadsAndFields , DocumentsWriter . FlushState state ) throws IOException ; abstract void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException ; abstract void abort ( ) ; abstract void setFieldInfos ( FieldInfos fieldInfos ) ; } 	0	['6', '1', '1', '7', '7', '15', '4', '4', '0', '2', '9', '0', '0', '0', '0.366666667', '0', '0', '0.5', '1', '0.8333', '0']
package org . apache . lucene . document ; import org . apache . lucene . analysis . TokenStream ; import org . apache . lucene . index . IndexWriter ; import org . apache . lucene . util . Parameter ; import java . io . Reader ; import java . io . Serializable ; public final class Field extends AbstractField implements Fieldable , Serializable { public static final class Store extends Parameter implements Serializable { private Store ( String name ) { super ( name ) ; } public static final Store COMPRESS = new Store ( "COMPRESS" ) ; public static final Store YES = new Store ( "YES" ) ; public static final Store NO = new Store ( "NO" ) ; } public static final class Index extends Parameter implements Serializable { private Index ( String name ) { super ( name ) ; } public static final Index NO = new Index ( "NO" ) ; public static final Index ANALYZED = new Index ( "ANALYZED" ) ; public static final Index TOKENIZED = ANALYZED ; public static final Index NOT_ANALYZED = new Index ( "NOT_ANALYZED" ) ; public static final Index UN_TOKENIZED = NOT_ANALYZED ; public static final Index NOT_ANALYZED_NO_NORMS = new Index ( "NOT_ANALYZED_NO_NORMS" ) ; public static final Index NO_NORMS = NOT_ANALYZED_NO_NORMS ; public static final Index ANALYZED_NO_NORMS = new Index ( "ANALYZED_NO_NORMS" ) ; } public static final class TermVector extends Parameter implements Serializable { private TermVector ( String name ) { super ( name ) ; } public static final TermVector NO = new TermVector ( "NO" ) ; public static final TermVector YES = new TermVector ( "YES" ) ; public static final TermVector WITH_POSITIONS = new TermVector ( "WITH_POSITIONS" ) ; public static final TermVector WITH_OFFSETS = new TermVector ( "WITH_OFFSETS" ) ; public static final TermVector WITH_POSITIONS_OFFSETS = new TermVector ( "WITH_POSITIONS_OFFSETS" ) ; } public String stringValue ( ) { return fieldsData instanceof String ? ( String ) fieldsData : null ; } public Reader readerValue ( ) { return fieldsData instanceof Reader ? ( Reader ) fieldsData : null ; } public byte [ ] binaryValue ( ) { if ( ! isBinary ) return null ; final byte [ ] data = ( byte [ ] ) fieldsData ; if ( binaryOffset == 0 && data . length == binaryLength ) return data ; final byte [ ] ret = new byte [ binaryLength ] ; System . arraycopy ( data , binaryOffset , ret , 0 , binaryLength ) ; return ret ; } public TokenStream tokenStreamValue ( ) { return fieldsData instanceof TokenStream ? ( TokenStream ) fieldsData : null ; } public void setValue ( String value ) { fieldsData = value ; } public void setValue ( Reader value ) { fieldsData = value ; } public void setValue ( byte [ ] value ) { fieldsData = value ; binaryLength = value . length ; binaryOffset = 0 ; } public void setValue ( byte [ ] value , int offset , int length ) { fieldsData = value ; binaryLength = length ; binaryOffset = offset ; } public void setValue ( TokenStream value ) { fieldsData = value ; } public Field ( String name , String value , Store store , Index index ) { this ( name , value , store , index , TermVector . NO ) ; } public Field ( String name , String value , Store store , Index index , TermVector termVector ) { if ( name == null ) throw new NullPointerException ( "name cannot be null" ) ; if ( value == null ) throw new NullPointerException ( "value cannot be null" ) ; if ( name . length ( ) == 0 && value . length ( ) == 0 ) throw new IllegalArgumentException ( "name and value cannot both be empty" ) ; if ( index == Index . NO && store == Store . NO ) throw new IllegalArgumentException ( "it doesn't make sense to have a field that " + "is neither indexed nor stored" ) ; if ( index == Index . NO && termVector != TermVector . NO ) throw new IllegalArgumentException ( "cannot store term vector information " + "for a field that is not indexed" ) ; this . name = name . intern ( ) ; this . fieldsData = value ; if ( store == Store . YES ) { this . isStored = true ; this . isCompressed = false ; } else if ( store == Store . COMPRESS ) { this . isStored = true ; this . isCompressed = true ; } else if ( store == Store . NO ) { this . isStored = false ; this . isCompressed = false ; } else throw new IllegalArgumentException ( "unknown store parameter " + store ) ; if ( index == Index . NO ) { this . isIndexed = false ; this . isTokenized = false ; } else if ( index == Index . ANALYZED ) { this . isIndexed = true ; this . isTokenized = true ; } else if ( index == Index . NOT_ANALYZED ) { this . isIndexed = true ; this . isTokenized = false ; } else if ( index == Index . NOT_ANALYZED_NO_NORMS ) { this . isIndexed = true ; this . isTokenized = false ; this . omitNorms = true ; } else if ( index == Index . ANALYZED_NO_NORMS ) { this . isIndexed = true ; this . isTokenized = true ; this . omitNorms = true ; } else { throw new IllegalArgumentException ( "unknown index parameter " + index ) ; } this . isBinary = false ; setStoreTermVector ( termVector ) ; } public Field ( String name , Reader reader ) { this ( name , reader , TermVector . NO ) ; } public Field ( String name , Reader reader , TermVector termVector ) { if ( name == null ) throw new NullPointerException ( "name cannot be null" ) ; if ( reader == null ) throw new NullPointerException ( "reader cannot be null" ) ; this . name = name . intern ( ) ; this . fieldsData = reader ; this . isStored = false ; this . isCompressed = false ; this . isIndexed = true ; this . isTokenized = true ; this . isBinary = false ; setStoreTermVector ( termVector ) ; } public Field ( String name , TokenStream tokenStream ) { this ( name , tokenStream , TermVector . NO ) ; } public Field ( String name , TokenStream tokenStream , TermVector termVector ) { if ( name == null ) throw new NullPointerException ( "name cannot be null" ) ; if ( tokenStream == null ) throw new NullPointerException ( "tokenStream cannot be null" ) ; this . name = name . intern ( ) ; this . fieldsData = tokenStream ; this . isStored = false ; this . isCompressed = false ; this . isIndexed = true ; this . isTokenized = true ; this . isBinary = false ; setStoreTermVector ( termVector ) ; } public Field ( String name , byte [ ] value , Store store ) { this ( name , value , 0 , value . length , store ) ; } public Field ( String name , byte [ ] value , int offset , int length , Store store ) { if ( name == null ) throw new IllegalArgumentException ( "name cannot be null" ) ; if ( value == null ) throw new IllegalArgumentException ( "value cannot be null" ) ; this . name = name . intern ( ) ; fieldsData = value ; if ( store == Store . YES ) { isStored = true ; isCompressed = false ; } else if ( store == Store . COMPRESS ) { isStored = true ; isCompressed = true ; } else if ( store == Store . NO ) throw new IllegalArgumentException ( "binary values can't be unstored" ) ; else throw new IllegalArgumentException ( "unknown store parameter " + store ) ; isIndexed = false ; isTokenized = false ; isBinary = true ; binaryLength = length ; binaryOffset = offset ; setStoreTermVector ( TermVector . NO ) ; } } 	1	['17', '2', '0', '8', '28', '0', '2', '6', '17', '2', '482', '0', '0', '0.709677419', '0.307189542', '1', '6', '27.35294118', '4', '0.8824', '5']
package org . apache . lucene . queryParser ; public interface QueryParserConstants { int EOF = 0 ; int _NUM_CHAR = 1 ; int _ESCAPED_CHAR = 2 ; int _TERM_START_CHAR = 3 ; int _TERM_CHAR = 4 ; int _WHITESPACE = 5 ; int _QUOTED_CHAR = 6 ; int AND = 8 ; int OR = 9 ; int NOT = 10 ; int PLUS = 11 ; int MINUS = 12 ; int LPAREN = 13 ; int RPAREN = 14 ; int COLON = 15 ; int STAR = 16 ; int CARAT = 17 ; int QUOTED = 18 ; int TERM = 19 ; int FUZZY_SLOP = 20 ; int PREFIXTERM = 21 ; int WILDTERM = 22 ; int RANGEIN_START = 23 ; int RANGEEX_START = 24 ; int NUMBER = 25 ; int RANGEIN_TO = 26 ; int RANGEIN_END = 27 ; int RANGEIN_QUOTED = 28 ; int RANGEIN_GOOP = 29 ; int RANGEEX_TO = 30 ; int RANGEEX_END = 31 ; int RANGEEX_QUOTED = 32 ; int RANGEEX_GOOP = 33 ; int Boost = 0 ; int RangeEx = 1 ; int RangeIn = 2 ; int DEFAULT = 3 ; String [ ] tokenImage = { "<EOF>" , "<_NUM_CHAR>" , "<_ESCAPED_CHAR>" , "<_TERM_START_CHAR>" , "<_TERM_CHAR>" , "<_WHITESPACE>" , "<_QUOTED_CHAR>" , "<token of kind 7>" , "<AND>" , "<OR>" , "<NOT>" , "\"+\"" , "\"-\"" , "\"(\"" , "\")\"" , "\":\"" , "\"*\"" , "\"^\"" , "<QUOTED>" , "<TERM>" , "<FUZZY_SLOP>" , "<PREFIXTERM>" , "<WILDTERM>" , "\"[\"" , "\"{\"" , "<NUMBER>" , "\"TO\"" , "\"]\"" , "<RANGEIN_QUOTED>" , "<RANGEIN_GOOP>" , "\"TO\"" , "\"}\"" , "<RANGEEX_QUOTED>" , "<RANGEEX_GOOP>" , } ; } 	0	['1', '1', '0', '2', '1', '0', '2', '0', '0', '2', '179', '0', '0', '0', '0', '0', '0', '140', '0', '0', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . RAMOutputStream ; abstract class MultiLevelSkipListWriter { private int numberOfSkipLevels ; private int skipInterval ; private RAMOutputStream [ ] skipBuffer ; protected MultiLevelSkipListWriter ( int skipInterval , int maxSkipLevels , int df ) { this . skipInterval = skipInterval ; numberOfSkipLevels = df == 0 ? 0 : ( int ) Math . floor ( Math . log ( df ) / Math . log ( skipInterval ) ) ; if ( numberOfSkipLevels > maxSkipLevels ) { numberOfSkipLevels = maxSkipLevels ; } } protected void init ( ) { skipBuffer = new RAMOutputStream [ numberOfSkipLevels ] ; for ( int i = 0 ; i < numberOfSkipLevels ; i ++ ) { skipBuffer [ i ] = new RAMOutputStream ( ) ; } } protected void resetSkip ( ) { if ( skipBuffer == null ) { init ( ) ; } else { for ( int i = 0 ; i < skipBuffer . length ; i ++ ) { skipBuffer [ i ] . reset ( ) ; } } } protected abstract void writeSkipData ( int level , IndexOutput skipBuffer ) throws IOException ; void bufferSkip ( int df ) throws IOException { int numLevels ; for ( numLevels = 0 ; ( df % skipInterval ) == 0 && numLevels < numberOfSkipLevels ; df /= skipInterval ) { numLevels ++ ; } long childPointer = 0 ; for ( int level = 0 ; level < numLevels ; level ++ ) { writeSkipData ( level , skipBuffer [ level ] ) ; long newChildPointer = skipBuffer [ level ] . getFilePointer ( ) ; if ( level != 0 ) { skipBuffer [ level ] . writeVLong ( childPointer ) ; } childPointer = newChildPointer ; } } long writeSkip ( IndexOutput output ) throws IOException { long skipPointer = output . getFilePointer ( ) ; if ( skipBuffer == null || skipBuffer . length == 0 ) return skipPointer ; for ( int level = numberOfSkipLevels - 1 ; level > 0 ; level -- ) { long length = skipBuffer [ level ] . getFilePointer ( ) ; if ( length > 0 ) { output . writeVLong ( length ) ; skipBuffer [ level ] . writeTo ( output ) ; } } skipBuffer [ 0 ] . writeTo ( output ) ; return skipPointer ; } } 	1	['6', '1', '1', '3', '16', '0', '1', '2', '0', '0.466666667', '178', '1', '1', '0', '0.611111111', '0', '0', '28.16666667', '3', '1.3333', '1']
package org . apache . lucene . index ; import java . util . Map ; import java . io . IOException ; abstract class InvertedDocConsumer { abstract InvertedDocConsumerPerThread addThread ( DocInverterPerThread docInverterPerThread ) ; abstract void abort ( ) ; abstract void flush ( Map threadsAndFields , DocumentsWriter . FlushState state ) throws IOException ; abstract void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException ; abstract boolean freeRAM ( ) ; FieldInfos fieldInfos ; void setFieldInfos ( FieldInfos fieldInfos ) { this . fieldInfos = fieldInfos ; } } 	0	['7', '1', '1', '7', '8', '21', '4', '4', '0', '1', '15', '0', '1', '0', '0.342857143', '0', '0', '1', '1', '0.8571', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . document . Fieldable ; final class StoredFieldsWriterPerField extends DocFieldConsumerPerField { final StoredFieldsWriterPerThread perThread ; final FieldInfo fieldInfo ; final DocumentsWriter . DocState docState ; public StoredFieldsWriterPerField ( StoredFieldsWriterPerThread perThread , FieldInfo fieldInfo ) { this . perThread = perThread ; this . fieldInfo = fieldInfo ; docState = perThread . docState ; } public void processFields ( Fieldable [ ] fields , int count ) throws IOException { final StoredFieldsWriter . PerDoc doc ; if ( perThread . doc == null ) { doc = perThread . doc = perThread . storedFieldsWriter . getPerDoc ( ) ; doc . docID = docState . docID ; perThread . localFieldsWriter . setFieldsStream ( doc . fdt ) ; assert doc . numStoredFields == 0 : "doc.numStoredFields=" + doc . numStoredFields ; assert 0 == doc . fdt . length ( ) ; assert 0 == doc . fdt . getFilePointer ( ) ; } else { doc = perThread . doc ; assert doc . docID == docState . docID : "doc.docID=" + doc . docID + " docState.docID=" + docState . docID ; } for ( int i = 0 ; i < count ; i ++ ) { final Fieldable field = fields [ i ] ; if ( field . isStored ( ) ) { perThread . localFieldsWriter . writeField ( fieldInfo , field ) ; assert docState . testPoint ( "StoredFieldsWriterPerField.processFields.writeField" ) ; doc . numStoredFields ++ ; } } } void abort ( ) { } } 	1	['5', '2', '0', '10', '23', '6', '1', '10', '2', '0.8', '185', '0', '3', '0.4', '0.333333333', '0', '0', '35', '1', '0.6', '1']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; public abstract class SpanFilter extends Filter { public abstract SpanFilterResult bitSpans ( IndexReader reader ) throws IOException ; } 	0	['2', '2', '2', '5', '3', '1', '2', '3', '2', '2', '5', '0', '0', '0.666666667', '0.75', '0', '0', '1.5', '1', '0.5', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . Explanation ; import org . apache . lucene . search . Query ; import org . apache . lucene . search . Scorer ; import org . apache . lucene . search . Searcher ; import org . apache . lucene . search . Similarity ; import org . apache . lucene . search . Weight ; import org . apache . lucene . util . ToStringUtils ; import java . util . Set ; public class MatchAllDocsQuery extends Query { public MatchAllDocsQuery ( ) { } private class MatchAllScorer extends Scorer { final IndexReader reader ; int id ; final int maxId ; final float score ; MatchAllScorer ( IndexReader reader , Similarity similarity , Weight w ) { super ( similarity ) ; this . reader = reader ; id = - 1 ; maxId = reader . maxDoc ( ) - 1 ; score = w . getValue ( ) ; } public Explanation explain ( int doc ) { return null ; } public int doc ( ) { return id ; } public boolean next ( ) { while ( id < maxId ) { id ++ ; if ( ! reader . isDeleted ( id ) ) { return true ; } } return false ; } public float score ( ) { return score ; } public boolean skipTo ( int target ) { id = target - 1 ; return next ( ) ; } } private class MatchAllDocsWeight implements Weight { private Similarity similarity ; private float queryWeight ; private float queryNorm ; public MatchAllDocsWeight ( Searcher searcher ) { this . similarity = searcher . getSimilarity ( ) ; } public String toString ( ) { return "weight(" + MatchAllDocsQuery . this + ")" ; } public Query getQuery ( ) { return MatchAllDocsQuery . this ; } public float getValue ( ) { return queryWeight ; } public float sumOfSquaredWeights ( ) { queryWeight = getBoost ( ) ; return queryWeight * queryWeight ; } public void normalize ( float queryNorm ) { this . queryNorm = queryNorm ; queryWeight *= this . queryNorm ; } public Scorer scorer ( IndexReader reader ) { return new MatchAllScorer ( reader , similarity , this ) ; } public Explanation explain ( IndexReader reader , int doc ) { Explanation queryExpl = new ComplexExplanation ( true , getValue ( ) , "MatchAllDocsQuery, product of:" ) ; if ( getBoost ( ) != 1.0f ) { queryExpl . addDetail ( new Explanation ( getBoost ( ) , "boost" ) ) ; } queryExpl . addDetail ( new Explanation ( queryNorm , "queryNorm" ) ) ; return queryExpl ; } } protected Weight createWeight ( Searcher searcher ) { return new MatchAllDocsWeight ( searcher ) ; } public void extractTerms ( Set terms ) { } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "MatchAllDocsQuery" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof MatchAllDocsQuery ) ) return false ; MatchAllDocsQuery other = ( MatchAllDocsQuery ) o ; return this . getBoost ( ) == other . getBoost ( ) ; } public int hashCode ( ) { return Float . floatToIntBits ( getBoost ( ) ) ^ 0x1AA71190 ; } } 	1	['6', '2', '0', '7', '14', '15', '3', '5', '5', '2', '57', '0', '0', '0.705882353', '0.333333333', '2', '3', '8.5', '3', '1.1667', '6']
package org . apache . lucene . index ; final class CharBlockPool { public char [ ] [ ] buffers = new char [ 10 ] [ ] ; int numBuffer ; int bufferUpto = - 1 ; public int charUpto = DocumentsWriter . CHAR_BLOCK_SIZE ; public char [ ] buffer ; public int charOffset = - DocumentsWriter . CHAR_BLOCK_SIZE ; final private DocumentsWriter docWriter ; public CharBlockPool ( DocumentsWriter docWriter ) { this . docWriter = docWriter ; } public void reset ( ) { docWriter . recycleCharBlocks ( buffers , 1 + bufferUpto ) ; bufferUpto = - 1 ; charUpto = DocumentsWriter . CHAR_BLOCK_SIZE ; charOffset = - DocumentsWriter . CHAR_BLOCK_SIZE ; } public void nextBuffer ( ) { if ( 1 + bufferUpto == buffers . length ) { char [ ] [ ] newBuffers = new char [ ( int ) ( buffers . length * 1.5 ) ] [ ] ; System . arraycopy ( buffers , 0 , newBuffers , 0 , buffers . length ) ; buffers = newBuffers ; } buffer = buffers [ 1 + bufferUpto ] = docWriter . getCharBlock ( ) ; bufferUpto ++ ; charUpto = 0 ; charOffset += DocumentsWriter . CHAR_BLOCK_SIZE ; } } 	0	['3', '1', '0', '5', '7', '0', '4', '1', '3', '0.357142857', '106', '0.142857143', '1', '0', '0.666666667', '0', '0', '32', '2', '1', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; import java . util . List ; import java . util . Collection ; import java . util . ArrayList ; import java . util . Iterator ; import java . util . Set ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . PriorityQueue ; import org . apache . lucene . util . ToStringUtils ; import org . apache . lucene . search . Query ; public class SpanOrQuery extends SpanQuery { private List clauses ; private String field ; public SpanOrQuery ( SpanQuery [ ] clauses ) { this . clauses = new ArrayList ( clauses . length ) ; for ( int i = 0 ; i < clauses . length ; i ++ ) { SpanQuery clause = clauses [ i ] ; if ( i == 0 ) { field = clause . getField ( ) ; } else if ( ! clause . getField ( ) . equals ( field ) ) { throw new IllegalArgumentException ( "Clauses must have same field." ) ; } this . clauses . add ( clause ) ; } } public SpanQuery [ ] getClauses ( ) { return ( SpanQuery [ ] ) clauses . toArray ( new SpanQuery [ clauses . size ( ) ] ) ; } public String getField ( ) { return field ; } public Collection getTerms ( ) { Collection terms = new ArrayList ( ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; terms . addAll ( clause . getTerms ( ) ) ; } return terms ; } public void extractTerms ( Set terms ) { Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; clause . extractTerms ( terms ) ; } } public Query rewrite ( IndexReader reader ) throws IOException { SpanOrQuery clone = null ; for ( int i = 0 ; i < clauses . size ( ) ; i ++ ) { SpanQuery c = ( SpanQuery ) clauses . get ( i ) ; SpanQuery query = ( SpanQuery ) c . rewrite ( reader ) ; if ( query != c ) { if ( clone == null ) clone = ( SpanOrQuery ) this . clone ( ) ; clone . clauses . set ( i , query ) ; } } if ( clone != null ) { return clone ; } else { return this ; } } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "spanOr([" ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { SpanQuery clause = ( SpanQuery ) i . next ( ) ; buffer . append ( clause . toString ( field ) ) ; if ( i . hasNext ( ) ) { buffer . append ( ", " ) ; } } buffer . append ( "])" ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( o == null || getClass ( ) != o . getClass ( ) ) return false ; final SpanOrQuery that = ( SpanOrQuery ) o ; if ( ! clauses . equals ( that . clauses ) ) return false ; if ( ! field . equals ( that . field ) ) return false ; return getBoost ( ) == that . getBoost ( ) ; } public int hashCode ( ) { int h = clauses . hashCode ( ) ; h ^= ( h << 10 ) | ( h > > > 23 ) ; h ^= Float . floatToRawIntBits ( getBoost ( ) ) ; return h ; } private class SpanQueue extends PriorityQueue { public SpanQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object o1 , Object o2 ) { Spans spans1 = ( Spans ) o1 ; Spans spans2 = ( Spans ) o2 ; if ( spans1 . doc ( ) == spans2 . doc ( ) ) { if ( spans1 . start ( ) == spans2 . start ( ) ) { return spans1 . end ( ) < spans2 . end ( ) ; } else { return spans1 . start ( ) < spans2 . start ( ) ; } } else { return spans1 . doc ( ) < spans2 . doc ( ) ; } } } public PayloadSpans getPayloadSpans ( final IndexReader reader ) throws IOException { return ( PayloadSpans ) getSpans ( reader ) ; } public Spans getSpans ( final IndexReader reader ) throws IOException { if ( clauses . size ( ) == 1 ) return ( ( SpanQuery ) clauses . get ( 0 ) ) . getPayloadSpans ( reader ) ; return new PayloadSpans ( ) { private SpanQueue queue = null ; private boolean initSpanQueue ( int target ) throws IOException { queue = new SpanQueue ( clauses . size ( ) ) ; Iterator i = clauses . iterator ( ) ; while ( i . hasNext ( ) ) { PayloadSpans spans = ( ( SpanQuery ) i . next ( ) ) . getPayloadSpans ( reader ) ; if ( ( ( target == - 1 ) && spans . next ( ) ) || ( ( target != - 1 ) && spans . skipTo ( target ) ) ) { queue . put ( spans ) ; } } return queue . size ( ) != 0 ; } public boolean next ( ) throws IOException { if ( queue == null ) { return initSpanQueue ( - 1 ) ; } if ( queue . size ( ) == 0 ) { return false ; } if ( top ( ) . next ( ) ) { queue . adjustTop ( ) ; return true ; } queue . pop ( ) ; return queue . size ( ) != 0 ; } private PayloadSpans top ( ) { return ( PayloadSpans ) queue . top ( ) ; } public boolean skipTo ( int target ) throws IOException { if ( queue == null ) { return initSpanQueue ( target ) ; } while ( queue . size ( ) != 0 && top ( ) . doc ( ) < target ) { if ( top ( ) . skipTo ( target ) ) { queue . adjustTop ( ) ; } else { queue . pop ( ) ; } } return queue . size ( ) != 0 ; } public int doc ( ) { return top ( ) . doc ( ) ; } public int start ( ) { return top ( ) . start ( ) ; } public int end ( ) { return top ( ) . end ( ) ; } public Collection getPayload ( ) throws IOException { ArrayList result = null ; PayloadSpans theTop = top ( ) ; if ( theTop != null && theTop . isPayloadAvailable ( ) ) { result = new ArrayList ( theTop . getPayload ( ) ) ; } return result ; } public boolean isPayloadAvailable ( ) { PayloadSpans top = top ( ) ; return top != null && top . isPayloadAvailable ( ) ; } public String toString ( ) { return "spans(" + SpanOrQuery . this + ")@" + ( ( queue == null ) ? "START" : ( queue . size ( ) > 0 ? ( doc ( ) + ":" + start ( ) + "-" + end ( ) ) : "END" ) ) ; } } ; } } 	1	['12', '3', '0', '10', '43', '0', '4', '7', '11', '0.454545455', '292', '1', '0', '0.607142857', '0.226190476', '2', '2', '23.16666667', '7', '1.6667', '5']
package org . apache . lucene . search ; import org . apache . lucene . util . Parameter ; public class BooleanClause implements java . io . Serializable { public static final class Occur extends Parameter implements java . io . Serializable { private Occur ( String name ) { super ( name ) ; } public String toString ( ) { if ( this == MUST ) return "+" ; if ( this == MUST_NOT ) return "-" ; return "" ; } public static final Occur MUST = new Occur ( "MUST" ) ; public static final Occur SHOULD = new Occur ( "SHOULD" ) ; public static final Occur MUST_NOT = new Occur ( "MUST_NOT" ) ; } private Query query ; private Occur occur ; public BooleanClause ( Query query , Occur occur ) { this . query = query ; this . occur = occur ; } public Occur getOccur ( ) { return occur ; } public void setOccur ( Occur occur ) { this . occur = occur ; } public Query getQuery ( ) { return query ; } public void setQuery ( Query query ) { this . query = query ; } public boolean isProhibited ( ) { return Occur . MUST_NOT . equals ( occur ) ; } public boolean isRequired ( ) { return Occur . MUST . equals ( occur ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof BooleanClause ) ) return false ; BooleanClause other = ( BooleanClause ) o ; return this . query . equals ( other . query ) && this . occur . equals ( other . occur ) ; } public int hashCode ( ) { return query . hashCode ( ) ^ ( Occur . MUST . equals ( occur ) ? 1 : 0 ) ^ ( Occur . MUST_NOT . equals ( occur ) ? 2 : 0 ) ; } public String toString ( ) { return occur . toString ( ) + query . toString ( ) ; } } 	0	['10', '1', '0', '7', '18', '0', '6', '2', '10', '0.333333333', '104', '1', '2', '0', '0.375', '1', '1', '9.2', '4', '1.4', '0']
package org . apache . lucene . util ; public class ToStringUtils { public static String boost ( float boost ) { if ( boost != 1.0f ) { return "^" + Float . toString ( boost ) ; } else return "" ; } } 	1	['2', '1', '0', '17', '7', '1', '17', '0', '2', '2', '21', '0', '0', '0', '0.5', '0', '0', '9.5', '2', '1', '1']
package org . apache . lucene . index ; import java . util . Collection ; import java . io . IOException ; public interface IndexCommitPoint { public String getSegmentsFileName ( ) ; public Collection getFileNames ( ) throws IOException ; public void delete ( ) ; } 	0	['3', '1', '0', '2', '3', '3', '2', '0', '3', '2', '3', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import java . io . IOException ; public class WildcardQuery extends MultiTermQuery { private boolean termContainsWildcard ; public WildcardQuery ( Term term ) { super ( term ) ; this . termContainsWildcard = ( term . text ( ) . indexOf ( '*' ) != - 1 ) || ( term . text ( ) . indexOf ( '?' ) != - 1 ) ; } protected FilteredTermEnum getEnum ( IndexReader reader ) throws IOException { return new WildcardTermEnum ( reader , getTerm ( ) ) ; } public boolean equals ( Object o ) { if ( o instanceof WildcardQuery ) return super . equals ( o ) ; return false ; } public Query rewrite ( IndexReader reader ) throws IOException { if ( this . termContainsWildcard ) { return super . rewrite ( reader ) ; } return new TermQuery ( getTerm ( ) ) ; } } 	1	['4', '3', '0', '8', '12', '4', '1', '7', '3', '0.666666667', '55', '1', '0', '0.857142857', '0.5', '1', '1', '12.5', '2', '1', '3']
package org . apache . lucene . store ; import java . io . IOException ; public class NoLockFactory extends LockFactory { private static NoLock singletonLock = new NoLock ( ) ; private static NoLockFactory singleton = new NoLockFactory ( ) ; public static NoLockFactory getNoLockFactory ( ) { return singleton ; } public Lock makeLock ( String lockName ) { return singletonLock ; } public void clearLock ( String lockName ) { } ; } ; class NoLock extends Lock { public boolean obtain ( ) throws IOException { return true ; } public void release ( ) { } public boolean isLocked ( ) { return false ; } public String toString ( ) { return "NoLock" ; } } 	0	['5', '2', '0', '4', '7', '6', '1', '3', '4', '0.75', '24', '1', '2', '0.571428571', '0.625', '0', '0', '3.4', '1', '0.6', '0']
package org . apache . lucene . search . spans ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; import java . util . Arrays ; import java . util . Comparator ; import java . util . LinkedList ; import java . util . List ; import java . util . Collection ; class NearSpansOrdered implements PayloadSpans { private final int allowedSlop ; private boolean firstTime = true ; private boolean more = false ; private final PayloadSpans [ ] subSpans ; private boolean inSameDoc = false ; private int matchDoc = - 1 ; private int matchStart = - 1 ; private int matchEnd = - 1 ; private List matchPayload ; private final PayloadSpans [ ] subSpansByDoc ; private final Comparator spanDocComparator = new Comparator ( ) { public int compare ( Object o1 , Object o2 ) { return ( ( Spans ) o1 ) . doc ( ) - ( ( Spans ) o2 ) . doc ( ) ; } } ; private SpanNearQuery query ; public NearSpansOrdered ( SpanNearQuery spanNearQuery , IndexReader reader ) throws IOException { if ( spanNearQuery . getClauses ( ) . length < 2 ) { throw new IllegalArgumentException ( "Less than 2 clauses: " + spanNearQuery ) ; } allowedSlop = spanNearQuery . getSlop ( ) ; SpanQuery [ ] clauses = spanNearQuery . getClauses ( ) ; subSpans = new PayloadSpans [ clauses . length ] ; matchPayload = new LinkedList ( ) ; subSpansByDoc = new PayloadSpans [ clauses . length ] ; for ( int i = 0 ; i < clauses . length ; i ++ ) { subSpans [ i ] = clauses [ i ] . getPayloadSpans ( reader ) ; subSpansByDoc [ i ] = subSpans [ i ] ; } query = spanNearQuery ; } public int doc ( ) { return matchDoc ; } public int start ( ) { return matchStart ; } public int end ( ) { return matchEnd ; } public Collection getPayload ( ) throws IOException { return matchPayload ; } public boolean isPayloadAvailable ( ) { return matchPayload . isEmpty ( ) == false ; } public boolean next ( ) throws IOException { if ( firstTime ) { firstTime = false ; for ( int i = 0 ; i < subSpans . length ; i ++ ) { if ( ! subSpans [ i ] . next ( ) ) { more = false ; return false ; } } more = true ; } matchPayload . clear ( ) ; return advanceAfterOrdered ( ) ; } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { firstTime = false ; for ( int i = 0 ; i < subSpans . length ; i ++ ) { if ( ! subSpans [ i ] . skipTo ( target ) ) { more = false ; return false ; } } more = true ; } else if ( more && ( subSpans [ 0 ] . doc ( ) < target ) ) { if ( subSpans [ 0 ] . skipTo ( target ) ) { inSameDoc = false ; } else { more = false ; return false ; } } matchPayload . clear ( ) ; return advanceAfterOrdered ( ) ; } private boolean advanceAfterOrdered ( ) throws IOException { while ( more && ( inSameDoc || toSameDoc ( ) ) ) { if ( stretchToOrder ( ) && shrinkToAfterShortestMatch ( ) ) { return true ; } } return false ; } private boolean toSameDoc ( ) throws IOException { Arrays . sort ( subSpansByDoc , spanDocComparator ) ; int firstIndex = 0 ; int maxDoc = subSpansByDoc [ subSpansByDoc . length - 1 ] . doc ( ) ; while ( subSpansByDoc [ firstIndex ] . doc ( ) != maxDoc ) { if ( ! subSpansByDoc [ firstIndex ] . skipTo ( maxDoc ) ) { more = false ; inSameDoc = false ; return false ; } maxDoc = subSpansByDoc [ firstIndex ] . doc ( ) ; if ( ++ firstIndex == subSpansByDoc . length ) { firstIndex = 0 ; } } for ( int i = 0 ; i < subSpansByDoc . length ; i ++ ) { assert ( subSpansByDoc [ i ] . doc ( ) == maxDoc ) : " NearSpansOrdered.toSameDoc() spans " + subSpansByDoc [ 0 ] + "\n at doc " + subSpansByDoc [ i ] . doc ( ) + ", but should be at " + maxDoc ; } inSameDoc = true ; return true ; } static final boolean docSpansOrdered ( Spans spans1 , Spans spans2 ) { assert spans1 . doc ( ) == spans2 . doc ( ) : "doc1 " + spans1 . doc ( ) + " != doc2 " + spans2 . doc ( ) ; int start1 = spans1 . start ( ) ; int start2 = spans2 . start ( ) ; return ( start1 == start2 ) ? ( spans1 . end ( ) < spans2 . end ( ) ) : ( start1 < start2 ) ; } private static final boolean docSpansOrdered ( int start1 , int end1 , int start2 , int end2 ) { return ( start1 == start2 ) ? ( end1 < end2 ) : ( start1 < start2 ) ; } private boolean stretchToOrder ( ) throws IOException { matchDoc = subSpans [ 0 ] . doc ( ) ; for ( int i = 1 ; inSameDoc && ( i < subSpans . length ) ; i ++ ) { while ( ! docSpansOrdered ( subSpans [ i - 1 ] , subSpans [ i ] ) ) { if ( ! subSpans [ i ] . next ( ) ) { inSameDoc = false ; more = false ; break ; } else if ( matchDoc != subSpans [ i ] . doc ( ) ) { inSameDoc = false ; break ; } } } return inSameDoc ; } private boolean shrinkToAfterShortestMatch ( ) throws IOException { matchStart = subSpans [ subSpans . length - 1 ] . start ( ) ; matchEnd = subSpans [ subSpans . length - 1 ] . end ( ) ; if ( subSpans [ subSpans . length - 1 ] . isPayloadAvailable ( ) ) { matchPayload . addAll ( subSpans [ subSpans . length - 1 ] . getPayload ( ) ) ; } int matchSlop = 0 ; int lastStart = matchStart ; int lastEnd = matchEnd ; for ( int i = subSpans . length - 2 ; i >= 0 ; i -- ) { PayloadSpans prevSpans = subSpans [ i ] ; if ( subSpans [ i ] . isPayloadAvailable ( ) ) { matchPayload . addAll ( 0 , subSpans [ i ] . getPayload ( ) ) ; } int prevStart = prevSpans . start ( ) ; int prevEnd = prevSpans . end ( ) ; while ( true ) { if ( ! prevSpans . next ( ) ) { inSameDoc = false ; more = false ; break ; } else if ( matchDoc != prevSpans . doc ( ) ) { inSameDoc = false ; break ; } else { int ppStart = prevSpans . start ( ) ; int ppEnd = prevSpans . end ( ) ; if ( ! docSpansOrdered ( ppStart , ppEnd , lastStart , lastEnd ) ) { break ; } else { prevStart = ppStart ; prevEnd = ppEnd ; } } } assert prevStart <= matchStart ; if ( matchStart > prevEnd ) { matchSlop += ( matchStart - prevEnd ) ; } matchStart = prevStart ; lastStart = prevStart ; lastEnd = prevEnd ; } return matchSlop <= allowedSlop ; } public String toString ( ) { return getClass ( ) . getName ( ) + "(" + query . toString ( ) + ")@" + ( firstTime ? "START" : ( more ? ( doc ( ) + ":" + start ( ) + "-" + end ( ) ) : "END" ) ) ; } } 	1	['17', '1', '0', '7', '53', '38', '3', '6', '9', '0.767857143', '725', '0.857142857', '3', '0', '0.197916667', '0', '0', '40.82352941', '6', '1.5294', '4']
package org . apache . lucene . index ; import java . util . * ; class SegmentTermVector implements TermFreqVector { private String field ; private String terms [ ] ; private int termFreqs [ ] ; SegmentTermVector ( String field , String terms [ ] , int termFreqs [ ] ) { this . field = field ; this . terms = terms ; this . termFreqs = termFreqs ; } public String getField ( ) { return field ; } public String toString ( ) { StringBuffer sb = new StringBuffer ( ) ; sb . append ( '{' ) ; sb . append ( field ) . append ( ": " ) ; if ( terms != null ) { for ( int i = 0 ; i < terms . length ; i ++ ) { if ( i > 0 ) sb . append ( ", " ) ; sb . append ( terms [ i ] ) . append ( '/' ) . append ( termFreqs [ i ] ) ; } } sb . append ( '}' ) ; return sb . toString ( ) ; } public int size ( ) { return terms == null ? 0 : terms . length ; } public String [ ] getTerms ( ) { return terms ; } public int [ ] getTermFrequencies ( ) { return termFreqs ; } public int indexOf ( String termText ) { if ( terms == null ) return - 1 ; int res = Arrays . binarySearch ( terms , termText ) ; return res >= 0 ? res : - 1 ; } public int [ ] indexesOf ( String [ ] termNumbers , int start , int len ) { int res [ ] = new int [ len ] ; for ( int i = 0 ; i < len ; i ++ ) { res [ i ] = indexOf ( termNumbers [ start + i ] ) ; } return res ; } } 	0	['8', '1', '1', '4', '15', '0', '3', '1', '7', '0.571428571', '133', '1', '0', '0', '0.35', '0', '0', '15.25', '4', '1.75', '0']
package org . apache . lucene . document ; import org . apache . lucene . search . PrefixQuery ; import org . apache . lucene . search . RangeQuery ; import java . util . Date ; public class DateField { private DateField ( ) { } private static int DATE_LEN = Long . toString ( 1000L * 365 * 24 * 60 * 60 * 1000 , Character . MAX_RADIX ) . length ( ) ; public static String MIN_DATE_STRING ( ) { return timeToString ( 0 ) ; } public static String MAX_DATE_STRING ( ) { char [ ] buffer = new char [ DATE_LEN ] ; char c = Character . forDigit ( Character . MAX_RADIX - 1 , Character . MAX_RADIX ) ; for ( int i = 0 ; i < DATE_LEN ; i ++ ) buffer [ i ] = c ; return new String ( buffer ) ; } public static String dateToString ( Date date ) { return timeToString ( date . getTime ( ) ) ; } public static String timeToString ( long time ) { if ( time < 0 ) throw new RuntimeException ( "time '" + time + "' is too early, must be >= 0" ) ; String s = Long . toString ( time , Character . MAX_RADIX ) ; if ( s . length ( ) > DATE_LEN ) throw new RuntimeException ( "time '" + time + "' is too late, length of string " + "representation must be <= " + DATE_LEN ) ; if ( s . length ( ) < DATE_LEN ) { StringBuffer sb = new StringBuffer ( s ) ; while ( sb . length ( ) < DATE_LEN ) sb . insert ( 0 , 0 ) ; s = sb . toString ( ) ; } return s ; } public static long stringToTime ( String s ) { return Long . parseLong ( s , Character . MAX_RADIX ) ; } public static Date stringToDate ( String s ) { return new Date ( stringToTime ( s ) ) ; } } 	1	['8', '1', '0', '1', '25', '22', '1', '0', '6', '0.428571429', '126', '1', '0', '0', '0.178571429', '0', '0', '14.625', '5', '1.375', '2']
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . function . DocValues ; import java . io . IOException ; import java . io . Serializable ; public abstract class ValueSource implements Serializable { public abstract DocValues getValues ( IndexReader reader ) throws IOException ; public abstract String description ( ) ; public String toString ( ) { return description ( ) ; } public abstract boolean equals ( Object o ) ; public abstract int hashCode ( ) ; } 	0	['6', '1', '3', '8', '7', '15', '6', '2', '6', '2', '12', '0', '0', '0', '0.444444444', '1', '1', '1', '1', '0.8333', '0']
package org . apache . lucene . search ; public class FieldDoc extends ScoreDoc { public Comparable [ ] fields ; public FieldDoc ( int doc , float score ) { super ( doc , score ) ; } public FieldDoc ( int doc , float score , Comparable [ ] fields ) { super ( doc , score ) ; this . fields = fields ; } } 	1	['2', '2', '0', '4', '3', '1', '3', '1', '2', '1', '16', '0', '0', '0', '0.875', '0', '0', '6.5', '0', '0', '1']
package org . apache . lucene . analysis ; import java . io . IOException ; public final class LengthFilter extends TokenFilter { final int min ; final int max ; public LengthFilter ( TokenStream in , int min , int max ) { super ( in ) ; this . min = min ; this . max = max ; } public final Token next ( final Token reusableToken ) throws IOException { assert reusableToken != null ; for ( Token nextToken = input . next ( reusableToken ) ; nextToken != null ; nextToken = input . next ( reusableToken ) ) { int len = nextToken . termLength ( ) ; if ( len >= min && len <= max ) { return nextToken ; } } return null ; } } 	0	['4', '3', '0', '3', '12', '2', '0', '3', '2', '0.75', '79', '0', '0', '0.777777778', '0.4', '1', '2', '17.75', '1', '0.5', '0']
package org . apache . lucene . queryParser ; public class TokenMgrError extends Error { static final int LEXICAL_ERROR = 0 ; static final int STATIC_LEXER_ERROR = 1 ; static final int INVALID_LEXICAL_STATE = 2 ; static final int LOOP_DETECTED = 3 ; int errorCode ; protected static final String addEscapes ( String str ) { StringBuffer retval = new StringBuffer ( ) ; char ch ; for ( int i = 0 ; i < str . length ( ) ; i ++ ) { switch ( str . charAt ( i ) ) { case 0 : continue ; case '\b' : retval . append ( "\\b" ) ; continue ; case '\t' : retval . append ( "\\t" ) ; continue ; case '\n' : retval . append ( "\\n" ) ; continue ; case '\f' : retval . append ( "\\f" ) ; continue ; case '\r' : retval . append ( "\\r" ) ; continue ; case '\"' : retval . append ( "\\\"" ) ; continue ; case '\'' : retval . append ( "\\\'" ) ; continue ; case '\\' : retval . append ( "\\\\" ) ; continue ; default : if ( ( ch = str . charAt ( i ) ) < 0x20 || ch > 0x7e ) { String s = "0000" + Integer . toString ( ch , 16 ) ; retval . append ( "\\u" + s . substring ( s . length ( ) - 4 , s . length ( ) ) ) ; } else { retval . append ( ch ) ; } continue ; } } return retval . toString ( ) ; } protected static String LexicalError ( boolean EOFSeen , int lexState , int errorLine , int errorColumn , String errorAfter , char curChar ) { return ( "Lexical error at line " + errorLine + ", column " + errorColumn + ".  Encountered: " + ( EOFSeen ? "<EOF> " : ( "\"" + addEscapes ( String . valueOf ( curChar ) ) + "\"" ) + " (" + ( int ) curChar + "), " ) + "after : \"" + addEscapes ( errorAfter ) + "\"" ) ; } public String getMessage ( ) { return super . getMessage ( ) ; } public TokenMgrError ( ) { } public TokenMgrError ( String message , int reason ) { super ( message ) ; errorCode = reason ; } public TokenMgrError ( boolean EOFSeen , int lexState , int errorLine , int errorColumn , String errorAfter , char curChar , int reason ) { this ( LexicalError ( EOFSeen , lexState , errorLine , errorColumn , errorAfter , curChar ) , reason ) ; } } 	1	['6', '3', '0', '2', '19', '15', '2', '0', '4', '1.12', '184', '0', '0', '0.8125', '0.5', '1', '1', '28.83333333', '14', '2.8333', '1']
package org . apache . lucene . index ; abstract class InvertedDocEndConsumerPerField { abstract void finish ( ) ; abstract void abort ( ) ; } 	0	['3', '1', '1', '5', '4', '3', '5', '0', '0', '2', '6', '0', '0', '0', '1', '0', '0', '1', '1', '0.6667', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . ArrayList ; import java . util . Collection ; import java . util . Iterator ; import java . util . List ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . FieldSelector ; import org . apache . lucene . document . FieldSelectorResult ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . IndexOutput ; final class SegmentMerger { static final byte [ ] NORMS_HEADER = new byte [ ] { 'N' , 'R' , 'M' , - 1 } ; private Directory directory ; private String segment ; private int termIndexInterval = IndexWriter . DEFAULT_TERM_INDEX_INTERVAL ; private List readers = new ArrayList ( ) ; private FieldInfos fieldInfos ; private int mergedDocs ; private CheckAbort checkAbort ; private boolean mergeDocStores ; private final static int MAX_RAW_MERGE_DOCS = 4192 ; SegmentMerger ( Directory dir , String name ) { directory = dir ; segment = name ; } SegmentMerger ( IndexWriter writer , String name , MergePolicy . OneMerge merge ) { directory = writer . getDirectory ( ) ; segment = name ; if ( merge != null ) checkAbort = new CheckAbort ( merge , directory ) ; termIndexInterval = writer . getTermIndexInterval ( ) ; } boolean hasProx ( ) { return fieldInfos . hasProx ( ) ; } final void add ( IndexReader reader ) { readers . add ( reader ) ; } final IndexReader segmentReader ( int i ) { return ( IndexReader ) readers . get ( i ) ; } final int merge ( ) throws CorruptIndexException , IOException { return merge ( true ) ; } final int merge ( boolean mergeDocStores ) throws CorruptIndexException , IOException { this . mergeDocStores = mergeDocStores ; mergedDocs = mergeFields ( ) ; mergeTerms ( ) ; mergeNorms ( ) ; if ( mergeDocStores && fieldInfos . hasVectors ( ) ) mergeVectors ( ) ; return mergedDocs ; } final void closeReaders ( ) throws IOException { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader reader = ( IndexReader ) readers . get ( i ) ; reader . close ( ) ; } } final List createCompoundFile ( String fileName ) throws IOException { CompoundFileWriter cfsWriter = new CompoundFileWriter ( directory , fileName , checkAbort ) ; List files = new ArrayList ( IndexFileNames . COMPOUND_EXTENSIONS . length + 1 ) ; for ( int i = 0 ; i < IndexFileNames . COMPOUND_EXTENSIONS . length ; i ++ ) { String ext = IndexFileNames . COMPOUND_EXTENSIONS [ i ] ; if ( ext . equals ( IndexFileNames . PROX_EXTENSION ) && ! hasProx ( ) ) continue ; if ( mergeDocStores || ( ! ext . equals ( IndexFileNames . FIELDS_EXTENSION ) && ! ext . equals ( IndexFileNames . FIELDS_INDEX_EXTENSION ) ) ) files . add ( segment + "." + ext ) ; } for ( int i = 0 ; i < fieldInfos . size ( ) ; i ++ ) { FieldInfo fi = fieldInfos . fieldInfo ( i ) ; if ( fi . isIndexed && ! fi . omitNorms ) { files . add ( segment + "." + IndexFileNames . NORMS_EXTENSION ) ; break ; } } if ( fieldInfos . hasVectors ( ) && mergeDocStores ) { for ( int i = 0 ; i < IndexFileNames . VECTOR_EXTENSIONS . length ; i ++ ) { files . add ( segment + "." + IndexFileNames . VECTOR_EXTENSIONS [ i ] ) ; } } Iterator it = files . iterator ( ) ; while ( it . hasNext ( ) ) { cfsWriter . addFile ( ( String ) it . next ( ) ) ; } cfsWriter . close ( ) ; return files ; } private void addIndexed ( IndexReader reader , FieldInfos fieldInfos , Collection names , boolean storeTermVectors , boolean storePositionWithTermVector , boolean storeOffsetWithTermVector , boolean storePayloads , boolean omitTf ) throws IOException { Iterator i = names . iterator ( ) ; while ( i . hasNext ( ) ) { String field = ( String ) i . next ( ) ; fieldInfos . add ( field , true , storeTermVectors , storePositionWithTermVector , storeOffsetWithTermVector , ! reader . hasNorms ( field ) , storePayloads , omitTf ) ; } } private SegmentReader [ ] matchingSegmentReaders ; private int [ ] rawDocLengths ; private int [ ] rawDocLengths2 ; private void setMatchingSegmentReaders ( ) { matchingSegmentReaders = new SegmentReader [ readers . size ( ) ] ; for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader reader = ( IndexReader ) readers . get ( i ) ; if ( reader instanceof SegmentReader ) { SegmentReader segmentReader = ( SegmentReader ) reader ; boolean same = true ; FieldInfos segmentFieldInfos = segmentReader . getFieldInfos ( ) ; for ( int j = 0 ; same && j < segmentFieldInfos . size ( ) ; j ++ ) same = fieldInfos . fieldName ( j ) . equals ( segmentFieldInfos . fieldName ( j ) ) ; if ( same ) matchingSegmentReaders [ i ] = segmentReader ; } } rawDocLengths = new int [ MAX_RAW_MERGE_DOCS ] ; rawDocLengths2 = new int [ MAX_RAW_MERGE_DOCS ] ; } private final int mergeFields ( ) throws CorruptIndexException , IOException { if ( ! mergeDocStores ) { final SegmentReader sr = ( SegmentReader ) readers . get ( readers . size ( ) - 1 ) ; fieldInfos = ( FieldInfos ) sr . fieldInfos . clone ( ) ; } else { fieldInfos = new FieldInfos ( ) ; } for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { IndexReader reader = ( IndexReader ) readers . get ( i ) ; if ( reader instanceof SegmentReader ) { SegmentReader segmentReader = ( SegmentReader ) reader ; for ( int j = 0 ; j < segmentReader . getFieldInfos ( ) . size ( ) ; j ++ ) { FieldInfo fi = segmentReader . getFieldInfos ( ) . fieldInfo ( j ) ; fieldInfos . add ( fi . name , fi . isIndexed , fi . storeTermVector , fi . storePositionWithTermVector , fi . storeOffsetWithTermVector , ! reader . hasNorms ( fi . name ) , fi . storePayloads , fi . omitTf ) ; } } else { addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . TERMVECTOR_WITH_POSITION_OFFSET ) , true , true , true , false , false ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . TERMVECTOR_WITH_POSITION ) , true , true , false , false , false ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . TERMVECTOR_WITH_OFFSET ) , true , false , true , false , false ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . TERMVECTOR ) , true , false , false , false , false ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . OMIT_TF ) , false , false , false , false , true ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . STORES_PAYLOADS ) , false , false , false , true , false ) ; addIndexed ( reader , fieldInfos , reader . getFieldNames ( IndexReader . FieldOption . INDEXED ) , false , false , false , false , false ) ; fieldInfos . add ( reader . getFieldNames ( IndexReader . FieldOption . UNINDEXED ) , false ) ; } } fieldInfos . write ( directory , segment + ".fnm" ) ; int docCount = 0 ; setMatchingSegmentReaders ( ) ; if ( mergeDocStores ) { FieldSelector fieldSelectorMerge = new FieldSelector ( ) { public FieldSelectorResult accept ( String fieldName ) { return FieldSelectorResult . LOAD_FOR_MERGE ; } } ; final FieldsWriter fieldsWriter = new FieldsWriter ( directory , segment , fieldInfos ) ; try { for ( int i = 0 ; i < readers . size ( ) ; i ++ ) { final IndexReader reader = ( IndexReader ) readers . get ( i ) ; final SegmentReader matchingSegmentReader = matchingSegmentReaders [ i ] ; final FieldsReader matchingFieldsReader ; final boolean hasMatchingReader ; if ( matchingSegmentReader != null ) { final FieldsReader fieldsReader = matchingSegmentReader . getFieldsReader ( ) ; if ( fieldsReader != null && ! fieldsReader . canReadRawDocs ( ) ) { matchingFieldsReader = null ; hasMatchingReader = false ; } else { matchingFieldsReader = fieldsReader ; hasMatchingReader = true ; } } else { hasMatchingReader = false ; matchingFieldsReader = null ; } final int maxDoc = reader . maxDoc ( ) ; final boolean hasDeletions = reader . hasDeletions ( ) ; for ( int j = 0 ; j < maxDoc ; ) { if ( ! hasDeletions || ! reader . isDeleted ( j ) ) { if ( hasMatchingReader ) { int start = j ; int numDocs = 0 ; do { j ++ ; numDocs ++ ; if ( j >= maxDoc ) break ; if ( hasDeletions && matchingSegmentReader . isDeleted ( j ) ) { j ++ ; break ; } } while ( numDocs < MAX_RAW_MERGE_DOCS ) ; IndexInput stream = matchingFieldsReader . rawDocs ( rawDocLengths , start , numDocs ) ; fieldsWriter . addRawDocuments ( stream , rawDocLengths , numDocs ) ; docCount += numDocs ; if ( checkAbort != null ) checkAbort . work ( 300 * numDocs ) ; } else { Document doc = reader . document ( j , fieldSelectorMerge ) ; fieldsWriter . addDocument ( doc ) ; j ++ ; docCount ++ ; if ( checkAbort != null ) checkAbort . work ( 300 ) ; } } else j ++ ; } } } finally { fieldsWriter . close ( ) ; } final long fdxFileLength = directory . fileLength ( segment + "." + IndexFileNames . FIELDS_INDEX_EXTENSION ) ; if ( 4 + docCount * 8 != fdxFileLength ) throw new RuntimeException ( "mergeFields produced an invalid result: docCount is " + docCount + " but fdx file size is " + fdxFileLength + "; now aborting this merge to prevent index corruption" ) ; } else for ( int i = 0 ; i < readers . size ( ) ; i ++ ) docCount += ( ( IndexReader ) readers . get ( i ) ) . numDocs ( ) ; return docCount ; } private final void mergeVectors ( ) throws IOException { TermVectorsWriter termVectorsWriter = new TermVectorsWriter ( directory , segment , fieldInfos ) ; try { for ( int r = 0 ; r < readers . size ( ) ; r ++ ) { final SegmentReader matchingSegmentReader = matchingSegmentReaders [ r ] ; TermVectorsReader matchingVectorsReader ; final boolean hasMatchingReader ; if ( matchingSegmentReader != null ) { matchingVectorsReader = matchingSegmentReader . termVectorsReaderOrig ; if ( matchingVectorsReader != null && ! matchingVectorsReader . canReadRawDocs ( ) ) { matchingVectorsReader = null ; hasMatchingReader = false ; } else hasMatchingReader = matchingVectorsReader != null ; } else { hasMatchingReader = false ; matchingVectorsReader = null ; } IndexReader reader = ( IndexReader ) readers . get ( r ) ; final boolean hasDeletions = reader . hasDeletions ( ) ; int maxDoc = reader . maxDoc ( ) ; for ( int docNum = 0 ; docNum < maxDoc ; ) { if ( ! hasDeletions || ! reader . isDeleted ( docNum ) ) { if ( hasMatchingReader ) { int start = docNum ; int numDocs = 0 ; do { docNum ++ ; numDocs ++ ; if ( docNum >= maxDoc ) break ; if ( hasDeletions && matchingSegmentReader . isDeleted ( docNum ) ) { docNum ++ ; break ; } } while ( numDocs < MAX_RAW_MERGE_DOCS ) ; matchingVectorsReader . rawDocs ( rawDocLengths , rawDocLengths2 , start , numDocs ) ; termVectorsWriter . addRawDocuments ( matchingVectorsReader , rawDocLengths , rawDocLengths2 , numDocs ) ; if ( checkAbort != null ) checkAbort . work ( 300 * numDocs ) ; } else { TermFreqVector [ ] vectors = reader . getTermFreqVectors ( docNum ) ; termVectorsWriter . addAllDocVectors ( vectors ) ; docNum ++ ; if ( checkAbort != null ) checkAbort . work ( 300 ) ; } } else docNum ++ ; } } } finally { termVectorsWriter . close ( ) ; } final long tvxSize = directory . fileLength ( segment + "." + IndexFileNames . VECTORS_INDEX_EXTENSION ) ; if ( 4 + mergedDocs * 16 != tvxSize ) throw new RuntimeException ( "mergeVectors produced an invalid result: mergedDocs is " + mergedDocs + " but tvx size is " + tvxSize + "; now aborting this merge to prevent index corruption" ) ; } private IndexOutput freqOutput = null ; private IndexOutput proxOutput = null ; private TermInfosWriter termInfosWriter = null ; private int skipInterval ; private int maxSkipLevels ; private SegmentMergeQueue queue = null ; private DefaultSkipListWriter skipListWriter = null ; private final void mergeTerms ( ) throws CorruptIndexException , IOException { try { freqOutput = directory . createOutput ( segment + ".frq" ) ; if ( hasProx ( ) ) proxOutput = directory . createOutput ( segment + ".prx" ) ; termInfosWriter = new TermInfosWriter ( directory , segment , fieldInfos , termIndexInterval ) ; skipInterval = termInfosWriter . skipInterval ; maxSkipLevels = termInfosWriter . maxSkipLevels ; skipListWriter = new DefaultSkipListWriter ( skipInterval , maxSkipLevels , mergedDocs , freqOutput , proxOutput ) ; queue = new SegmentMergeQueue ( readers . size ( ) ) ; mergeTermInfos ( ) ; } finally { if ( freqOutput != null ) freqOutput . close ( ) ; if ( proxOutput != null ) proxOutput . close ( ) ; if ( termInfosWriter != null ) termInfosWriter . close ( ) ; if ( queue != null ) queue . close ( ) ; } } private final void mergeTermInfos ( ) throws CorruptIndexException , IOException { int base = 0 ; final int readerCount = readers . size ( ) ; for ( int i = 0 ; i < readerCount ; i ++ ) { IndexReader reader = ( IndexReader ) readers . get ( i ) ; TermEnum termEnum = reader . terms ( ) ; SegmentMergeInfo smi = new SegmentMergeInfo ( base , termEnum , reader ) ; int [ ] docMap = smi . getDocMap ( ) ; if ( docMap != null ) { if ( docMaps == null ) { docMaps = new int [ readerCount ] [ ] ; delCounts = new int [ readerCount ] ; } docMaps [ i ] = docMap ; delCounts [ i ] = smi . reader . maxDoc ( ) - smi . reader . numDocs ( ) ; } base += reader . numDocs ( ) ; if ( smi . next ( ) ) queue . put ( smi ) ; else smi . close ( ) ; } SegmentMergeInfo [ ] match = new SegmentMergeInfo [ readers . size ( ) ] ; while ( queue . size ( ) > 0 ) { int matchSize = 0 ; match [ matchSize ++ ] = ( SegmentMergeInfo ) queue . pop ( ) ; Term term = match [ 0 ] . term ; SegmentMergeInfo top = ( SegmentMergeInfo ) queue . top ( ) ; while ( top != null && term . compareTo ( top . term ) == 0 ) { match [ matchSize ++ ] = ( SegmentMergeInfo ) queue . pop ( ) ; top = ( SegmentMergeInfo ) queue . top ( ) ; } final int df = mergeTermInfo ( match , matchSize ) ; if ( checkAbort != null ) checkAbort . work ( df / 3.0 ) ; while ( matchSize > 0 ) { SegmentMergeInfo smi = match [ -- matchSize ] ; if ( smi . next ( ) ) queue . put ( smi ) ; else smi . close ( ) ; } } } private final TermInfo termInfo = new TermInfo ( ) ; private final int mergeTermInfo ( SegmentMergeInfo [ ] smis , int n ) throws CorruptIndexException , IOException { final long freqPointer = freqOutput . getFilePointer ( ) ; final long proxPointer ; if ( proxOutput != null ) proxPointer = proxOutput . getFilePointer ( ) ; else proxPointer = 0 ; int df ; if ( fieldInfos . fieldInfo ( smis [ 0 ] . term . field ) . omitTf ) { df = appendPostingsNoTf ( smis , n ) ; } else { df = appendPostings ( smis , n ) ; } long skipPointer = skipListWriter . writeSkip ( freqOutput ) ; if ( df > 0 ) { termInfo . set ( df , freqPointer , proxPointer , ( int ) ( skipPointer - freqPointer ) ) ; termInfosWriter . add ( smis [ 0 ] . term , termInfo ) ; } return df ; } private byte [ ] payloadBuffer ; private int [ ] [ ] docMaps ; int [ ] [ ] getDocMaps ( ) { return docMaps ; } private int [ ] delCounts ; int [ ] getDelCounts ( ) { return delCounts ; } private final int appendPostings ( SegmentMergeInfo [ ] smis , int n ) throws CorruptIndexException , IOException { int lastDoc = 0 ; int df = 0 ; skipListWriter . resetSkip ( ) ; boolean storePayloads = fieldInfos . fieldInfo ( smis [ 0 ] . term . field ) . storePayloads ; int lastPayloadLength = - 1 ; for ( int i = 0 ; i < n ; i ++ ) { SegmentMergeInfo smi = smis [ i ] ; TermPositions postings = smi . getPositions ( ) ; assert postings != null ; int base = smi . base ; int [ ] docMap = smi . getDocMap ( ) ; postings . seek ( smi . termEnum ) ; while ( postings . next ( ) ) { int doc = postings . doc ( ) ; if ( docMap != null ) doc = docMap [ doc ] ; doc += base ; if ( doc < 0 || ( df > 0 && doc <= lastDoc ) ) throw new CorruptIndexException ( "docs out of order (" + doc + " <= " + lastDoc + " )" ) ; df ++ ; if ( ( df % skipInterval ) == 0 ) { skipListWriter . setSkipData ( lastDoc , storePayloads , lastPayloadLength ) ; skipListWriter . bufferSkip ( df ) ; } int docCode = ( doc - lastDoc ) << 1 ; lastDoc = doc ; int freq = postings . freq ( ) ; if ( freq == 1 ) { freqOutput . writeVInt ( docCode | 1 ) ; } else { freqOutput . writeVInt ( docCode ) ; freqOutput . writeVInt ( freq ) ; } int lastPosition = 0 ; for ( int j = 0 ; j < freq ; j ++ ) { int position = postings . nextPosition ( ) ; int delta = position - lastPosition ; if ( storePayloads ) { int payloadLength = postings . getPayloadLength ( ) ; if ( payloadLength == lastPayloadLength ) { proxOutput . writeVInt ( delta * 2 ) ; } else { proxOutput . writeVInt ( delta * 2 + 1 ) ; proxOutput . writeVInt ( payloadLength ) ; lastPayloadLength = payloadLength ; } if ( payloadLength > 0 ) { if ( payloadBuffer == null || payloadBuffer . length < payloadLength ) { payloadBuffer = new byte [ payloadLength ] ; } postings . getPayload ( payloadBuffer , 0 ) ; proxOutput . writeBytes ( payloadBuffer , 0 , payloadLength ) ; } } else { proxOutput . writeVInt ( delta ) ; } lastPosition = position ; } } } return df ; } private final int appendPostingsNoTf ( SegmentMergeInfo [ ] smis , int n ) throws CorruptIndexException , IOException { int lastDoc = 0 ; int df = 0 ; skipListWriter . resetSkip ( ) ; int lastPayloadLength = - 1 ; for ( int i = 0 ; i < n ; i ++ ) { SegmentMergeInfo smi = smis [ i ] ; TermPositions postings = smi . getPositions ( ) ; assert postings != null ; int base = smi . base ; int [ ] docMap = smi . getDocMap ( ) ; postings . seek ( smi . termEnum ) ; while ( postings . next ( ) ) { int doc = postings . doc ( ) ; if ( docMap != null ) doc = docMap [ doc ] ; doc += base ; if ( doc < 0 || ( df > 0 && doc <= lastDoc ) ) throw new CorruptIndexException ( "docs out of order (" + doc + " <= " + lastDoc + " )" ) ; df ++ ; if ( ( df % skipInterval ) == 0 ) { skipListWriter . setSkipData ( lastDoc , false , lastPayloadLength ) ; skipListWriter . bufferSkip ( df ) ; } int docCode = ( doc - lastDoc ) ; lastDoc = doc ; freqOutput . writeVInt ( docCode ) ; } } return df ; } private void mergeNorms ( ) throws IOException { byte [ ] normBuffer = null ; IndexOutput output = null ; try { for ( int i = 0 ; i < fieldInfos . size ( ) ; i ++ ) { FieldInfo fi = fieldInfos . fieldInfo ( i ) ; if ( fi . isIndexed && ! fi . omitNorms ) { if ( output == null ) { output = directory . createOutput ( segment + "." + IndexFileNames . NORMS_EXTENSION ) ; output . writeBytes ( NORMS_HEADER , NORMS_HEADER . length ) ; } for ( int j = 0 ; j < readers . size ( ) ; j ++ ) { IndexReader reader = ( IndexReader ) readers . get ( j ) ; int maxDoc = reader . maxDoc ( ) ; if ( normBuffer == null || normBuffer . length < maxDoc ) { normBuffer = new byte [ maxDoc ] ; } reader . norms ( fi . name , normBuffer , 0 ) ; if ( ! reader . hasDeletions ( ) ) { output . writeBytes ( normBuffer , maxDoc ) ; } else { for ( int k = 0 ; k < maxDoc ; k ++ ) { if ( ! reader . isDeleted ( k ) ) { output . writeByte ( normBuffer [ k ] ) ; } } } if ( checkAbort != null ) checkAbort . work ( maxDoc ) ; } } } } finally { if ( output != null ) { output . close ( ) ; } } } final static class CheckAbort { private double workCount ; private MergePolicy . OneMerge merge ; private Directory dir ; public CheckAbort ( MergePolicy . OneMerge merge , Directory dir ) { this . merge = merge ; this . dir = dir ; } public void work ( double units ) throws MergePolicy . MergeAbortedException { workCount += units ; if ( workCount >= 10000.0 ) { merge . checkAborted ( dir ) ; workCount = 0 ; } } } } 	1	['23', '1', '0', '31', '128', '39', '4', '30', '0', '0.812937063', '1901', '0.884615385', '10', '0', '0.169421488', '0', '0', '80.52173913', '6', '1.087', '4']
package org . apache . lucene . index ; import java . io . IOException ; final class FreqProxFieldMergeState { final FreqProxTermsWriterPerField field ; final int numPostings ; final CharBlockPool charPool ; final RawPostingList [ ] postings ; private FreqProxTermsWriter . PostingList p ; char [ ] text ; int textOffset ; private int postingUpto = - 1 ; final ByteSliceReader freq = new ByteSliceReader ( ) ; final ByteSliceReader prox = new ByteSliceReader ( ) ; int docID ; int termFreq ; public FreqProxFieldMergeState ( FreqProxTermsWriterPerField field ) { this . field = field ; this . charPool = field . perThread . termsHashPerThread . charPool ; this . numPostings = field . termsHashPerField . numPostings ; this . postings = field . termsHashPerField . sortPostings ( ) ; } boolean nextTerm ( ) throws IOException { postingUpto ++ ; if ( postingUpto == numPostings ) return false ; p = ( FreqProxTermsWriter . PostingList ) postings [ postingUpto ] ; docID = 0 ; text = charPool . buffers [ p . textStart > > DocumentsWriter . CHAR_BLOCK_SHIFT ] ; textOffset = p . textStart & DocumentsWriter . CHAR_BLOCK_MASK ; field . termsHashPerField . initReader ( freq , p , 0 ) ; if ( ! field . fieldInfo . omitTf ) field . termsHashPerField . initReader ( prox , p , 1 ) ; boolean result = nextDoc ( ) ; assert result ; return true ; } public boolean nextDoc ( ) throws IOException { if ( freq . eof ( ) ) { if ( p . lastDocCode != - 1 ) { docID = p . lastDocID ; if ( ! field . omitTf ) termFreq = p . docFreq ; p . lastDocCode = - 1 ; return true ; } else return false ; } final int code = freq . readVInt ( ) ; if ( field . omitTf ) docID += code ; else { docID += code > > > 1 ; if ( ( code & 1 ) != 0 ) termFreq = 1 ; else termFreq = freq . readVInt ( ) ; } assert docID != p . lastDocID ; return true ; } } 	0	['5', '1', '0', '10', '16', '0', '1', '9', '2', '0.75', '238', '0.142857143', '6', '0', '0.416666667', '0', '0', '43.8', '1', '0.6', '0']
package org . apache . lucene . index ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . document . Document ; import org . apache . lucene . search . Similarity ; import org . apache . lucene . search . Query ; import org . apache . lucene . search . IndexSearcher ; import org . apache . lucene . search . Scorer ; import org . apache . lucene . search . Weight ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . AlreadyClosedException ; import org . apache . lucene . util . ArrayUtil ; import java . io . IOException ; import java . io . PrintStream ; import java . util . Collection ; import java . util . Iterator ; import java . util . List ; import java . util . HashMap ; import java . util . HashSet ; import java . util . ArrayList ; import java . util . Map . Entry ; import java . text . NumberFormat ; final class DocumentsWriter { IndexWriter writer ; Directory directory ; String segment ; private String docStoreSegment ; private int docStoreOffset ; private int nextDocID ; private int numDocsInRAM ; int numDocsInStore ; private final static int MAX_THREAD_STATE = 5 ; private DocumentsWriterThreadState [ ] threadStates = new DocumentsWriterThreadState [ 0 ] ; private final HashMap threadBindings = new HashMap ( ) ; private int pauseThreads ; boolean flushPending ; boolean bufferIsFull ; private boolean aborting ; private DocFieldProcessor docFieldProcessor ; PrintStream infoStream ; int maxFieldLength = IndexWriter . DEFAULT_MAX_FIELD_LENGTH ; Similarity similarity ; List newFiles ; static class DocState { DocumentsWriter docWriter ; Analyzer analyzer ; int maxFieldLength ; PrintStream infoStream ; Similarity similarity ; int docID ; Document doc ; String maxTermPrefix ; public boolean testPoint ( String name ) { return docWriter . writer . testPoint ( name ) ; } } static class FlushState { DocumentsWriter docWriter ; Directory directory ; String segmentName ; String docStoreSegmentName ; int numDocsInRAM ; int numDocsInStore ; Collection flushedFiles ; public String segmentFileName ( String ext ) { return segmentName + "." + ext ; } } abstract static class DocWriter { DocWriter next ; int docID ; abstract void finish ( ) throws IOException ; abstract void abort ( ) ; abstract long sizeInBytes ( ) ; void setNext ( DocWriter next ) { this . next = next ; } } ; final DocConsumer consumer ; private BufferedDeletes deletesInRAM = new BufferedDeletes ( ) ; private BufferedDeletes deletesFlushed = new BufferedDeletes ( ) ; private int maxBufferedDeleteTerms = IndexWriter . DEFAULT_MAX_BUFFERED_DELETE_TERMS ; private long ramBufferSize = ( long ) ( IndexWriter . DEFAULT_RAM_BUFFER_SIZE_MB * 1024 * 1024 ) ; private long waitQueuePauseBytes = ( long ) ( ramBufferSize * 0.1 ) ; private long waitQueueResumeBytes = ( long ) ( ramBufferSize * 0.05 ) ; private long freeTrigger = ( long ) ( IndexWriter . DEFAULT_RAM_BUFFER_SIZE_MB * 1024 * 1024 * 1.05 ) ; private long freeLevel = ( long ) ( IndexWriter . DEFAULT_RAM_BUFFER_SIZE_MB * 1024 * 1024 * 0.95 ) ; private int maxBufferedDocs = IndexWriter . DEFAULT_MAX_BUFFERED_DOCS ; private int flushedDocCount ; synchronized void updateFlushedDocCount ( int n ) { flushedDocCount += n ; } synchronized int getFlushedDocCount ( ) { return flushedDocCount ; } synchronized void setFlushedDocCount ( int n ) { flushedDocCount = n ; } private boolean closed ; DocumentsWriter ( Directory directory , IndexWriter writer ) throws IOException { this . directory = directory ; this . writer = writer ; this . similarity = writer . getSimilarity ( ) ; flushedDocCount = writer . maxDoc ( ) ; final TermsHashConsumer termVectorsWriter = new TermVectorsTermsWriter ( this ) ; final TermsHashConsumer freqProxWriter = new FreqProxTermsWriter ( ) ; final InvertedDocConsumer termsHash = new TermsHash ( this , true , freqProxWriter , new TermsHash ( this , false , termVectorsWriter , null ) ) ; final NormsWriter normsWriter = new NormsWriter ( ) ; final DocInverter docInverter = new DocInverter ( termsHash , normsWriter ) ; final StoredFieldsWriter fieldsWriter = new StoredFieldsWriter ( this ) ; final DocFieldConsumers docFieldConsumers = new DocFieldConsumers ( docInverter , fieldsWriter ) ; consumer = docFieldProcessor = new DocFieldProcessor ( this , docFieldConsumers ) ; } boolean hasProx ( ) { return docFieldProcessor . fieldInfos . hasProx ( ) ; } synchronized void setInfoStream ( PrintStream infoStream ) { this . infoStream = infoStream ; for ( int i = 0 ; i < threadStates . length ; i ++ ) threadStates [ i ] . docState . infoStream = infoStream ; } synchronized void setMaxFieldLength ( int maxFieldLength ) { this . maxFieldLength = maxFieldLength ; for ( int i = 0 ; i < threadStates . length ; i ++ ) threadStates [ i ] . docState . maxFieldLength = maxFieldLength ; } synchronized void setSimilarity ( Similarity similarity ) { this . similarity = similarity ; for ( int i = 0 ; i < threadStates . length ; i ++ ) threadStates [ i ] . docState . similarity = similarity ; } synchronized void setRAMBufferSizeMB ( double mb ) { if ( mb == IndexWriter . DISABLE_AUTO_FLUSH ) { ramBufferSize = IndexWriter . DISABLE_AUTO_FLUSH ; waitQueuePauseBytes = 4 * 1024 * 1024 ; waitQueueResumeBytes = 2 * 1024 * 1024 ; } else { ramBufferSize = ( long ) ( mb * 1024 * 1024 ) ; waitQueuePauseBytes = ( long ) ( ramBufferSize * 0.1 ) ; waitQueueResumeBytes = ( long ) ( ramBufferSize * 0.05 ) ; freeTrigger = ( long ) ( 1.05 * ramBufferSize ) ; freeLevel = ( long ) ( 0.95 * ramBufferSize ) ; } } synchronized double getRAMBufferSizeMB ( ) { if ( ramBufferSize == IndexWriter . DISABLE_AUTO_FLUSH ) { return ramBufferSize ; } else { return ramBufferSize / 1024. / 1024. ; } } void setMaxBufferedDocs ( int count ) { maxBufferedDocs = count ; } int getMaxBufferedDocs ( ) { return maxBufferedDocs ; } String getSegment ( ) { return segment ; } int getNumDocsInRAM ( ) { return numDocsInRAM ; } synchronized String getDocStoreSegment ( ) { return docStoreSegment ; } int getDocStoreOffset ( ) { return docStoreOffset ; } synchronized String closeDocStore ( ) throws IOException { assert allThreadsIdle ( ) ; if ( infoStream != null ) message ( "closeDocStore: " + openFiles . size ( ) + " files to flush to segment " + docStoreSegment + " numDocs=" + numDocsInStore ) ; boolean success = false ; try { initFlushState ( true ) ; closedFiles . clear ( ) ; consumer . closeDocStore ( flushState ) ; assert 0 == openFiles . size ( ) ; String s = docStoreSegment ; docStoreSegment = null ; docStoreOffset = 0 ; numDocsInStore = 0 ; success = true ; return s ; } finally { if ( ! success ) { abort ( ) ; } } } private Collection abortedFiles ; private FlushState flushState ; Collection abortedFiles ( ) { return abortedFiles ; } void message ( String message ) { writer . message ( "DW: " + message ) ; } final List openFiles = new ArrayList ( ) ; final List closedFiles = new ArrayList ( ) ; synchronized List openFiles ( ) { return ( List ) ( ( ArrayList ) openFiles ) . clone ( ) ; } synchronized List closedFiles ( ) { return ( List ) ( ( ArrayList ) closedFiles ) . clone ( ) ; } synchronized void addOpenFile ( String name ) { assert ! openFiles . contains ( name ) ; openFiles . add ( name ) ; } synchronized void removeOpenFile ( String name ) { assert openFiles . contains ( name ) ; openFiles . remove ( name ) ; closedFiles . add ( name ) ; } synchronized void setAborting ( ) { aborting = true ; } synchronized void abort ( ) throws IOException { try { message ( "docWriter: now abort" ) ; waitQueue . abort ( ) ; pauseAllThreads ( ) ; try { assert 0 == waitQueue . numWaiting ; waitQueue . waitingBytes = 0 ; try { abortedFiles = openFiles ( ) ; } catch ( Throwable t ) { abortedFiles = null ; } deletesInRAM . clear ( ) ; openFiles . clear ( ) ; for ( int i = 0 ; i < threadStates . length ; i ++ ) try { threadStates [ i ] . consumer . abort ( ) ; } catch ( Throwable t ) { } try { consumer . abort ( ) ; } catch ( Throwable t ) { } docStoreSegment = null ; numDocsInStore = 0 ; docStoreOffset = 0 ; doAfterFlush ( ) ; } finally { resumeAllThreads ( ) ; } } finally { aborting = false ; notifyAll ( ) ; } } private void doAfterFlush ( ) throws IOException { assert allThreadsIdle ( ) ; threadBindings . clear ( ) ; waitQueue . reset ( ) ; segment = null ; numDocsInRAM = 0 ; nextDocID = 0 ; bufferIsFull = false ; flushPending = false ; for ( int i = 0 ; i < threadStates . length ; i ++ ) threadStates [ i ] . doAfterFlush ( ) ; numBytesUsed = 0 ; } synchronized boolean pauseAllThreads ( ) { pauseThreads ++ ; while ( ! allThreadsIdle ( ) ) { try { wait ( ) ; } catch ( InterruptedException e ) { Thread . currentThread ( ) . interrupt ( ) ; } } return aborting ; } synchronized void resumeAllThreads ( ) { pauseThreads -- ; assert pauseThreads >= 0 ; if ( 0 == pauseThreads ) notifyAll ( ) ; } private synchronized boolean allThreadsIdle ( ) { for ( int i = 0 ; i < threadStates . length ; i ++ ) if ( ! threadStates [ i ] . isIdle ) return false ; return true ; } synchronized private void initFlushState ( boolean onlyDocStore ) { initSegmentName ( onlyDocStore ) ; if ( flushState == null ) { flushState = new FlushState ( ) ; flushState . directory = directory ; flushState . docWriter = this ; } flushState . docStoreSegmentName = docStoreSegment ; flushState . segmentName = segment ; flushState . numDocsInRAM = numDocsInRAM ; flushState . numDocsInStore = numDocsInStore ; flushState . flushedFiles = new HashSet ( ) ; } synchronized int flush ( boolean closeDocStore ) throws IOException { assert allThreadsIdle ( ) ; assert numDocsInRAM > 0 ; assert nextDocID == numDocsInRAM ; assert waitQueue . numWaiting == 0 ; assert waitQueue . waitingBytes == 0 ; initFlushState ( false ) ; docStoreOffset = numDocsInStore ; if ( infoStream != null ) message ( "flush postings as segment " + flushState . segmentName + " numDocs=" + numDocsInRAM ) ; boolean success = false ; try { if ( closeDocStore ) { assert flushState . docStoreSegmentName != null ; assert flushState . docStoreSegmentName . equals ( flushState . segmentName ) ; closeDocStore ( ) ; flushState . numDocsInStore = 0 ; } Collection threads = new HashSet ( ) ; for ( int i = 0 ; i < threadStates . length ; i ++ ) threads . add ( threadStates [ i ] . consumer ) ; consumer . flush ( threads , flushState ) ; if ( infoStream != null ) { final long newSegmentSize = segmentSize ( flushState . segmentName ) ; String message = "  oldRAMSize=" + numBytesUsed + " newFlushedSize=" + newSegmentSize + " docs/MB=" + nf . format ( numDocsInRAM / ( newSegmentSize / 1024. / 1024. ) ) + " new/old=" + nf . format ( 100.0 * newSegmentSize / numBytesUsed ) + "%" ; message ( message ) ; } flushedDocCount += flushState . numDocsInRAM ; doAfterFlush ( ) ; success = true ; } finally { if ( ! success ) { abort ( ) ; } } assert waitQueue . waitingBytes == 0 ; return flushState . numDocsInRAM ; } void createCompoundFile ( String segment ) throws IOException { CompoundFileWriter cfsWriter = new CompoundFileWriter ( directory , segment + "." + IndexFileNames . COMPOUND_FILE_EXTENSION ) ; Iterator it = flushState . flushedFiles . iterator ( ) ; while ( it . hasNext ( ) ) cfsWriter . addFile ( ( String ) it . next ( ) ) ; cfsWriter . close ( ) ; } synchronized boolean setFlushPending ( ) { if ( flushPending ) return false ; else { flushPending = true ; return true ; } } synchronized void clearFlushPending ( ) { flushPending = false ; } synchronized void pushDeletes ( ) { deletesFlushed . update ( deletesInRAM ) ; } synchronized void close ( ) { closed = true ; notifyAll ( ) ; } synchronized void initSegmentName ( boolean onlyDocStore ) { if ( segment == null && ( ! onlyDocStore || docStoreSegment == null ) ) { segment = writer . newSegmentName ( ) ; assert numDocsInRAM == 0 ; } if ( docStoreSegment == null ) { docStoreSegment = segment ; assert numDocsInStore == 0 ; } } synchronized DocumentsWriterThreadState getThreadState ( Document doc , Term delTerm ) throws IOException { DocumentsWriterThreadState state = ( DocumentsWriterThreadState ) threadBindings . get ( Thread . currentThread ( ) ) ; if ( state == null ) { DocumentsWriterThreadState minThreadState = null ; for ( int i = 0 ; i < threadStates . length ; i ++ ) { DocumentsWriterThreadState ts = threadStates [ i ] ; if ( minThreadState == null || ts . numThreads < minThreadState . numThreads ) minThreadState = ts ; } if ( minThreadState != null && ( minThreadState . numThreads == 0 || threadStates . length >= MAX_THREAD_STATE ) ) { state = minThreadState ; state . numThreads ++ ; } else { DocumentsWriterThreadState [ ] newArray = new DocumentsWriterThreadState [ 1 + threadStates . length ] ; if ( threadStates . length > 0 ) System . arraycopy ( threadStates , 0 , newArray , 0 , threadStates . length ) ; state = newArray [ threadStates . length ] = new DocumentsWriterThreadState ( this ) ; threadStates = newArray ; } threadBindings . put ( Thread . currentThread ( ) , state ) ; } waitReady ( state ) ; initSegmentName ( false ) ; state . isIdle = false ; boolean success = false ; try { state . docState . docID = nextDocID ; assert writer . testPoint ( "DocumentsWriter.ThreadState.init start" ) ; if ( delTerm != null ) { addDeleteTerm ( delTerm , state . docState . docID ) ; state . doFlushAfter = timeToFlushDeletes ( ) ; } assert writer . testPoint ( "DocumentsWriter.ThreadState.init after delTerm" ) ; nextDocID ++ ; numDocsInRAM ++ ; if ( ! flushPending && maxBufferedDocs != IndexWriter . DISABLE_AUTO_FLUSH && numDocsInRAM >= maxBufferedDocs ) { flushPending = true ; state . doFlushAfter = true ; } success = true ; } finally { if ( ! success ) { state . isIdle = true ; notifyAll ( ) ; if ( state . doFlushAfter ) { state . doFlushAfter = false ; flushPending = false ; } } } return state ; } boolean addDocument ( Document doc , Analyzer analyzer ) throws CorruptIndexException , IOException { return updateDocument ( doc , analyzer , null ) ; } boolean updateDocument ( Term t , Document doc , Analyzer analyzer ) throws CorruptIndexException , IOException { return updateDocument ( doc , analyzer , t ) ; } boolean updateDocument ( Document doc , Analyzer analyzer , Term delTerm ) throws CorruptIndexException , IOException { final DocumentsWriterThreadState state = getThreadState ( doc , delTerm ) ; final DocState docState = state . docState ; docState . doc = doc ; docState . analyzer = analyzer ; boolean success = false ; try { final DocWriter perDoc = state . consumer . processDocument ( ) ; finishDocument ( state , perDoc ) ; success = true ; } finally { if ( ! success ) { synchronized ( this ) { if ( aborting ) { state . isIdle = true ; notifyAll ( ) ; abort ( ) ; } else { skipDocWriter . docID = docState . docID ; boolean success2 = false ; try { waitQueue . add ( skipDocWriter ) ; success2 = true ; } finally { if ( ! success2 ) { state . isIdle = true ; notifyAll ( ) ; abort ( ) ; return false ; } } state . isIdle = true ; notifyAll ( ) ; if ( state . doFlushAfter ) { state . doFlushAfter = false ; flushPending = false ; notifyAll ( ) ; } addDeleteDocID ( state . docState . docID ) ; } } } } return state . doFlushAfter || timeToFlushDeletes ( ) ; } synchronized int getNumBufferedDeleteTerms ( ) { return deletesInRAM . numTerms ; } synchronized HashMap getBufferedDeleteTerms ( ) { return deletesInRAM . terms ; } synchronized void remapDeletes ( SegmentInfos infos , int [ ] [ ] docMaps , int [ ] delCounts , MergePolicy . OneMerge merge , int mergeDocCount ) { if ( docMaps == null ) return ; MergeDocIDRemapper mapper = new MergeDocIDRemapper ( infos , docMaps , delCounts , merge , mergeDocCount ) ; deletesInRAM . remap ( mapper , infos , docMaps , delCounts , merge , mergeDocCount ) ; deletesFlushed . remap ( mapper , infos , docMaps , delCounts , merge , mergeDocCount ) ; flushedDocCount -= mapper . docShift ; } synchronized private void waitReady ( DocumentsWriterThreadState state ) { while ( ! closed && ( ( state != null && ! state . isIdle ) || pauseThreads != 0 || flushPending || aborting ) ) { try { wait ( ) ; } catch ( InterruptedException e ) { Thread . currentThread ( ) . interrupt ( ) ; } } if ( closed ) throw new AlreadyClosedException ( "this IndexWriter is closed" ) ; } synchronized boolean bufferDeleteTerms ( Term [ ] terms ) throws IOException { waitReady ( null ) ; for ( int i = 0 ; i < terms . length ; i ++ ) addDeleteTerm ( terms [ i ] , numDocsInRAM ) ; return timeToFlushDeletes ( ) ; } synchronized boolean bufferDeleteTerm ( Term term ) throws IOException { waitReady ( null ) ; addDeleteTerm ( term , numDocsInRAM ) ; return timeToFlushDeletes ( ) ; } synchronized boolean bufferDeleteQueries ( Query [ ] queries ) throws IOException { waitReady ( null ) ; for ( int i = 0 ; i < queries . length ; i ++ ) addDeleteQuery ( queries [ i ] , numDocsInRAM ) ; return timeToFlushDeletes ( ) ; } synchronized boolean bufferDeleteQuery ( Query query ) throws IOException { waitReady ( null ) ; addDeleteQuery ( query , numDocsInRAM ) ; return timeToFlushDeletes ( ) ; } synchronized boolean deletesFull ( ) { return maxBufferedDeleteTerms != IndexWriter . DISABLE_AUTO_FLUSH && ( ( deletesInRAM . numTerms + deletesInRAM . queries . size ( ) + deletesInRAM . docIDs . size ( ) ) >= maxBufferedDeleteTerms ) ; } synchronized private boolean timeToFlushDeletes ( ) { return ( bufferIsFull || deletesFull ( ) ) && setFlushPending ( ) ; } void setMaxBufferedDeleteTerms ( int maxBufferedDeleteTerms ) { this . maxBufferedDeleteTerms = maxBufferedDeleteTerms ; } int getMaxBufferedDeleteTerms ( ) { return maxBufferedDeleteTerms ; } synchronized boolean hasDeletes ( ) { return deletesFlushed . any ( ) ; } synchronized boolean applyDeletes ( SegmentInfos infos ) throws IOException { if ( ! hasDeletes ( ) ) return false ; if ( infoStream != null ) message ( "apply " + deletesFlushed . numTerms + " buffered deleted terms and " + deletesFlushed . docIDs . size ( ) + " deleted docIDs and " + deletesFlushed . queries . size ( ) + " deleted queries on " + + infos . size ( ) + " segments." ) ; final int infosEnd = infos . size ( ) ; int docStart = 0 ; boolean any = false ; for ( int i = 0 ; i < infosEnd ; i ++ ) { IndexReader reader = SegmentReader . get ( infos . info ( i ) , false ) ; boolean success = false ; try { any |= applyDeletes ( reader , docStart ) ; docStart += reader . maxDoc ( ) ; success = true ; } finally { if ( reader != null ) { try { if ( success ) reader . doCommit ( ) ; } finally { reader . doClose ( ) ; } } } } deletesFlushed . clear ( ) ; return any ; } private final synchronized boolean applyDeletes ( IndexReader reader , int docIDStart ) throws CorruptIndexException , IOException { final int docEnd = docIDStart + reader . maxDoc ( ) ; boolean any = false ; Iterator iter = deletesFlushed . terms . entrySet ( ) . iterator ( ) ; while ( iter . hasNext ( ) ) { Entry entry = ( Entry ) iter . next ( ) ; Term term = ( Term ) entry . getKey ( ) ; TermDocs docs = reader . termDocs ( term ) ; if ( docs != null ) { int limit = ( ( BufferedDeletes . Num ) entry . getValue ( ) ) . getNum ( ) ; try { while ( docs . next ( ) ) { int docID = docs . doc ( ) ; if ( docIDStart + docID >= limit ) break ; reader . deleteDocument ( docID ) ; any = true ; } } finally { docs . close ( ) ; } } } iter = deletesFlushed . docIDs . iterator ( ) ; while ( iter . hasNext ( ) ) { int docID = ( ( Integer ) iter . next ( ) ) . intValue ( ) ; if ( docID >= docIDStart && docID < docEnd ) { reader . deleteDocument ( docID - docIDStart ) ; any = true ; } } IndexSearcher searcher = new IndexSearcher ( reader ) ; iter = deletesFlushed . queries . entrySet ( ) . iterator ( ) ; while ( iter . hasNext ( ) ) { Entry entry = ( Entry ) iter . next ( ) ; Query query = ( Query ) entry . getKey ( ) ; int limit = ( ( Integer ) entry . getValue ( ) ) . intValue ( ) ; Weight weight = query . weight ( searcher ) ; Scorer scorer = weight . scorer ( reader ) ; while ( scorer . next ( ) ) { final int docID = scorer . doc ( ) ; if ( docIDStart + docID >= limit ) break ; reader . deleteDocument ( docID ) ; any = true ; } } searcher . close ( ) ; return any ; } synchronized private void addDeleteTerm ( Term term , int docCount ) { BufferedDeletes . Num num = ( BufferedDeletes . Num ) deletesInRAM . terms . get ( term ) ; final int docIDUpto = flushedDocCount + docCount ; if ( num == null ) deletesInRAM . terms . put ( term , new BufferedDeletes . Num ( docIDUpto ) ) ; else num . setNum ( docIDUpto ) ; deletesInRAM . numTerms ++ ; } synchronized private void addDeleteDocID ( int docID ) { deletesInRAM . docIDs . add ( new Integer ( flushedDocCount + docID ) ) ; } synchronized private void addDeleteQuery ( Query query , int docID ) { deletesInRAM . queries . put ( query , new Integer ( flushedDocCount + docID ) ) ; } synchronized boolean doBalanceRAM ( ) { return ramBufferSize != IndexWriter . DISABLE_AUTO_FLUSH && ! bufferIsFull && ( numBytesUsed >= ramBufferSize || numBytesAlloc >= freeTrigger ) ; } private void finishDocument ( DocumentsWriterThreadState perThread , DocWriter docWriter ) throws IOException { if ( doBalanceRAM ( ) ) balanceRAM ( ) ; synchronized ( this ) { assert docWriter == null || docWriter . docID == perThread . docState . docID ; if ( aborting ) { if ( docWriter != null ) try { docWriter . abort ( ) ; } catch ( Throwable t ) { } perThread . isIdle = true ; notifyAll ( ) ; return ; } final boolean doPause ; if ( docWriter != null ) doPause = waitQueue . add ( docWriter ) ; else { skipDocWriter . docID = perThread . docState . docID ; doPause = waitQueue . add ( skipDocWriter ) ; } if ( doPause ) waitForWaitQueue ( ) ; if ( bufferIsFull && ! flushPending ) { flushPending = true ; perThread . doFlushAfter = true ; } perThread . isIdle = true ; notifyAll ( ) ; } } synchronized void waitForWaitQueue ( ) { do { try { wait ( ) ; } catch ( InterruptedException e ) { Thread . currentThread ( ) . interrupt ( ) ; } } while ( ! waitQueue . doResume ( ) ) ; } private static class SkipDocWriter extends DocWriter { void finish ( ) { } void abort ( ) { } long sizeInBytes ( ) { return 0 ; } } final SkipDocWriter skipDocWriter = new SkipDocWriter ( ) ; long getRAMUsed ( ) { return numBytesUsed ; } long numBytesAlloc ; long numBytesUsed ; NumberFormat nf = NumberFormat . getInstance ( ) ; private long segmentSize ( String segmentName ) throws IOException { assert infoStream != null ; long size = directory . fileLength ( segmentName + ".tii" ) + directory . fileLength ( segmentName + ".tis" ) + directory . fileLength ( segmentName + ".frq" ) + directory . fileLength ( segmentName + ".prx" ) ; final String normFileName = segmentName + ".nrm" ; if ( directory . fileExists ( normFileName ) ) size += directory . fileLength ( normFileName ) ; return size ; } final static int OBJECT_HEADER_BYTES = 8 ; final static int POINTER_NUM_BYTE = 4 ; final static int INT_NUM_BYTE = 4 ; final static int CHAR_NUM_BYTE = 2 ; final static int BYTE_BLOCK_SHIFT = 15 ; final static int BYTE_BLOCK_SIZE = ( int ) ( 1 << BYTE_BLOCK_SHIFT ) ; final static int BYTE_BLOCK_MASK = BYTE_BLOCK_SIZE - 1 ; final static int BYTE_BLOCK_NOT_MASK = ~ BYTE_BLOCK_MASK ; private class ByteBlockAllocator extends ByteBlockPool . Allocator { ArrayList freeByteBlocks = new ArrayList ( ) ; byte [ ] getByteBlock ( boolean trackAllocations ) { synchronized ( DocumentsWriter . this ) { final int size = freeByteBlocks . size ( ) ; final byte [ ] b ; if ( 0 == size ) { numBytesAlloc += BYTE_BLOCK_SIZE ; b = new byte [ BYTE_BLOCK_SIZE ] ; } else b = ( byte [ ] ) freeByteBlocks . remove ( size - 1 ) ; if ( trackAllocations ) numBytesUsed += BYTE_BLOCK_SIZE ; assert numBytesUsed <= numBytesAlloc ; return b ; } } void recycleByteBlocks ( byte [ ] [ ] blocks , int start , int end ) { synchronized ( DocumentsWriter . this ) { for ( int i = start ; i < end ; i ++ ) freeByteBlocks . add ( blocks [ i ] ) ; } } } final static int INT_BLOCK_SHIFT = 13 ; final static int INT_BLOCK_SIZE = ( int ) ( 1 << INT_BLOCK_SHIFT ) ; final static int INT_BLOCK_MASK = INT_BLOCK_SIZE - 1 ; private ArrayList freeIntBlocks = new ArrayList ( ) ; synchronized int [ ] getIntBlock ( boolean trackAllocations ) { final int size = freeIntBlocks . size ( ) ; final int [ ] b ; if ( 0 == size ) { numBytesAlloc += INT_BLOCK_SIZE * INT_NUM_BYTE ; b = new int [ INT_BLOCK_SIZE ] ; } else b = ( int [ ] ) freeIntBlocks . remove ( size - 1 ) ; if ( trackAllocations ) numBytesUsed += INT_BLOCK_SIZE * INT_NUM_BYTE ; assert numBytesUsed <= numBytesAlloc ; return b ; } synchronized void bytesAllocated ( long numBytes ) { numBytesAlloc += numBytes ; assert numBytesUsed <= numBytesAlloc ; } synchronized void bytesUsed ( long numBytes ) { numBytesUsed += numBytes ; assert numBytesUsed <= numBytesAlloc ; } synchronized void recycleIntBlocks ( int [ ] [ ] blocks , int start , int end ) { for ( int i = start ; i < end ; i ++ ) freeIntBlocks . add ( blocks [ i ] ) ; } ByteBlockAllocator byteBlockAllocator = new ByteBlockAllocator ( ) ; final static int CHAR_BLOCK_SHIFT = 14 ; final static int CHAR_BLOCK_SIZE = ( int ) ( 1 << CHAR_BLOCK_SHIFT ) ; final static int CHAR_BLOCK_MASK = CHAR_BLOCK_SIZE - 1 ; final static int MAX_TERM_LENGTH = CHAR_BLOCK_SIZE - 1 ; private ArrayList freeCharBlocks = new ArrayList ( ) ; synchronized char [ ] getCharBlock ( ) { final int size = freeCharBlocks . size ( ) ; final char [ ] c ; if ( 0 == size ) { numBytesAlloc += CHAR_BLOCK_SIZE * CHAR_NUM_BYTE ; c = new char [ CHAR_BLOCK_SIZE ] ; } else c = ( char [ ] ) freeCharBlocks . remove ( size - 1 ) ; numBytesUsed += CHAR_BLOCK_SIZE * CHAR_NUM_BYTE ; assert numBytesUsed <= numBytesAlloc ; return c ; } synchronized void recycleCharBlocks ( char [ ] [ ] blocks , int numBlocks ) { for ( int i = 0 ; i < numBlocks ; i ++ ) freeCharBlocks . add ( blocks [ i ] ) ; } String toMB ( long v ) { return nf . format ( v / 1024. / 1024. ) ; } void balanceRAM ( ) { final long flushTrigger = ( long ) ramBufferSize ; if ( numBytesAlloc > freeTrigger ) { if ( infoStream != null ) message ( "  RAM: now balance allocations: usedMB=" + toMB ( numBytesUsed ) + " vs trigger=" + toMB ( flushTrigger ) + " allocMB=" + toMB ( numBytesAlloc ) + " vs trigger=" + toMB ( freeTrigger ) + " byteBlockFree=" + toMB ( byteBlockAllocator . freeByteBlocks . size ( ) * BYTE_BLOCK_SIZE ) + " charBlockFree=" + toMB ( freeCharBlocks . size ( ) * CHAR_BLOCK_SIZE * CHAR_NUM_BYTE ) ) ; final long startBytesAlloc = numBytesAlloc ; int iter = 0 ; boolean any = true ; while ( numBytesAlloc > freeLevel ) { synchronized ( this ) { if ( 0 == byteBlockAllocator . freeByteBlocks . size ( ) && 0 == freeCharBlocks . size ( ) && 0 == freeIntBlocks . size ( ) && ! any ) { bufferIsFull = numBytesUsed > flushTrigger ; if ( infoStream != null ) { if ( numBytesUsed > flushTrigger ) message ( "    nothing to free; now set bufferIsFull" ) ; else message ( "    nothing to free" ) ; } assert numBytesUsed <= numBytesAlloc ; break ; } if ( ( 0 == iter % 4 ) && byteBlockAllocator . freeByteBlocks . size ( ) > 0 ) { byteBlockAllocator . freeByteBlocks . remove ( byteBlockAllocator . freeByteBlocks . size ( ) - 1 ) ; numBytesAlloc -= BYTE_BLOCK_SIZE ; } if ( ( 1 == iter % 4 ) && freeCharBlocks . size ( ) > 0 ) { freeCharBlocks . remove ( freeCharBlocks . size ( ) - 1 ) ; numBytesAlloc -= CHAR_BLOCK_SIZE * CHAR_NUM_BYTE ; } if ( ( 2 == iter % 4 ) && freeIntBlocks . size ( ) > 0 ) { freeIntBlocks . remove ( freeIntBlocks . size ( ) - 1 ) ; numBytesAlloc -= INT_BLOCK_SIZE * INT_NUM_BYTE ; } } if ( ( 3 == iter % 4 ) && any ) any = consumer . freeRAM ( ) ; iter ++ ; } if ( infoStream != null ) message ( "    after free: freedMB=" + nf . format ( ( startBytesAlloc - numBytesAlloc ) / 1024. / 1024. ) + " usedMB=" + nf . format ( numBytesUsed / 1024. / 1024. ) + " allocMB=" + nf . format ( numBytesAlloc / 1024. / 1024. ) ) ; } else { synchronized ( this ) { if ( numBytesUsed > flushTrigger ) { if ( infoStream != null ) message ( "  RAM: now flush @ usedMB=" + nf . format ( numBytesUsed / 1024. / 1024. ) + " allocMB=" + nf . format ( numBytesAlloc / 1024. / 1024. ) + " triggerMB=" + nf . format ( flushTrigger / 1024. / 1024. ) ) ; bufferIsFull = true ; } } } } final WaitQueue waitQueue = new WaitQueue ( ) ; private class WaitQueue { DocWriter [ ] waiting ; int nextWriteDocID ; int nextWriteLoc ; int numWaiting ; long waitingBytes ; public WaitQueue ( ) { waiting = new DocWriter [ 10 ] ; } synchronized void reset ( ) { assert numWaiting == 0 ; assert waitingBytes == 0 ; nextWriteDocID = 0 ; } synchronized boolean doResume ( ) { return waitingBytes <= waitQueueResumeBytes ; } synchronized boolean doPause ( ) { return waitingBytes > waitQueuePauseBytes ; } synchronized void abort ( ) { int count = 0 ; for ( int i = 0 ; i < waiting . length ; i ++ ) { final DocWriter doc = waiting [ i ] ; if ( doc != null ) { doc . abort ( ) ; waiting [ i ] = null ; count ++ ; } } waitingBytes = 0 ; assert count == numWaiting ; numWaiting = 0 ; } private void writeDocument ( DocWriter doc ) throws IOException { assert doc == skipDocWriter || nextWriteDocID == doc . docID ; boolean success = false ; try { doc . finish ( ) ; nextWriteDocID ++ ; numDocsInStore ++ ; nextWriteLoc ++ ; assert nextWriteLoc <= waiting . length ; if ( nextWriteLoc == waiting . length ) nextWriteLoc = 0 ; success = true ; } finally { if ( ! success ) setAborting ( ) ; } } synchronized public boolean add ( DocWriter doc ) throws IOException { assert doc . docID >= nextWriteDocID ; if ( doc . docID == nextWriteDocID ) { writeDocument ( doc ) ; while ( true ) { doc = waiting [ nextWriteLoc ] ; if ( doc != null ) { numWaiting -- ; waiting [ nextWriteLoc ] = null ; waitingBytes -= doc . sizeInBytes ( ) ; writeDocument ( doc ) ; } else break ; } } else { int gap = doc . docID - nextWriteDocID ; if ( gap >= waiting . length ) { DocWriter [ ] newArray = new DocWriter [ ArrayUtil . getNextSize ( gap ) ] ; assert nextWriteLoc >= 0 ; System . arraycopy ( waiting , nextWriteLoc , newArray , 0 , waiting . length - nextWriteLoc ) ; System . arraycopy ( waiting , 0 , newArray , waiting . length - nextWriteLoc , nextWriteLoc ) ; nextWriteLoc = 0 ; waiting = newArray ; gap = doc . docID - nextWriteDocID ; } int loc = nextWriteLoc + gap ; if ( loc >= waiting . length ) loc -= waiting . length ; assert loc < waiting . length ; assert waiting [ loc ] == null ; waiting [ loc ] = doc ; numWaiting ++ ; waitingBytes += doc . sizeInBytes ( ) ; } return doPause ( ) ; } } } 	1	['76', '1', '0', '53', '185', '1998', '18', '46', '0', '0.941202186', '2835', '0.409836066', '12', '0', '0.069866667', '0', '0', '35.5', '23', '2.0132', '9']
package org . apache . lucene . index ; import java . io . File ; import java . io . FilenameFilter ; import java . util . HashSet ; public class IndexFileNameFilter implements FilenameFilter { static IndexFileNameFilter singleton = new IndexFileNameFilter ( ) ; private HashSet extensions ; private HashSet extensionsInCFS ; public IndexFileNameFilter ( ) { extensions = new HashSet ( ) ; for ( int i = 0 ; i < IndexFileNames . INDEX_EXTENSIONS . length ; i ++ ) { extensions . add ( IndexFileNames . INDEX_EXTENSIONS [ i ] ) ; } extensionsInCFS = new HashSet ( ) ; for ( int i = 0 ; i < IndexFileNames . INDEX_EXTENSIONS_IN_COMPOUND_FILE . length ; i ++ ) { extensionsInCFS . add ( IndexFileNames . INDEX_EXTENSIONS_IN_COMPOUND_FILE [ i ] ) ; } } public boolean accept ( File dir , String name ) { int i = name . lastIndexOf ( '.' ) ; if ( i != - 1 ) { String extension = name . substring ( 1 + i ) ; if ( extensions . contains ( extension ) ) { return true ; } else if ( extension . startsWith ( "f" ) && extension . matches ( "f\\d+" ) ) { return true ; } else if ( extension . startsWith ( "s" ) && extension . matches ( "s\\d+" ) ) { return true ; } } else { if ( name . equals ( IndexFileNames . DELETABLE ) ) return true ; else if ( name . startsWith ( IndexFileNames . SEGMENTS ) ) return true ; } return false ; } public boolean isCFSFile ( String name ) { int i = name . lastIndexOf ( '.' ) ; if ( i != - 1 ) { String extension = name . substring ( 1 + i ) ; if ( extensionsInCFS . contains ( extension ) ) { return true ; } if ( extension . startsWith ( "f" ) && extension . matches ( "f\\d+" ) ) { return true ; } } return false ; } public static IndexFileNameFilter getFilter ( ) { return singleton ; } } 	0	['5', '1', '0', '3', '14', '4', '2', '1', '4', '0.583333333', '145', '0.666666667', '1', '0', '0.5', '0', '0', '27.4', '7', '2.6', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; import java . util . BitSet ; import java . util . Map ; import java . util . WeakHashMap ; public class CachingSpanFilter extends SpanFilter { protected SpanFilter filter ; protected transient Map cache ; public CachingSpanFilter ( SpanFilter filter ) { this . filter = filter ; } public BitSet bits ( IndexReader reader ) throws IOException { SpanFilterResult result = getCachedResult ( reader ) ; return result != null ? result . getBits ( ) : null ; } public DocIdSet getDocIdSet ( IndexReader reader ) throws IOException { SpanFilterResult result = getCachedResult ( reader ) ; return result != null ? result . getDocIdSet ( ) : null ; } private SpanFilterResult getCachedResult ( IndexReader reader ) throws IOException { SpanFilterResult result = null ; if ( cache == null ) { cache = new WeakHashMap ( ) ; } synchronized ( cache ) { result = ( SpanFilterResult ) cache . get ( reader ) ; if ( result == null ) { result = filter . bitSpans ( reader ) ; cache . put ( reader , result ) ; } } return result ; } public SpanFilterResult bitSpans ( IndexReader reader ) throws IOException { return getCachedResult ( reader ) ; } public String toString ( ) { return "CachingSpanFilter(" + filter + ")" ; } public boolean equals ( Object o ) { if ( ! ( o instanceof CachingSpanFilter ) ) return false ; return this . filter . equals ( ( ( CachingSpanFilter ) o ) . filter ) ; } public int hashCode ( ) { return filter . hashCode ( ) ^ 0x1117BF25 ; } } 	1	['8', '3', '0', '4', '21', '8', '0', '4', '7', '0.285714286', '116', '1', '1', '0.3', '0.4375', '2', '2', '13.25', '2', '1', '1']
package org . apache . lucene . index ; public abstract class TermVectorMapper { private boolean ignoringPositions ; private boolean ignoringOffsets ; protected TermVectorMapper ( ) { } protected TermVectorMapper ( boolean ignoringPositions , boolean ignoringOffsets ) { this . ignoringPositions = ignoringPositions ; this . ignoringOffsets = ignoringOffsets ; } public abstract void setExpectations ( String field , int numTerms , boolean storeOffsets , boolean storePositions ) ; public abstract void map ( String term , int frequency , TermVectorOffsetInfo [ ] offsets , int [ ] positions ) ; public boolean isIgnoringPositions ( ) { return ignoringPositions ; } public boolean isIgnoringOffsets ( ) { return ignoringOffsets ; } public void setDocumentNumber ( int documentNumber ) { } } 	0	['7', '1', '4', '12', '8', '17', '11', '1', '5', '0.833333333', '28', '1', '0', '0', '0.380952381', '0', '0', '2.714285714', '1', '0.7143', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; import java . io . IOException ; public abstract class Tokenizer extends TokenStream { protected Reader input ; protected Tokenizer ( ) { } protected Tokenizer ( Reader input ) { this . input = input ; } public void close ( ) throws IOException { input . close ( ) ; } public void reset ( Reader input ) throws IOException { this . input = input ; } } 	1	['4', '2', '4', '10', '6', '0', '9', '1', '2', '0.333333333', '22', '1', '0', '0.714285714', '0.75', '0', '0', '4.25', '1', '0.5', '4']
package org . apache . lucene . util ; public abstract class StringHelper { public static final int bytesDifference ( byte [ ] bytes1 , int len1 , byte [ ] bytes2 , int len2 ) { int len = len1 < len2 ? len1 : len2 ; for ( int i = 0 ; i < len ; i ++ ) if ( bytes1 [ i ] != bytes2 [ i ] ) return i ; return len ; } public static final int stringDifference ( String s1 , String s2 ) { int len1 = s1 . length ( ) ; int len2 = s2 . length ( ) ; int len = len1 < len2 ? len1 : len2 ; for ( int i = 0 ; i < len ; i ++ ) { if ( s1 . charAt ( i ) != s2 . charAt ( i ) ) { return i ; } } return len ; } private StringHelper ( ) { } } 	0	['3', '1', '0', '1', '6', '3', '1', '0', '2', '2', '62', '0', '0', '0', '0.333333333', '0', '0', '19.66666667', '4', '2.6667', '0']
package org . apache . lucene . search ; public class ComplexExplanation extends Explanation { private Boolean match ; public ComplexExplanation ( ) { super ( ) ; } public ComplexExplanation ( boolean match , float value , String description ) { super ( value , description ) ; this . match = Boolean . valueOf ( match ) ; } public Boolean getMatch ( ) { return match ; } public void setMatch ( Boolean match ) { this . match = match ; } public boolean isMatch ( ) { Boolean m = getMatch ( ) ; return ( null != m ? m . booleanValue ( ) : super . isMatch ( ) ) ; } protected String getSummary ( ) { if ( null == getMatch ( ) ) return super . getSummary ( ) ; return getValue ( ) + " = " + ( isMatch ( ) ? "(MATCH) " : "(NON-MATCH) " ) + getDescription ( ) ; } } 	1	['6', '2', '0', '11', '18', '9', '10', '1', '5', '0.2', '65', '1', '0', '0.733333333', '0.333333333', '1', '1', '9.666666667', '3', '1.1667', '1']
package org . apache . lucene . search ; import java . util . ArrayList ; import java . util . BitSet ; import java . util . List ; public class SpanFilterResult { private BitSet bits ; private DocIdSet docIdSet ; private List positions ; public SpanFilterResult ( BitSet bits , List positions ) { this . bits = bits ; this . positions = positions ; } public SpanFilterResult ( DocIdSet docIdSet , List positions ) { this . docIdSet = docIdSet ; this . positions = positions ; } public List getPositions ( ) { return positions ; } public BitSet getBits ( ) { return bits ; } public DocIdSet getDocIdSet ( ) { return docIdSet ; } public static class PositionInfo { private int doc ; private List positions ; public PositionInfo ( int doc ) { this . doc = doc ; positions = new ArrayList ( ) ; } public void addPosition ( int start , int end ) { positions . add ( new StartEnd ( start , end ) ) ; } public int getDoc ( ) { return doc ; } public List getPositions ( ) { return positions ; } } public static class StartEnd { private int start ; private int end ; public StartEnd ( int start , int end ) { this . start = start ; this . end = end ; } public int getEnd ( ) { return end ; } public int getStart ( ) { return start ; } } } 	0	['5', '1', '0', '4', '6', '0', '3', '1', '5', '0.666666667', '35', '1', '1', '0', '0.45', '0', '0', '5.4', '1', '0.6', '0']
package org . apache . lucene . search . spans ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . util . PriorityQueue ; import java . io . IOException ; import java . util . ArrayList ; import java . util . Collection ; import java . util . List ; import java . util . Set ; import java . util . HashSet ; class NearSpansUnordered implements PayloadSpans { private SpanNearQuery query ; private List ordered = new ArrayList ( ) ; private int slop ; private SpansCell first ; private SpansCell last ; private int totalLength ; private CellQueue queue ; private SpansCell max ; private boolean more = true ; private boolean firstTime = true ; private class CellQueue extends PriorityQueue { public CellQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object o1 , Object o2 ) { SpansCell spans1 = ( SpansCell ) o1 ; SpansCell spans2 = ( SpansCell ) o2 ; if ( spans1 . doc ( ) == spans2 . doc ( ) ) { return NearSpansOrdered . docSpansOrdered ( spans1 , spans2 ) ; } else { return spans1 . doc ( ) < spans2 . doc ( ) ; } } } private class SpansCell implements PayloadSpans { private PayloadSpans spans ; private SpansCell next ; private int length = - 1 ; private int index ; public SpansCell ( PayloadSpans spans , int index ) { this . spans = spans ; this . index = index ; } public boolean next ( ) throws IOException { return adjust ( spans . next ( ) ) ; } public boolean skipTo ( int target ) throws IOException { return adjust ( spans . skipTo ( target ) ) ; } private boolean adjust ( boolean condition ) { if ( length != - 1 ) { totalLength -= length ; } if ( condition ) { length = end ( ) - start ( ) ; totalLength += length ; if ( max == null || doc ( ) > max . doc ( ) || ( doc ( ) == max . doc ( ) ) && ( end ( ) > max . end ( ) ) ) { max = this ; } } more = condition ; return condition ; } public int doc ( ) { return spans . doc ( ) ; } public int start ( ) { return spans . start ( ) ; } public int end ( ) { return spans . end ( ) ; } public Collection getPayload ( ) throws IOException { return new ArrayList ( spans . getPayload ( ) ) ; } public boolean isPayloadAvailable ( ) { return spans . isPayloadAvailable ( ) ; } public String toString ( ) { return spans . toString ( ) + "#" + index ; } } public NearSpansUnordered ( SpanNearQuery query , IndexReader reader ) throws IOException { this . query = query ; this . slop = query . getSlop ( ) ; SpanQuery [ ] clauses = query . getClauses ( ) ; queue = new CellQueue ( clauses . length ) ; for ( int i = 0 ; i < clauses . length ; i ++ ) { SpansCell cell = new SpansCell ( clauses [ i ] . getPayloadSpans ( reader ) , i ) ; ordered . add ( cell ) ; } } public boolean next ( ) throws IOException { if ( firstTime ) { initList ( true ) ; listToQueue ( ) ; firstTime = false ; } else if ( more ) { if ( min ( ) . next ( ) ) { queue . adjustTop ( ) ; } else { more = false ; } } while ( more ) { boolean queueStale = false ; if ( min ( ) . doc ( ) != max . doc ( ) ) { queueToList ( ) ; queueStale = true ; } while ( more && first . doc ( ) < last . doc ( ) ) { more = first . skipTo ( last . doc ( ) ) ; firstToLast ( ) ; queueStale = true ; } if ( ! more ) return false ; if ( queueStale ) { listToQueue ( ) ; queueStale = false ; } if ( atMatch ( ) ) { return true ; } more = min ( ) . next ( ) ; if ( more ) { queue . adjustTop ( ) ; } } return false ; } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) { initList ( false ) ; for ( SpansCell cell = first ; more && cell != null ; cell = cell . next ) { more = cell . skipTo ( target ) ; } if ( more ) { listToQueue ( ) ; } firstTime = false ; } else { while ( more && min ( ) . doc ( ) < target ) { if ( min ( ) . skipTo ( target ) ) { queue . adjustTop ( ) ; } else { more = false ; } } } return more && ( atMatch ( ) || next ( ) ) ; } private SpansCell min ( ) { return ( SpansCell ) queue . top ( ) ; } public int doc ( ) { return min ( ) . doc ( ) ; } public int start ( ) { return min ( ) . start ( ) ; } public int end ( ) { return max . end ( ) ; } public Collection getPayload ( ) throws IOException { Set matchPayload = new HashSet ( ) ; for ( SpansCell cell = first ; cell != null ; cell = cell . next ) { if ( cell . isPayloadAvailable ( ) ) { matchPayload . addAll ( cell . getPayload ( ) ) ; } } return matchPayload ; } public boolean isPayloadAvailable ( ) { SpansCell pointer = min ( ) ; while ( pointer != null ) { if ( pointer . isPayloadAvailable ( ) ) { return true ; } pointer = pointer . next ; } return false ; } public String toString ( ) { return getClass ( ) . getName ( ) + "(" + query . toString ( ) + ")@" + ( firstTime ? "START" : ( more ? ( doc ( ) + ":" + start ( ) + "-" + end ( ) ) : "END" ) ) ; } private void initList ( boolean next ) throws IOException { for ( int i = 0 ; more && i < ordered . size ( ) ; i ++ ) { SpansCell cell = ( SpansCell ) ordered . get ( i ) ; if ( next ) more = cell . next ( ) ; if ( more ) { addToList ( cell ) ; } } } private void addToList ( SpansCell cell ) throws IOException { if ( last != null ) { last . next = cell ; } else first = cell ; last = cell ; cell . next = null ; } private void firstToLast ( ) { last . next = first ; last = first ; first = first . next ; last . next = null ; } private void queueToList ( ) throws IOException { last = first = null ; while ( queue . top ( ) != null ) { addToList ( ( SpansCell ) queue . pop ( ) ) ; } } private void listToQueue ( ) { queue . clear ( ) ; for ( SpansCell cell = first ; cell != null ; cell = cell . next ) { queue . put ( cell ) ; } } private boolean atMatch ( ) { return ( min ( ) . doc ( ) == max . doc ( ) ) && ( ( max . end ( ) - min ( ) . start ( ) - totalLength ) <= slop ) ; } } 	1	['21', '1', '0', '6', '54', '98', '3', '6', '9', '0.765', '509', '1', '5', '0', '0.204081633', '0', '0', '22.76190476', '3', '1.2857', '2']
package org . apache . lucene . store ; import java . io . IOException ; class RAMInputStream extends IndexInput implements Cloneable { static final int BUFFER_SIZE = RAMOutputStream . BUFFER_SIZE ; private RAMFile file ; private long length ; private byte [ ] currentBuffer ; private int currentBufferIndex ; private int bufferPosition ; private long bufferStart ; private int bufferLength ; RAMInputStream ( RAMFile f ) throws IOException { file = f ; length = file . length ; if ( length / BUFFER_SIZE >= Integer . MAX_VALUE ) { throw new IOException ( "Too large RAMFile! " + length ) ; } currentBufferIndex = - 1 ; currentBuffer = null ; } public void close ( ) { } public long length ( ) { return length ; } public byte readByte ( ) throws IOException { if ( bufferPosition >= bufferLength ) { currentBufferIndex ++ ; switchCurrentBuffer ( true ) ; } return currentBuffer [ bufferPosition ++ ] ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { while ( len > 0 ) { if ( bufferPosition >= bufferLength ) { currentBufferIndex ++ ; switchCurrentBuffer ( true ) ; } int remainInBuffer = bufferLength - bufferPosition ; int bytesToCopy = len < remainInBuffer ? len : remainInBuffer ; System . arraycopy ( currentBuffer , bufferPosition , b , offset , bytesToCopy ) ; offset += bytesToCopy ; len -= bytesToCopy ; bufferPosition += bytesToCopy ; } } private final void switchCurrentBuffer ( boolean enforceEOF ) throws IOException { if ( currentBufferIndex >= file . numBuffers ( ) ) { if ( enforceEOF ) throw new IOException ( "Read past EOF" ) ; else { currentBufferIndex -- ; bufferPosition = BUFFER_SIZE ; } } else { currentBuffer = ( byte [ ] ) file . getBuffer ( currentBufferIndex ) ; bufferPosition = 0 ; bufferStart = ( long ) BUFFER_SIZE * ( long ) currentBufferIndex ; long buflen = length - bufferStart ; bufferLength = buflen > BUFFER_SIZE ? BUFFER_SIZE : ( int ) buflen ; } } public long getFilePointer ( ) { return currentBufferIndex < 0 ? 0 : bufferStart + bufferPosition ; } public void seek ( long pos ) throws IOException { if ( currentBuffer == null || pos < bufferStart || pos >= bufferStart + BUFFER_SIZE ) { currentBufferIndex = ( int ) ( pos / BUFFER_SIZE ) ; switchCurrentBuffer ( false ) ; } bufferPosition = ( int ) ( pos % BUFFER_SIZE ) ; } } 	0	['8', '2', '0', '3', '17', '0', '1', '2', '6', '0.5', '236', '0.875', '1', '0.708333333', '0.270833333', '1', '4', '27.5', '2', '1', '0']
package org . apache . lucene . document ; import org . apache . lucene . analysis . TokenStream ; import java . io . Reader ; import java . io . Serializable ; public interface Fieldable extends Serializable { void setBoost ( float boost ) ; float getBoost ( ) ; String name ( ) ; public String stringValue ( ) ; public Reader readerValue ( ) ; public byte [ ] binaryValue ( ) ; public TokenStream tokenStreamValue ( ) ; boolean isStored ( ) ; boolean isIndexed ( ) ; boolean isTokenized ( ) ; boolean isCompressed ( ) ; boolean isTermVectorStored ( ) ; boolean isStoreOffsetWithTermVector ( ) ; boolean isStorePositionWithTermVector ( ) ; boolean isBinary ( ) ; boolean getOmitNorms ( ) ; void setOmitNorms ( boolean omitNorms ) ; void setOmitTf ( boolean omitTf ) ; boolean getOmitTf ( ) ; boolean isLazy ( ) ; abstract int getBinaryOffset ( ) ; abstract int getBinaryLength ( ) ; abstract byte [ ] getBinaryValue ( ) ; abstract byte [ ] getBinaryValue ( byte [ ] result ) ; } 	1	['24', '1', '0', '20', '24', '276', '19', '1', '24', '2', '24', '0', '0', '0', '0.291666667', '0', '0', '0', '1', '1', '1']
package org . apache . lucene . store ; import java . io . IOException ; public class LockReleaseFailedException extends IOException { public LockReleaseFailedException ( String message ) { super ( message ) ; } } 	0	['1', '4', '0', '2', '2', '0', '2', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . util ; public final class Constants { private Constants ( ) { } public static final String JAVA_VERSION = System . getProperty ( "java.version" ) ; public static final boolean JAVA_1_1 = JAVA_VERSION . startsWith ( "1.1." ) ; public static final boolean JAVA_1_2 = JAVA_VERSION . startsWith ( "1.2." ) ; public static final boolean JAVA_1_3 = JAVA_VERSION . startsWith ( "1.3." ) ; public static final String OS_NAME = System . getProperty ( "os.name" ) ; public static final boolean LINUX = OS_NAME . startsWith ( "Linux" ) ; public static final boolean WINDOWS = OS_NAME . startsWith ( "Windows" ) ; public static final boolean SUN_OS = OS_NAME . startsWith ( "SunOS" ) ; } 	1	['2', '1', '0', '1', '5', '1', '1', '0', '0', '1', '44', '0', '0', '0', '1', '0', '0', '17', '0', '0', '4']
package org . apache . lucene . index ; import java . io . IOException ; final class DocFieldConsumersPerThread extends DocFieldConsumerPerThread { final DocFieldConsumerPerThread one ; final DocFieldConsumerPerThread two ; final DocFieldConsumers parent ; final DocumentsWriter . DocState docState ; public DocFieldConsumersPerThread ( DocFieldProcessorPerThread docFieldProcessorPerThread , DocFieldConsumers parent , DocFieldConsumerPerThread one , DocFieldConsumerPerThread two ) { this . parent = parent ; this . one = one ; this . two = two ; docState = docFieldProcessorPerThread . docState ; } public void startDocument ( ) throws IOException { one . startDocument ( ) ; two . startDocument ( ) ; } public void abort ( ) { try { one . abort ( ) ; } finally { two . abort ( ) ; } } public DocumentsWriter . DocWriter finishDocument ( ) throws IOException { final DocumentsWriter . DocWriter oneDoc = one . finishDocument ( ) ; final DocumentsWriter . DocWriter twoDoc = two . finishDocument ( ) ; if ( oneDoc == null ) return twoDoc ; else if ( twoDoc == null ) return oneDoc ; else { DocFieldConsumers . PerDoc both = parent . getPerDoc ( ) ; both . docID = docState . docID ; assert oneDoc . docID == docState . docID ; assert twoDoc . docID == docState . docID ; both . one = oneDoc ; both . two = twoDoc ; return both ; } } public DocFieldConsumerPerField addField ( FieldInfo fi ) { return new DocFieldConsumersPerField ( this , one . addField ( fi ) , two . addField ( fi ) ) ; } } 	0	['7', '2', '0', '9', '19', '0', '2', '9', '5', '0.694444444', '146', '0', '4', '0.444444444', '0.277777778', '0', '0', '19', '3', '1', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . Term ; import org . apache . lucene . util . ToStringUtils ; public abstract class MultiTermQuery extends Query { private Term term ; public MultiTermQuery ( Term term ) { this . term = term ; } public Term getTerm ( ) { return term ; } protected abstract FilteredTermEnum getEnum ( IndexReader reader ) throws IOException ; public Query rewrite ( IndexReader reader ) throws IOException { FilteredTermEnum enumerator = getEnum ( reader ) ; BooleanQuery query = new BooleanQuery ( true ) ; try { do { Term t = enumerator . term ( ) ; if ( t != null ) { TermQuery tq = new TermQuery ( t ) ; tq . setBoost ( getBoost ( ) * enumerator . difference ( ) ) ; query . add ( tq , BooleanClause . Occur . SHOULD ) ; } } while ( enumerator . next ( ) ) ; } finally { enumerator . close ( ) ; } return query ; } public String toString ( String field ) { StringBuffer buffer = new StringBuffer ( ) ; if ( ! term . field ( ) . equals ( field ) ) { buffer . append ( term . field ( ) ) ; buffer . append ( ":" ) ; } buffer . append ( term . text ( ) ) ; buffer . append ( ToStringUtils . boost ( getBoost ( ) ) ) ; return buffer . toString ( ) ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( ! ( o instanceof MultiTermQuery ) ) return false ; final MultiTermQuery multiTermQuery = ( MultiTermQuery ) o ; if ( ! term . equals ( multiTermQuery . term ) ) return false ; return getBoost ( ) == multiTermQuery . getBoost ( ) ; } public int hashCode ( ) { return term . hashCode ( ) + Float . floatToRawIntBits ( getBoost ( ) ) ; } } 	1	['7', '2', '2', '10', '27', '1', '2', '8', '6', '0.333333333', '136', '1', '1', '0.666666667', '0.342857143', '2', '3', '18.28571429', '5', '1.5714', '5']
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; final class PhraseQueue extends PriorityQueue { PhraseQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object o1 , Object o2 ) { PhrasePositions pp1 = ( PhrasePositions ) o1 ; PhrasePositions pp2 = ( PhrasePositions ) o2 ; if ( pp1 . doc == pp2 . doc ) if ( pp1 . position == pp2 . position ) return pp1 . offset < pp2 . offset ; else return pp1 . position < pp2 . position ; else return pp1 . doc < pp2 . doc ; } } 	0	['2', '2', '0', '5', '4', '1', '3', '2', '0', '2', '51', '0', '0', '0.923076923', '0.666666667', '1', '3', '24.5', '6', '3', '0']
package org . apache . lucene . queryParser ; public class Token { public int kind ; public int beginLine ; public int beginColumn ; public int endLine ; public int endColumn ; public String image ; public Token next ; public Token specialToken ; public Object getValue ( ) { return null ; } public Token ( ) { } public Token ( int kind ) { this ( kind , null ) ; } public Token ( int kind , String image ) { this . kind = kind ; this . image = image ; } public String toString ( ) { return image ; } public static Token newToken ( int ofKind , String image ) { switch ( ofKind ) { default : return new Token ( ofKind , image ) ; } } public static Token newToken ( int ofKind ) { return newToken ( ofKind , null ) ; } } 	1	['7', '1', '0', '4', '8', '19', '4', '0', '7', '0.979166667', '49', '0', '2', '0', '0.523809524', '0', '0', '4.857142857', '2', '0.7143', '1']
package org . apache . lucene . store ; import java . io . IOException ; public class RAMOutputStream extends IndexOutput { static final int BUFFER_SIZE = 1024 ; private RAMFile file ; private byte [ ] currentBuffer ; private int currentBufferIndex ; private int bufferPosition ; private long bufferStart ; private int bufferLength ; public RAMOutputStream ( ) { this ( new RAMFile ( ) ) ; } RAMOutputStream ( RAMFile f ) { file = f ; currentBufferIndex = - 1 ; currentBuffer = null ; } public void writeTo ( IndexOutput out ) throws IOException { flush ( ) ; final long end = file . length ; long pos = 0 ; int buffer = 0 ; while ( pos < end ) { int length = BUFFER_SIZE ; long nextPos = pos + length ; if ( nextPos > end ) { length = ( int ) ( end - pos ) ; } out . writeBytes ( ( byte [ ] ) file . getBuffer ( buffer ++ ) , length ) ; pos = nextPos ; } } public void reset ( ) { try { seek ( 0 ) ; } catch ( IOException e ) { throw new RuntimeException ( e . toString ( ) ) ; } file . setLength ( 0 ) ; } public void close ( ) throws IOException { flush ( ) ; } public void seek ( long pos ) throws IOException { setFileLength ( ) ; if ( pos < bufferStart || pos >= bufferStart + bufferLength ) { currentBufferIndex = ( int ) ( pos / BUFFER_SIZE ) ; switchCurrentBuffer ( ) ; } bufferPosition = ( int ) ( pos % BUFFER_SIZE ) ; } public long length ( ) { return file . length ; } public void writeByte ( byte b ) throws IOException { if ( bufferPosition == bufferLength ) { currentBufferIndex ++ ; switchCurrentBuffer ( ) ; } currentBuffer [ bufferPosition ++ ] = b ; } public void writeBytes ( byte [ ] b , int offset , int len ) throws IOException { assert b != null ; while ( len > 0 ) { if ( bufferPosition == bufferLength ) { currentBufferIndex ++ ; switchCurrentBuffer ( ) ; } int remainInBuffer = currentBuffer . length - bufferPosition ; int bytesToCopy = len < remainInBuffer ? len : remainInBuffer ; System . arraycopy ( b , offset , currentBuffer , bufferPosition , bytesToCopy ) ; offset += bytesToCopy ; len -= bytesToCopy ; bufferPosition += bytesToCopy ; } } private final void switchCurrentBuffer ( ) throws IOException { if ( currentBufferIndex == file . numBuffers ( ) ) { currentBuffer = file . addBuffer ( BUFFER_SIZE ) ; } else { currentBuffer = ( byte [ ] ) file . getBuffer ( currentBufferIndex ) ; } bufferPosition = 0 ; bufferStart = ( long ) BUFFER_SIZE * ( long ) currentBufferIndex ; bufferLength = currentBuffer . length ; } private void setFileLength ( ) { long pointer = bufferStart + bufferPosition ; if ( pointer > file . length ) { file . setLength ( pointer ) ; } } public void flush ( ) throws IOException { file . setLastModified ( System . currentTimeMillis ( ) ) ; setFileLength ( ) ; } public long getFilePointer ( ) { return currentBufferIndex < 0 ? 0 : bufferStart + bufferPosition ; } public long sizeInBytes ( ) { return file . numBuffers ( ) * BUFFER_SIZE ; } } 	0	['16', '2', '0', '11', '33', '26', '9', '2', '11', '0.688888889', '332', '0.666666667', '1', '0.566666667', '0.175', '1', '5', '19.1875', '2', '0.9375', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . util . PriorityQueue ; public class ParallelMultiSearcher extends MultiSearcher { private Searchable [ ] searchables ; private int [ ] starts ; public ParallelMultiSearcher ( Searchable [ ] searchables ) throws IOException { super ( searchables ) ; this . searchables = searchables ; this . starts = getStarts ( ) ; } public int docFreq ( Term term ) throws IOException { return super . docFreq ( term ) ; } public TopDocs search ( Weight weight , Filter filter , int nDocs ) throws IOException { HitQueue hq = new HitQueue ( nDocs ) ; int totalHits = 0 ; MultiSearcherThread [ ] msta = new MultiSearcherThread [ searchables . length ] ; for ( int i = 0 ; i < searchables . length ; i ++ ) { msta [ i ] = new MultiSearcherThread ( searchables [ i ] , weight , filter , nDocs , hq , i , starts , "MultiSearcher thread #" + ( i + 1 ) ) ; msta [ i ] . start ( ) ; } for ( int i = 0 ; i < searchables . length ; i ++ ) { try { msta [ i ] . join ( ) ; } catch ( InterruptedException ie ) { ; } IOException ioe = msta [ i ] . getIOException ( ) ; if ( ioe == null ) { totalHits += msta [ i ] . hits ( ) ; } else { throw ioe ; } } ScoreDoc [ ] scoreDocs = new ScoreDoc [ hq . size ( ) ] ; for ( int i = hq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = ( ScoreDoc ) hq . pop ( ) ; float maxScore = ( totalHits == 0 ) ? Float . NEGATIVE_INFINITY : scoreDocs [ 0 ] . score ; return new TopDocs ( totalHits , scoreDocs , maxScore ) ; } public TopFieldDocs search ( Weight weight , Filter filter , int nDocs , Sort sort ) throws IOException { FieldDocSortedHitQueue hq = new FieldDocSortedHitQueue ( null , nDocs ) ; int totalHits = 0 ; MultiSearcherThread [ ] msta = new MultiSearcherThread [ searchables . length ] ; for ( int i = 0 ; i < searchables . length ; i ++ ) { msta [ i ] = new MultiSearcherThread ( searchables [ i ] , weight , filter , nDocs , hq , sort , i , starts , "MultiSearcher thread #" + ( i + 1 ) ) ; msta [ i ] . start ( ) ; } float maxScore = Float . NEGATIVE_INFINITY ; for ( int i = 0 ; i < searchables . length ; i ++ ) { try { msta [ i ] . join ( ) ; } catch ( InterruptedException ie ) { ; } IOException ioe = msta [ i ] . getIOException ( ) ; if ( ioe == null ) { totalHits += msta [ i ] . hits ( ) ; maxScore = Math . max ( maxScore , msta [ i ] . getMaxScore ( ) ) ; } else { throw ioe ; } } ScoreDoc [ ] scoreDocs = new ScoreDoc [ hq . size ( ) ] ; for ( int i = hq . size ( ) - 1 ; i >= 0 ; i -- ) scoreDocs [ i ] = ( ScoreDoc ) hq . pop ( ) ; return new TopFieldDocs ( totalHits , scoreDocs , hq . getFields ( ) , maxScore ) ; } public void search ( Weight weight , Filter filter , final HitCollector results ) throws IOException { for ( int i = 0 ; i < searchables . length ; i ++ ) { final int start = starts [ i ] ; searchables [ i ] . search ( weight , filter , new HitCollector ( ) { public void collect ( int doc , float score ) { results . collect ( doc + start , score ) ; } } ) ; } } public Query rewrite ( Query original ) throws IOException { return super . rewrite ( original ) ; } } class MultiSearcherThread extends Thread { private Searchable searchable ; private Weight weight ; private Filter filter ; private int nDocs ; private TopDocs docs ; private int i ; private PriorityQueue hq ; private int [ ] starts ; private IOException ioe ; private Sort sort ; public MultiSearcherThread ( Searchable searchable , Weight weight , Filter filter , int nDocs , HitQueue hq , int i , int [ ] starts , String name ) { super ( name ) ; this . searchable = searchable ; this . weight = weight ; this . filter = filter ; this . nDocs = nDocs ; this . hq = hq ; this . i = i ; this . starts = starts ; } public MultiSearcherThread ( Searchable searchable , Weight weight , Filter filter , int nDocs , FieldDocSortedHitQueue hq , Sort sort , int i , int [ ] starts , String name ) { super ( name ) ; this . searchable = searchable ; this . weight = weight ; this . filter = filter ; this . nDocs = nDocs ; this . hq = hq ; this . i = i ; this . starts = starts ; this . sort = sort ; } public void run ( ) { try { docs = ( sort == null ) ? searchable . search ( weight , filter , nDocs ) : searchable . search ( weight , filter , nDocs , sort ) ; } catch ( IOException ioe ) { this . ioe = ioe ; } if ( ioe == null ) { if ( sort != null ) { ( ( FieldDocSortedHitQueue ) hq ) . setFields ( ( ( TopFieldDocs ) docs ) . fields ) ; } ScoreDoc [ ] scoreDocs = docs . scoreDocs ; for ( int j = 0 ; j < scoreDocs . length ; j ++ ) { ScoreDoc scoreDoc = scoreDocs [ j ] ; scoreDoc . doc += starts [ i ] ; synchronized ( hq ) { if ( ! hq . insert ( scoreDoc ) ) break ; } } } } public int hits ( ) { return docs . totalHits ; } public float getMaxScore ( ) { return docs . getMaxScore ( ) ; } public IOException getIOException ( ) { return ioe ; } } 	1	['6', '3', '0', '16', '33', '3', '1', '16', '6', '0.4', '297', '1', '1', '0.88372093', '0.351851852', '2', '3', '48.16666667', '1', '0.8333', '5']
package org . apache . lucene . store ; public class AlreadyClosedException extends IllegalStateException { public AlreadyClosedException ( String message ) { super ( message ) ; } } 	0	['1', '5', '0', '5', '2', '0', '5', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . search ; import java . io . IOException ; import java . util . Collection ; import java . util . Arrays ; import java . util . Comparator ; class ConjunctionScorer extends Scorer { private final Scorer [ ] scorers ; private boolean firstTime = true ; private boolean more ; private final float coord ; private int lastDoc = - 1 ; public ConjunctionScorer ( Similarity similarity , Collection scorers ) throws IOException { this ( similarity , ( Scorer [ ] ) scorers . toArray ( new Scorer [ scorers . size ( ) ] ) ) ; } public ConjunctionScorer ( Similarity similarity , Scorer [ ] scorers ) throws IOException { super ( similarity ) ; this . scorers = scorers ; coord = getSimilarity ( ) . coord ( this . scorers . length , this . scorers . length ) ; } public int doc ( ) { return lastDoc ; } public boolean next ( ) throws IOException { if ( firstTime ) return init ( 0 ) ; else if ( more ) more = scorers [ ( scorers . length - 1 ) ] . next ( ) ; return doNext ( ) ; } private boolean doNext ( ) throws IOException { int first = 0 ; Scorer lastScorer = scorers [ scorers . length - 1 ] ; Scorer firstScorer ; while ( more && ( firstScorer = scorers [ first ] ) . doc ( ) < ( lastDoc = lastScorer . doc ( ) ) ) { more = firstScorer . skipTo ( lastDoc ) ; lastScorer = firstScorer ; first = ( first == ( scorers . length - 1 ) ) ? 0 : first + 1 ; } return more ; } public boolean skipTo ( int target ) throws IOException { if ( firstTime ) return init ( target ) ; else if ( more ) more = scorers [ ( scorers . length - 1 ) ] . skipTo ( target ) ; return doNext ( ) ; } private boolean init ( int target ) throws IOException { firstTime = false ; more = scorers . length > 1 ; for ( int i = 0 ; i < scorers . length ; i ++ ) { more = target == 0 ? scorers [ i ] . next ( ) : scorers [ i ] . skipTo ( target ) ; if ( ! more ) return false ; } Arrays . sort ( scorers , new Comparator ( ) { public int compare ( Object o1 , Object o2 ) { return ( ( Scorer ) o1 ) . doc ( ) - ( ( Scorer ) o2 ) . doc ( ) ; } } ) ; doNext ( ) ; int end = ( scorers . length - 1 ) ; for ( int i = 0 ; i < ( end > > 1 ) ; i ++ ) { Scorer tmp = scorers [ i ] ; scorers [ i ] = scorers [ end - i - 1 ] ; scorers [ end - i - 1 ] = tmp ; } return more ; } public float score ( ) throws IOException { float sum = 0.0f ; for ( int i = 0 ; i < scorers . length ; i ++ ) { sum += scorers [ i ] . score ( ) ; } return sum * coord ; } public Explanation explain ( int doc ) { throw new UnsupportedOperationException ( ) ; } } 	1	['9', '3', '1', '6', '21', '2', '3', '4', '7', '0.475', '278', '1', '1', '0.533333333', '0.355555556', '1', '3', '29.33333333', '1', '0.7778', '1']
package org . apache . lucene . index ; public class FieldReaderException extends RuntimeException { public FieldReaderException ( ) { } public FieldReaderException ( Throwable cause ) { super ( cause ) ; } public FieldReaderException ( String message ) { super ( message ) ; } public FieldReaderException ( String message , Throwable cause ) { super ( message , cause ) ; } } 	0	['4', '4', '0', '1', '8', '6', '1', '0', '4', '2', '20', '0', '0', '1', '0.666666667', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import java . io . IOException ; import java . io . FileNotFoundException ; import java . io . PrintStream ; import java . util . Map ; import java . util . HashMap ; import java . util . Iterator ; import java . util . List ; import java . util . ArrayList ; import java . util . Collections ; import java . util . Collection ; final class IndexFileDeleter { private List deletable ; private Map refCounts = new HashMap ( ) ; private List commits = new ArrayList ( ) ; private List lastFiles = new ArrayList ( ) ; private List commitsToDelete = new ArrayList ( ) ; private PrintStream infoStream ; private Directory directory ; private IndexDeletionPolicy policy ; private DocumentsWriter docWriter ; public static boolean VERBOSE_REF_COUNTS = false ; void setInfoStream ( PrintStream infoStream ) { this . infoStream = infoStream ; if ( infoStream != null ) message ( "setInfoStream deletionPolicy=" + policy ) ; } private void message ( String message ) { infoStream . println ( "IFD [" + Thread . currentThread ( ) . getName ( ) + "]: " + message ) ; } public IndexFileDeleter ( Directory directory , IndexDeletionPolicy policy , SegmentInfos segmentInfos , PrintStream infoStream , DocumentsWriter docWriter ) throws CorruptIndexException , IOException { this . docWriter = docWriter ; this . infoStream = infoStream ; if ( infoStream != null ) message ( "init: current segments file is \"" + segmentInfos . getCurrentSegmentFileName ( ) + "\"; deletionPolicy=" + policy ) ; this . policy = policy ; this . directory = directory ; long currentGen = segmentInfos . getGeneration ( ) ; IndexFileNameFilter filter = IndexFileNameFilter . getFilter ( ) ; String [ ] files = directory . list ( ) ; if ( files == null ) throw new IOException ( "cannot read directory " + directory + ": list() returned null" ) ; CommitPoint currentCommitPoint = null ; for ( int i = 0 ; i < files . length ; i ++ ) { String fileName = files [ i ] ; if ( filter . accept ( null , fileName ) && ! fileName . equals ( IndexFileNames . SEGMENTS_GEN ) ) { getRefCount ( fileName ) ; if ( fileName . startsWith ( IndexFileNames . SEGMENTS ) ) { if ( SegmentInfos . generationFromSegmentsFileName ( fileName ) <= currentGen ) { if ( infoStream != null ) { message ( "init: load commit \"" + fileName + "\"" ) ; } SegmentInfos sis = new SegmentInfos ( ) ; try { sis . read ( directory , fileName ) ; } catch ( FileNotFoundException e ) { if ( infoStream != null ) { message ( "init: hit FileNotFoundException when loading commit \"" + fileName + "\"; skipping this commit point" ) ; } sis = null ; } if ( sis != null ) { CommitPoint commitPoint = new CommitPoint ( commitsToDelete , directory , sis ) ; if ( sis . getGeneration ( ) == segmentInfos . getGeneration ( ) ) { currentCommitPoint = commitPoint ; } commits . add ( commitPoint ) ; incRef ( sis , true ) ; } } } } } if ( currentCommitPoint == null ) { SegmentInfos sis = new SegmentInfos ( ) ; try { sis . read ( directory , segmentInfos . getCurrentSegmentFileName ( ) ) ; } catch ( IOException e ) { throw new CorruptIndexException ( "failed to locate current segments_N file" ) ; } if ( infoStream != null ) message ( "forced open of current segments file " + segmentInfos . getCurrentSegmentFileName ( ) ) ; currentCommitPoint = new CommitPoint ( commitsToDelete , directory , sis ) ; commits . add ( currentCommitPoint ) ; incRef ( sis , true ) ; } Collections . sort ( commits ) ; Iterator it = refCounts . keySet ( ) . iterator ( ) ; while ( it . hasNext ( ) ) { String fileName = ( String ) it . next ( ) ; RefCount rc = ( RefCount ) refCounts . get ( fileName ) ; if ( 0 == rc . count ) { if ( infoStream != null ) { message ( "init: removing unreferenced file \"" + fileName + "\"" ) ; } deleteFile ( fileName ) ; } } policy . onInit ( commits ) ; if ( currentCommitPoint . deleted ) { checkpoint ( segmentInfos , false ) ; } deleteCommits ( ) ; } private void deleteCommits ( ) throws IOException { int size = commitsToDelete . size ( ) ; if ( size > 0 ) { for ( int i = 0 ; i < size ; i ++ ) { CommitPoint commit = ( CommitPoint ) commitsToDelete . get ( i ) ; if ( infoStream != null ) { message ( "deleteCommits: now decRef commit \"" + commit . getSegmentsFileName ( ) + "\"" ) ; } int size2 = commit . files . size ( ) ; for ( int j = 0 ; j < size2 ; j ++ ) { decRef ( ( String ) commit . files . get ( j ) ) ; } } commitsToDelete . clear ( ) ; size = commits . size ( ) ; int readFrom = 0 ; int writeTo = 0 ; while ( readFrom < size ) { CommitPoint commit = ( CommitPoint ) commits . get ( readFrom ) ; if ( ! commit . deleted ) { if ( writeTo != readFrom ) { commits . set ( writeTo , commits . get ( readFrom ) ) ; } writeTo ++ ; } readFrom ++ ; } while ( size > writeTo ) { commits . remove ( size - 1 ) ; size -- ; } } } public void refresh ( String segmentName ) throws IOException { String [ ] files = directory . list ( ) ; if ( files == null ) throw new IOException ( "cannot read directory " + directory + ": list() returned null" ) ; IndexFileNameFilter filter = IndexFileNameFilter . getFilter ( ) ; String segmentPrefix1 ; String segmentPrefix2 ; if ( segmentName != null ) { segmentPrefix1 = segmentName + "." ; segmentPrefix2 = segmentName + "_" ; } else { segmentPrefix1 = null ; segmentPrefix2 = null ; } for ( int i = 0 ; i < files . length ; i ++ ) { String fileName = files [ i ] ; if ( filter . accept ( null , fileName ) && ( segmentName == null || fileName . startsWith ( segmentPrefix1 ) || fileName . startsWith ( segmentPrefix2 ) ) && ! refCounts . containsKey ( fileName ) && ! fileName . equals ( IndexFileNames . SEGMENTS_GEN ) ) { if ( infoStream != null ) { message ( "refresh [prefix=" + segmentName + "]: removing newly created unreferenced file \"" + fileName + "\"" ) ; } deleteFile ( fileName ) ; } } } public void refresh ( ) throws IOException { refresh ( null ) ; } public void close ( ) throws IOException { deletePendingFiles ( ) ; } private void deletePendingFiles ( ) throws IOException { if ( deletable != null ) { List oldDeletable = deletable ; deletable = null ; int size = oldDeletable . size ( ) ; for ( int i = 0 ; i < size ; i ++ ) { if ( infoStream != null ) message ( "delete pending file " + oldDeletable . get ( i ) ) ; deleteFile ( ( String ) oldDeletable . get ( i ) ) ; } } } public void checkpoint ( SegmentInfos segmentInfos , boolean isCommit ) throws IOException { if ( infoStream != null ) { message ( "now checkpoint \"" + segmentInfos . getCurrentSegmentFileName ( ) + "\" [" + segmentInfos . size ( ) + " segments " + "; isCommit = " + isCommit + "]" ) ; } deletePendingFiles ( ) ; incRef ( segmentInfos , isCommit ) ; if ( isCommit ) { commits . add ( new CommitPoint ( commitsToDelete , directory , segmentInfos ) ) ; policy . onCommit ( commits ) ; deleteCommits ( ) ; } else { final List docWriterFiles ; if ( docWriter != null ) { docWriterFiles = docWriter . openFiles ( ) ; if ( docWriterFiles != null ) incRef ( docWriterFiles ) ; } else docWriterFiles = null ; int size = lastFiles . size ( ) ; if ( size > 0 ) { for ( int i = 0 ; i < size ; i ++ ) decRef ( ( List ) lastFiles . get ( i ) ) ; lastFiles . clear ( ) ; } size = segmentInfos . size ( ) ; for ( int i = 0 ; i < size ; i ++ ) { SegmentInfo segmentInfo = segmentInfos . info ( i ) ; if ( segmentInfo . dir == directory ) { lastFiles . add ( segmentInfo . files ( ) ) ; } } if ( docWriterFiles != null ) lastFiles . add ( docWriterFiles ) ; } } void incRef ( SegmentInfos segmentInfos , boolean isCommit ) throws IOException { int size = segmentInfos . size ( ) ; for ( int i = 0 ; i < size ; i ++ ) { SegmentInfo segmentInfo = segmentInfos . info ( i ) ; if ( segmentInfo . dir == directory ) { incRef ( segmentInfo . files ( ) ) ; } } if ( isCommit ) { getRefCount ( segmentInfos . getCurrentSegmentFileName ( ) ) . IncRef ( ) ; } } void incRef ( List files ) throws IOException { int size = files . size ( ) ; for ( int i = 0 ; i < size ; i ++ ) { String fileName = ( String ) files . get ( i ) ; RefCount rc = getRefCount ( fileName ) ; if ( infoStream != null && VERBOSE_REF_COUNTS ) { message ( "  IncRef \"" + fileName + "\": pre-incr count is " + rc . count ) ; } rc . IncRef ( ) ; } } void decRef ( List files ) throws IOException { int size = files . size ( ) ; for ( int i = 0 ; i < size ; i ++ ) { decRef ( ( String ) files . get ( i ) ) ; } } void decRef ( String fileName ) throws IOException { RefCount rc = getRefCount ( fileName ) ; if ( infoStream != null && VERBOSE_REF_COUNTS ) { message ( "  DecRef \"" + fileName + "\": pre-decr count is " + rc . count ) ; } if ( 0 == rc . DecRef ( ) ) { deleteFile ( fileName ) ; refCounts . remove ( fileName ) ; } } void decRef ( SegmentInfos segmentInfos ) throws IOException { final int size = segmentInfos . size ( ) ; for ( int i = 0 ; i < size ; i ++ ) { SegmentInfo segmentInfo = segmentInfos . info ( i ) ; if ( segmentInfo . dir == directory ) { decRef ( segmentInfo . files ( ) ) ; } } } private RefCount getRefCount ( String fileName ) { RefCount rc ; if ( ! refCounts . containsKey ( fileName ) ) { rc = new RefCount ( ) ; refCounts . put ( fileName , rc ) ; } else { rc = ( RefCount ) refCounts . get ( fileName ) ; } return rc ; } void deleteFiles ( List files ) throws IOException { final int size = files . size ( ) ; for ( int i = 0 ; i < size ; i ++ ) deleteFile ( ( String ) files . get ( i ) ) ; } void deleteNewFiles ( Collection files ) throws IOException { final Iterator it = files . iterator ( ) ; while ( it . hasNext ( ) ) { final String fileName = ( String ) it . next ( ) ; if ( ! refCounts . containsKey ( fileName ) ) deleteFile ( fileName ) ; } } void deleteFile ( String fileName ) throws IOException { try { if ( infoStream != null ) { message ( "delete \"" + fileName + "\"" ) ; } directory . deleteFile ( fileName ) ; } catch ( IOException e ) { if ( directory . fileExists ( fileName ) ) { if ( infoStream != null ) { message ( "IndexFileDeleter: unable to remove file \"" + fileName + "\": " + e . toString ( ) + "; Will re-try later." ) ; } if ( deletable == null ) { deletable = new ArrayList ( ) ; } deletable . add ( fileName ) ; } } } final private static class RefCount { int count ; public int IncRef ( ) { return ++ count ; } public int DecRef ( ) { assert count > 0 ; return -- count ; } } final private static class CommitPoint extends IndexCommit implements Comparable { long gen ; List files ; String segmentsFileName ; boolean deleted ; Directory directory ; Collection commitsToDelete ; long version ; long generation ; final boolean isOptimized ; public CommitPoint ( Collection commitsToDelete , Directory directory , SegmentInfos segmentInfos ) throws IOException { this . directory = directory ; this . commitsToDelete = commitsToDelete ; segmentsFileName = segmentInfos . getCurrentSegmentFileName ( ) ; version = segmentInfos . getVersion ( ) ; generation = segmentInfos . getGeneration ( ) ; int size = segmentInfos . size ( ) ; files = new ArrayList ( size ) ; files . add ( segmentsFileName ) ; gen = segmentInfos . getGeneration ( ) ; for ( int i = 0 ; i < size ; i ++ ) { SegmentInfo segmentInfo = segmentInfos . info ( i ) ; if ( segmentInfo . dir == directory ) { files . addAll ( segmentInfo . files ( ) ) ; } } isOptimized = segmentInfos . size ( ) == 1 && ! segmentInfos . info ( 0 ) . hasDeletions ( ) ; } public boolean isOptimized ( ) { return isOptimized ; } public String getSegmentsFileName ( ) { return segmentsFileName ; } public Collection getFileNames ( ) throws IOException { return Collections . unmodifiableCollection ( files ) ; } public Directory getDirectory ( ) { return directory ; } public long getVersion ( ) { return version ; } public long getGeneration ( ) { return generation ; } public void delete ( ) { if ( ! deleted ) { deleted = true ; commitsToDelete . add ( this ) ; } } public boolean isDeleted ( ) { return deleted ; } public int compareTo ( Object obj ) { CommitPoint commit = ( CommitPoint ) obj ; if ( gen < commit . gen ) { return - 1 ; } else if ( gen > commit . gen ) { return 1 ; } else { return 0 ; } } } } 	1	['20', '1', '0', '12', '77', '64', '3', '10', '5', '0.71291866', '1021', '0.818181818', '3', '0', '0.205263158', '0', '0', '49.5', '2', '1', '5']
package org . apache . lucene . search ; import java . util . Iterator ; import java . util . NoSuchElementException ; public class HitIterator implements Iterator { private Hits hits ; private int hitNumber = 0 ; HitIterator ( Hits hits ) { this . hits = hits ; } public boolean hasNext ( ) { return hitNumber < hits . length ( ) ; } public Object next ( ) { if ( hitNumber == hits . length ( ) ) throw new NoSuchElementException ( ) ; Object next = new Hit ( hits , hitNumber ) ; hitNumber ++ ; return next ; } public void remove ( ) { throw new UnsupportedOperationException ( ) ; } public int length ( ) { return hits . length ( ) ; } } 	0	['5', '1', '0', '2', '10', '0', '1', '2', '4', '0.375', '60', '1', '1', '0', '0.6', '0', '0', '10.6', '2', '1.2', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . RAMOutputStream ; import org . apache . lucene . util . ArrayUtil ; import java . io . IOException ; import java . util . Collection ; import java . util . Iterator ; import java . util . Map ; final class TermVectorsTermsWriter extends TermsHashConsumer { final DocumentsWriter docWriter ; TermVectorsWriter termVectorsWriter ; PerDoc [ ] docFreeList = new PerDoc [ 1 ] ; int freeCount ; IndexOutput tvx ; IndexOutput tvd ; IndexOutput tvf ; int lastDocID ; public TermVectorsTermsWriter ( DocumentsWriter docWriter ) { this . docWriter = docWriter ; } public TermsHashConsumerPerThread addThread ( TermsHashPerThread termsHashPerThread ) { return new TermVectorsTermsWriterPerThread ( termsHashPerThread , this ) ; } void createPostings ( RawPostingList [ ] postings , int start , int count ) { final int end = start + count ; for ( int i = start ; i < end ; i ++ ) postings [ i ] = new PostingList ( ) ; } synchronized void flush ( Map threadsAndFields , final DocumentsWriter . FlushState state ) throws IOException { if ( tvx != null ) { if ( state . numDocsInStore > 0 ) fill ( state . numDocsInStore - docWriter . getDocStoreOffset ( ) ) ; tvx . flush ( ) ; tvd . flush ( ) ; tvf . flush ( ) ; } Iterator it = threadsAndFields . entrySet ( ) . iterator ( ) ; while ( it . hasNext ( ) ) { Map . Entry entry = ( Map . Entry ) it . next ( ) ; Iterator it2 = ( ( Collection ) entry . getValue ( ) ) . iterator ( ) ; while ( it2 . hasNext ( ) ) { TermVectorsTermsWriterPerField perField = ( TermVectorsTermsWriterPerField ) it2 . next ( ) ; perField . termsHashPerField . reset ( ) ; perField . shrinkHash ( ) ; } TermVectorsTermsWriterPerThread perThread = ( TermVectorsTermsWriterPerThread ) entry . getKey ( ) ; perThread . termsHashPerThread . reset ( true ) ; } } synchronized void closeDocStore ( final DocumentsWriter . FlushState state ) throws IOException { if ( tvx != null ) { fill ( state . numDocsInStore - docWriter . getDocStoreOffset ( ) ) ; tvx . close ( ) ; tvf . close ( ) ; tvd . close ( ) ; tvx = null ; assert state . docStoreSegmentName != null ; if ( 4 + state . numDocsInStore * 16 != state . directory . fileLength ( state . docStoreSegmentName + "." + IndexFileNames . VECTORS_INDEX_EXTENSION ) ) throw new RuntimeException ( "after flush: tvx size mismatch: " + state . numDocsInStore + " docs vs " + state . directory . fileLength ( state . docStoreSegmentName + "." + IndexFileNames . VECTORS_INDEX_EXTENSION ) + " length in bytes of " + state . docStoreSegmentName + "." + IndexFileNames . VECTORS_INDEX_EXTENSION ) ; state . flushedFiles . add ( state . docStoreSegmentName + "." + IndexFileNames . VECTORS_INDEX_EXTENSION ) ; state . flushedFiles . add ( state . docStoreSegmentName + "." + IndexFileNames . VECTORS_FIELDS_EXTENSION ) ; state . flushedFiles . add ( state . docStoreSegmentName + "." + IndexFileNames . VECTORS_DOCUMENTS_EXTENSION ) ; docWriter . removeOpenFile ( state . docStoreSegmentName + "." + IndexFileNames . VECTORS_INDEX_EXTENSION ) ; docWriter . removeOpenFile ( state . docStoreSegmentName + "." + IndexFileNames . VECTORS_FIELDS_EXTENSION ) ; docWriter . removeOpenFile ( state . docStoreSegmentName + "." + IndexFileNames . VECTORS_DOCUMENTS_EXTENSION ) ; lastDocID = 0 ; } } int allocCount ; synchronized PerDoc getPerDoc ( ) { if ( freeCount == 0 ) { allocCount ++ ; if ( allocCount > docFreeList . length ) { assert allocCount == 1 + docFreeList . length ; docFreeList = new PerDoc [ ArrayUtil . getNextSize ( allocCount ) ] ; } return new PerDoc ( ) ; } else return docFreeList [ -- freeCount ] ; } void fill ( int docID ) throws IOException { final int docStoreOffset = docWriter . getDocStoreOffset ( ) ; final int end = docID + docStoreOffset ; if ( lastDocID < end ) { final long tvfPosition = tvf . getFilePointer ( ) ; while ( lastDocID < end ) { tvx . writeLong ( tvd . getFilePointer ( ) ) ; tvd . writeVInt ( 0 ) ; tvx . writeLong ( tvfPosition ) ; lastDocID ++ ; } } } synchronized void initTermVectorsWriter ( ) throws IOException { if ( tvx == null ) { final String docStoreSegment = docWriter . getDocStoreSegment ( ) ; if ( docStoreSegment == null ) return ; assert docStoreSegment != null ; tvx = docWriter . directory . createOutput ( docStoreSegment + "." + IndexFileNames . VECTORS_INDEX_EXTENSION ) ; tvd = docWriter . directory . createOutput ( docStoreSegment + "." + IndexFileNames . VECTORS_DOCUMENTS_EXTENSION ) ; tvf = docWriter . directory . createOutput ( docStoreSegment + "." + IndexFileNames . VECTORS_FIELDS_EXTENSION ) ; tvx . writeInt ( TermVectorsReader . FORMAT_CURRENT ) ; tvd . writeInt ( TermVectorsReader . FORMAT_CURRENT ) ; tvf . writeInt ( TermVectorsReader . FORMAT_CURRENT ) ; docWriter . addOpenFile ( docStoreSegment + "." + IndexFileNames . VECTORS_INDEX_EXTENSION ) ; docWriter . addOpenFile ( docStoreSegment + "." + IndexFileNames . VECTORS_FIELDS_EXTENSION ) ; docWriter . addOpenFile ( docStoreSegment + "." + IndexFileNames . VECTORS_DOCUMENTS_EXTENSION ) ; lastDocID = 0 ; } } synchronized void finishDocument ( PerDoc perDoc ) throws IOException { assert docWriter . writer . testPoint ( "TermVectorsTermsWriter.finishDocument start" ) ; initTermVectorsWriter ( ) ; fill ( perDoc . docID ) ; tvx . writeLong ( tvd . getFilePointer ( ) ) ; tvx . writeLong ( tvf . getFilePointer ( ) ) ; tvd . writeVInt ( perDoc . numVectorFields ) ; if ( perDoc . numVectorFields > 0 ) { for ( int i = 0 ; i < perDoc . numVectorFields ; i ++ ) tvd . writeVInt ( perDoc . fieldNumbers [ i ] ) ; assert 0 == perDoc . fieldPointers [ 0 ] ; long lastPos = perDoc . fieldPointers [ 0 ] ; for ( int i = 1 ; i < perDoc . numVectorFields ; i ++ ) { long pos = perDoc . fieldPointers [ i ] ; tvd . writeVLong ( pos - lastPos ) ; lastPos = pos ; } perDoc . tvf . writeTo ( tvf ) ; perDoc . tvf . reset ( ) ; perDoc . numVectorFields = 0 ; } assert lastDocID == perDoc . docID + docWriter . getDocStoreOffset ( ) ; lastDocID ++ ; free ( perDoc ) ; assert docWriter . writer . testPoint ( "TermVectorsTermsWriter.finishDocument end" ) ; } public boolean freeRAM ( ) { return false ; } public void abort ( ) { if ( tvx != null ) { try { tvx . close ( ) ; } catch ( Throwable t ) { } tvx = null ; } if ( tvd != null ) { try { tvd . close ( ) ; } catch ( Throwable t ) { } tvd = null ; } if ( tvf != null ) { try { tvf . close ( ) ; } catch ( Throwable t ) { } tvf = null ; } lastDocID = 0 ; } synchronized void free ( PerDoc doc ) { assert freeCount < docFreeList . length ; docFreeList [ freeCount ++ ] = doc ; } class PerDoc extends DocumentsWriter . DocWriter { RAMOutputStream tvf = new RAMOutputStream ( ) ; int numVectorFields ; int [ ] fieldNumbers = new int [ 1 ] ; long [ ] fieldPointers = new long [ 1 ] ; void reset ( ) { tvf . reset ( ) ; numVectorFields = 0 ; } void abort ( ) { reset ( ) ; free ( this ) ; } void addField ( final int fieldNumber ) { if ( numVectorFields == fieldNumbers . length ) { fieldNumbers = ArrayUtil . grow ( fieldNumbers ) ; fieldPointers = ArrayUtil . grow ( fieldPointers ) ; } fieldNumbers [ numVectorFields ] = fieldNumber ; fieldPointers [ numVectorFields ] = tvf . getFilePointer ( ) ; numVectorFields ++ ; } public long sizeInBytes ( ) { return tvf . sizeInBytes ( ) ; } public void finish ( ) throws IOException { finishDocument ( this ) ; } } static final class PostingList extends RawPostingList { int freq ; int lastOffset ; int lastPosition ; } int bytesPerPosting ( ) { return RawPostingList . BYTES_SIZE + 3 * DocumentsWriter . INT_NUM_BYTE ; } } 	1	['15', '2', '0', '17', '58', '37', '4', '17', '4', '0.779220779', '751', '0', '6', '0.35', '0.19047619', '0', '0', '48.33333333', '5', '1.5333', '2']
package org . apache . lucene . index ; import java . io . IOException ; abstract class InvertedDocConsumerPerThread { abstract void startDocument ( ) throws IOException ; abstract InvertedDocConsumerPerField addField ( DocInverterPerField docInverterPerField , FieldInfo fieldInfo ) ; abstract DocumentsWriter . DocWriter finishDocument ( ) throws IOException ; abstract void abort ( ) ; } 	0	['5', '1', '1', '9', '6', '10', '6', '4', '0', '2', '8', '0', '0', '0', '0.466666667', '0', '0', '0.6', '1', '0.8', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . BufferedIndexInput ; import org . apache . lucene . util . cache . Cache ; import org . apache . lucene . util . cache . SimpleLRUCache ; import org . apache . lucene . util . CloseableThreadLocal ; final class TermInfosReader { private Directory directory ; private String segment ; private FieldInfos fieldInfos ; private CloseableThreadLocal threadResources = new CloseableThreadLocal ( ) ; private SegmentTermEnum origEnum ; private long size ; private Term [ ] indexTerms = null ; private TermInfo [ ] indexInfos ; private long [ ] indexPointers ; private SegmentTermEnum indexEnum ; private int indexDivisor = 1 ; private int totalIndexInterval ; private final static int DEFAULT_CACHE_SIZE = 1024 ; private static final class ThreadResources { SegmentTermEnum termEnum ; Cache termInfoCache ; } TermInfosReader ( Directory dir , String seg , FieldInfos fis ) throws CorruptIndexException , IOException { this ( dir , seg , fis , BufferedIndexInput . BUFFER_SIZE ) ; } TermInfosReader ( Directory dir , String seg , FieldInfos fis , int readBufferSize ) throws CorruptIndexException , IOException { boolean success = false ; try { directory = dir ; segment = seg ; fieldInfos = fis ; origEnum = new SegmentTermEnum ( directory . openInput ( segment + "." + IndexFileNames . TERMS_EXTENSION , readBufferSize ) , fieldInfos , false ) ; size = origEnum . size ; totalIndexInterval = origEnum . indexInterval ; indexEnum = new SegmentTermEnum ( directory . openInput ( segment + "." + IndexFileNames . TERMS_INDEX_EXTENSION , readBufferSize ) , fieldInfos , true ) ; success = true ; } finally { if ( ! success ) { close ( ) ; } } } public int getSkipInterval ( ) { return origEnum . skipInterval ; } public int getMaxSkipLevels ( ) { return origEnum . maxSkipLevels ; } public void setIndexDivisor ( int indexDivisor ) throws IllegalStateException { if ( indexDivisor < 1 ) throw new IllegalArgumentException ( "indexDivisor must be > 0: got " + indexDivisor ) ; if ( indexTerms != null ) throw new IllegalStateException ( "index terms are already loaded" ) ; this . indexDivisor = indexDivisor ; totalIndexInterval = origEnum . indexInterval * indexDivisor ; } public int getIndexDivisor ( ) { return indexDivisor ; } final void close ( ) throws IOException { if ( origEnum != null ) origEnum . close ( ) ; if ( indexEnum != null ) indexEnum . close ( ) ; threadResources . close ( ) ; } final long size ( ) { return size ; } private ThreadResources getThreadResources ( ) { ThreadResources resources = ( ThreadResources ) threadResources . get ( ) ; if ( resources == null ) { resources = new ThreadResources ( ) ; resources . termEnum = terms ( ) ; resources . termInfoCache = new SimpleLRUCache ( DEFAULT_CACHE_SIZE ) ; threadResources . set ( resources ) ; } return resources ; } private synchronized void ensureIndexIsRead ( ) throws IOException { if ( indexTerms != null ) return ; try { int indexSize = 1 + ( ( int ) indexEnum . size - 1 ) / indexDivisor ; indexTerms = new Term [ indexSize ] ; indexInfos = new TermInfo [ indexSize ] ; indexPointers = new long [ indexSize ] ; for ( int i = 0 ; indexEnum . next ( ) ; i ++ ) { indexTerms [ i ] = indexEnum . term ( ) ; indexInfos [ i ] = indexEnum . termInfo ( ) ; indexPointers [ i ] = indexEnum . indexPointer ; for ( int j = 1 ; j < indexDivisor ; j ++ ) if ( ! indexEnum . next ( ) ) break ; } } finally { indexEnum . close ( ) ; indexEnum = null ; } } private final int getIndexOffset ( Term term ) { int lo = 0 ; int hi = indexTerms . length - 1 ; while ( hi >= lo ) { int mid = ( lo + hi ) > > 1 ; int delta = term . compareTo ( indexTerms [ mid ] ) ; if ( delta < 0 ) hi = mid - 1 ; else if ( delta > 0 ) lo = mid + 1 ; else return mid ; } return hi ; } private final void seekEnum ( SegmentTermEnum enumerator , int indexOffset ) throws IOException { enumerator . seek ( indexPointers [ indexOffset ] , ( indexOffset * totalIndexInterval ) - 1 , indexTerms [ indexOffset ] , indexInfos [ indexOffset ] ) ; } TermInfo get ( Term term ) throws IOException { return get ( term , true ) ; } private TermInfo get ( Term term , boolean useCache ) throws IOException { if ( size == 0 ) return null ; ensureIndexIsRead ( ) ; TermInfo ti ; ThreadResources resources = getThreadResources ( ) ; Cache cache = null ; if ( useCache ) { cache = resources . termInfoCache ; ti = ( TermInfo ) cache . get ( term ) ; if ( ti != null ) { return ti ; } } SegmentTermEnum enumerator = resources . termEnum ; if ( enumerator . term ( ) != null && ( ( enumerator . prev ( ) != null && term . compareTo ( enumerator . prev ( ) ) > 0 ) || term . compareTo ( enumerator . term ( ) ) >= 0 ) ) { int enumOffset = ( int ) ( enumerator . position / totalIndexInterval ) + 1 ; if ( indexTerms . length == enumOffset || term . compareTo ( indexTerms [ enumOffset ] ) < 0 ) { int numScans = enumerator . scanTo ( term ) ; if ( enumerator . term ( ) != null && term . compareTo ( enumerator . term ( ) ) == 0 ) { ti = enumerator . termInfo ( ) ; if ( cache != null && numScans > 1 ) { cache . put ( term , ti ) ; } } else { ti = null ; } return ti ; } } seekEnum ( enumerator , getIndexOffset ( term ) ) ; enumerator . scanTo ( term ) ; if ( enumerator . term ( ) != null && term . compareTo ( enumerator . term ( ) ) == 0 ) { ti = enumerator . termInfo ( ) ; if ( cache != null ) { cache . put ( term , ti ) ; } } else { ti = null ; } return ti ; } final Term get ( int position ) throws IOException { if ( size == 0 ) return null ; SegmentTermEnum enumerator = getThreadResources ( ) . termEnum ; if ( enumerator != null && enumerator . term ( ) != null && position >= enumerator . position && position < ( enumerator . position + totalIndexInterval ) ) return scanEnum ( enumerator , position ) ; seekEnum ( enumerator , position / totalIndexInterval ) ; return scanEnum ( enumerator , position ) ; } private final Term scanEnum ( SegmentTermEnum enumerator , int position ) throws IOException { while ( enumerator . position < position ) if ( ! enumerator . next ( ) ) return null ; return enumerator . term ( ) ; } final long getPosition ( Term term ) throws IOException { if ( size == 0 ) return - 1 ; ensureIndexIsRead ( ) ; int indexOffset = getIndexOffset ( term ) ; SegmentTermEnum enumerator = getThreadResources ( ) . termEnum ; seekEnum ( enumerator , indexOffset ) ; while ( term . compareTo ( enumerator . term ( ) ) > 0 && enumerator . next ( ) ) { } if ( term . compareTo ( enumerator . term ( ) ) == 0 ) return enumerator . position ; else return - 1 ; } public SegmentTermEnum terms ( ) { return ( SegmentTermEnum ) origEnum . clone ( ) ; } public SegmentTermEnum terms ( Term term ) throws IOException { get ( term , false ) ; return ( SegmentTermEnum ) getThreadResources ( ) . termEnum . clone ( ) ; } } 	1	['19', '1', '0', '14', '45', '79', '2', '12', '6', '0.709401709', '629', '1', '7', '0', '0.25', '0', '0', '31.42105263', '4', '1.1053', '2']
package org . apache . lucene . analysis ; import java . io . IOException ; public class TeeTokenFilter extends TokenFilter { SinkTokenizer sink ; public TeeTokenFilter ( TokenStream input , SinkTokenizer sink ) { super ( input ) ; this . sink = sink ; } public Token next ( final Token reusableToken ) throws IOException { assert reusableToken != null ; Token nextToken = input . next ( reusableToken ) ; sink . add ( nextToken ) ; return nextToken ; } } 	0	['4', '3', '0', '4', '12', '2', '0', '4', '2', '0.777777778', '58', '0', '1', '0.777777778', '0.4', '1', '2', '12.75', '1', '0.5', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import java . io . IOException ; import java . util . List ; import java . util . ArrayList ; import java . util . Set ; public abstract class MergePolicy { public static class OneMerge { SegmentInfo info ; boolean mergeDocStores ; boolean optimize ; SegmentInfos segmentsClone ; boolean increfDone ; boolean registerDone ; long mergeGen ; boolean isExternal ; int maxNumSegmentsOptimize ; final SegmentInfos segments ; final boolean useCompoundFile ; boolean aborted ; Throwable error ; public OneMerge ( SegmentInfos segments , boolean useCompoundFile ) { if ( 0 == segments . size ( ) ) throw new RuntimeException ( "segments must include at least one segment" ) ; this . segments = segments ; this . useCompoundFile = useCompoundFile ; } synchronized void setException ( Throwable error ) { this . error = error ; } synchronized Throwable getException ( ) { return error ; } synchronized void abort ( ) { aborted = true ; } synchronized boolean isAborted ( ) { return aborted ; } synchronized void checkAborted ( Directory dir ) throws MergeAbortedException { if ( aborted ) throw new MergeAbortedException ( "merge is aborted: " + segString ( dir ) ) ; } String segString ( Directory dir ) { StringBuffer b = new StringBuffer ( ) ; final int numSegments = segments . size ( ) ; for ( int i = 0 ; i < numSegments ; i ++ ) { if ( i > 0 ) b . append ( ' ' ) ; b . append ( segments . info ( i ) . segString ( dir ) ) ; } if ( info != null ) b . append ( " into " ) . append ( info . name ) ; if ( optimize ) b . append ( " [optimize]" ) ; return b . toString ( ) ; } } public static class MergeSpecification { public List merges = new ArrayList ( ) ; public void add ( OneMerge merge ) { merges . add ( merge ) ; } public String segString ( Directory dir ) { StringBuffer b = new StringBuffer ( ) ; b . append ( "MergeSpec:\n" ) ; final int count = merges . size ( ) ; for ( int i = 0 ; i < count ; i ++ ) b . append ( "  " ) . append ( 1 + i ) . append ( ": " ) . append ( ( ( OneMerge ) merges . get ( i ) ) . segString ( dir ) ) ; return b . toString ( ) ; } } public static class MergeException extends RuntimeException { private Directory dir ; public MergeException ( String message ) { super ( message ) ; } public MergeException ( String message , Directory dir ) { super ( message ) ; this . dir = dir ; } public MergeException ( Throwable exc ) { super ( exc ) ; } public MergeException ( Throwable exc , Directory dir ) { super ( exc ) ; this . dir = dir ; } public Directory getDirectory ( ) { return dir ; } } public static class MergeAbortedException extends IOException { public MergeAbortedException ( ) { super ( "merge is aborted" ) ; } public MergeAbortedException ( String message ) { super ( message ) ; } } abstract MergeSpecification findMerges ( SegmentInfos segmentInfos , IndexWriter writer ) throws CorruptIndexException , IOException ; abstract MergeSpecification findMergesForOptimize ( SegmentInfos segmentInfos , IndexWriter writer , int maxSegmentCount , Set segmentsToOptimize ) throws CorruptIndexException , IOException ; MergeSpecification findMergesToExpungeDeletes ( SegmentInfos segmentInfos , IndexWriter writer ) throws CorruptIndexException , IOException { throw new RuntimeException ( "not implemented" ) ; } abstract void close ( ) ; abstract boolean useCompoundFile ( SegmentInfos segments , SegmentInfo newSegment ) ; abstract boolean useCompoundDocStore ( SegmentInfos segments ) ; } 	1	['7', '1', '1', '6', '9', '21', '2', '5', '1', '2', '15', '0', '0', '0', '0.428571429', '0', '0', '1.142857143', '1', '0.8571', '3']
package org . apache . lucene . index ; public interface TermPositionVector extends TermFreqVector { public int [ ] getTermPositions ( int index ) ; public TermVectorOffsetInfo [ ] getOffsets ( int index ) ; } 	0	['2', '1', '0', '4', '2', '1', '2', '2', '2', '2', '2', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search ; public abstract class DocIdSet { public abstract DocIdSetIterator iterator ( ) ; } 	1	['2', '1', '3', '16', '3', '1', '15', '1', '2', '2', '5', '0', '0', '0', '1', '0', '0', '1.5', '1', '0.5', '5']
package org . apache . lucene . queryParser ; import java . io . * ; public final class FastCharStream implements CharStream { char [ ] buffer = null ; int bufferLength = 0 ; int bufferPosition = 0 ; int tokenStart = 0 ; int bufferStart = 0 ; Reader input ; public FastCharStream ( Reader r ) { input = r ; } public final char readChar ( ) throws IOException { if ( bufferPosition >= bufferLength ) refill ( ) ; return buffer [ bufferPosition ++ ] ; } private final void refill ( ) throws IOException { int newPosition = bufferLength - tokenStart ; if ( tokenStart == 0 ) { if ( buffer == null ) { buffer = new char [ 2048 ] ; } else if ( bufferLength == buffer . length ) { char [ ] newBuffer = new char [ buffer . length * 2 ] ; System . arraycopy ( buffer , 0 , newBuffer , 0 , bufferLength ) ; buffer = newBuffer ; } } else { System . arraycopy ( buffer , tokenStart , buffer , 0 , newPosition ) ; } bufferLength = newPosition ; bufferPosition = newPosition ; bufferStart += tokenStart ; tokenStart = 0 ; int charsRead = input . read ( buffer , newPosition , buffer . length - newPosition ) ; if ( charsRead == - 1 ) throw new IOException ( "read past eof" ) ; else bufferLength += charsRead ; } public final char BeginToken ( ) throws IOException { tokenStart = bufferPosition ; return readChar ( ) ; } public final void backup ( int amount ) { bufferPosition -= amount ; } public final String GetImage ( ) { return new String ( buffer , tokenStart , bufferPosition - tokenStart ) ; } public final char [ ] GetSuffix ( int len ) { char [ ] value = new char [ len ] ; System . arraycopy ( buffer , bufferPosition - len , value , 0 , len ) ; return value ; } public final void Done ( ) { try { input . close ( ) ; } catch ( IOException e ) { System . err . println ( "Caught: " + e + "; ignoring." ) ; } } public final int getColumn ( ) { return bufferStart + bufferPosition ; } public final int getLine ( ) { return 1 ; } public final int getEndColumn ( ) { return bufferStart + bufferPosition ; } public final int getEndLine ( ) { return 1 ; } public final int getBeginColumn ( ) { return bufferStart + tokenStart ; } public final int getBeginLine ( ) { return 1 ; } } 	0	['14', '1', '0', '2', '25', '3', '1', '1', '13', '0.602564103', '237', '0', '0', '0', '0.404761905', '0', '0', '15.5', '1', '0.9286', '0']
package org . apache . lucene . search ; import java . util . ArrayList ; public class Explanation implements java . io . Serializable { private float value ; private String description ; private ArrayList details ; public Explanation ( ) { } public Explanation ( float value , String description ) { this . value = value ; this . description = description ; } public boolean isMatch ( ) { return ( 0.0f < getValue ( ) ) ; } public float getValue ( ) { return value ; } public void setValue ( float value ) { this . value = value ; } public String getDescription ( ) { return description ; } public void setDescription ( String description ) { this . description = description ; } protected String getSummary ( ) { return getValue ( ) + " = " + getDescription ( ) ; } public Explanation [ ] getDetails ( ) { if ( details == null ) return null ; return ( Explanation [ ] ) details . toArray ( new Explanation [ 0 ] ) ; } public void addDetail ( Explanation detail ) { if ( details == null ) details = new ArrayList ( ) ; details . add ( detail ) ; } public String toString ( ) { return toString ( 0 ) ; } protected String toString ( int depth ) { StringBuffer buffer = new StringBuffer ( ) ; for ( int i = 0 ; i < depth ; i ++ ) { buffer . append ( "  " ) ; } buffer . append ( getSummary ( ) ) ; buffer . append ( "\n" ) ; Explanation [ ] details = getDetails ( ) ; if ( details != null ) { for ( int i = 0 ; i < details . length ; i ++ ) { buffer . append ( details [ i ] . toString ( depth + 1 ) ) ; } } return buffer . toString ( ) ; } public String toHtml ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "<ul>\n" ) ; buffer . append ( "<li>" ) ; buffer . append ( getSummary ( ) ) ; buffer . append ( "<br />\n" ) ; Explanation [ ] details = getDetails ( ) ; if ( details != null ) { for ( int i = 0 ; i < details . length ; i ++ ) { buffer . append ( details [ i ] . toHtml ( ) ) ; } } buffer . append ( "</li>\n" ) ; buffer . append ( "</ul>\n" ) ; return buffer . toString ( ) ; } } 	1	['13', '1', '1', '41', '21', '64', '41', '0', '11', '0.611111111', '197', '1', '0', '0', '0.292307692', '0', '0', '13.92307692', '4', '1.4615', '1']
package org . apache . lucene . search ; public class DefaultSimilarity extends Similarity { public float lengthNorm ( String fieldName , int numTerms ) { return ( float ) ( 1.0 / Math . sqrt ( numTerms ) ) ; } public float queryNorm ( float sumOfSquaredWeights ) { return ( float ) ( 1.0 / Math . sqrt ( sumOfSquaredWeights ) ) ; } public float tf ( float freq ) { return ( float ) Math . sqrt ( freq ) ; } public float sloppyFreq ( int distance ) { return 1.0f / ( distance + 1 ) ; } public float idf ( int docFreq , int numDocs ) { return ( float ) ( Math . log ( numDocs / ( double ) ( docFreq + 1 ) ) + 1.0 ) ; } public float coord ( int overlap , int maxOverlap ) { return overlap / ( float ) maxOverlap ; } } 	0	['7', '2', '0', '3', '10', '21', '3', '1', '7', '2', '54', '0', '0', '0.714285714', '0.5', '1', '2', '6.714285714', '1', '0.8571', '0']
package org . apache . lucene . search . spans ; import java . io . IOException ; public interface Spans { boolean next ( ) throws IOException ; boolean skipTo ( int target ) throws IOException ; int doc ( ) ; int start ( ) ; int end ( ) ; } 	1	['5', '1', '0', '17', '5', '10', '17', '0', '5', '2', '5', '0', '0', '0', '0.6', '0', '0', '0', '1', '1', '2']
package org . apache . lucene . index ; import java . io . IOException ; final class TermsHashPerThread extends InvertedDocConsumerPerThread { final TermsHash termsHash ; final TermsHashConsumerPerThread consumer ; final TermsHashPerThread nextPerThread ; final CharBlockPool charPool ; final IntBlockPool intPool ; final ByteBlockPool bytePool ; final boolean primary ; final DocumentsWriter . DocState docState ; final RawPostingList freePostings [ ] = new RawPostingList [ 256 ] ; int freePostingsCount ; public TermsHashPerThread ( DocInverterPerThread docInverterPerThread , final TermsHash termsHash , final TermsHash nextTermsHash , final TermsHashPerThread primaryPerThread ) { docState = docInverterPerThread . docState ; this . termsHash = termsHash ; this . consumer = termsHash . consumer . addThread ( this ) ; if ( nextTermsHash != null ) { charPool = new CharBlockPool ( termsHash . docWriter ) ; primary = true ; } else { charPool = primaryPerThread . charPool ; primary = false ; } intPool = new IntBlockPool ( termsHash . docWriter , termsHash . trackAllocations ) ; bytePool = new ByteBlockPool ( termsHash . docWriter . byteBlockAllocator , termsHash . trackAllocations ) ; if ( nextTermsHash != null ) nextPerThread = nextTermsHash . addThread ( docInverterPerThread , this ) ; else nextPerThread = null ; } InvertedDocConsumerPerField addField ( DocInverterPerField docInverterPerField , final FieldInfo fieldInfo ) { return new TermsHashPerField ( docInverterPerField , this , nextPerThread , fieldInfo ) ; } synchronized public void abort ( ) { reset ( true ) ; consumer . abort ( ) ; if ( nextPerThread != null ) nextPerThread . abort ( ) ; } void morePostings ( ) throws IOException { assert freePostingsCount == 0 ; termsHash . getPostings ( freePostings ) ; freePostingsCount = freePostings . length ; assert noNullPostings ( freePostings , freePostingsCount , "consumer=" + consumer ) ; } private static boolean noNullPostings ( RawPostingList [ ] postings , int count , String details ) { for ( int i = 0 ; i < count ; i ++ ) assert postings [ i ] != null : "postings[" + i + "] of " + count + " is null: " + details ; return true ; } public void startDocument ( ) throws IOException { consumer . startDocument ( ) ; if ( nextPerThread != null ) nextPerThread . consumer . startDocument ( ) ; } public DocumentsWriter . DocWriter finishDocument ( ) throws IOException { final DocumentsWriter . DocWriter doc = consumer . finishDocument ( ) ; final DocumentsWriter . DocWriter doc2 ; if ( nextPerThread != null ) doc2 = nextPerThread . consumer . finishDocument ( ) ; else doc2 = null ; if ( doc == null ) return doc2 ; else { doc . setNext ( doc2 ) ; return doc ; } } void reset ( boolean recyclePostings ) { intPool . reset ( ) ; bytePool . reset ( ) ; if ( primary ) charPool . reset ( ) ; if ( recyclePostings ) { termsHash . recyclePostings ( freePostings , freePostingsCount ) ; freePostingsCount = 0 ; } } } 	0	['10', '2', '0', '24', '37', '7', '9', '18', '4', '0.759259259', '276', '0', '8', '0.333333333', '0.188888889', '0', '0', '25.4', '4', '1.4', '0']
package org . apache . lucene . index ; import java . io . IOException ; final class SegmentMergeInfo { Term term ; int base ; TermEnum termEnum ; IndexReader reader ; private TermPositions postings ; private int [ ] docMap ; SegmentMergeInfo ( int b , TermEnum te , IndexReader r ) throws IOException { base = b ; reader = r ; termEnum = te ; term = te . term ( ) ; } int [ ] getDocMap ( ) { if ( docMap == null ) { if ( reader . hasDeletions ( ) ) { int maxDoc = reader . maxDoc ( ) ; docMap = new int [ maxDoc ] ; int j = 0 ; for ( int i = 0 ; i < maxDoc ; i ++ ) { if ( reader . isDeleted ( i ) ) docMap [ i ] = - 1 ; else docMap [ i ] = j ++ ; } } } return docMap ; } TermPositions getPositions ( ) throws IOException { if ( postings == null ) { postings = reader . termPositions ( ) ; } return postings ; } final boolean next ( ) throws IOException { if ( termEnum . next ( ) ) { term = termEnum . term ( ) ; return true ; } else { term = null ; return false ; } } final void close ( ) throws IOException { termEnum . close ( ) ; if ( postings != null ) { postings . close ( ) ; } } } 	1	['5', '1', '0', '7', '14', '0', '3', '4', '0', '0.75', '108', '0.333333333', '4', '0', '0.4', '0', '0', '19.4', '5', '1.6', '1']
