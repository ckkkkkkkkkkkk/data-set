package org . apache . lucene . search ; import java . io . IOException ; import java . io . StringReader ; import java . util . ArrayList ; import java . util . Arrays ; import java . util . HashMap ; import java . util . Iterator ; import java . util . List ; import java . util . Map ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . analysis . TokenStream ; import org . apache . lucene . index . TermFreqVector ; public class QueryTermVector implements TermFreqVector { private String [ ] terms = new String [ 0 ] ; private int [ ] termFreqs = new int [ 0 ] ; public String getField ( ) { return null ; } public QueryTermVector ( String [ ] queryTerms ) { processTerms ( queryTerms ) ; } public QueryTermVector ( String queryString , Analyzer analyzer ) { if ( analyzer != null ) { TokenStream stream = analyzer . tokenStream ( "" , new StringReader ( queryString ) ) ; if ( stream != null ) { List terms = new ArrayList ( ) ; try { final Token reusableToken = new Token ( ) ; for ( Token nextToken = stream . next ( reusableToken ) ; nextToken != null ; nextToken = stream . next ( reusableToken ) ) { terms . add ( nextToken . term ( ) ) ; } processTerms ( ( String [ ] ) terms . toArray ( new String [ terms . size ( ) ] ) ) ; } catch ( IOException e ) { } } } } private void processTerms ( String [ ] queryTerms ) { if ( queryTerms != null ) { Arrays . sort ( queryTerms ) ; Map tmpSet = new HashMap ( queryTerms . length ) ; List tmpList = new ArrayList ( queryTerms . length ) ; List tmpFreqs = new ArrayList ( queryTerms . length ) ; int j = 0 ; for ( int i = 0 ; i < queryTerms . length ; i ++ ) { String term = queryTerms [ i ] ; Integer position = ( Integer ) tmpSet . get ( term ) ; if ( position == null ) { tmpSet . put ( term , new Integer ( j ++ ) ) ; tmpList . add ( term ) ; tmpFreqs . add ( new Integer ( 1 ) ) ; } else { Integer integer = ( Integer ) tmpFreqs . get ( position . intValue ( ) ) ; tmpFreqs . set ( position . intValue ( ) , new Integer ( integer . intValue ( ) + 1 ) ) ; } } terms = ( String [ ] ) tmpList . toArray ( terms ) ; termFreqs = new int [ tmpFreqs . size ( ) ] ; int i = 0 ; for ( Iterator iter = tmpFreqs . iterator ( ) ; iter . hasNext ( ) ; ) { Integer integer = ( Integer ) iter . next ( ) ; termFreqs [ i ++ ] = integer . intValue ( ) ; } } } public final String toString ( ) { StringBuffer sb = new StringBuffer ( ) ; sb . append ( '{' ) ; for ( int i = 0 ; i < terms . length ; i ++ ) { if ( i > 0 ) sb . append ( ", " ) ; sb . append ( terms [ i ] ) . append ( '/' ) . append ( termFreqs [ i ] ) ; } sb . append ( '}' ) ; return sb . toString ( ) ; } public int size ( ) { return terms . length ; } public String [ ] getTerms ( ) { return terms ; } public int [ ] getTermFrequencies ( ) { return termFreqs ; } public int indexOf ( String term ) { int res = Arrays . binarySearch ( terms , term ) ; return res >= 0 ? res : - 1 ; } public int [ ] indexesOf ( String [ ] terms , int start , int len ) { int res [ ] = new int [ len ] ; for ( int i = 0 ; i < len ; i ++ ) { res [ i ] = indexOf ( terms [ i ] ) ; } return res ; } } 	0	['10', '1', '0', '4', '38', '0', '0', '4', '9', '0.388888889', '287', '1', '0', '0', '0.34', '0', '0', '27.5', '5', '1.6', '0']
package org . apache . lucene . index ; class ReadOnlySegmentReader extends SegmentReader { static void noWrite ( ) { throw new UnsupportedOperationException ( "This IndexReader cannot make any changes to the index (it was opened with readOnly = true)" ) ; } protected void acquireWriteLock ( ) { noWrite ( ) ; } public boolean isDeleted ( int n ) { return deletedDocs != null && deletedDocs . get ( n ) ; } } 	0	['4', '4', '0', '3', '7', '6', '2', '2', '1', '2', '26', '0', '0', '0.98125', '0.5', '2', '3', '5.5', '3', '1.25', '0']
package org . apache . lucene . index ; import java . io . IOException ; abstract class DocFieldConsumerPerThread { abstract void startDocument ( ) throws IOException ; abstract DocumentsWriter . DocWriter finishDocument ( ) throws IOException ; abstract DocFieldConsumerPerField addField ( FieldInfo fi ) ; abstract void abort ( ) ; } 	0	['5', '1', '3', '13', '6', '10', '10', '3', '0', '2', '8', '0', '0', '0', '0.6', '0', '0', '0.6', '1', '0.8', '0']
package org . apache . lucene . index ; import org . apache . lucene . util . PriorityQueue ; import java . io . IOException ; import java . util . Arrays ; import java . util . Iterator ; import java . util . LinkedList ; import java . util . List ; public class MultipleTermPositions implements TermPositions { private static final class TermPositionsQueue extends PriorityQueue { TermPositionsQueue ( List termPositions ) throws IOException { initialize ( termPositions . size ( ) ) ; Iterator i = termPositions . iterator ( ) ; while ( i . hasNext ( ) ) { TermPositions tp = ( TermPositions ) i . next ( ) ; if ( tp . next ( ) ) put ( tp ) ; } } final TermPositions peek ( ) { return ( TermPositions ) top ( ) ; } public final boolean lessThan ( Object a , Object b ) { return ( ( TermPositions ) a ) . doc ( ) < ( ( TermPositions ) b ) . doc ( ) ; } } private static final class IntQueue { private int _arraySize = 16 ; private int _index = 0 ; private int _lastIndex = 0 ; private int [ ] _array = new int [ _arraySize ] ; final void add ( int i ) { if ( _lastIndex == _arraySize ) growArray ( ) ; _array [ _lastIndex ++ ] = i ; } final int next ( ) { return _array [ _index ++ ] ; } final void sort ( ) { Arrays . sort ( _array , _index , _lastIndex ) ; } final void clear ( ) { _index = 0 ; _lastIndex = 0 ; } final int size ( ) { return ( _lastIndex - _index ) ; } private void growArray ( ) { int [ ] newArray = new int [ _arraySize * 2 ] ; System . arraycopy ( _array , 0 , newArray , 0 , _arraySize ) ; _array = newArray ; _arraySize *= 2 ; } } private int _doc ; private int _freq ; private TermPositionsQueue _termPositionsQueue ; private IntQueue _posList ; public MultipleTermPositions ( IndexReader indexReader , Term [ ] terms ) throws IOException { List termPositions = new LinkedList ( ) ; for ( int i = 0 ; i < terms . length ; i ++ ) termPositions . add ( indexReader . termPositions ( terms [ i ] ) ) ; _termPositionsQueue = new TermPositionsQueue ( termPositions ) ; _posList = new IntQueue ( ) ; } public final boolean next ( ) throws IOException { if ( _termPositionsQueue . size ( ) == 0 ) return false ; _posList . clear ( ) ; _doc = _termPositionsQueue . peek ( ) . doc ( ) ; TermPositions tp ; do { tp = _termPositionsQueue . peek ( ) ; for ( int i = 0 ; i < tp . freq ( ) ; i ++ ) _posList . add ( tp . nextPosition ( ) ) ; if ( tp . next ( ) ) _termPositionsQueue . adjustTop ( ) ; else { _termPositionsQueue . pop ( ) ; tp . close ( ) ; } } while ( _termPositionsQueue . size ( ) > 0 && _termPositionsQueue . peek ( ) . doc ( ) == _doc ) ; _posList . sort ( ) ; _freq = _posList . size ( ) ; return true ; } public final int nextPosition ( ) { return _posList . next ( ) ; } public final boolean skipTo ( int target ) throws IOException { while ( _termPositionsQueue . peek ( ) != null && target > _termPositionsQueue . peek ( ) . doc ( ) ) { TermPositions tp = ( TermPositions ) _termPositionsQueue . pop ( ) ; if ( tp . skipTo ( target ) ) _termPositionsQueue . put ( tp ) ; else tp . close ( ) ; } return next ( ) ; } public final int doc ( ) { return _doc ; } public final int freq ( ) { return _freq ; } public final void close ( ) throws IOException { while ( _termPositionsQueue . size ( ) > 0 ) ( ( TermPositions ) _termPositionsQueue . pop ( ) ) . close ( ) ; } public void seek ( Term arg0 ) throws IOException { throw new UnsupportedOperationException ( ) ; } public void seek ( TermEnum termEnum ) throws IOException { throw new UnsupportedOperationException ( ) ; } public int read ( int [ ] arg0 , int [ ] arg1 ) throws IOException { throw new UnsupportedOperationException ( ) ; } public int getPayloadLength ( ) { throw new UnsupportedOperationException ( ) ; } public byte [ ] getPayload ( byte [ ] data , int offset ) throws IOException { throw new UnsupportedOperationException ( ) ; } public boolean isPayloadAvailable ( ) { return false ; } } 	0	['13', '1', '0', '8', '36', '58', '1', '7', '13', '0.791666667', '191', '1', '2', '0', '0.201923077', '0', '0', '13.38461538', '1', '0.9231', '0']
package org . apache . lucene . index ; import java . io . IOException ; public interface TermDocs { void seek ( Term term ) throws IOException ; void seek ( TermEnum termEnum ) throws IOException ; int doc ( ) ; int freq ( ) ; boolean next ( ) throws IOException ; int read ( int [ ] docs , int [ ] freqs ) throws IOException ; boolean skipTo ( int target ) throws IOException ; void close ( ) throws IOException ; } 	0	['8', '1', '0', '30', '8', '28', '28', '2', '8', '2', '8', '0', '0', '0', '0.3', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . index ; final class IntBlockPool { public int [ ] [ ] buffers = new int [ 10 ] [ ] ; int bufferUpto = - 1 ; public int intUpto = DocumentsWriter . INT_BLOCK_SIZE ; public int [ ] buffer ; public int intOffset = - DocumentsWriter . INT_BLOCK_SIZE ; final private DocumentsWriter docWriter ; final boolean trackAllocations ; public IntBlockPool ( DocumentsWriter docWriter , boolean trackAllocations ) { this . docWriter = docWriter ; this . trackAllocations = trackAllocations ; } public void reset ( ) { if ( bufferUpto != - 1 ) { if ( bufferUpto > 0 ) docWriter . recycleIntBlocks ( buffers , 1 , 1 + bufferUpto ) ; bufferUpto = 0 ; intUpto = 0 ; intOffset = 0 ; buffer = buffers [ 0 ] ; } } public void nextBuffer ( ) { if ( 1 + bufferUpto == buffers . length ) { int [ ] [ ] newBuffers = new int [ ( int ) ( buffers . length * 1.5 ) ] [ ] ; System . arraycopy ( buffers , 0 , newBuffers , 0 , buffers . length ) ; buffers = newBuffers ; } buffer = buffers [ 1 + bufferUpto ] = docWriter . getIntBlock ( trackAllocations ) ; bufferUpto ++ ; intUpto = 0 ; intOffset += DocumentsWriter . INT_BLOCK_SIZE ; } } 	0	['3', '1', '0', '3', '7', '0', '2', '1', '3', '0.142857143', '125', '0.142857143', '1', '0', '0.555555556', '0', '0', '38.33333333', '3', '1.6667', '0']
package org . apache . lucene . store ; import java . io . IOException ; import java . util . zip . CRC32 ; import java . util . zip . Checksum ; public class ChecksumIndexInput extends IndexInput { IndexInput main ; Checksum digest ; public ChecksumIndexInput ( IndexInput main ) { this . main = main ; digest = new CRC32 ( ) ; } public byte readByte ( ) throws IOException { final byte b = main . readByte ( ) ; digest . update ( b ) ; return b ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { main . readBytes ( b , offset , len ) ; digest . update ( b , offset , len ) ; } public long getChecksum ( ) { return digest . getValue ( ) ; } public void close ( ) throws IOException { main . close ( ) ; } public long getFilePointer ( ) { return main . getFilePointer ( ) ; } public void seek ( long pos ) { throw new RuntimeException ( "not allowed" ) ; } public long length ( ) { return main . length ( ) ; } } 	0	['8', '2', '0', '2', '19', '0', '1', '1', '8', '0.428571429', '65', '0', '1', '0.708333333', '0.3', '1', '4', '6.875', '1', '0.875', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Collection ; abstract class DocConsumer { abstract DocConsumerPerThread addThread ( DocumentsWriterThreadState perThread ) throws IOException ; abstract void flush ( final Collection threads , final DocumentsWriter . FlushState state ) throws IOException ; abstract void closeDocStore ( final DocumentsWriter . FlushState state ) throws IOException ; abstract void abort ( ) ; abstract boolean freeRAM ( ) ; } 	0	['6', '1', '1', '5', '7', '15', '3', '3', '0', '2', '9', '0', '0', '0', '0.416666667', '0', '0', '0.5', '1', '0.8333', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . analysis . Token ; abstract class TermsHashConsumerPerField { abstract boolean start ( Fieldable [ ] fields , int count ) throws IOException ; abstract void finish ( ) throws IOException ; abstract void skippingLongTerm ( Token t ) throws IOException ; abstract void newTerm ( Token t , RawPostingList p ) throws IOException ; abstract void addTerm ( Token t , RawPostingList p ) throws IOException ; abstract int getStreamCount ( ) ; } 	0	['7', '1', '2', '10', '8', '21', '7', '3', '0', '2', '10', '0', '0', '0', '0.4', '0', '0', '0.428571429', '1', '0.8571', '0']
package org . apache . lucene . index ; final class TermInfo { int docFreq = 0 ; long freqPointer = 0 ; long proxPointer = 0 ; int skipOffset ; TermInfo ( ) { } TermInfo ( int df , long fp , long pp ) { docFreq = df ; freqPointer = fp ; proxPointer = pp ; } TermInfo ( TermInfo ti ) { docFreq = ti . docFreq ; freqPointer = ti . freqPointer ; proxPointer = ti . proxPointer ; skipOffset = ti . skipOffset ; } final void set ( int docFreq , long freqPointer , long proxPointer , int skipOffset ) { this . docFreq = docFreq ; this . freqPointer = freqPointer ; this . proxPointer = proxPointer ; this . skipOffset = skipOffset ; } final void set ( TermInfo ti ) { docFreq = ti . docFreq ; freqPointer = ti . freqPointer ; proxPointer = ti . proxPointer ; skipOffset = ti . skipOffset ; } } 	0	['5', '1', '0', '8', '6', '0', '8', '0', '0', '0.125', '100', '0', '0', '0', '0.55', '0', '0', '18.2', '1', '0.4', '0']
package org . apache . lucene . index ; import java . io . IOException ; public class CorruptIndexException extends IOException { public CorruptIndexException ( String message ) { super ( message ) ; } } 	0	['1', '4', '0', '36', '2', '0', '36', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . queryParser ; import java . util . ArrayList ; import java . util . List ; import java . util . Map ; import java . util . Vector ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . search . BooleanClause ; import org . apache . lucene . search . BooleanQuery ; import org . apache . lucene . search . MultiPhraseQuery ; import org . apache . lucene . search . PhraseQuery ; import org . apache . lucene . search . Query ; public class MultiFieldQueryParser extends QueryParser { protected String [ ] fields ; protected Map boosts ; public MultiFieldQueryParser ( String [ ] fields , Analyzer analyzer , Map boosts ) { this ( fields , analyzer ) ; this . boosts = boosts ; } public MultiFieldQueryParser ( String [ ] fields , Analyzer analyzer ) { super ( null , analyzer ) ; this . fields = fields ; } protected Query getFieldQuery ( String field , String queryText , int slop ) throws ParseException { if ( field == null ) { List clauses = new ArrayList ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { Query q = super . getFieldQuery ( fields [ i ] , queryText ) ; if ( q != null ) { if ( boosts != null ) { Float boost = ( Float ) boosts . get ( fields [ i ] ) ; if ( boost != null ) { q . setBoost ( boost . floatValue ( ) ) ; } } applySlop ( q , slop ) ; clauses . add ( new BooleanClause ( q , BooleanClause . Occur . SHOULD ) ) ; } } if ( clauses . size ( ) == 0 ) return null ; return getBooleanQuery ( clauses , true ) ; } Query q = super . getFieldQuery ( field , queryText ) ; applySlop ( q , slop ) ; return q ; } private void applySlop ( Query q , int slop ) { if ( q instanceof PhraseQuery ) { ( ( PhraseQuery ) q ) . setSlop ( slop ) ; } else if ( q instanceof MultiPhraseQuery ) { ( ( MultiPhraseQuery ) q ) . setSlop ( slop ) ; } } protected Query getFieldQuery ( String field , String queryText ) throws ParseException { return getFieldQuery ( field , queryText , 0 ) ; } protected Query getFuzzyQuery ( String field , String termStr , float minSimilarity ) throws ParseException { if ( field == null ) { List clauses = new ArrayList ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( getFuzzyQuery ( fields [ i ] , termStr , minSimilarity ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getFuzzyQuery ( field , termStr , minSimilarity ) ; } protected Query getPrefixQuery ( String field , String termStr ) throws ParseException { if ( field == null ) { List clauses = new ArrayList ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( getPrefixQuery ( fields [ i ] , termStr ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getPrefixQuery ( field , termStr ) ; } protected Query getWildcardQuery ( String field , String termStr ) throws ParseException { if ( field == null ) { List clauses = new ArrayList ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( getWildcardQuery ( fields [ i ] , termStr ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getWildcardQuery ( field , termStr ) ; } protected Query getRangeQuery ( String field , String part1 , String part2 , boolean inclusive ) throws ParseException { if ( field == null ) { List clauses = new ArrayList ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { clauses . add ( new BooleanClause ( getRangeQuery ( fields [ i ] , part1 , part2 , inclusive ) , BooleanClause . Occur . SHOULD ) ) ; } return getBooleanQuery ( clauses , true ) ; } return super . getRangeQuery ( field , part1 , part2 , inclusive ) ; } public static Query parse ( String [ ] queries , String [ ] fields , Analyzer analyzer ) throws ParseException { if ( queries . length != fields . length ) throw new IllegalArgumentException ( "queries.length != fields.length" ) ; BooleanQuery bQuery = new BooleanQuery ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { QueryParser qp = new QueryParser ( fields [ i ] , analyzer ) ; Query q = qp . parse ( queries [ i ] ) ; if ( q != null && ( ! ( q instanceof BooleanQuery ) || ( ( BooleanQuery ) q ) . getClauses ( ) . length > 0 ) ) { bQuery . add ( q , BooleanClause . Occur . SHOULD ) ; } } return bQuery ; } public static Query parse ( String query , String [ ] fields , BooleanClause . Occur [ ] flags , Analyzer analyzer ) throws ParseException { if ( fields . length != flags . length ) throw new IllegalArgumentException ( "fields.length != flags.length" ) ; BooleanQuery bQuery = new BooleanQuery ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { QueryParser qp = new QueryParser ( fields [ i ] , analyzer ) ; Query q = qp . parse ( query ) ; if ( q != null && ( ! ( q instanceof BooleanQuery ) || ( ( BooleanQuery ) q ) . getClauses ( ) . length > 0 ) ) { bQuery . add ( q , flags [ i ] ) ; } } return bQuery ; } public static Query parse ( String [ ] queries , String [ ] fields , BooleanClause . Occur [ ] flags , Analyzer analyzer ) throws ParseException { if ( ! ( queries . length == fields . length && queries . length == flags . length ) ) throw new IllegalArgumentException ( "queries, fields, and flags array have have different length" ) ; BooleanQuery bQuery = new BooleanQuery ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { QueryParser qp = new QueryParser ( fields [ i ] , analyzer ) ; Query q = qp . parse ( queries [ i ] ) ; if ( q != null && ( ! ( q instanceof BooleanQuery ) || ( ( BooleanQuery ) q ) . getClauses ( ) . length > 0 ) ) { bQuery . add ( q , flags [ i ] ) ; } } return bQuery ; } } 	0	['12', '2', '0', '9', '33', '34', '0', '9', '5', '0.590909091', '453', '1', '0', '0.885057471', '0.283333333', '1', '5', '36.58333333', '3', '1', '0']
package org . apache . lucene . search ; public interface ScoreDocComparator { static final ScoreDocComparator RELEVANCE = new ScoreDocComparator ( ) { public int compare ( ScoreDoc i , ScoreDoc j ) { if ( i . score > j . score ) return - 1 ; if ( i . score < j . score ) return 1 ; return 0 ; } public Comparable sortValue ( ScoreDoc i ) { return new Float ( i . score ) ; } public int sortType ( ) { return SortField . SCORE ; } } ; static final ScoreDocComparator INDEXORDER = new ScoreDocComparator ( ) { public int compare ( ScoreDoc i , ScoreDoc j ) { if ( i . doc < j . doc ) return - 1 ; if ( i . doc > j . doc ) return 1 ; return 0 ; } public Comparable sortValue ( ScoreDoc i ) { return new Integer ( i . doc ) ; } public int sortType ( ) { return SortField . DOC ; } } ; int compare ( ScoreDoc i , ScoreDoc j ) ; Comparable sortValue ( ScoreDoc i ) ; int sortType ( ) ; } 	0	['4', '1', '0', '16', '6', '6', '15', '3', '3', '1', '15', '0', '2', '0', '0.833333333', '0', '0', '2.25', '1', '0.75', '0']
package org . apache . lucene . index ; import java . io . IOException ; abstract class TermsHashConsumerPerThread { abstract void startDocument ( ) throws IOException ; abstract DocumentsWriter . DocWriter finishDocument ( ) throws IOException ; abstract public TermsHashConsumerPerField addField ( TermsHashPerField termsHashPerField , FieldInfo fieldInfo ) ; abstract public void abort ( ) ; } 	0	['5', '1', '2', '11', '6', '10', '8', '4', '2', '2', '8', '0', '0', '0', '0.466666667', '0', '0', '0.6', '1', '0.8', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; public final class LowerCaseFilter extends TokenFilter { public LowerCaseFilter ( TokenStream in ) { super ( in ) ; } public final Token next ( final Token reusableToken ) throws IOException { assert reusableToken != null ; Token nextToken = input . next ( reusableToken ) ; if ( nextToken != null ) { final char [ ] buffer = nextToken . termBuffer ( ) ; final int length = nextToken . termLength ( ) ; for ( int i = 0 ; i < length ; i ++ ) buffer [ i ] = Character . toLowerCase ( buffer [ i ] ) ; return nextToken ; } else return null ; } } 	0	['4', '3', '0', '4', '14', '4', '1', '3', '2', '0.833333333', '74', '0', '0', '0.777777778', '0.416666667', '1', '2', '17', '1', '0.5', '0']
package org . apache . lucene . index ; import org . apache . lucene . util . ArrayUtil ; import org . apache . lucene . search . Similarity ; final class NormsWriterPerField extends InvertedDocEndConsumerPerField implements Comparable { final NormsWriterPerThread perThread ; final FieldInfo fieldInfo ; final DocumentsWriter . DocState docState ; int [ ] docIDs = new int [ 1 ] ; byte [ ] norms = new byte [ 1 ] ; int upto ; final DocInverter . FieldInvertState fieldState ; public void reset ( ) { docIDs = ArrayUtil . shrink ( docIDs , upto ) ; norms = ArrayUtil . shrink ( norms , upto ) ; upto = 0 ; } public NormsWriterPerField ( final DocInverterPerField docInverterPerField , final NormsWriterPerThread perThread , final FieldInfo fieldInfo ) { this . perThread = perThread ; this . fieldInfo = fieldInfo ; docState = perThread . docState ; fieldState = docInverterPerField . fieldState ; } void abort ( ) { upto = 0 ; } public int compareTo ( Object other ) { return fieldInfo . name . compareTo ( ( ( NormsWriterPerField ) other ) . fieldInfo . name ) ; } void finish ( ) { assert docIDs . length == norms . length ; if ( fieldInfo . isIndexed && ! fieldInfo . omitNorms ) { if ( docIDs . length <= upto ) { assert docIDs . length == upto ; docIDs = ArrayUtil . grow ( docIDs , 1 + upto ) ; norms = ArrayUtil . grow ( norms , 1 + upto ) ; } final float norm = fieldState . boost * docState . similarity . lengthNorm ( fieldInfo . name , fieldState . length ) ; norms [ upto ] = Similarity . encodeNorm ( norm ) ; docIDs [ upto ] = docState . docID ; upto ++ ; } } } 	0	['7', '2', '0', '9', '20', '5', '2', '8', '3', '0.796296296', '191', '0', '4', '0.285714286', '0.277777778', '0', '0', '25', '8', '1.7143', '0']
package org . apache . lucene . index ; import java . io . IOException ; public class SerialMergeScheduler extends MergeScheduler { synchronized public void merge ( IndexWriter writer ) throws CorruptIndexException , IOException { while ( true ) { MergePolicy . OneMerge merge = writer . getNextMerge ( ) ; if ( merge == null ) break ; writer . merge ( merge ) ; } } public void close ( ) { } } 	0	['3', '2', '0', '5', '6', '3', '1', '4', '3', '2', '18', '0', '0', '0.5', '0.666666667', '0', '0', '5', '1', '0.6667', '0']
package org . apache . lucene . search ; public class TopFieldDocs extends TopDocs { public SortField [ ] fields ; TopFieldDocs ( int totalHits , ScoreDoc [ ] scoreDocs , SortField [ ] fields , float maxScore ) { super ( totalHits , scoreDocs , maxScore ) ; this . fields = fields ; } } 	0	['1', '2', '0', '14', '2', '0', '11', '3', '0', '2', '11', '0', '1', '1', '1', '0', '0', '9', '0', '0', '0']
package org . apache . lucene . document ; import java . util . * ; import org . apache . lucene . search . ScoreDoc ; import org . apache . lucene . search . Searcher ; import org . apache . lucene . index . IndexReader ; public final class Document implements java . io . Serializable { List fields = new ArrayList ( ) ; private float boost = 1.0f ; public Document ( ) { } public void setBoost ( float boost ) { this . boost = boost ; } public float getBoost ( ) { return boost ; } public final void add ( Fieldable field ) { fields . add ( field ) ; } public final void removeField ( String name ) { Iterator it = fields . iterator ( ) ; while ( it . hasNext ( ) ) { Fieldable field = ( Fieldable ) it . next ( ) ; if ( field . name ( ) . equals ( name ) ) { it . remove ( ) ; return ; } } } public final void removeFields ( String name ) { Iterator it = fields . iterator ( ) ; while ( it . hasNext ( ) ) { Fieldable field = ( Fieldable ) it . next ( ) ; if ( field . name ( ) . equals ( name ) ) { it . remove ( ) ; } } } public final Field getField ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Field field = ( Field ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) return field ; } return null ; } public Fieldable getFieldable ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) return field ; } return null ; } public final String get ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( ! field . isBinary ( ) ) ) return field . stringValue ( ) ; } return null ; } public final Enumeration fields ( ) { return new Enumeration ( ) { final Iterator iter = fields . iterator ( ) ; public boolean hasMoreElements ( ) { return iter . hasNext ( ) ; } public Object nextElement ( ) { return iter . next ( ) ; } } ; } public final List getFields ( ) { return fields ; } private final static Field [ ] NO_FIELDS = new Field [ 0 ] ; public final Field [ ] getFields ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Field field = ( Field ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) { result . add ( field ) ; } } if ( result . size ( ) == 0 ) return NO_FIELDS ; return ( Field [ ] ) result . toArray ( new Field [ result . size ( ) ] ) ; } private final static Fieldable [ ] NO_FIELDABLES = new Fieldable [ 0 ] ; public Fieldable [ ] getFieldables ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) ) { result . add ( field ) ; } } if ( result . size ( ) == 0 ) return NO_FIELDABLES ; return ( Fieldable [ ] ) result . toArray ( new Fieldable [ result . size ( ) ] ) ; } private final static String [ ] NO_STRINGS = new String [ 0 ] ; public final String [ ] getValues ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( ! field . isBinary ( ) ) ) result . add ( field . stringValue ( ) ) ; } if ( result . size ( ) == 0 ) return NO_STRINGS ; return ( String [ ] ) result . toArray ( new String [ result . size ( ) ] ) ; } private final static byte [ ] [ ] NO_BYTES = new byte [ 0 ] [ ] ; public final byte [ ] [ ] getBinaryValues ( String name ) { List result = new ArrayList ( ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( field . isBinary ( ) ) ) result . add ( field . binaryValue ( ) ) ; } if ( result . size ( ) == 0 ) return NO_BYTES ; return ( byte [ ] [ ] ) result . toArray ( new byte [ result . size ( ) ] [ ] ) ; } public final byte [ ] getBinaryValue ( String name ) { for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; if ( field . name ( ) . equals ( name ) && ( field . isBinary ( ) ) ) return field . binaryValue ( ) ; } return null ; } public final String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "Document<" ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) { Fieldable field = ( Fieldable ) fields . get ( i ) ; buffer . append ( field . toString ( ) ) ; if ( i != fields . size ( ) - 1 ) buffer . append ( " " ) ; } buffer . append ( ">" ) ; return buffer . toString ( ) ; } } 	0	['18', '1', '0', '30', '39', '0', '28', '3', '17', '0.81372549', '432', '0.833333333', '2', '0', '0.426470588', '0', '0', '22.66666667', '5', '2.4444', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . * ; final class PhrasePositions { int doc ; int position ; int count ; int offset ; TermPositions tp ; PhrasePositions next ; boolean repeats ; PhrasePositions ( TermPositions t , int o ) { tp = t ; offset = o ; } final boolean next ( ) throws IOException { if ( ! tp . next ( ) ) { tp . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } doc = tp . doc ( ) ; position = 0 ; return true ; } final boolean skipTo ( int target ) throws IOException { if ( ! tp . skipTo ( target ) ) { tp . close ( ) ; doc = Integer . MAX_VALUE ; return false ; } doc = tp . doc ( ) ; position = 0 ; return true ; } final void firstPosition ( ) throws IOException { count = tp . freq ( ) ; nextPosition ( ) ; } final boolean nextPosition ( ) throws IOException { if ( count -- > 0 ) { position = tp . nextPosition ( ) - offset ; return true ; } else return false ; } } 	0	['5', '1', '0', '5', '12', '0', '4', '1', '0', '0.678571429', '95', '0', '2', '0', '0.533333333', '0', '0', '16.6', '1', '0.8', '0']
package org . apache . lucene . document ; import java . io . Serializable ; public interface FieldSelector extends Serializable { FieldSelectorResult accept ( String fieldName ) ; } 	0	['1', '1', '0', '19', '1', '0', '18', '1', '1', '2', '1', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . IndexInput ; import java . util . LinkedList ; import java . util . HashSet ; import java . util . Iterator ; import java . io . IOException ; final class CompoundFileWriter { private static final class FileEntry { String file ; long directoryOffset ; long dataOffset ; } private Directory directory ; private String fileName ; private HashSet ids ; private LinkedList entries ; private boolean merged = false ; private SegmentMerger . CheckAbort checkAbort ; public CompoundFileWriter ( Directory dir , String name ) { this ( dir , name , null ) ; } CompoundFileWriter ( Directory dir , String name , SegmentMerger . CheckAbort checkAbort ) { if ( dir == null ) throw new NullPointerException ( "directory cannot be null" ) ; if ( name == null ) throw new NullPointerException ( "name cannot be null" ) ; this . checkAbort = checkAbort ; directory = dir ; fileName = name ; ids = new HashSet ( ) ; entries = new LinkedList ( ) ; } public Directory getDirectory ( ) { return directory ; } public String getName ( ) { return fileName ; } public void addFile ( String file ) { if ( merged ) throw new IllegalStateException ( "Can't add extensions after merge has been called" ) ; if ( file == null ) throw new NullPointerException ( "file cannot be null" ) ; if ( ! ids . add ( file ) ) throw new IllegalArgumentException ( "File " + file + " already added" ) ; FileEntry entry = new FileEntry ( ) ; entry . file = file ; entries . add ( entry ) ; } public void close ( ) throws IOException { if ( merged ) throw new IllegalStateException ( "Merge already performed" ) ; if ( entries . isEmpty ( ) ) throw new IllegalStateException ( "No entries to merge have been defined" ) ; merged = true ; IndexOutput os = null ; try { os = directory . createOutput ( fileName ) ; os . writeVInt ( entries . size ( ) ) ; Iterator it = entries . iterator ( ) ; long totalSize = 0 ; while ( it . hasNext ( ) ) { FileEntry fe = ( FileEntry ) it . next ( ) ; fe . directoryOffset = os . getFilePointer ( ) ; os . writeLong ( 0 ) ; os . writeString ( fe . file ) ; totalSize += directory . fileLength ( fe . file ) ; } final long finalLength = totalSize + os . getFilePointer ( ) ; os . setLength ( finalLength ) ; byte buffer [ ] = new byte [ 16384 ] ; it = entries . iterator ( ) ; while ( it . hasNext ( ) ) { FileEntry fe = ( FileEntry ) it . next ( ) ; fe . dataOffset = os . getFilePointer ( ) ; copyFile ( fe , os , buffer ) ; } it = entries . iterator ( ) ; while ( it . hasNext ( ) ) { FileEntry fe = ( FileEntry ) it . next ( ) ; os . seek ( fe . directoryOffset ) ; os . writeLong ( fe . dataOffset ) ; } assert finalLength == os . length ( ) ; IndexOutput tmp = os ; os = null ; tmp . close ( ) ; } finally { if ( os != null ) try { os . close ( ) ; } catch ( IOException e ) { } } } private void copyFile ( FileEntry source , IndexOutput os , byte buffer [ ] ) throws IOException { IndexInput is = null ; try { long startPtr = os . getFilePointer ( ) ; is = directory . openInput ( source . file ) ; long length = is . length ( ) ; long remainder = length ; int chunk = buffer . length ; while ( remainder > 0 ) { int len = ( int ) Math . min ( chunk , remainder ) ; is . readBytes ( buffer , 0 , len , false ) ; os . writeBytes ( buffer , len ) ; remainder -= len ; if ( checkAbort != null ) checkAbort . work ( 80 ) ; } if ( remainder != 0 ) throw new IOException ( "Non-zero remainder length after copying: " + remainder + " (id: " + source . file + ", length: " + length + ", buffer size: " + chunk + ")" ) ; long endPtr = os . getFilePointer ( ) ; long diff = endPtr - startPtr ; if ( diff != length ) throw new IOException ( "Difference in the output file offsets " + diff + " does not match the original file length " + length ) ; } finally { if ( is != null ) is . close ( ) ; } } } 	0	['9', '1', '0', '9', '51', '14', '3', '6', '5', '0.703125', '414', '0.75', '2', '0', '0.303571429', '0', '0', '44.11111111', '4', '1', '0']
package org . apache . lucene . index ; import org . apache . lucene . document . Fieldable ; final class DocFieldProcessorPerField { final DocFieldConsumerPerField consumer ; final FieldInfo fieldInfo ; DocFieldProcessorPerField next ; int lastGen = - 1 ; int fieldCount ; Fieldable [ ] fields = new Fieldable [ 1 ] ; public DocFieldProcessorPerField ( final DocFieldProcessorPerThread perThread , final FieldInfo fieldInfo ) { this . consumer = perThread . consumer . addField ( fieldInfo ) ; this . fieldInfo = fieldInfo ; } public void abort ( ) { consumer . abort ( ) ; } } 	0	['2', '1', '0', '5', '5', '0', '1', '5', '2', '1.166666667', '31', '0', '4', '0', '0.666666667', '0', '0', '11.5', '1', '0.5', '0']
package org . apache . lucene . index ; import java . util . HashMap ; import java . util . Collection ; import java . util . Iterator ; import java . util . Map ; import java . util . HashSet ; import java . io . IOException ; import org . apache . lucene . util . ArrayUtil ; final class DocFieldConsumers extends DocFieldConsumer { final DocFieldConsumer one ; final DocFieldConsumer two ; public DocFieldConsumers ( DocFieldConsumer one , DocFieldConsumer two ) { this . one = one ; this . two = two ; } void setFieldInfos ( FieldInfos fieldInfos ) { super . setFieldInfos ( fieldInfos ) ; one . setFieldInfos ( fieldInfos ) ; two . setFieldInfos ( fieldInfos ) ; } public void flush ( Map threadsAndFields , DocumentsWriter . FlushState state ) throws IOException { Map oneThreadsAndFields = new HashMap ( ) ; Map twoThreadsAndFields = new HashMap ( ) ; Iterator it = threadsAndFields . entrySet ( ) . iterator ( ) ; while ( it . hasNext ( ) ) { Map . Entry entry = ( Map . Entry ) it . next ( ) ; DocFieldConsumersPerThread perThread = ( DocFieldConsumersPerThread ) entry . getKey ( ) ; Collection fields = ( Collection ) entry . getValue ( ) ; Iterator fieldsIt = fields . iterator ( ) ; Collection oneFields = new HashSet ( ) ; Collection twoFields = new HashSet ( ) ; while ( fieldsIt . hasNext ( ) ) { DocFieldConsumersPerField perField = ( DocFieldConsumersPerField ) fieldsIt . next ( ) ; oneFields . add ( perField . one ) ; twoFields . add ( perField . two ) ; } oneThreadsAndFields . put ( perThread . one , oneFields ) ; twoThreadsAndFields . put ( perThread . two , twoFields ) ; } one . flush ( oneThreadsAndFields , state ) ; two . flush ( twoThreadsAndFields , state ) ; } public void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException { try { one . closeDocStore ( state ) ; } finally { two . closeDocStore ( state ) ; } } public void abort ( ) { try { one . abort ( ) ; } finally { two . abort ( ) ; } } public boolean freeRAM ( ) { boolean any = one . freeRAM ( ) ; any |= two . freeRAM ( ) ; return any ; } public DocFieldConsumerPerThread addThread ( DocFieldProcessorPerThread docFieldProcessorPerThread ) throws IOException { return new DocFieldConsumersPerThread ( docFieldProcessorPerThread , this , one . addThread ( docFieldProcessorPerThread ) , two . addThread ( docFieldProcessorPerThread ) ) ; } PerDoc [ ] docFreeList = new PerDoc [ 1 ] ; int freeCount ; int allocCount ; synchronized PerDoc getPerDoc ( ) { if ( freeCount == 0 ) { allocCount ++ ; if ( allocCount > docFreeList . length ) { assert allocCount == 1 + docFreeList . length ; docFreeList = new PerDoc [ ArrayUtil . getNextSize ( allocCount ) ] ; } return new PerDoc ( ) ; } else return docFreeList [ -- freeCount ] ; } synchronized void freePerDoc ( PerDoc perDoc ) { assert freeCount < docFreeList . length ; docFreeList [ freeCount ++ ] = perDoc ; } class PerDoc extends DocumentsWriter . DocWriter { DocumentsWriter . DocWriter one ; DocumentsWriter . DocWriter two ; public long sizeInBytes ( ) { return one . sizeInBytes ( ) + two . sizeInBytes ( ) ; } public void finish ( ) throws IOException { try { try { one . finish ( ) ; } finally { two . finish ( ) ; } } finally { freePerDoc ( this ) ; } } public void abort ( ) { try { try { one . abort ( ) ; } finally { two . abort ( ) ; } } finally { freePerDoc ( this ) ; } } } } 	0	['11', '2', '0', '11', '37', '3', '3', '10', '6', '0.757142857', '281', '0', '3', '0.4', '0.2125', '0', '0', '23.90909091', '5', '1.5455', '0']
package org . apache . lucene . index ; abstract class InvertedDocEndConsumerPerThread { abstract void startDocument ( ) ; abstract InvertedDocEndConsumerPerField addField ( DocInverterPerField docInverterPerField , FieldInfo fieldInfo ) ; abstract void finishDocument ( ) ; abstract void abort ( ) ; } 	0	['5', '1', '1', '8', '6', '10', '6', '3', '0', '2', '8', '0', '0', '0', '0.466666667', '0', '0', '0.6', '1', '0.8', '0']
package org . apache . lucene . index ; import java . io . IOException ; public abstract class MergeScheduler { abstract void merge ( IndexWriter writer ) throws CorruptIndexException , IOException ; abstract void close ( ) throws CorruptIndexException , IOException ; } 	0	['3', '1', '2', '5', '4', '3', '4', '2', '1', '2', '6', '0', '0', '0', '0.666666667', '0', '0', '1', '1', '0.6667', '0']
package org . apache . lucene . index ; import java . util . Map ; import java . util . HashMap ; import java . util . HashSet ; import java . util . Collection ; import java . util . Iterator ; import java . io . IOException ; final class DocInverter extends DocFieldConsumer { final InvertedDocConsumer consumer ; final InvertedDocEndConsumer endConsumer ; public DocInverter ( InvertedDocConsumer consumer , InvertedDocEndConsumer endConsumer ) { this . consumer = consumer ; this . endConsumer = endConsumer ; } void setFieldInfos ( FieldInfos fieldInfos ) { super . setFieldInfos ( fieldInfos ) ; consumer . setFieldInfos ( fieldInfos ) ; endConsumer . setFieldInfos ( fieldInfos ) ; } void flush ( Map threadsAndFields , DocumentsWriter . FlushState state ) throws IOException { Map childThreadsAndFields = new HashMap ( ) ; Map endChildThreadsAndFields = new HashMap ( ) ; Iterator it = threadsAndFields . entrySet ( ) . iterator ( ) ; while ( it . hasNext ( ) ) { Map . Entry entry = ( Map . Entry ) it . next ( ) ; DocInverterPerThread perThread = ( DocInverterPerThread ) entry . getKey ( ) ; Collection fields = ( Collection ) entry . getValue ( ) ; Iterator fieldsIt = fields . iterator ( ) ; Collection childFields = new HashSet ( ) ; Collection endChildFields = new HashSet ( ) ; while ( fieldsIt . hasNext ( ) ) { DocInverterPerField perField = ( DocInverterPerField ) fieldsIt . next ( ) ; childFields . add ( perField . consumer ) ; endChildFields . add ( perField . endConsumer ) ; } childThreadsAndFields . put ( perThread . consumer , childFields ) ; endChildThreadsAndFields . put ( perThread . endConsumer , endChildFields ) ; } consumer . flush ( childThreadsAndFields , state ) ; endConsumer . flush ( endChildThreadsAndFields , state ) ; } public void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException { consumer . closeDocStore ( state ) ; endConsumer . closeDocStore ( state ) ; } void abort ( ) { consumer . abort ( ) ; endConsumer . abort ( ) ; } public boolean freeRAM ( ) { return consumer . freeRAM ( ) ; } public DocFieldConsumerPerThread addThread ( DocFieldProcessorPerThread docFieldProcessorPerThread ) { return new DocInverterPerThread ( docFieldProcessorPerThread , this ) ; } final static class FieldInvertState { int position ; int length ; int offset ; float boost ; void reset ( float docBoost ) { position = 0 ; length = 0 ; offset = 0 ; boost = docBoost ; } } } 	0	['7', '2', '0', '14', '30', '0', '2', '13', '4', '0.25', '136', '0', '2', '0.5', '0.285714286', '0', '0', '18.14285714', '1', '0.8571', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . document . Fieldable ; final class DocFieldConsumersPerField extends DocFieldConsumerPerField { final DocFieldConsumerPerField one ; final DocFieldConsumerPerField two ; final DocFieldConsumersPerThread perThread ; public DocFieldConsumersPerField ( DocFieldConsumersPerThread perThread , DocFieldConsumerPerField one , DocFieldConsumerPerField two ) { this . perThread = perThread ; this . one = one ; this . two = two ; } public void processFields ( Fieldable [ ] fields , int count ) throws IOException { one . processFields ( fields , count ) ; two . processFields ( fields , count ) ; } public void abort ( ) { try { one . abort ( ) ; } finally { two . abort ( ) ; } } } 	0	['3', '2', '0', '4', '6', '0', '2', '3', '3', '0.333333333', '44', '0', '3', '0.5', '0.466666667', '0', '0', '12.66666667', '3', '1.3333', '0']
package org . apache . lucene . document ; public class LoadFirstFieldSelector implements FieldSelector { public FieldSelectorResult accept ( String fieldName ) { return FieldSelectorResult . LOAD_AND_BREAK ; } } 	0	['2', '1', '0', '2', '3', '1', '0', '2', '2', '2', '7', '0', '0', '0', '0.75', '0', '0', '2.5', '1', '0.5', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class BufferedIndexInput extends IndexInput { public static final int BUFFER_SIZE = 1024 ; private int bufferSize = BUFFER_SIZE ; protected byte [ ] buffer ; private long bufferStart = 0 ; private int bufferLength = 0 ; private int bufferPosition = 0 ; public byte readByte ( ) throws IOException { if ( bufferPosition >= bufferLength ) refill ( ) ; return buffer [ bufferPosition ++ ] ; } public BufferedIndexInput ( ) { } public BufferedIndexInput ( int bufferSize ) { checkBufferSize ( bufferSize ) ; this . bufferSize = bufferSize ; } public void setBufferSize ( int newSize ) { assert buffer == null || bufferSize == buffer . length : "buffer=" + buffer + " bufferSize=" + bufferSize + " buffer.length=" + ( buffer != null ? buffer . length : 0 ) ; if ( newSize != bufferSize ) { checkBufferSize ( newSize ) ; bufferSize = newSize ; if ( buffer != null ) { byte [ ] newBuffer = new byte [ newSize ] ; final int leftInBuffer = bufferLength - bufferPosition ; final int numToCopy ; if ( leftInBuffer > newSize ) numToCopy = newSize ; else numToCopy = leftInBuffer ; System . arraycopy ( buffer , bufferPosition , newBuffer , 0 , numToCopy ) ; bufferStart += bufferPosition ; bufferPosition = 0 ; bufferLength = numToCopy ; newBuffer ( newBuffer ) ; } } } protected void newBuffer ( byte [ ] newBuffer ) { buffer = newBuffer ; } public int getBufferSize ( ) { return bufferSize ; } private void checkBufferSize ( int bufferSize ) { if ( bufferSize <= 0 ) throw new IllegalArgumentException ( "bufferSize must be greater than 0 (got " + bufferSize + ")" ) ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { readBytes ( b , offset , len , true ) ; } public void readBytes ( byte [ ] b , int offset , int len , boolean useBuffer ) throws IOException { if ( len <= ( bufferLength - bufferPosition ) ) { if ( len > 0 ) System . arraycopy ( buffer , bufferPosition , b , offset , len ) ; bufferPosition += len ; } else { int available = bufferLength - bufferPosition ; if ( available > 0 ) { System . arraycopy ( buffer , bufferPosition , b , offset , available ) ; offset += available ; len -= available ; bufferPosition += available ; } if ( useBuffer && len < bufferSize ) { refill ( ) ; if ( bufferLength < len ) { System . arraycopy ( buffer , 0 , b , offset , bufferLength ) ; throw new IOException ( "read past EOF" ) ; } else { System . arraycopy ( buffer , 0 , b , offset , len ) ; bufferPosition = len ; } } else { long after = bufferStart + bufferPosition + len ; if ( after > length ( ) ) throw new IOException ( "read past EOF" ) ; readInternal ( b , offset , len ) ; bufferStart = after ; bufferPosition = 0 ; bufferLength = 0 ; } } } private void refill ( ) throws IOException { long start = bufferStart + bufferPosition ; long end = start + bufferSize ; if ( end > length ( ) ) end = length ( ) ; int newLength = ( int ) ( end - start ) ; if ( newLength <= 0 ) throw new IOException ( "read past EOF" ) ; if ( buffer == null ) { newBuffer ( new byte [ bufferSize ] ) ; seekInternal ( bufferStart ) ; } readInternal ( buffer , 0 , newLength ) ; bufferLength = newLength ; bufferStart = start ; bufferPosition = 0 ; } protected abstract void readInternal ( byte [ ] b , int offset , int length ) throws IOException ; public long getFilePointer ( ) { return bufferStart + bufferPosition ; } public void seek ( long pos ) throws IOException { if ( pos >= bufferStart && pos < ( bufferStart + bufferLength ) ) bufferPosition = ( int ) ( pos - bufferStart ) ; else { bufferStart = pos ; bufferPosition = 0 ; bufferLength = 0 ; seekInternal ( pos ) ; } } protected abstract void seekInternal ( long pos ) throws IOException ; public Object clone ( ) { BufferedIndexInput clone = ( BufferedIndexInput ) super . clone ( ) ; clone . buffer = null ; clone . bufferLength = 0 ; clone . bufferPosition = 0 ; clone . bufferStart = getFilePointer ( ) ; return clone ; } } 	0	['17', '2', '2', '4', '33', '42', '3', '1', '10', '0.6953125', '478', '0.625', '0', '0.548387097', '0.302083333', '1', '5', '26.64705882', '8', '1.2941', '0']
package org . apache . lucene . index ; import java . io . Reader ; final class ReusableStringReader extends Reader { int upto ; int left ; String s ; void init ( String s ) { this . s = s ; left = s . length ( ) ; this . upto = 0 ; } public int read ( char [ ] c ) { return read ( c , 0 , c . length ) ; } public int read ( char [ ] c , int off , int len ) { if ( left > len ) { s . getChars ( upto , upto + len , c , off ) ; upto += len ; left -= len ; return len ; } else if ( 0 == left ) { return - 1 ; } else { s . getChars ( upto , upto + left , c , off ) ; int r = left ; left = 0 ; upto = s . length ( ) ; return r ; } } public void close ( ) { } ; } 	0	['5', '2', '0', '2', '8', '8', '2', '0', '3', '0.5', '90', '0', '0', '0.714285714', '0.45', '1', '2', '16.4', '3', '1.2', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . document . Fieldable ; abstract class DocFieldConsumerPerField { abstract void processFields ( Fieldable [ ] fields , int count ) throws IOException ; abstract void abort ( ) ; } 	0	['3', '1', '3', '11', '4', '3', '10', '1', '0', '2', '6', '0', '0', '0', '0.555555556', '0', '0', '1', '1', '0.6667', '0']
package org . apache . lucene . index ; class SegmentTermPositionVector extends SegmentTermVector implements TermPositionVector { protected int [ ] [ ] positions ; protected TermVectorOffsetInfo [ ] [ ] offsets ; public static final int [ ] EMPTY_TERM_POS = new int [ 0 ] ; public SegmentTermPositionVector ( String field , String terms [ ] , int termFreqs [ ] , int [ ] [ ] positions , TermVectorOffsetInfo [ ] [ ] offsets ) { super ( field , terms , termFreqs ) ; this . offsets = offsets ; this . positions = positions ; } public TermVectorOffsetInfo [ ] getOffsets ( int index ) { TermVectorOffsetInfo [ ] result = TermVectorOffsetInfo . EMPTY_OFFSET_INFO ; if ( offsets == null ) return null ; if ( index >= 0 && index < offsets . length ) { result = offsets [ index ] ; } return result ; } public int [ ] getTermPositions ( int index ) { int [ ] result = EMPTY_TERM_POS ; if ( positions == null ) return null ; if ( index >= 0 && index < positions . length ) { result = positions [ index ] ; } return result ; } } 	0	['4', '2', '0', '4', '5', '0', '1', '3', '3', '0.666666667', '65', '0.666666667', '1', '0.777777778', '0.476190476', '0', '0', '14.5', '4', '2', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . IndexOutput ; import java . io . IOException ; final class ByteSliceReader extends IndexInput { ByteBlockPool pool ; int bufferUpto ; byte [ ] buffer ; public int upto ; int limit ; int level ; public int bufferOffset ; public int endIndex ; public void init ( ByteBlockPool pool , int startIndex , int endIndex ) { assert endIndex - startIndex >= 0 ; assert startIndex >= 0 ; assert endIndex >= 0 ; this . pool = pool ; this . endIndex = endIndex ; level = 0 ; bufferUpto = startIndex / DocumentsWriter . BYTE_BLOCK_SIZE ; bufferOffset = bufferUpto * DocumentsWriter . BYTE_BLOCK_SIZE ; buffer = pool . buffers [ bufferUpto ] ; upto = startIndex & DocumentsWriter . BYTE_BLOCK_MASK ; final int firstSize = ByteBlockPool . levelSizeArray [ 0 ] ; if ( startIndex + firstSize >= endIndex ) { limit = endIndex & DocumentsWriter . BYTE_BLOCK_MASK ; } else limit = upto + firstSize - 4 ; } public boolean eof ( ) { assert upto + bufferOffset <= endIndex ; return upto + bufferOffset == endIndex ; } public byte readByte ( ) { assert ! eof ( ) ; assert upto <= limit ; if ( upto == limit ) nextSlice ( ) ; return buffer [ upto ++ ] ; } public long writeTo ( IndexOutput out ) throws IOException { long size = 0 ; while ( true ) { if ( limit + bufferOffset == endIndex ) { assert endIndex - bufferOffset >= upto ; out . writeBytes ( buffer , upto , limit - upto ) ; size += limit - upto ; break ; } else { out . writeBytes ( buffer , upto , limit - upto ) ; size += limit - upto ; nextSlice ( ) ; } } return size ; } public void nextSlice ( ) { final int nextIndex = ( ( buffer [ limit ] & 0xff ) << 24 ) + ( ( buffer [ 1 + limit ] & 0xff ) << 16 ) + ( ( buffer [ 2 + limit ] & 0xff ) << 8 ) + ( buffer [ 3 + limit ] & 0xff ) ; level = ByteBlockPool . nextLevelArray [ level ] ; final int newSize = ByteBlockPool . levelSizeArray [ level ] ; bufferUpto = nextIndex / DocumentsWriter . BYTE_BLOCK_SIZE ; bufferOffset = bufferUpto * DocumentsWriter . BYTE_BLOCK_SIZE ; buffer = pool . buffers [ bufferUpto ] ; upto = nextIndex & DocumentsWriter . BYTE_BLOCK_MASK ; if ( nextIndex + newSize >= endIndex ) { assert endIndex - nextIndex > 0 ; limit = endIndex - bufferOffset ; } else { limit = upto + newSize - 4 ; } } public void readBytes ( byte [ ] b , int offset , int len ) { while ( len > 0 ) { final int numLeft = limit - upto ; if ( numLeft < len ) { System . arraycopy ( buffer , upto , b , offset , numLeft ) ; offset += numLeft ; len -= numLeft ; nextSlice ( ) ; } else { System . arraycopy ( buffer , upto , b , offset , len ) ; upto += len ; break ; } } } public long getFilePointer ( ) { throw new RuntimeException ( "not implemented" ) ; } public long length ( ) { throw new RuntimeException ( "not implemented" ) ; } public void seek ( long pos ) { throw new RuntimeException ( "not implemented" ) ; } public void close ( ) { throw new RuntimeException ( "not implemented" ) ; } } 	0	['13', '2', '0', '8', '22', '38', '5', '3', '10', '0.658333333', '447', '0', '1', '0.607142857', '0.214285714', '1', '4', '32.61538462', '8', '2.3846', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . util . UnicodeUtil ; final class TermBuffer implements Cloneable { private String field ; private Term term ; private boolean preUTF8Strings ; private boolean dirty ; private UnicodeUtil . UTF16Result text = new UnicodeUtil . UTF16Result ( ) ; private UnicodeUtil . UTF8Result bytes = new UnicodeUtil . UTF8Result ( ) ; public final int compareTo ( TermBuffer other ) { if ( field == other . field ) return compareChars ( text . result , text . length , other . text . result , other . text . length ) ; else return field . compareTo ( other . field ) ; } private static final int compareChars ( char [ ] chars1 , int len1 , char [ ] chars2 , int len2 ) { final int end = len1 < len2 ? len1 : len2 ; for ( int k = 0 ; k < end ; k ++ ) { char c1 = chars1 [ k ] ; char c2 = chars2 [ k ] ; if ( c1 != c2 ) { return c1 - c2 ; } } return len1 - len2 ; } void setPreUTF8Strings ( ) { preUTF8Strings = true ; } public final void read ( IndexInput input , FieldInfos fieldInfos ) throws IOException { this . term = null ; int start = input . readVInt ( ) ; int length = input . readVInt ( ) ; int totalLength = start + length ; if ( preUTF8Strings ) { text . setLength ( totalLength ) ; input . readChars ( text . result , start , length ) ; } else { if ( dirty ) { UnicodeUtil . UTF16toUTF8 ( text . result , 0 , text . length , bytes ) ; bytes . setLength ( totalLength ) ; input . readBytes ( bytes . result , start , length ) ; UnicodeUtil . UTF8toUTF16 ( bytes . result , 0 , totalLength , text ) ; dirty = false ; } else { bytes . setLength ( totalLength ) ; input . readBytes ( bytes . result , start , length ) ; UnicodeUtil . UTF8toUTF16 ( bytes . result , start , length , text ) ; } } this . field = fieldInfos . fieldName ( input . readVInt ( ) ) ; } public final void set ( Term term ) { if ( term == null ) { reset ( ) ; return ; } final String termText = term . text ( ) ; final int termLen = termText . length ( ) ; text . setLength ( termLen ) ; termText . getChars ( 0 , termLen , text . result , 0 ) ; dirty = true ; field = term . field ( ) ; this . term = term ; } public final void set ( TermBuffer other ) { text . copyText ( other . text ) ; dirty = true ; field = other . field ; term = other . term ; } public void reset ( ) { field = null ; text . setLength ( 0 ) ; term = null ; dirty = true ; } public Term toTerm ( ) { if ( field == null ) return null ; if ( term == null ) term = new Term ( field , new String ( text . result , 0 , text . length ) , false ) ; return term ; } protected Object clone ( ) { TermBuffer clone = null ; try { clone = ( TermBuffer ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . dirty = true ; clone . bytes = new UnicodeUtil . UTF8Result ( ) ; clone . text = new UnicodeUtil . UTF16Result ( ) ; clone . text . copyText ( text ) ; return clone ; } } 	0	['10', '1', '0', '7', '30', '0', '1', '6', '6', '0.574074074', '303', '1', '3', '0', '0.228571429', '0', '0', '28.7', '4', '1.6', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Map ; abstract class DocFieldConsumer { FieldInfos fieldInfos ; abstract void flush ( Map threadsAndFields , DocumentsWriter . FlushState state ) throws IOException ; abstract void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException ; abstract void abort ( ) ; abstract DocFieldConsumerPerThread addThread ( DocFieldProcessorPerThread docFieldProcessorPerThread ) throws IOException ; abstract boolean freeRAM ( ) ; void setFieldInfos ( FieldInfos fieldInfos ) { this . fieldInfos = fieldInfos ; } } 	0	['7', '1', '3', '9', '8', '21', '6', '4', '0', '1', '15', '0', '1', '0', '0.342857143', '0', '0', '1', '1', '0.8571', '0']
package org . apache . lucene . store ; import java . io . IOException ; public abstract class BufferedIndexOutput extends IndexOutput { static final int BUFFER_SIZE = 16384 ; private final byte [ ] buffer = new byte [ BUFFER_SIZE ] ; private long bufferStart = 0 ; private int bufferPosition = 0 ; public void writeByte ( byte b ) throws IOException { if ( bufferPosition >= BUFFER_SIZE ) flush ( ) ; buffer [ bufferPosition ++ ] = b ; } public void writeBytes ( byte [ ] b , int offset , int length ) throws IOException { int bytesLeft = BUFFER_SIZE - bufferPosition ; if ( bytesLeft >= length ) { System . arraycopy ( b , offset , buffer , bufferPosition , length ) ; bufferPosition += length ; if ( BUFFER_SIZE - bufferPosition == 0 ) flush ( ) ; } else { if ( length > BUFFER_SIZE ) { if ( bufferPosition > 0 ) flush ( ) ; flushBuffer ( b , offset , length ) ; bufferStart += length ; } else { int pos = 0 ; int pieceLength ; while ( pos < length ) { pieceLength = ( length - pos < bytesLeft ) ? length - pos : bytesLeft ; System . arraycopy ( b , pos + offset , buffer , bufferPosition , pieceLength ) ; pos += pieceLength ; bufferPosition += pieceLength ; bytesLeft = BUFFER_SIZE - bufferPosition ; if ( bytesLeft == 0 ) { flush ( ) ; bytesLeft = BUFFER_SIZE ; } } } } } public void flush ( ) throws IOException { flushBuffer ( buffer , bufferPosition ) ; bufferStart += bufferPosition ; bufferPosition = 0 ; } private void flushBuffer ( byte [ ] b , int len ) throws IOException { flushBuffer ( b , 0 , len ) ; } protected abstract void flushBuffer ( byte [ ] b , int offset , int len ) throws IOException ; public void close ( ) throws IOException { flush ( ) ; } public long getFilePointer ( ) { return bufferStart + bufferPosition ; } public void seek ( long pos ) throws IOException { flush ( ) ; bufferStart = pos ; } public abstract long length ( ) throws IOException ; } 	0	['10', '2', '1', '2', '12', '17', '1', '1', '8', '0.555555556', '185', '0.75', '0', '0.653846154', '0.36', '1', '5', '17.1', '1', '0.9', '0']
package org . apache . lucene . index ; final class IndexFileNames { static final String SEGMENTS = "segments" ; static final String SEGMENTS_GEN = "segments.gen" ; static final String DELETABLE = "deletable" ; static final String NORMS_EXTENSION = "nrm" ; static final String FREQ_EXTENSION = "frq" ; static final String PROX_EXTENSION = "prx" ; static final String TERMS_EXTENSION = "tis" ; static final String TERMS_INDEX_EXTENSION = "tii" ; static final String FIELDS_INDEX_EXTENSION = "fdx" ; static final String FIELDS_EXTENSION = "fdt" ; static final String VECTORS_FIELDS_EXTENSION = "tvf" ; static final String VECTORS_DOCUMENTS_EXTENSION = "tvd" ; static final String VECTORS_INDEX_EXTENSION = "tvx" ; static final String COMPOUND_FILE_EXTENSION = "cfs" ; static final String COMPOUND_FILE_STORE_EXTENSION = "cfx" ; static final String DELETES_EXTENSION = "del" ; static final String FIELD_INFOS_EXTENSION = "fnm" ; static final String PLAIN_NORMS_EXTENSION = "f" ; static final String SEPARATE_NORMS_EXTENSION = "s" ; static final String GEN_EXTENSION = "gen" ; static final String INDEX_EXTENSIONS [ ] = new String [ ] { COMPOUND_FILE_EXTENSION , FIELD_INFOS_EXTENSION , FIELDS_INDEX_EXTENSION , FIELDS_EXTENSION , TERMS_INDEX_EXTENSION , TERMS_EXTENSION , FREQ_EXTENSION , PROX_EXTENSION , DELETES_EXTENSION , VECTORS_INDEX_EXTENSION , VECTORS_DOCUMENTS_EXTENSION , VECTORS_FIELDS_EXTENSION , GEN_EXTENSION , NORMS_EXTENSION , COMPOUND_FILE_STORE_EXTENSION , } ; static final String [ ] INDEX_EXTENSIONS_IN_COMPOUND_FILE = new String [ ] { FIELD_INFOS_EXTENSION , FIELDS_INDEX_EXTENSION , FIELDS_EXTENSION , TERMS_INDEX_EXTENSION , TERMS_EXTENSION , FREQ_EXTENSION , PROX_EXTENSION , VECTORS_INDEX_EXTENSION , VECTORS_DOCUMENTS_EXTENSION , VECTORS_FIELDS_EXTENSION , NORMS_EXTENSION } ; static final String [ ] STORE_INDEX_EXTENSIONS = new String [ ] { VECTORS_INDEX_EXTENSION , VECTORS_FIELDS_EXTENSION , VECTORS_DOCUMENTS_EXTENSION , FIELDS_INDEX_EXTENSION , FIELDS_EXTENSION } ; static final String [ ] NON_STORE_INDEX_EXTENSIONS = new String [ ] { FIELD_INFOS_EXTENSION , FREQ_EXTENSION , PROX_EXTENSION , TERMS_EXTENSION , TERMS_INDEX_EXTENSION , NORMS_EXTENSION } ; static final String COMPOUND_EXTENSIONS [ ] = new String [ ] { FIELD_INFOS_EXTENSION , FREQ_EXTENSION , PROX_EXTENSION , FIELDS_INDEX_EXTENSION , FIELDS_EXTENSION , TERMS_INDEX_EXTENSION , TERMS_EXTENSION } ; static final String VECTOR_EXTENSIONS [ ] = new String [ ] { VECTORS_INDEX_EXTENSION , VECTORS_DOCUMENTS_EXTENSION , VECTORS_FIELDS_EXTENSION } ; static final String fileNameFromGeneration ( String base , String extension , long gen ) { if ( gen == SegmentInfo . NO ) { return null ; } else if ( gen == SegmentInfo . WITHOUT_GEN ) { return base + extension ; } else { return base + "_" + Long . toString ( gen , Character . MAX_RADIX ) + extension ; } } static final boolean isDocStoreFile ( String fileName ) { if ( fileName . endsWith ( COMPOUND_FILE_STORE_EXTENSION ) ) return true ; for ( int i = 0 ; i < STORE_INDEX_EXTENSIONS . length ; i ++ ) if ( fileName . endsWith ( STORE_INDEX_EXTENSIONS [ i ] ) ) return true ; return false ; } } 	0	['4', '1', '0', '5', '10', '4', '5', '0', '0', '1.243589744', '298', '0', '0', '0', '0.444444444', '0', '0', '67', '4', '1.75', '0']
package org . apache . lucene . index ; import java . util . List ; import java . io . IOException ; public interface IndexDeletionPolicy { public void onInit ( List commits ) throws IOException ; public void onCommit ( List commits ) throws IOException ; } 	0	['2', '1', '0', '8', '2', '1', '8', '0', '2', '2', '2', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . analysis ; import java . io . Reader ; public class LetterTokenizer extends CharTokenizer { public LetterTokenizer ( Reader in ) { super ( in ) ; } protected boolean isTokenChar ( char c ) { return Character . isLetter ( c ) ; } } 	0	['2', '4', '1', '2', '4', '1', '1', '1', '1', '2', '9', '0', '0', '0.923076923', '0.666666667', '1', '1', '3.5', '1', '0.5', '0']
package org . apache . lucene . index ; import org . apache . lucene . util . UnicodeUtil ; final class TermVectorsTermsWriterPerThread extends TermsHashConsumerPerThread { final TermVectorsTermsWriter termsWriter ; final TermsHashPerThread termsHashPerThread ; final DocumentsWriter . DocState docState ; TermVectorsTermsWriter . PerDoc doc ; public TermVectorsTermsWriterPerThread ( TermsHashPerThread termsHashPerThread , TermVectorsTermsWriter termsWriter ) { this . termsWriter = termsWriter ; this . termsHashPerThread = termsHashPerThread ; docState = termsHashPerThread . docState ; } final ByteSliceReader vectorSliceReader = new ByteSliceReader ( ) ; final UnicodeUtil . UTF8Result utf8Results [ ] = { new UnicodeUtil . UTF8Result ( ) , new UnicodeUtil . UTF8Result ( ) } ; public void startDocument ( ) { assert clearLastVectorFieldName ( ) ; if ( doc != null ) { doc . reset ( ) ; doc . docID = docState . docID ; } } public DocumentsWriter . DocWriter finishDocument ( ) { try { return doc ; } finally { doc = null ; } } public TermsHashConsumerPerField addField ( TermsHashPerField termsHashPerField , FieldInfo fieldInfo ) { return new TermVectorsTermsWriterPerField ( termsHashPerField , this , fieldInfo ) ; } public void abort ( ) { if ( doc != null ) { doc . abort ( ) ; doc = null ; } } final boolean clearLastVectorFieldName ( ) { lastVectorFieldName = null ; return true ; } String lastVectorFieldName ; final boolean vectorFieldsInOrder ( FieldInfo fi ) { try { if ( lastVectorFieldName != null ) return lastVectorFieldName . compareTo ( fi . name ) < 0 ; else return true ; } finally { lastVectorFieldName = fi . name ; } } } 	0	['9', '2', '0', '12', '21', '24', '2', '12', '5', '0.916666667', '167', '0', '6', '0.363636364', '0.270833333', '0', '0', '16.55555556', '6', '2', '0']
package org . apache . lucene . index ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . analysis . Token ; import java . io . IOException ; abstract class InvertedDocConsumerPerField { abstract boolean start ( Fieldable [ ] fields , int count ) throws IOException ; abstract void add ( Token token ) throws IOException ; abstract void finish ( ) throws IOException ; abstract void abort ( ) ; } 	0	['5', '1', '1', '7', '6', '10', '5', '2', '0', '2', '8', '0', '0', '0', '0.4', '0', '0', '0.6', '1', '0.8', '0']
package org . apache . lucene . analysis . standard ; import org . apache . lucene . analysis . Token ; class StandardTokenizerImpl { public static final int YYEOF = - 1 ; private static final int ZZ_BUFFERSIZE = 16384 ; public static final int YYINITIAL = 0 ; private static final String ZZ_CMAP_PACKED = "\11\0\1\0\1\15\1\0\1\0\1\14\22\0\1\0\5\0\1\5" + "\1\3\4\0\1\11\1\7\1\4\1\11\12\2\6\0\1\6\32\12" + "\4\0\1\10\1\0\32\12\57\0\1\12\12\0\1\12\4\0\1\12" + "\5\0\27\12\1\0\37\12\1\0Ĩ\12\2\0\22\12\34\0\136\12" + "\2\0\11\12\2\0\7\12\16\0\2\12\16\0\5\12\11\0\1\12" + "\213\0\1\12\13\0\1\12\1\0\3\12\1\0\1\12\1\0\24\12" + "\1\0\54\12\1\0\10\12\2\0\32\12\14\0\202\12\12\0\71\12" + "\2\0\2\12\2\0\2\12\3\0\46\12\2\0\2\12\67\0\46\12" + "\2\0\1\12\7\0\47\12\110\0\33\12\5\0\3\12\56\0\32\12" + "\5\0\13\12\25\0\12\2\7\0\143\12\1\0\1\12\17\0\2\12" + "\11\0\12\2\3\12\23\0\1\12\1\0\33\12\123\0\46\12ş\0" + "\65\12\3\0\1\12\22\0\1\12\7\0\12\12\4\0\12\2\25\0" + "\10\12\2\0\2\12\2\0\26\12\1\0\7\12\1\0\1\12\3\0" + "\4\12\42\0\2\12\1\0\3\12\4\0\12\2\2\12\23\0\6\12" + "\4\0\2\12\2\0\26\12\1\0\7\12\1\0\2\12\1\0\2\12" + "\1\0\2\12\37\0\4\12\1\0\1\12\7\0\12\2\2\0\3\12" + "\20\0\7\12\1\0\1\12\1\0\3\12\1\0\26\12\1\0\7\12" + "\1\0\2\12\1\0\5\12\3\0\1\12\22\0\1\12\17\0\1\12" + "\5\0\12\2\25\0\10\12\2\0\2\12\2\0\26\12\1\0\7\12" + "\1\0\2\12\2\0\4\12\3\0\1\12\36\0\2\12\1\0\3\12" + "\4\0\12\2\25\0\6\12\3\0\3\12\1\0\4\12\3\0\2\12" + "\1\0\1\12\1\0\2\12\3\0\2\12\3\0\3\12\3\0\10\12" + "\1\0\3\12\55\0\11\2\25\0\10\12\1\0\3\12\1\0\27\12" + "\1\0\12\12\1\0\5\12\46\0\2\12\4\0\12\2\25\0\10\12" + "\1\0\3\12\1\0\27\12\1\0\12\12\1\0\5\12\44\0\1\12" + "\1\0\2\12\4\0\12\2\25\0\10\12\1\0\3\12\1\0\27\12" + "\1\0\20\12\46\0\2\12\4\0\12\2\25\0\22\12\3\0\30\12" + "\1\0\11\12\1\0\1\12\2\0\7\12\71\0\1\1\60\12\1\1" + "\2\12\14\1\7\12\11\1\12\2\47\0\2\12\1\0\1\12\2\0" + "\2\12\1\0\1\12\2\0\1\12\6\0\4\12\1\0\7\12\1\0" + "\3\12\1\0\1\12\1\0\1\12\2\0\2\12\1\0\4\12\1\0" + "\2\12\11\0\1\12\2\0\5\12\1\0\1\12\11\0\12\2\2\0" + "\2\12\42\0\1\12\37\0\12\2\26\0\10\12\1\0\42\12\35\0" + "\4\12\164\0\42\12\1\0\5\12\1\0\2\12\25\0\12\2\6\0" + "\6\12\112\0\46\12\12\0\47\12\11\0\132\12\5\0\104\12\5\0" + "\122\12\6\0\7\12\1\0\77\12\1\0\1\12\1\0\4\12\2\0" + "\7\12\1\0\1\12\1\0\4\12\2\0\47\12\1\0\1\12\1\0" + "\4\12\2\0\37\12\1\0\1\12\1\0\4\12\2\0\7\12\1\0" + "\1\12\1\0\4\12\2\0\7\12\1\0\7\12\1\0\27\12\1\0" + "\37\12\1\0\1\12\1\0\4\12\2\0\7\12\1\0\47\12\1\0" + "\23\12\16\0\11\2\56\0\125\12\14\0ɬ\12\2\0\10\12\12\0" + "\32\12\5\0\113\12\225\0\64\12\54\0\12\2\46\0\12\2\6\0" + "\130\12\10\0\51\12՗\0\234\12\4\0\132\12\6\0\26\12\2\0" + "\6\12\2\0\46\12\2\0\6\12\2\0\10\12\1\0\1\12\1\0" + "\1\12\1\0\1\12\1\0\37\12\2\0\65\12\1\0\7\12\1\0" + "\1\12\3\0\3\12\1\0\7\12\3\0\4\12\2\0\6\12\4\0" + "\15\12\5\0\3\12\1\0\7\12\202\0\1\12\202\0\1\12\4\0" + "\1\12\2\0\12\12\1\0\1\12\3\0\5\12\6\0\1\12\1\0" + "\1\12\1\0\1\12\1\0\4\12\1\0\3\12\1\0\7\12໋\0" + "\2\12\52\0\5\12\12\0\1\13\124\13\10\13\2\13\2\13\132\13" + "\1\13\3\13\6\13\50\13\3\13\1\0\136\12\21\0\30\12\70\0" + "\20\13Ā\0\200\13\200\0ᦶ\13\12\13\100\0冦\13\132\13ҍ\12" + "ݳ\0⮤\12⅜\0Į\13\322\13\7\12\14\0\5\12\5\0\1\12" + "\1\0\12\12\1\0\15\12\1\0\5\12\1\0\1\12\1\0\2\12" + "\1\0\2\12\1\0\154\12\41\0ū\12\22\0\100\12\2\0\66\12" + "\50\0\14\12\164\0\3\12\1\0\1\12\1\0\207\12\23\0\12\2" + "\7\0\32\12\6\0\32\12\12\0\1\13\72\13\37\12\3\0\6\12" + "\2\0\6\12\2\0\6\12\2\0\3\12\43\0" ; private static final char [ ] ZZ_CMAP = zzUnpackCMap ( ZZ_CMAP_PACKED ) ; private static final int [ ] ZZ_ACTION = zzUnpackAction ( ) ; private static final String ZZ_ACTION_PACKED_0 = "\1\0\1\1\3\2\1\3\1\1\13\0\1\2\3\4" + "\2\0\1\5\1\0\1\5\3\4\6\5\1\6\1\4" + "\2\7\1\10\1\0\1\10\3\0\2\10\1\11\1\12" + "\1\4" ; private static int [ ] zzUnpackAction ( ) { int [ ] result = new int [ 51 ] ; int offset = 0 ; offset = zzUnpackAction ( ZZ_ACTION_PACKED_0 , offset , result ) ; return result ; } private static int zzUnpackAction ( String packed , int offset , int [ ] result ) { int i = 0 ; int j = offset ; int l = packed . length ( ) ; while ( i < l ) { int count = packed . charAt ( i ++ ) ; int value = packed . charAt ( i ++ ) ; do result [ j ++ ] = value ; while ( -- count > 0 ) ; } return j ; } private static final int [ ] ZZ_ROWMAP = zzUnpackRowMap ( ) ; private static final String ZZ_ROWMAP_PACKED_0 = "\0\0\0\16\0\34\0\52\0\70\0\16\0\106\0\124" + "\0\142\0\160\0\176\0\214\0\232\0\250\0\266\0\304" + "\0\322\0\340\0\356\0\374\0Ċ\0Ę\0Ħ\0Ĵ" + "\0ł\0Ő\0Ş\0Ŭ\0ź\0ƈ\0Ɩ\0Ƥ" + "\0Ʋ\0ǀ\0ǎ\0ǜ\0Ǫ\0Ǹ\0\322\0Ȇ" + "\0Ȕ\0Ȣ\0Ȱ\0Ⱦ\0Ɍ\0ɚ\0\124\0\214" + "\0ɨ\0ɶ\0ʄ" ; private static int [ ] zzUnpackRowMap ( ) { int [ ] result = new int [ 51 ] ; int offset = 0 ; offset = zzUnpackRowMap ( ZZ_ROWMAP_PACKED_0 , offset , result ) ; return result ; } private static int zzUnpackRowMap ( String packed , int offset , int [ ] result ) { int i = 0 ; int j = offset ; int l = packed . length ( ) ; while ( i < l ) { int high = packed . charAt ( i ++ ) << 16 ; result [ j ++ ] = high | packed . charAt ( i ++ ) ; } return j ; } private static final int [ ] ZZ_TRANS = zzUnpackTrans ( ) ; private static final String ZZ_TRANS_PACKED_0 = "\1\2\1\3\1\4\7\2\1\5\1\6\1\7\1\2" + "\17\0\2\3\1\0\1\10\1\0\1\11\2\12\1\13" + "\1\3\4\0\1\3\1\4\1\0\1\14\1\0\1\11" + "\2\15\1\16\1\4\4\0\1\3\1\4\1\17\1\20" + "\1\21\1\22\2\12\1\13\1\23\20\0\1\2\1\0" + "\1\24\1\25\7\0\1\26\4\0\2\27\7\0\1\27" + "\4\0\1\30\1\31\7\0\1\32\5\0\1\33\7\0" + "\1\13\4\0\1\34\1\35\7\0\1\36\4\0\1\37" + "\1\40\7\0\1\41\4\0\1\42\1\43\7\0\1\44" + "\15\0\1\45\4\0\1\24\1\25\7\0\1\46\15\0" + "\1\47\4\0\2\27\7\0\1\50\4\0\1\3\1\4" + "\1\17\1\10\1\21\1\22\2\12\1\13\1\23\4\0" + "\2\24\1\0\1\51\1\0\1\11\2\52\1\0\1\24" + "\4\0\1\24\1\25\1\0\1\53\1\0\1\11\2\54" + "\1\55\1\25\4\0\1\24\1\25\1\0\1\51\1\0" + "\1\11\2\52\1\0\1\26\4\0\2\27\1\0\1\56" + "\2\0\1\56\2\0\1\27\4\0\2\30\1\0\1\52" + "\1\0\1\11\2\52\1\0\1\30\4\0\1\30\1\31" + "\1\0\1\54\1\0\1\11\2\54\1\55\1\31\4\0" + "\1\30\1\31\1\0\1\52\1\0\1\11\2\52\1\0" + "\1\32\5\0\1\33\1\0\1\55\2\0\3\55\1\33" + "\4\0\2\34\1\0\1\57\1\0\1\11\2\12\1\13" + "\1\34\4\0\1\34\1\35\1\0\1\60\1\0\1\11" + "\2\15\1\16\1\35\4\0\1\34\1\35\1\0\1\57" + "\1\0\1\11\2\12\1\13\1\36\4\0\2\37\1\0" + "\1\12\1\0\1\11\2\12\1\13\1\37\4\0\1\37" + "\1\40\1\0\1\15\1\0\1\11\2\15\1\16\1\40" + "\4\0\1\37\1\40\1\0\1\12\1\0\1\11\2\12" + "\1\13\1\41\4\0\2\42\1\0\1\13\2\0\3\13" + "\1\42\4\0\1\42\1\43\1\0\1\16\2\0\3\16" + "\1\43\4\0\1\42\1\43\1\0\1\13\2\0\3\13" + "\1\44\6\0\1\17\6\0\1\45\4\0\1\24\1\25" + "\1\0\1\61\1\0\1\11\2\52\1\0\1\26\4\0" + "\2\27\1\0\1\56\2\0\1\56\2\0\1\50\4\0" + "\2\24\7\0\1\24\4\0\2\30\7\0\1\30\4\0" + "\2\34\7\0\1\34\4\0\2\37\7\0\1\37\4\0" + "\2\42\7\0\1\42\4\0\2\62\7\0\1\62\4\0" + "\2\24\7\0\1\63\4\0\2\62\1\0\1\56\2\0" + "\1\56\2\0\1\62\4\0\2\24\1\0\1\61\1\0" + "\1\11\2\52\1\0\1\24\3\0" ; private static int [ ] zzUnpackTrans ( ) { int [ ] result = new int [ 658 ] ; int offset = 0 ; offset = zzUnpackTrans ( ZZ_TRANS_PACKED_0 , offset , result ) ; return result ; } private static int zzUnpackTrans ( String packed , int offset , int [ ] result ) { int i = 0 ; int j = offset ; int l = packed . length ( ) ; while ( i < l ) { int count = packed . charAt ( i ++ ) ; int value = packed . charAt ( i ++ ) ; value -- ; do result [ j ++ ] = value ; while ( -- count > 0 ) ; } return j ; } private static final int ZZ_UNKNOWN_ERROR = 0 ; private static final int ZZ_NO_MATCH = 1 ; private static final int ZZ_PUSHBACK_2BIG = 2 ; private static final String ZZ_ERROR_MSG [ ] = { "Unkown internal scanner error" , "Error: could not match input" , "Error: pushback value was too large" } ; private static final int [ ] ZZ_ATTRIBUTE = zzUnpackAttribute ( ) ; private static final String ZZ_ATTRIBUTE_PACKED_0 = "\1\0\1\11\3\1\1\11\1\1\13\0\4\1\2\0" + "\1\1\1\0\17\1\1\0\1\1\3\0\5\1" ; private static int [ ] zzUnpackAttribute ( ) { int [ ] result = new int [ 51 ] ; int offset = 0 ; offset = zzUnpackAttribute ( ZZ_ATTRIBUTE_PACKED_0 , offset , result ) ; return result ; } private static int zzUnpackAttribute ( String packed , int offset , int [ ] result ) { int i = 0 ; int j = offset ; int l = packed . length ( ) ; while ( i < l ) { int count = packed . charAt ( i ++ ) ; int value = packed . charAt ( i ++ ) ; do result [ j ++ ] = value ; while ( -- count > 0 ) ; } return j ; } private java . io . Reader zzReader ; private int zzState ; private int zzLexicalState = YYINITIAL ; private char zzBuffer [ ] = new char [ ZZ_BUFFERSIZE ] ; private int zzMarkedPos ; private int zzPushbackPos ; private int zzCurrentPos ; private int zzStartRead ; private int zzEndRead ; private int yyline ; private int yychar ; private int yycolumn ; private boolean zzAtBOL = true ; private boolean zzAtEOF ; public static final int ALPHANUM = StandardTokenizer . ALPHANUM ; public static final int APOSTROPHE = StandardTokenizer . APOSTROPHE ; public static final int ACRONYM = StandardTokenizer . ACRONYM ; public static final int COMPANY = StandardTokenizer . COMPANY ; public static final int EMAIL = StandardTokenizer . EMAIL ; public static final int HOST = StandardTokenizer . HOST ; public static final int NUM = StandardTokenizer . NUM ; public static final int CJ = StandardTokenizer . CJ ; public static final int ACRONYM_DEP = StandardTokenizer . ACRONYM_DEP ; public static final String [ ] TOKEN_TYPES = StandardTokenizer . TOKEN_TYPES ; public final int yychar ( ) { return yychar ; } final void getText ( Token t ) { t . setTermBuffer ( zzBuffer , zzStartRead , zzMarkedPos - zzStartRead ) ; } StandardTokenizerImpl ( java . io . Reader in ) { this . zzReader = in ; } StandardTokenizerImpl ( java . io . InputStream in ) { this ( new java . io . InputStreamReader ( in ) ) ; } private static char [ ] zzUnpackCMap ( String packed ) { char [ ] map = new char [ 0x10000 ] ; int i = 0 ; int j = 0 ; while ( i < 1154 ) { int count = packed . charAt ( i ++ ) ; char value = packed . charAt ( i ++ ) ; do map [ j ++ ] = value ; while ( -- count > 0 ) ; } return map ; } private boolean zzRefill ( ) throws java . io . IOException { if ( zzStartRead > 0 ) { System . arraycopy ( zzBuffer , zzStartRead , zzBuffer , 0 , zzEndRead - zzStartRead ) ; zzEndRead -= zzStartRead ; zzCurrentPos -= zzStartRead ; zzMarkedPos -= zzStartRead ; zzPushbackPos -= zzStartRead ; zzStartRead = 0 ; } if ( zzCurrentPos >= zzBuffer . length ) { char newBuffer [ ] = new char [ zzCurrentPos * 2 ] ; System . arraycopy ( zzBuffer , 0 , newBuffer , 0 , zzBuffer . length ) ; zzBuffer = newBuffer ; } int numRead = zzReader . read ( zzBuffer , zzEndRead , zzBuffer . length - zzEndRead ) ; if ( numRead < 0 ) { return true ; } else { zzEndRead += numRead ; return false ; } } public final void yyclose ( ) throws java . io . IOException { zzAtEOF = true ; zzEndRead = zzStartRead ; if ( zzReader != null ) zzReader . close ( ) ; } public final void yyreset ( java . io . Reader reader ) { zzReader = reader ; zzAtBOL = true ; zzAtEOF = false ; zzEndRead = zzStartRead = 0 ; zzCurrentPos = zzMarkedPos = zzPushbackPos = 0 ; yyline = yychar = yycolumn = 0 ; zzLexicalState = YYINITIAL ; } public final int yystate ( ) { return zzLexicalState ; } public final void yybegin ( int newState ) { zzLexicalState = newState ; } public final String yytext ( ) { return new String ( zzBuffer , zzStartRead , zzMarkedPos - zzStartRead ) ; } public final char yycharat ( int pos ) { return zzBuffer [ zzStartRead + pos ] ; } public final int yylength ( ) { return zzMarkedPos - zzStartRead ; } private void zzScanError ( int errorCode ) { String message ; try { message = ZZ_ERROR_MSG [ errorCode ] ; } catch ( ArrayIndexOutOfBoundsException e ) { message = ZZ_ERROR_MSG [ ZZ_UNKNOWN_ERROR ] ; } throw new Error ( message ) ; } public void yypushback ( int number ) { if ( number > yylength ( ) ) zzScanError ( ZZ_PUSHBACK_2BIG ) ; zzMarkedPos -= number ; } public int getNextToken ( ) throws java . io . IOException { int zzInput ; int zzAction ; int zzCurrentPosL ; int zzMarkedPosL ; int zzEndReadL = zzEndRead ; char [ ] zzBufferL = zzBuffer ; char [ ] zzCMapL = ZZ_CMAP ; int [ ] zzTransL = ZZ_TRANS ; int [ ] zzRowMapL = ZZ_ROWMAP ; int [ ] zzAttrL = ZZ_ATTRIBUTE ; while ( true ) { zzMarkedPosL = zzMarkedPos ; yychar += zzMarkedPosL - zzStartRead ; zzAction = - 1 ; zzCurrentPosL = zzCurrentPos = zzStartRead = zzMarkedPosL ; zzState = zzLexicalState ; zzForAction : { while ( true ) { if ( zzCurrentPosL < zzEndReadL ) zzInput = zzBufferL [ zzCurrentPosL ++ ] ; else if ( zzAtEOF ) { zzInput = YYEOF ; break zzForAction ; } else { zzCurrentPos = zzCurrentPosL ; zzMarkedPos = zzMarkedPosL ; boolean eof = zzRefill ( ) ; zzCurrentPosL = zzCurrentPos ; zzMarkedPosL = zzMarkedPos ; zzBufferL = zzBuffer ; zzEndReadL = zzEndRead ; if ( eof ) { zzInput = YYEOF ; break zzForAction ; } else { zzInput = zzBufferL [ zzCurrentPosL ++ ] ; } } int zzNext = zzTransL [ zzRowMapL [ zzState ] + zzCMapL [ zzInput ] ] ; if ( zzNext == - 1 ) break zzForAction ; zzState = zzNext ; int zzAttributes = zzAttrL [ zzState ] ; if ( ( zzAttributes & 1 ) == 1 ) { zzAction = zzState ; zzMarkedPosL = zzCurrentPosL ; if ( ( zzAttributes & 8 ) == 8 ) break zzForAction ; } } } zzMarkedPos = zzMarkedPosL ; switch ( zzAction < 0 ? zzAction : ZZ_ACTION [ zzAction ] ) { case 4 : { return HOST ; } case 11 : break ; case 9 : { return ACRONYM ; } case 12 : break ; case 8 : { return ACRONYM_DEP ; } case 13 : break ; case 1 : { } case 14 : break ; case 5 : { return NUM ; } case 15 : break ; case 3 : { return CJ ; } case 16 : break ; case 2 : { return ALPHANUM ; } case 17 : break ; case 7 : { return COMPANY ; } case 18 : break ; case 6 : { return APOSTROPHE ; } case 19 : break ; case 10 : { return EMAIL ; } case 20 : break ; default : if ( zzInput == YYEOF && zzStartRead == zzCurrentPos ) { zzAtEOF = true ; return YYEOF ; } else { zzScanError ( ZZ_NO_MATCH ) ; } } } } } 	0	['25', '1', '0', '3', '35', '196', '2', '2', '10', '0.968495935', '729', '0.707317073', '0', '0', '0.214285714', '0', '0', '26.52', '3', '1.28', '0']
package org . apache . lucene . store ; import java . io . IOException ; import java . util . zip . CRC32 ; import java . util . zip . Checksum ; public class ChecksumIndexOutput extends IndexOutput { IndexOutput main ; Checksum digest ; public ChecksumIndexOutput ( IndexOutput main ) { this . main = main ; digest = new CRC32 ( ) ; } public void writeByte ( byte b ) throws IOException { digest . update ( b ) ; main . writeByte ( b ) ; } public void writeBytes ( byte [ ] b , int offset , int length ) throws IOException { digest . update ( b , offset , length ) ; main . writeBytes ( b , offset , length ) ; } public long getChecksum ( ) { return digest . getValue ( ) ; } public void flush ( ) throws IOException { main . flush ( ) ; } public void close ( ) throws IOException { main . close ( ) ; } public long getFilePointer ( ) { return main . getFilePointer ( ) ; } public void seek ( long pos ) { throw new RuntimeException ( "not allowed" ) ; } public void prepareCommit ( ) throws IOException { final long checksum = getChecksum ( ) ; final long pos = main . getFilePointer ( ) ; main . writeLong ( checksum - 1 ) ; main . flush ( ) ; main . seek ( pos ) ; } public void finishCommit ( ) throws IOException { main . writeLong ( getChecksum ( ) ) ; } public long length ( ) throws IOException { return main . length ( ) ; } } 	0	['11', '2', '0', '2', '25', '0', '1', '1', '11', '0.35', '98', '0', '1', '0.62962963', '0.242424242', '1', '5', '7.727272727', '1', '0.9091', '0']
package org . apache . lucene . index ; import java . io . IOException ; abstract class DocConsumerPerThread { abstract DocumentsWriter . DocWriter processDocument ( ) throws IOException ; abstract void abort ( ) ; } 	0	['3', '1', '1', '6', '4', '3', '5', '1', '0', '2', '6', '0', '0', '0', '1', '0', '0', '1', '1', '0.6667', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; import java . util . ArrayList ; import java . util . Iterator ; import java . util . List ; public class SinkTokenizer extends Tokenizer { protected List lst = new ArrayList ( ) ; protected Iterator iter ; public SinkTokenizer ( List input ) { this . lst = input ; if ( this . lst == null ) this . lst = new ArrayList ( ) ; } public SinkTokenizer ( ) { this . lst = new ArrayList ( ) ; } public SinkTokenizer ( int initCap ) { this . lst = new ArrayList ( initCap ) ; } public List getTokens ( ) { return lst ; } public Token next ( final Token reusableToken ) throws IOException { assert reusableToken != null ; if ( iter == null ) iter = lst . iterator ( ) ; if ( iter . hasNext ( ) ) { Token nextToken = ( Token ) iter . next ( ) ; return ( Token ) nextToken . clone ( ) ; } return null ; } public void add ( Token t ) { if ( t == null ) return ; lst . add ( ( Token ) t . clone ( ) ) ; } public void close ( ) throws IOException { input = null ; lst = null ; } public void reset ( ) throws IOException { iter = lst . iterator ( ) ; } } 	0	['10', '3', '0', '3', '23', '0', '1', '2', '8', '0.75', '143', '0.5', '0', '0.538461538', '0.288888889', '2', '2', '12.9', '2', '0.7', '0']
package org . apache . lucene . index ; import java . util . Comparator ; public class TermVectorEntryFreqSortedComparator implements Comparator { public int compare ( Object object , Object object1 ) { int result = 0 ; TermVectorEntry entry = ( TermVectorEntry ) object ; TermVectorEntry entry1 = ( TermVectorEntry ) object1 ; result = entry1 . getFrequency ( ) - entry . getFrequency ( ) ; if ( result == 0 ) { result = entry . getTerm ( ) . compareTo ( entry1 . getTerm ( ) ) ; if ( result == 0 ) { result = entry . getField ( ) . compareTo ( entry1 . getField ( ) ) ; } } return result ; } } 	0	['2', '1', '0', '1', '7', '1', '0', '1', '2', '2', '37', '0', '0', '0', '0.75', '0', '0', '17.5', '3', '1.5', '0']
package org . apache . lucene ; public final class LucenePackage { private LucenePackage ( ) { } public static Package get ( ) { return LucenePackage . class . getPackage ( ) ; } } 	0	['3', '1', '0', '0', '8', '3', '0', '0', '1', '1', '27', '0', '0', '0', '0.333333333', '0', '0', '7.666666667', '2', '1', '0']
package org . apache . lucene . index ; abstract class RawPostingList { final static int BYTES_SIZE = DocumentsWriter . OBJECT_HEADER_BYTES + 3 * DocumentsWriter . INT_NUM_BYTE ; int textStart ; int intStart ; int byteStart ; } 	0	['1', '1', '2', '12', '2', '0', '12', '0', '0', '2', '8', '0', '0', '0', '1', '0', '0', '3', '0', '0', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . document . Document ; import org . apache . lucene . index . CorruptIndexException ; public class Hit implements java . io . Serializable { private Document doc = null ; private boolean resolved = false ; private Hits hits = null ; private int hitNumber ; Hit ( Hits hits , int hitNumber ) { this . hits = hits ; this . hitNumber = hitNumber ; } public Document getDocument ( ) throws CorruptIndexException , IOException { if ( ! resolved ) fetchTheHit ( ) ; return doc ; } public float getScore ( ) throws IOException { return hits . score ( hitNumber ) ; } public int getId ( ) throws IOException { return hits . id ( hitNumber ) ; } private void fetchTheHit ( ) throws CorruptIndexException , IOException { doc = hits . doc ( hitNumber ) ; resolved = true ; } public float getBoost ( ) throws CorruptIndexException , IOException { return getDocument ( ) . getBoost ( ) ; } public String get ( String name ) throws CorruptIndexException , IOException { return getDocument ( ) . get ( name ) ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "Hit<" ) ; buffer . append ( hits . toString ( ) ) ; buffer . append ( " [" ) ; buffer . append ( hitNumber ) ; buffer . append ( "] " ) ; if ( resolved ) { buffer . append ( "resolved" ) ; } else { buffer . append ( "unresolved" ) ; } buffer . append ( ">" ) ; return buffer . toString ( ) ; } } 	0	['8', '1', '0', '4', '19', '2', '1', '3', '6', '0.178571429', '116', '1', '2', '0', '0.34375', '0', '0', '13', '2', '1', '0']
package org . apache . lucene . util ; import java . io . ObjectStreamException ; import java . io . Serializable ; import java . io . StreamCorruptedException ; import java . util . HashMap ; import java . util . Map ; public abstract class Parameter implements Serializable { static Map allParameters = new HashMap ( ) ; private String name ; private Parameter ( ) { } protected Parameter ( String name ) { this . name = name ; String key = makeKey ( name ) ; if ( allParameters . containsKey ( key ) ) throw new IllegalArgumentException ( "Parameter name " + key + " already used!" ) ; allParameters . put ( key , this ) ; } private String makeKey ( String name ) { return getClass ( ) + " " + name ; } public String toString ( ) { return name ; } protected Object readResolve ( ) throws ObjectStreamException { Object par = allParameters . get ( makeKey ( name ) ) ; if ( par == null ) throw new StreamCorruptedException ( "Unknown parameter value: " + name ) ; return par ; } } 	0	['6', '1', '5', '5', '18', '5', '5', '0', '1', '0.6', '88', '0.5', '0', '0', '0.7', '0', '0', '13.33333333', '1', '0.5', '0']
package org . apache . lucene . search ; public class QueryFilter extends CachingWrapperFilter { public QueryFilter ( Query query ) { super ( new QueryWrapperFilter ( query ) ) ; } public boolean equals ( Object o ) { return super . equals ( ( QueryFilter ) o ) ; } public int hashCode ( ) { return super . hashCode ( ) ^ 0x923F64B9 ; } } 	0	['3', '3', '0', '4', '7', '3', '0', '4', '3', '2', '20', '0', '0', '0.777777778', '0.555555556', '2', '3', '5.666666667', '1', '0.6667', '0']
package org . apache . lucene . index ; import java . util . * ; public class FieldSortedTermVectorMapper extends TermVectorMapper { private Map fieldToTerms = new HashMap ( ) ; private SortedSet currentSet ; private String currentField ; private Comparator comparator ; public FieldSortedTermVectorMapper ( Comparator comparator ) { this ( false , false , comparator ) ; } public FieldSortedTermVectorMapper ( boolean ignoringPositions , boolean ignoringOffsets , Comparator comparator ) { super ( ignoringPositions , ignoringOffsets ) ; this . comparator = comparator ; } public void map ( String term , int frequency , TermVectorOffsetInfo [ ] offsets , int [ ] positions ) { TermVectorEntry entry = new TermVectorEntry ( currentField , term , frequency , offsets , positions ) ; currentSet . add ( entry ) ; } public void setExpectations ( String field , int numTerms , boolean storeOffsets , boolean storePositions ) { currentSet = new TreeSet ( comparator ) ; currentField = field ; fieldToTerms . put ( field , currentSet ) ; } public Map getFieldToTerms ( ) { return fieldToTerms ; } public Comparator getComparator ( ) { return comparator ; } } 	0	['6', '2', '0', '3', '12', '3', '0', '3', '6', '0.6', '69', '1', '0', '0.555555556', '0.380952381', '0', '0', '9.833333333', '1', '0.6667', '0']
package org . apache . lucene . util ; public class SmallFloat { public static byte floatToByte ( float f , int numMantissaBits , int zeroExp ) { int fzero = ( 63 - zeroExp ) << numMantissaBits ; int bits = Float . floatToRawIntBits ( f ) ; int smallfloat = bits > > ( 24 - numMantissaBits ) ; if ( smallfloat < fzero ) { return ( bits <= 0 ) ? ( byte ) 0 : ( byte ) 1 ; } else if ( smallfloat >= fzero + 0x100 ) { return - 1 ; } else { return ( byte ) ( smallfloat - fzero ) ; } } public static float byteToFloat ( byte b , int numMantissaBits , int zeroExp ) { if ( b == 0 ) return 0.0f ; int bits = ( b & 0xff ) << ( 24 - numMantissaBits ) ; bits += ( 63 - zeroExp ) << 24 ; return Float . intBitsToFloat ( bits ) ; } public static byte floatToByte315 ( float f ) { int bits = Float . floatToRawIntBits ( f ) ; int smallfloat = bits > > ( 24 - 3 ) ; if ( smallfloat < ( 63 - 15 ) << 3 ) { return ( bits <= 0 ) ? ( byte ) 0 : ( byte ) 1 ; } if ( smallfloat >= ( ( 63 - 15 ) << 3 ) + 0x100 ) { return - 1 ; } return ( byte ) ( smallfloat - ( ( 63 - 15 ) << 3 ) ) ; } public static float byte315ToFloat ( byte b ) { if ( b == 0 ) return 0.0f ; int bits = ( b & 0xff ) << ( 24 - 3 ) ; bits += ( 63 - 15 ) << 24 ; return Float . intBitsToFloat ( bits ) ; } public static byte floatToByte52 ( float f ) { int bits = Float . floatToRawIntBits ( f ) ; int smallfloat = bits > > ( 24 - 5 ) ; if ( smallfloat < ( 63 - 2 ) << 5 ) { return ( bits <= 0 ) ? ( byte ) 0 : ( byte ) 1 ; } if ( smallfloat >= ( ( 63 - 2 ) << 5 ) + 0x100 ) { return - 1 ; } return ( byte ) ( smallfloat - ( ( 63 - 2 ) << 5 ) ) ; } public static float byte52ToFloat ( byte b ) { if ( b == 0 ) return 0.0f ; int bits = ( b & 0xff ) << ( 24 - 5 ) ; bits += ( 63 - 2 ) << 24 ; return Float . intBitsToFloat ( bits ) ; } } 	0	['7', '1', '0', '1', '10', '21', '1', '0', '7', '2', '155', '0', '0', '0', '0.321428571', '0', '0', '21.14285714', '4', '2.5714', '0']
package org . apache . lucene . store ; import java . io . IOException ; import java . io . File ; public class LockStressTest { public static void main ( String [ ] args ) throws Exception { if ( args . length != 6 ) { System . out . println ( "\nUsage: java org.apache.lucene.store.LockStressTest myID verifierHostOrIP verifierPort lockFactoryClassName lockDirName sleepTime\n" + "\n" + "  myID = int from 0 .. 255 (should be unique for test process)\n" + "  verifierHostOrIP = host name or IP address where LockVerifyServer is running\n" + "  verifierPort = port that LockVerifyServer is listening on\n" + "  lockFactoryClassName = primary LockFactory class that we will use\n" + "  lockDirName = path to the lock directory (only set for Simple/NativeFSLockFactory\n" + "  sleepTimeMS = milliseconds to pause betweeen each lock obtain/release\n" + "\n" + "You should run multiple instances of this process, each with its own\n" + "unique ID, and each pointing to the same lock directory, to verify\n" + "that locking is working correctly.\n" + "\n" + "Make sure you are first running LockVerifyServer.\n" + "\n" ) ; System . exit ( 1 ) ; } final int myID = Integer . parseInt ( args [ 0 ] ) ; if ( myID < 0 || myID > 255 ) { System . out . println ( "myID must be a unique int 0..255" ) ; System . exit ( 1 ) ; } final String verifierHost = args [ 1 ] ; final int verifierPort = Integer . parseInt ( args [ 2 ] ) ; final String lockFactoryClassName = args [ 3 ] ; final String lockDirName = args [ 4 ] ; final int sleepTimeMS = Integer . parseInt ( args [ 5 ] ) ; Class c ; try { c = Class . forName ( lockFactoryClassName ) ; } catch ( ClassNotFoundException e ) { throw new IOException ( "unable to find LockClass " + lockFactoryClassName ) ; } LockFactory lockFactory ; try { lockFactory = ( LockFactory ) c . newInstance ( ) ; } catch ( IllegalAccessException e ) { throw new IOException ( "IllegalAccessException when instantiating LockClass " + lockFactoryClassName ) ; } catch ( InstantiationException e ) { throw new IOException ( "InstantiationException when instantiating LockClass " + lockFactoryClassName ) ; } catch ( ClassCastException e ) { throw new IOException ( "unable to cast LockClass " + lockFactoryClassName + " instance to a LockFactory" ) ; } File lockDir = new File ( lockDirName ) ; if ( lockFactory instanceof NativeFSLockFactory ) { ( ( NativeFSLockFactory ) lockFactory ) . setLockDir ( lockDir ) ; } else if ( lockFactory instanceof SimpleFSLockFactory ) { ( ( SimpleFSLockFactory ) lockFactory ) . setLockDir ( lockDir ) ; } lockFactory . setLockPrefix ( "test" ) ; LockFactory verifyLF = new VerifyingLockFactory ( ( byte ) myID , lockFactory , verifierHost , verifierPort ) ; Lock l = verifyLF . makeLock ( "test.lock" ) ; while ( true ) { boolean obtained = false ; try { obtained = l . obtain ( 10 ) ; } catch ( LockObtainFailedException e ) { System . out . print ( "x" ) ; } if ( obtained ) { System . out . print ( "l" ) ; l . release ( ) ; } Thread . sleep ( sleepTimeMS ) ; } } } 	0	['2', '1', '0', '6', '22', '1', '0', '6', '2', '2', '172', '0', '0', '0', '0.5', '0', '0', '85', '1', '0.5', '0']
package org . apache . lucene . index ; final class NormsWriterPerThread extends InvertedDocEndConsumerPerThread { final NormsWriter normsWriter ; final DocumentsWriter . DocState docState ; public NormsWriterPerThread ( DocInverterPerThread docInverterPerThread , NormsWriter normsWriter ) { this . normsWriter = normsWriter ; docState = docInverterPerThread . docState ; } InvertedDocEndConsumerPerField addField ( DocInverterPerField docInverterPerField , final FieldInfo fieldInfo ) { return new NormsWriterPerField ( docInverterPerField , this , fieldInfo ) ; } void abort ( ) { } void startDocument ( ) { } void finishDocument ( ) { } boolean freeRAM ( ) { return false ; } } 	0	['6', '2', '0', '8', '8', '15', '2', '8', '1', '1', '30', '0', '2', '0.444444444', '0.333333333', '0', '0', '3.666666667', '1', '0.8333', '0']
package org . apache . lucene . index ; import java . util . List ; public final class KeepOnlyLastCommitDeletionPolicy implements IndexDeletionPolicy { public void onInit ( List commits ) { onCommit ( commits ) ; } public void onCommit ( List commits ) { int size = commits . size ( ) ; for ( int i = 0 ; i < size - 1 ; i ++ ) { ( ( IndexCommit ) commits . get ( i ) ) . delete ( ) ; } } } 	0	['3', '1', '0', '4', '7', '3', '2', '2', '3', '2', '28', '0', '0', '0', '0.833333333', '0', '0', '8.333333333', '2', '1', '0']
package org . apache . lucene . document ; import java . util . Set ; public class SetBasedFieldSelector implements FieldSelector { private Set fieldsToLoad ; private Set lazyFieldsToLoad ; public SetBasedFieldSelector ( Set fieldsToLoad , Set lazyFieldsToLoad ) { this . fieldsToLoad = fieldsToLoad ; this . lazyFieldsToLoad = lazyFieldsToLoad ; } public FieldSelectorResult accept ( String fieldName ) { FieldSelectorResult result = FieldSelectorResult . NO_LOAD ; if ( fieldsToLoad . contains ( fieldName ) == true ) { result = FieldSelectorResult . LOAD ; } if ( lazyFieldsToLoad . contains ( fieldName ) == true ) { result = FieldSelectorResult . LAZY_LOAD ; } return result ; } } 	0	['2', '1', '0', '2', '4', '0', '0', '2', '2', '0', '33', '1', '0', '0', '0.666666667', '0', '0', '14.5', '3', '1.5', '0']
package org . apache . lucene . index ; import java . util . Arrays ; final class ByteBlockPool { abstract static class Allocator { abstract void recycleByteBlocks ( byte [ ] [ ] blocks , int start , int end ) ; abstract byte [ ] getByteBlock ( boolean trackAllocations ) ; } public byte [ ] [ ] buffers = new byte [ 10 ] [ ] ; int bufferUpto = - 1 ; public int byteUpto = DocumentsWriter . BYTE_BLOCK_SIZE ; public byte [ ] buffer ; public int byteOffset = - DocumentsWriter . BYTE_BLOCK_SIZE ; private final boolean trackAllocations ; private final Allocator allocator ; public ByteBlockPool ( Allocator allocator , boolean trackAllocations ) { this . allocator = allocator ; this . trackAllocations = trackAllocations ; } public void reset ( ) { if ( bufferUpto != - 1 ) { for ( int i = 0 ; i < bufferUpto ; i ++ ) Arrays . fill ( buffers [ i ] , ( byte ) 0 ) ; Arrays . fill ( buffers [ bufferUpto ] , 0 , byteUpto , ( byte ) 0 ) ; if ( bufferUpto > 0 ) allocator . recycleByteBlocks ( buffers , 1 , 1 + bufferUpto ) ; bufferUpto = 0 ; byteUpto = 0 ; byteOffset = 0 ; buffer = buffers [ 0 ] ; } } public void nextBuffer ( ) { if ( 1 + bufferUpto == buffers . length ) { byte [ ] [ ] newBuffers = new byte [ ( int ) ( buffers . length * 1.5 ) ] [ ] ; System . arraycopy ( buffers , 0 , newBuffers , 0 , buffers . length ) ; buffers = newBuffers ; } buffer = buffers [ 1 + bufferUpto ] = allocator . getByteBlock ( trackAllocations ) ; bufferUpto ++ ; byteUpto = 0 ; byteOffset += DocumentsWriter . BYTE_BLOCK_SIZE ; } public int newSlice ( final int size ) { if ( byteUpto > DocumentsWriter . BYTE_BLOCK_SIZE - size ) nextBuffer ( ) ; final int upto = byteUpto ; byteUpto += size ; buffer [ byteUpto - 1 ] = 16 ; return upto ; } final static int [ ] nextLevelArray = { 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 9 } ; final static int [ ] levelSizeArray = { 5 , 14 , 20 , 30 , 40 , 40 , 80 , 80 , 120 , 200 } ; final static int FIRST_LEVEL_SIZE = levelSizeArray [ 0 ] ; public int allocSlice ( final byte [ ] slice , final int upto ) { final int level = slice [ upto ] & 15 ; final int newLevel = nextLevelArray [ level ] ; final int newSize = levelSizeArray [ newLevel ] ; if ( byteUpto > DocumentsWriter . BYTE_BLOCK_SIZE - newSize ) nextBuffer ( ) ; final int newUpto = byteUpto ; final int offset = newUpto + byteOffset ; byteUpto += newSize ; buffer [ newUpto ] = slice [ upto - 3 ] ; buffer [ newUpto + 1 ] = slice [ upto - 2 ] ; buffer [ newUpto + 2 ] = slice [ upto - 1 ] ; slice [ upto - 3 ] = ( byte ) ( offset > > > 24 ) ; slice [ upto - 2 ] = ( byte ) ( offset > > > 16 ) ; slice [ upto - 1 ] = ( byte ) ( offset > > > 8 ) ; slice [ upto ] = ( byte ) offset ; buffer [ byteUpto - 1 ] = ( byte ) ( 16 | newLevel ) ; return newUpto + 3 ; } } 	0	['6', '1', '0', '5', '12', '0', '4', '1', '5', '0.44', '387', '0.2', '1', '0', '0.4', '0', '0', '61.83333333', '4', '1.6667', '0']
package org . apache . lucene . store ; import java . net . ServerSocket ; import java . net . Socket ; import java . io . OutputStream ; import java . io . InputStream ; import java . io . IOException ; public class LockVerifyServer { private static String getTime ( long startTime ) { return "[" + ( ( System . currentTimeMillis ( ) - startTime ) / 1000 ) + "s] " ; } public static void main ( String [ ] args ) throws IOException { if ( args . length != 1 ) { System . out . println ( "\nUsage: java org.apache.lucene.store.LockVerifyServer port\n" ) ; System . exit ( 1 ) ; } final int port = Integer . parseInt ( args [ 0 ] ) ; ServerSocket s = new ServerSocket ( port ) ; s . setReuseAddress ( true ) ; System . out . println ( "\nReady on port " + port + "..." ) ; int lockedID = 0 ; long startTime = System . currentTimeMillis ( ) ; while ( true ) { Socket cs = s . accept ( ) ; OutputStream out = cs . getOutputStream ( ) ; InputStream in = cs . getInputStream ( ) ; int id = in . read ( ) ; int command = in . read ( ) ; boolean err = false ; if ( command == 1 ) { if ( lockedID != 0 ) { err = true ; System . out . println ( getTime ( startTime ) + " ERROR: id " + id + " got lock, but " + lockedID + " already holds the lock" ) ; } lockedID = id ; } else if ( command == 0 ) { if ( lockedID != id ) { err = true ; System . out . println ( getTime ( startTime ) + " ERROR: id " + id + " released the lock, but " + lockedID + " is the one holding the lock" ) ; } lockedID = 0 ; } else throw new RuntimeException ( "unrecognized command " + command ) ; System . out . print ( "." ) ; if ( err ) out . write ( 1 ) ; else out . write ( 0 ) ; out . close ( ) ; in . close ( ) ; cs . close ( ) ; } } } 	0	['3', '1', '0', '0', '25', '3', '0', '0', '2', '2', '165', '0', '0', '0', '0.333333333', '0', '0', '54', '1', '0.6667', '0']
package org . apache . lucene . index ; final class ByteSliceWriter { private byte [ ] slice ; private int upto ; private final ByteBlockPool pool ; int offset0 ; public ByteSliceWriter ( ByteBlockPool pool ) { this . pool = pool ; } public void init ( int address ) { slice = pool . buffers [ address > > DocumentsWriter . BYTE_BLOCK_SHIFT ] ; assert slice != null ; upto = address & DocumentsWriter . BYTE_BLOCK_MASK ; offset0 = address ; assert upto < slice . length ; } public void writeByte ( byte b ) { assert slice != null ; if ( slice [ upto ] != 0 ) { upto = pool . allocSlice ( slice , upto ) ; slice = pool . buffer ; offset0 = pool . byteOffset ; assert slice != null ; } slice [ upto ++ ] = b ; assert upto != slice . length ; } public void writeBytes ( final byte [ ] b , int offset , final int len ) { final int offsetEnd = offset + len ; while ( offset < offsetEnd ) { if ( slice [ upto ] != 0 ) { upto = pool . allocSlice ( slice , upto ) ; slice = pool . buffer ; offset0 = pool . byteOffset ; } slice [ upto ++ ] = b [ offset ++ ] ; assert upto != slice . length ; } } public int getAddress ( ) { return upto + ( offset0 & DocumentsWriter . BYTE_BLOCK_NOT_MASK ) ; } public void writeVInt ( int i ) { while ( ( i & ~ 0x7F ) != 0 ) { writeByte ( ( byte ) ( ( i & 0x7f ) | 0x80 ) ) ; i >>>= 7 ; } writeByte ( ( byte ) i ) ; } } 	0	['8', '1', '0', '1', '15', '4', '0', '1', '6', '0.547619048', '240', '0.5', '1', '0', '0.30952381', '0', '0', '28.25', '8', '2.75', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . BufferedIndexInput ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . Lock ; import java . util . HashMap ; import java . io . IOException ; class CompoundFileReader extends Directory { private int readBufferSize ; private static final class FileEntry { long offset ; long length ; } private Directory directory ; private String fileName ; private IndexInput stream ; private HashMap entries = new HashMap ( ) ; public CompoundFileReader ( Directory dir , String name ) throws IOException { this ( dir , name , BufferedIndexInput . BUFFER_SIZE ) ; } public CompoundFileReader ( Directory dir , String name , int readBufferSize ) throws IOException { directory = dir ; fileName = name ; this . readBufferSize = readBufferSize ; boolean success = false ; try { stream = dir . openInput ( name , readBufferSize ) ; int count = stream . readVInt ( ) ; FileEntry entry = null ; for ( int i = 0 ; i < count ; i ++ ) { long offset = stream . readLong ( ) ; String id = stream . readString ( ) ; if ( entry != null ) { entry . length = offset - entry . offset ; } entry = new FileEntry ( ) ; entry . offset = offset ; entries . put ( id , entry ) ; } if ( entry != null ) { entry . length = stream . length ( ) - entry . offset ; } success = true ; } finally { if ( ! success && ( stream != null ) ) { try { stream . close ( ) ; } catch ( IOException e ) { } } } } public Directory getDirectory ( ) { return directory ; } public String getName ( ) { return fileName ; } public synchronized void close ( ) throws IOException { if ( stream == null ) throw new IOException ( "Already closed" ) ; entries . clear ( ) ; stream . close ( ) ; stream = null ; } public synchronized IndexInput openInput ( String id ) throws IOException { return openInput ( id , readBufferSize ) ; } public synchronized IndexInput openInput ( String id , int readBufferSize ) throws IOException { if ( stream == null ) throw new IOException ( "Stream closed" ) ; FileEntry entry = ( FileEntry ) entries . get ( id ) ; if ( entry == null ) throw new IOException ( "No sub-file with id " + id + " found" ) ; return new CSIndexInput ( stream , entry . offset , entry . length , readBufferSize ) ; } public String [ ] list ( ) { String res [ ] = new String [ entries . size ( ) ] ; return ( String [ ] ) entries . keySet ( ) . toArray ( res ) ; } public boolean fileExists ( String name ) { return entries . containsKey ( name ) ; } public long fileModified ( String name ) throws IOException { return directory . fileModified ( fileName ) ; } public void touchFile ( String name ) throws IOException { directory . touchFile ( fileName ) ; } public void deleteFile ( String name ) { throw new UnsupportedOperationException ( ) ; } public void renameFile ( String from , String to ) { throw new UnsupportedOperationException ( ) ; } public long fileLength ( String name ) throws IOException { FileEntry e = ( FileEntry ) entries . get ( name ) ; if ( e == null ) throw new IOException ( "File " + name + " does not exist" ) ; return e . length ; } public IndexOutput createOutput ( String name ) { throw new UnsupportedOperationException ( ) ; } public Lock makeLock ( String name ) { throw new UnsupportedOperationException ( ) ; } static final class CSIndexInput extends BufferedIndexInput { IndexInput base ; long fileOffset ; long length ; CSIndexInput ( final IndexInput base , final long fileOffset , final long length ) { this ( base , fileOffset , length , BufferedIndexInput . BUFFER_SIZE ) ; } CSIndexInput ( final IndexInput base , final long fileOffset , final long length , int readBufferSize ) { super ( readBufferSize ) ; this . base = ( IndexInput ) base . clone ( ) ; this . fileOffset = fileOffset ; this . length = length ; } public Object clone ( ) { CSIndexInput clone = ( CSIndexInput ) super . clone ( ) ; clone . base = ( IndexInput ) base . clone ( ) ; clone . fileOffset = fileOffset ; clone . length = length ; return clone ; } protected void readInternal ( byte [ ] b , int offset , int len ) throws IOException { long start = getFilePointer ( ) ; if ( start + len > length ) throw new IOException ( "read past EOF" ) ; base . seek ( fileOffset + start ) ; base . readBytes ( b , offset , len , false ) ; } protected void seekInternal ( long pos ) { } public void close ( ) throws IOException { base . close ( ) ; } public long length ( ) { return length ; } } } 	0	['16', '2', '0', '9', '40', '70', '2', '7', '16', '0.72', '267', '1', '2', '0.575757576', '0.5', '1', '5', '15.375', '1', '0.875', '0']
package org . apache . lucene . index ; import java . util . Collection ; import java . util . Map ; import java . util . HashMap ; import java . util . Iterator ; import java . util . HashSet ; import java . util . Arrays ; import java . io . IOException ; import org . apache . lucene . util . ArrayUtil ; final class TermsHash extends InvertedDocConsumer { final TermsHashConsumer consumer ; final TermsHash nextTermsHash ; final int bytesPerPosting ; final int postingsFreeChunk ; final DocumentsWriter docWriter ; private TermsHash primaryTermsHash ; private RawPostingList [ ] postingsFreeList = new RawPostingList [ 1 ] ; private int postingsFreeCount ; private int postingsAllocCount ; boolean trackAllocations ; public TermsHash ( final DocumentsWriter docWriter , boolean trackAllocations , final TermsHashConsumer consumer , final TermsHash nextTermsHash ) { this . docWriter = docWriter ; this . consumer = consumer ; this . nextTermsHash = nextTermsHash ; this . trackAllocations = trackAllocations ; bytesPerPosting = consumer . bytesPerPosting ( ) + 4 * DocumentsWriter . POINTER_NUM_BYTE ; postingsFreeChunk = ( int ) ( DocumentsWriter . BYTE_BLOCK_SIZE / bytesPerPosting ) ; } InvertedDocConsumerPerThread addThread ( DocInverterPerThread docInverterPerThread ) { return new TermsHashPerThread ( docInverterPerThread , this , nextTermsHash , null ) ; } TermsHashPerThread addThread ( DocInverterPerThread docInverterPerThread , TermsHashPerThread primaryPerThread ) { return new TermsHashPerThread ( docInverterPerThread , this , nextTermsHash , primaryPerThread ) ; } void setFieldInfos ( FieldInfos fieldInfos ) { this . fieldInfos = fieldInfos ; consumer . setFieldInfos ( fieldInfos ) ; } synchronized public void abort ( ) { consumer . abort ( ) ; if ( nextTermsHash != null ) nextTermsHash . abort ( ) ; } void shrinkFreePostings ( Map threadsAndFields , DocumentsWriter . FlushState state ) { assert postingsFreeCount == postingsAllocCount : Thread . currentThread ( ) . getName ( ) + ": postingsFreeCount=" + postingsFreeCount + " postingsAllocCount=" + postingsAllocCount + " consumer=" + consumer ; final int newSize = ArrayUtil . getShrinkSize ( postingsFreeList . length , postingsAllocCount ) ; if ( newSize != postingsFreeList . length ) { RawPostingList [ ] newArray = new RawPostingList [ newSize ] ; System . arraycopy ( postingsFreeList , 0 , newArray , 0 , postingsFreeCount ) ; postingsFreeList = newArray ; } } synchronized void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException { consumer . closeDocStore ( state ) ; if ( nextTermsHash != null ) nextTermsHash . closeDocStore ( state ) ; } synchronized void flush ( Map threadsAndFields , final DocumentsWriter . FlushState state ) throws IOException { Map childThreadsAndFields = new HashMap ( ) ; Map nextThreadsAndFields ; if ( nextTermsHash != null ) nextThreadsAndFields = new HashMap ( ) ; else nextThreadsAndFields = null ; Iterator it = threadsAndFields . entrySet ( ) . iterator ( ) ; while ( it . hasNext ( ) ) { Map . Entry entry = ( Map . Entry ) it . next ( ) ; TermsHashPerThread perThread = ( TermsHashPerThread ) entry . getKey ( ) ; Collection fields = ( Collection ) entry . getValue ( ) ; Iterator fieldsIt = fields . iterator ( ) ; Collection childFields = new HashSet ( ) ; Collection nextChildFields ; if ( nextTermsHash != null ) nextChildFields = new HashSet ( ) ; else nextChildFields = null ; while ( fieldsIt . hasNext ( ) ) { TermsHashPerField perField = ( TermsHashPerField ) fieldsIt . next ( ) ; childFields . add ( perField . consumer ) ; if ( nextTermsHash != null ) nextChildFields . add ( perField . nextPerField ) ; } childThreadsAndFields . put ( perThread . consumer , childFields ) ; if ( nextTermsHash != null ) nextThreadsAndFields . put ( perThread . nextPerThread , nextChildFields ) ; } consumer . flush ( childThreadsAndFields , state ) ; shrinkFreePostings ( threadsAndFields , state ) ; if ( nextTermsHash != null ) nextTermsHash . flush ( nextThreadsAndFields , state ) ; } synchronized public boolean freeRAM ( ) { if ( ! trackAllocations ) return false ; boolean any ; final int numToFree ; if ( postingsFreeCount >= postingsFreeChunk ) numToFree = postingsFreeChunk ; else numToFree = postingsFreeCount ; any = numToFree > 0 ; if ( any ) { Arrays . fill ( postingsFreeList , postingsFreeCount - numToFree , postingsFreeCount , null ) ; postingsFreeCount -= numToFree ; postingsAllocCount -= numToFree ; docWriter . bytesAllocated ( - numToFree * bytesPerPosting ) ; any = true ; } if ( nextTermsHash != null ) any |= nextTermsHash . freeRAM ( ) ; return any ; } synchronized public void recyclePostings ( final RawPostingList [ ] postings , final int numPostings ) { assert postings . length >= numPostings ; assert postingsFreeCount + numPostings <= postingsFreeList . length ; System . arraycopy ( postings , 0 , postingsFreeList , postingsFreeCount , numPostings ) ; postingsFreeCount += numPostings ; } synchronized public void getPostings ( final RawPostingList [ ] postings ) { assert docWriter . writer . testPoint ( "TermsHash.getPostings start" ) ; assert postingsFreeCount <= postingsFreeList . length ; assert postingsFreeCount <= postingsAllocCount : "postingsFreeCount=" + postingsFreeCount + " postingsAllocCount=" + postingsAllocCount ; final int numToCopy ; if ( postingsFreeCount < postings . length ) numToCopy = postingsFreeCount ; else numToCopy = postings . length ; final int start = postingsFreeCount - numToCopy ; assert start >= 0 ; assert start + numToCopy <= postingsFreeList . length ; assert numToCopy <= postings . length ; System . arraycopy ( postingsFreeList , start , postings , 0 , numToCopy ) ; if ( numToCopy != postings . length ) { final int extra = postings . length - numToCopy ; final int newPostingsAllocCount = postingsAllocCount + extra ; consumer . createPostings ( postings , numToCopy , extra ) ; assert docWriter . writer . testPoint ( "TermsHash.getPostings after create" ) ; postingsAllocCount += extra ; if ( trackAllocations ) docWriter . bytesAllocated ( extra * bytesPerPosting ) ; if ( newPostingsAllocCount > postingsFreeList . length ) postingsFreeList = new RawPostingList [ ArrayUtil . getNextSize ( newPostingsAllocCount ) ] ; } postingsFreeCount -= numToCopy ; if ( trackAllocations ) docWriter . bytesUsed ( postings . length * bytesPerPosting ) ; } } 	0	['13', '2', '0', '14', '52', '0', '3', '14', '5', '0.763888889', '584', '0.333333333', '5', '0.352941176', '0.179487179', '0', '0', '43', '20', '3.3077', '0']
package org . apache . lucene . index ; import java . util . ArrayList ; import java . util . HashMap ; import java . util . List ; import java . util . Map ; public class PositionBasedTermVectorMapper extends TermVectorMapper { private Map fieldToTerms ; private String currentField ; private Map currentPositions ; private boolean storeOffsets ; public PositionBasedTermVectorMapper ( ) { super ( false , false ) ; } public PositionBasedTermVectorMapper ( boolean ignoringOffsets ) { super ( false , ignoringOffsets ) ; } public boolean isIgnoringPositions ( ) { return false ; } public void map ( String term , int frequency , TermVectorOffsetInfo [ ] offsets , int [ ] positions ) { for ( int i = 0 ; i < positions . length ; i ++ ) { Integer posVal = new Integer ( positions [ i ] ) ; TVPositionInfo pos = ( TVPositionInfo ) currentPositions . get ( posVal ) ; if ( pos == null ) { pos = new TVPositionInfo ( positions [ i ] , storeOffsets ) ; currentPositions . put ( posVal , pos ) ; } pos . addTerm ( term , offsets != null ? offsets [ i ] : null ) ; } } public void setExpectations ( String field , int numTerms , boolean storeOffsets , boolean storePositions ) { if ( storePositions == false ) { throw new RuntimeException ( "You must store positions in order to use this Mapper" ) ; } if ( storeOffsets == true ) { } fieldToTerms = new HashMap ( numTerms ) ; this . storeOffsets = storeOffsets ; currentField = field ; currentPositions = new HashMap ( ) ; fieldToTerms . put ( currentField , currentPositions ) ; } public Map getFieldToTerms ( ) { return fieldToTerms ; } public static class TVPositionInfo { private int position ; private List terms ; private List offsets ; public TVPositionInfo ( int position , boolean storeOffsets ) { this . position = position ; terms = new ArrayList ( ) ; if ( storeOffsets ) { offsets = new ArrayList ( ) ; } } void addTerm ( String term , TermVectorOffsetInfo info ) { terms . add ( term ) ; if ( offsets != null ) { offsets . add ( info ) ; } } public int getPosition ( ) { return position ; } public List getTerms ( ) { return terms ; } public List getOffsets ( ) { return offsets ; } } } 	0	['6', '2', '0', '3', '15', '11', '0', '3', '6', '0.85', '110', '1', '0', '0.555555556', '0.388888889', '0', '0', '16.66666667', '4', '1.5', '0']
package org . apache . lucene . index ; import org . apache . lucene . store . BufferedIndexInput ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . IndexInput ; import java . io . IOException ; import java . util . Arrays ; class TermVectorsReader implements Cloneable { static final int FORMAT_VERSION = 2 ; static final int FORMAT_VERSION2 = 3 ; static final int FORMAT_UTF8_LENGTH_IN_BYTES = 4 ; static final int FORMAT_CURRENT = FORMAT_UTF8_LENGTH_IN_BYTES ; static final int FORMAT_SIZE = 4 ; static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x1 ; static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x2 ; private FieldInfos fieldInfos ; private IndexInput tvx ; private IndexInput tvd ; private IndexInput tvf ; private int size ; private int numTotalDocs ; private int docStoreOffset ; private final int format ; TermVectorsReader ( Directory d , String segment , FieldInfos fieldInfos ) throws CorruptIndexException , IOException { this ( d , segment , fieldInfos , BufferedIndexInput . BUFFER_SIZE ) ; } TermVectorsReader ( Directory d , String segment , FieldInfos fieldInfos , int readBufferSize ) throws CorruptIndexException , IOException { this ( d , segment , fieldInfos , readBufferSize , - 1 , 0 ) ; } TermVectorsReader ( Directory d , String segment , FieldInfos fieldInfos , int readBufferSize , int docStoreOffset , int size ) throws CorruptIndexException , IOException { boolean success = false ; try { if ( d . fileExists ( segment + "." + IndexFileNames . VECTORS_INDEX_EXTENSION ) ) { tvx = d . openInput ( segment + "." + IndexFileNames . VECTORS_INDEX_EXTENSION , readBufferSize ) ; format = checkValidFormat ( tvx ) ; tvd = d . openInput ( segment + "." + IndexFileNames . VECTORS_DOCUMENTS_EXTENSION , readBufferSize ) ; final int tvdFormat = checkValidFormat ( tvd ) ; tvf = d . openInput ( segment + "." + IndexFileNames . VECTORS_FIELDS_EXTENSION , readBufferSize ) ; final int tvfFormat = checkValidFormat ( tvf ) ; assert format == tvdFormat ; assert format == tvfFormat ; if ( format >= FORMAT_VERSION2 ) { assert ( tvx . length ( ) - FORMAT_SIZE ) % 16 == 0 ; numTotalDocs = ( int ) ( tvx . length ( ) > > 4 ) ; } else { assert ( tvx . length ( ) - FORMAT_SIZE ) % 8 == 0 ; numTotalDocs = ( int ) ( tvx . length ( ) > > 3 ) ; } if ( - 1 == docStoreOffset ) { this . docStoreOffset = 0 ; this . size = numTotalDocs ; assert size == 0 || numTotalDocs == size ; } else { this . docStoreOffset = docStoreOffset ; this . size = size ; assert numTotalDocs >= size + docStoreOffset : "numTotalDocs=" + numTotalDocs + " size=" + size + " docStoreOffset=" + docStoreOffset ; } } else format = 0 ; this . fieldInfos = fieldInfos ; success = true ; } finally { if ( ! success ) { close ( ) ; } } } IndexInput getTvdStream ( ) { return tvd ; } IndexInput getTvfStream ( ) { return tvf ; } final private void seekTvx ( final int docNum ) throws IOException { if ( format < FORMAT_VERSION2 ) tvx . seek ( ( docNum + docStoreOffset ) * 8L + FORMAT_SIZE ) ; else tvx . seek ( ( docNum + docStoreOffset ) * 16L + FORMAT_SIZE ) ; } boolean canReadRawDocs ( ) { return format >= FORMAT_UTF8_LENGTH_IN_BYTES ; } final void rawDocs ( int [ ] tvdLengths , int [ ] tvfLengths , int startDocID , int numDocs ) throws IOException { if ( tvx == null ) { Arrays . fill ( tvdLengths , 0 ) ; Arrays . fill ( tvfLengths , 0 ) ; return ; } if ( format < FORMAT_VERSION2 ) throw new IllegalStateException ( "cannot read raw docs with older term vector formats" ) ; seekTvx ( startDocID ) ; long tvdPosition = tvx . readLong ( ) ; tvd . seek ( tvdPosition ) ; long tvfPosition = tvx . readLong ( ) ; tvf . seek ( tvfPosition ) ; long lastTvdPosition = tvdPosition ; long lastTvfPosition = tvfPosition ; int count = 0 ; while ( count < numDocs ) { final int docID = docStoreOffset + startDocID + count + 1 ; assert docID <= numTotalDocs ; if ( docID < numTotalDocs ) { tvdPosition = tvx . readLong ( ) ; tvfPosition = tvx . readLong ( ) ; } else { tvdPosition = tvd . length ( ) ; tvfPosition = tvf . length ( ) ; assert count == numDocs - 1 ; } tvdLengths [ count ] = ( int ) ( tvdPosition - lastTvdPosition ) ; tvfLengths [ count ] = ( int ) ( tvfPosition - lastTvfPosition ) ; count ++ ; lastTvdPosition = tvdPosition ; lastTvfPosition = tvfPosition ; } } private int checkValidFormat ( IndexInput in ) throws CorruptIndexException , IOException { int format = in . readInt ( ) ; if ( format > FORMAT_CURRENT ) { throw new CorruptIndexException ( "Incompatible format version: " + format + " expected " + FORMAT_CURRENT + " or less" ) ; } return format ; } void close ( ) throws IOException { IOException keep = null ; if ( tvx != null ) try { tvx . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tvd != null ) try { tvd . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( tvf != null ) try { tvf . close ( ) ; } catch ( IOException e ) { if ( keep == null ) keep = e ; } if ( keep != null ) throw ( IOException ) keep . fillInStackTrace ( ) ; } int size ( ) { return size ; } public void get ( int docNum , String field , TermVectorMapper mapper ) throws IOException { if ( tvx != null ) { int fieldNumber = fieldInfos . fieldNumber ( field ) ; seekTvx ( docNum ) ; long tvdPosition = tvx . readLong ( ) ; tvd . seek ( tvdPosition ) ; int fieldCount = tvd . readVInt ( ) ; int number = 0 ; int found = - 1 ; for ( int i = 0 ; i < fieldCount ; i ++ ) { if ( format >= FORMAT_VERSION ) number = tvd . readVInt ( ) ; else number += tvd . readVInt ( ) ; if ( number == fieldNumber ) found = i ; } if ( found != - 1 ) { long position ; if ( format >= FORMAT_VERSION2 ) position = tvx . readLong ( ) ; else position = tvd . readVLong ( ) ; for ( int i = 1 ; i <= found ; i ++ ) position += tvd . readVLong ( ) ; mapper . setDocumentNumber ( docNum ) ; readTermVector ( field , position , mapper ) ; } else { } } else { } } TermFreqVector get ( int docNum , String field ) throws IOException { ParallelArrayTermVectorMapper mapper = new ParallelArrayTermVectorMapper ( ) ; get ( docNum , field , mapper ) ; return mapper . materializeVector ( ) ; } final private String [ ] readFields ( int fieldCount ) throws IOException { int number = 0 ; String [ ] fields = new String [ fieldCount ] ; for ( int i = 0 ; i < fieldCount ; i ++ ) { if ( format >= FORMAT_VERSION ) number = tvd . readVInt ( ) ; else number += tvd . readVInt ( ) ; fields [ i ] = fieldInfos . fieldName ( number ) ; } return fields ; } final private long [ ] readTvfPointers ( int fieldCount ) throws IOException { long position ; if ( format >= FORMAT_VERSION2 ) position = tvx . readLong ( ) ; else position = tvd . readVLong ( ) ; long [ ] tvfPointers = new long [ fieldCount ] ; tvfPointers [ 0 ] = position ; for ( int i = 1 ; i < fieldCount ; i ++ ) { position += tvd . readVLong ( ) ; tvfPointers [ i ] = position ; } return tvfPointers ; } TermFreqVector [ ] get ( int docNum ) throws IOException { TermFreqVector [ ] result = null ; if ( tvx != null ) { seekTvx ( docNum ) ; long tvdPosition = tvx . readLong ( ) ; tvd . seek ( tvdPosition ) ; int fieldCount = tvd . readVInt ( ) ; if ( fieldCount != 0 ) { final String [ ] fields = readFields ( fieldCount ) ; final long [ ] tvfPointers = readTvfPointers ( fieldCount ) ; result = readTermVectors ( docNum , fields , tvfPointers ) ; } } else { } return result ; } public void get ( int docNumber , TermVectorMapper mapper ) throws IOException { if ( tvx != null ) { seekTvx ( docNumber ) ; long tvdPosition = tvx . readLong ( ) ; tvd . seek ( tvdPosition ) ; int fieldCount = tvd . readVInt ( ) ; if ( fieldCount != 0 ) { final String [ ] fields = readFields ( fieldCount ) ; final long [ ] tvfPointers = readTvfPointers ( fieldCount ) ; mapper . setDocumentNumber ( docNumber ) ; readTermVectors ( fields , tvfPointers , mapper ) ; } } else { } } private SegmentTermVector [ ] readTermVectors ( int docNum , String fields [ ] , long tvfPointers [ ] ) throws IOException { SegmentTermVector res [ ] = new SegmentTermVector [ fields . length ] ; for ( int i = 0 ; i < fields . length ; i ++ ) { ParallelArrayTermVectorMapper mapper = new ParallelArrayTermVectorMapper ( ) ; mapper . setDocumentNumber ( docNum ) ; readTermVector ( fields [ i ] , tvfPointers [ i ] , mapper ) ; res [ i ] = ( SegmentTermVector ) mapper . materializeVector ( ) ; } return res ; } private void readTermVectors ( String fields [ ] , long tvfPointers [ ] , TermVectorMapper mapper ) throws IOException { for ( int i = 0 ; i < fields . length ; i ++ ) { readTermVector ( fields [ i ] , tvfPointers [ i ] , mapper ) ; } } private void readTermVector ( String field , long tvfPointer , TermVectorMapper mapper ) throws IOException { tvf . seek ( tvfPointer ) ; int numTerms = tvf . readVInt ( ) ; if ( numTerms == 0 ) return ; boolean storePositions ; boolean storeOffsets ; if ( format >= FORMAT_VERSION ) { byte bits = tvf . readByte ( ) ; storePositions = ( bits & STORE_POSITIONS_WITH_TERMVECTOR ) != 0 ; storeOffsets = ( bits & STORE_OFFSET_WITH_TERMVECTOR ) != 0 ; } else { tvf . readVInt ( ) ; storePositions = false ; storeOffsets = false ; } mapper . setExpectations ( field , numTerms , storeOffsets , storePositions ) ; int start = 0 ; int deltaLength = 0 ; int totalLength = 0 ; byte [ ] byteBuffer ; char [ ] charBuffer ; final boolean preUTF8 = format < FORMAT_UTF8_LENGTH_IN_BYTES ; if ( preUTF8 ) { charBuffer = new char [ 10 ] ; byteBuffer = null ; } else { charBuffer = null ; byteBuffer = new byte [ 20 ] ; } for ( int i = 0 ; i < numTerms ; i ++ ) { start = tvf . readVInt ( ) ; deltaLength = tvf . readVInt ( ) ; totalLength = start + deltaLength ; final String term ; if ( preUTF8 ) { if ( charBuffer . length < totalLength ) { char [ ] newCharBuffer = new char [ ( int ) ( 1.5 * totalLength ) ] ; System . arraycopy ( charBuffer , 0 , newCharBuffer , 0 , start ) ; charBuffer = newCharBuffer ; } tvf . readChars ( charBuffer , start , deltaLength ) ; term = new String ( charBuffer , 0 , totalLength ) ; } else { if ( byteBuffer . length < totalLength ) { byte [ ] newByteBuffer = new byte [ ( int ) ( 1.5 * totalLength ) ] ; System . arraycopy ( byteBuffer , 0 , newByteBuffer , 0 , start ) ; byteBuffer = newByteBuffer ; } tvf . readBytes ( byteBuffer , start , deltaLength ) ; term = new String ( byteBuffer , 0 , totalLength , "UTF-8" ) ; } int freq = tvf . readVInt ( ) ; int [ ] positions = null ; if ( storePositions ) { if ( mapper . isIgnoringPositions ( ) == false ) { positions = new int [ freq ] ; int prevPosition = 0 ; for ( int j = 0 ; j < freq ; j ++ ) { positions [ j ] = prevPosition + tvf . readVInt ( ) ; prevPosition = positions [ j ] ; } } else { for ( int j = 0 ; j < freq ; j ++ ) { tvf . readVInt ( ) ; } } } TermVectorOffsetInfo [ ] offsets = null ; if ( storeOffsets ) { if ( mapper . isIgnoringOffsets ( ) == false ) { offsets = new TermVectorOffsetInfo [ freq ] ; int prevOffset = 0 ; for ( int j = 0 ; j < freq ; j ++ ) { int startOffset = prevOffset + tvf . readVInt ( ) ; int endOffset = startOffset + tvf . readVInt ( ) ; offsets [ j ] = new TermVectorOffsetInfo ( startOffset , endOffset ) ; prevOffset = endOffset ; } } else { for ( int j = 0 ; j < freq ; j ++ ) { tvf . readVInt ( ) ; tvf . readVInt ( ) ; } } } mapper . map ( term , freq , offsets , positions ) ; } } protected Object clone ( ) throws CloneNotSupportedException { final TermVectorsReader clone = ( TermVectorsReader ) super . clone ( ) ; if ( tvx != null && tvd != null && tvf != null ) { clone . tvx = ( IndexInput ) tvx . clone ( ) ; clone . tvd = ( IndexInput ) tvd . clone ( ) ; clone . tvf = ( IndexInput ) tvf . clone ( ) ; } return clone ; } } class ParallelArrayTermVectorMapper extends TermVectorMapper { private String [ ] terms ; private int [ ] termFreqs ; private int positions [ ] [ ] ; private TermVectorOffsetInfo offsets [ ] [ ] ; private int currentPosition ; private boolean storingOffsets ; private boolean storingPositions ; private String field ; public void setExpectations ( String field , int numTerms , boolean storeOffsets , boolean storePositions ) { this . field = field ; terms = new String [ numTerms ] ; termFreqs = new int [ numTerms ] ; this . storingOffsets = storeOffsets ; this . storingPositions = storePositions ; if ( storePositions ) this . positions = new int [ numTerms ] [ ] ; if ( storeOffsets ) this . offsets = new TermVectorOffsetInfo [ numTerms ] [ ] ; } public void map ( String term , int frequency , TermVectorOffsetInfo [ ] offsets , int [ ] positions ) { terms [ currentPosition ] = term ; termFreqs [ currentPosition ] = frequency ; if ( storingOffsets ) { this . offsets [ currentPosition ] = offsets ; } if ( storingPositions ) { this . positions [ currentPosition ] = positions ; } currentPosition ++ ; } public TermFreqVector materializeVector ( ) { SegmentTermVector tv = null ; if ( field != null && terms != null ) { if ( storingPositions || storingOffsets ) { tv = new SegmentTermPositionVector ( field , terms , termFreqs , positions , offsets ) ; } else { tv = new SegmentTermVector ( field , terms , termFreqs ) ; } } return tv ; } } 	0	['23', '1', '0', '12', '66', '99', '3', '9', '2', '0.847593583', '1164', '0.470588235', '4', '0', '0.231404959', '0', '0', '48.86956522', '2', '0.8696', '0']
package org . apache . lucene . index ; public class TermVectorEntry { private String field ; private String term ; private int frequency ; private TermVectorOffsetInfo [ ] offsets ; int [ ] positions ; public TermVectorEntry ( ) { } public TermVectorEntry ( String field , String term , int frequency , TermVectorOffsetInfo [ ] offsets , int [ ] positions ) { this . field = field ; this . term = term ; this . frequency = frequency ; this . offsets = offsets ; this . positions = positions ; } public String getField ( ) { return field ; } public int getFrequency ( ) { return frequency ; } public TermVectorOffsetInfo [ ] getOffsets ( ) { return offsets ; } public int [ ] getPositions ( ) { return positions ; } public String getTerm ( ) { return term ; } void setFrequency ( int frequency ) { this . frequency = frequency ; } void setOffsets ( TermVectorOffsetInfo [ ] offsets ) { this . offsets = offsets ; } void setPositions ( int [ ] positions ) { this . positions = positions ; } public boolean equals ( Object o ) { if ( this == o ) return true ; if ( o == null || getClass ( ) != o . getClass ( ) ) return false ; TermVectorEntry that = ( TermVectorEntry ) o ; if ( term != null ? ! term . equals ( that . term ) : that . term != null ) return false ; return true ; } public int hashCode ( ) { return ( term != null ? term . hashCode ( ) : 0 ) ; } public String toString ( ) { return "TermVectorEntry{" + "field='" + field + '\'' + ", term='" + term + '\'' + ", frequency=" + frequency + '}' ; } } 	0	['13', '1', '0', '4', '22', '32', '3', '1', '10', '0.783333333', '135', '0.8', '1', '0', '0.269230769', '1', '1', '9', '7', '1.3846', '0']
package org . apache . lucene . analysis ; public class ISOLatin1AccentFilter extends TokenFilter { public ISOLatin1AccentFilter ( TokenStream input ) { super ( input ) ; } private char [ ] output = new char [ 256 ] ; private int outputPos ; public final Token next ( final Token reusableToken ) throws java . io . IOException { assert reusableToken != null ; Token nextToken = input . next ( reusableToken ) ; if ( nextToken != null ) { final char [ ] buffer = nextToken . termBuffer ( ) ; final int length = nextToken . termLength ( ) ; for ( int i = 0 ; i < length ; i ++ ) { final char c = buffer [ i ] ; if ( c >= 'À' && c <= 'ﬆ' ) { removeAccents ( buffer , length ) ; nextToken . setTermBuffer ( output , 0 , outputPos ) ; break ; } } return nextToken ; } else return null ; } public final void removeAccents ( char [ ] input , int length ) { final int maxSizeNeeded = 2 * length ; int size = output . length ; while ( size < maxSizeNeeded ) size *= 2 ; if ( size != output . length ) output = new char [ size ] ; outputPos = 0 ; int pos = 0 ; for ( int i = 0 ; i < length ; i ++ , pos ++ ) { final char c = input [ pos ] ; if ( c < 'À' || c > 'ﬆ' ) output [ outputPos ++ ] = c ; else { switch ( c ) { case 'À' : case 'Á' : case 'Â' : case 'Ã' : case 'Ä' : case 'Å' : output [ outputPos ++ ] = 'A' ; break ; case 'Æ' : output [ outputPos ++ ] = 'A' ; output [ outputPos ++ ] = 'E' ; break ; case 'Ç' : output [ outputPos ++ ] = 'C' ; break ; case 'È' : case 'É' : case 'Ê' : case 'Ë' : output [ outputPos ++ ] = 'E' ; break ; case 'Ì' : case 'Í' : case 'Î' : case 'Ï' : output [ outputPos ++ ] = 'I' ; break ; case 'Ĳ' : output [ outputPos ++ ] = 'I' ; output [ outputPos ++ ] = 'J' ; break ; case 'Ð' : output [ outputPos ++ ] = 'D' ; break ; case 'Ñ' : output [ outputPos ++ ] = 'N' ; break ; case 'Ò' : case 'Ó' : case 'Ô' : case 'Õ' : case 'Ö' : case 'Ø' : output [ outputPos ++ ] = 'O' ; break ; case 'Œ' : output [ outputPos ++ ] = 'O' ; output [ outputPos ++ ] = 'E' ; break ; case 'Þ' : output [ outputPos ++ ] = 'T' ; output [ outputPos ++ ] = 'H' ; break ; case 'Ù' : case 'Ú' : case 'Û' : case 'Ü' : output [ outputPos ++ ] = 'U' ; break ; case 'Ý' : case 'Ÿ' : output [ outputPos ++ ] = 'Y' ; break ; case 'à' : case 'á' : case 'â' : case 'ã' : case 'ä' : case 'å' : output [ outputPos ++ ] = 'a' ; break ; case 'æ' : output [ outputPos ++ ] = 'a' ; output [ outputPos ++ ] = 'e' ; break ; case 'ç' : output [ outputPos ++ ] = 'c' ; break ; case 'è' : case 'é' : case 'ê' : case 'ë' : output [ outputPos ++ ] = 'e' ; break ; case 'ì' : case 'í' : case 'î' : case 'ï' : output [ outputPos ++ ] = 'i' ; break ; case 'ĳ' : output [ outputPos ++ ] = 'i' ; output [ outputPos ++ ] = 'j' ; break ; case 'ð' : output [ outputPos ++ ] = 'd' ; break ; case 'ñ' : output [ outputPos ++ ] = 'n' ; break ; case 'ò' : case 'ó' : case 'ô' : case 'õ' : case 'ö' : case 'ø' : output [ outputPos ++ ] = 'o' ; break ; case 'œ' : output [ outputPos ++ ] = 'o' ; output [ outputPos ++ ] = 'e' ; break ; case 'ß' : output [ outputPos ++ ] = 's' ; output [ outputPos ++ ] = 's' ; break ; case 'þ' : output [ outputPos ++ ] = 't' ; output [ outputPos ++ ] = 'h' ; break ; case 'ù' : case 'ú' : case 'û' : case 'ü' : output [ outputPos ++ ] = 'u' ; break ; case 'ý' : case 'ÿ' : output [ outputPos ++ ] = 'y' ; break ; case 'ﬀ' : output [ outputPos ++ ] = 'f' ; output [ outputPos ++ ] = 'f' ; break ; case 'ﬁ' : output [ outputPos ++ ] = 'f' ; output [ outputPos ++ ] = 'i' ; break ; case 'ﬂ' : output [ outputPos ++ ] = 'f' ; output [ outputPos ++ ] = 'l' ; break ; case 'ﬅ' : output [ outputPos ++ ] = 'f' ; output [ outputPos ++ ] = 't' ; break ; case 'ﬆ' : output [ outputPos ++ ] = 's' ; output [ outputPos ++ ] = 't' ; break ; default : output [ outputPos ++ ] = c ; break ; } } } } } 	0	['5', '3', '0', '3', '15', '2', '0', '3', '3', '0.75', '708', '0.5', '0', '0.7', '0.333333333', '1', '2', '139.8', '79', '16.2', '0']
package org . apache . lucene . util . cache ; import java . util . LinkedHashMap ; import java . util . Map ; public class SimpleLRUCache extends SimpleMapCache { private final static float LOADFACTOR = 0.75f ; private int cacheSize ; public SimpleLRUCache ( int cacheSize ) { super ( null ) ; this . cacheSize = cacheSize ; int capacity = ( int ) Math . ceil ( cacheSize / LOADFACTOR ) + 1 ; super . map = new LinkedHashMap ( capacity , LOADFACTOR , true ) { protected boolean removeEldestEntry ( Map . Entry eldest ) { return size ( ) > SimpleLRUCache . this . cacheSize ; } } ; } } 	0	['2', '3', '0', '3', '5', '0', '2', '2', '1', '1', '33', '1', '0', '0.923076923', '0.5', '1', '4', '14.5', '1', '0.5', '0']
package org . apache . lucene . store ; import java . util . ArrayList ; import java . io . Serializable ; class RAMFile implements Serializable { private static final long serialVersionUID = 1l ; private ArrayList buffers = new ArrayList ( ) ; long length ; RAMDirectory directory ; long sizeInBytes ; private long lastModified = System . currentTimeMillis ( ) ; RAMFile ( ) { } RAMFile ( RAMDirectory directory ) { this . directory = directory ; } synchronized long getLength ( ) { return length ; } synchronized void setLength ( long length ) { this . length = length ; } synchronized long getLastModified ( ) { return lastModified ; } synchronized void setLastModified ( long lastModified ) { this . lastModified = lastModified ; } final synchronized byte [ ] addBuffer ( int size ) { byte [ ] buffer = newBuffer ( size ) ; if ( directory != null ) synchronized ( directory ) { buffers . add ( buffer ) ; directory . sizeInBytes += size ; sizeInBytes += size ; } else buffers . add ( buffer ) ; return buffer ; } final synchronized byte [ ] getBuffer ( int index ) { return ( byte [ ] ) buffers . get ( index ) ; } final synchronized int numBuffers ( ) { return buffers . size ( ) ; } byte [ ] newBuffer ( int size ) { return new byte [ size ] ; } long getSizeInBytes ( ) { synchronized ( directory ) { return sizeInBytes ; } } } 	0	['11', '1', '0', '3', '17', '19', '3', '1', '0', '0.833333333', '133', '0.5', '1', '0', '0.386363636', '0', '0', '10.54545455', '2', '0.9091', '0']
package org . apache . lucene . store ; import java . io . IOException ; public class LockObtainFailedException extends IOException { public LockObtainFailedException ( String message ) { super ( message ) ; } } 	0	['1', '4', '0', '8', '2', '0', '8', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import org . apache . lucene . store . BufferedIndexInput ; import org . apache . lucene . store . IndexInput ; abstract class MultiLevelSkipListReader { private int maxNumberOfSkipLevels ; private int numberOfSkipLevels ; private int numberOfLevelsToBuffer = 1 ; private int docCount ; private boolean haveSkipped ; private IndexInput [ ] skipStream ; private long skipPointer [ ] ; private int skipInterval [ ] ; private int [ ] numSkipped ; private int [ ] skipDoc ; private int lastDoc ; private long [ ] childPointer ; private long lastChildPointer ; private boolean inputIsBuffered ; public MultiLevelSkipListReader ( IndexInput skipStream , int maxSkipLevels , int skipInterval ) { this . skipStream = new IndexInput [ maxSkipLevels ] ; this . skipPointer = new long [ maxSkipLevels ] ; this . childPointer = new long [ maxSkipLevels ] ; this . numSkipped = new int [ maxSkipLevels ] ; this . maxNumberOfSkipLevels = maxSkipLevels ; this . skipInterval = new int [ maxSkipLevels ] ; this . skipStream [ 0 ] = skipStream ; this . inputIsBuffered = ( skipStream instanceof BufferedIndexInput ) ; this . skipInterval [ 0 ] = skipInterval ; for ( int i = 1 ; i < maxSkipLevels ; i ++ ) { this . skipInterval [ i ] = this . skipInterval [ i - 1 ] * skipInterval ; } skipDoc = new int [ maxSkipLevels ] ; } int getDoc ( ) { return lastDoc ; } int skipTo ( int target ) throws IOException { if ( ! haveSkipped ) { loadSkipLevels ( ) ; haveSkipped = true ; } int level = 0 ; while ( level < numberOfSkipLevels - 1 && target > skipDoc [ level + 1 ] ) { level ++ ; } while ( level >= 0 ) { if ( target > skipDoc [ level ] ) { if ( ! loadNextSkip ( level ) ) { continue ; } } else { if ( level > 0 && lastChildPointer > skipStream [ level - 1 ] . getFilePointer ( ) ) { seekChild ( level - 1 ) ; } level -- ; } } return numSkipped [ 0 ] - skipInterval [ 0 ] - 1 ; } private boolean loadNextSkip ( int level ) throws IOException { setLastSkipData ( level ) ; numSkipped [ level ] += skipInterval [ level ] ; if ( numSkipped [ level ] > docCount ) { skipDoc [ level ] = Integer . MAX_VALUE ; if ( numberOfSkipLevels > level ) numberOfSkipLevels = level ; return false ; } skipDoc [ level ] += readSkipData ( level , skipStream [ level ] ) ; if ( level != 0 ) { childPointer [ level ] = skipStream [ level ] . readVLong ( ) + skipPointer [ level - 1 ] ; } return true ; } protected void seekChild ( int level ) throws IOException { skipStream [ level ] . seek ( lastChildPointer ) ; numSkipped [ level ] = numSkipped [ level + 1 ] - skipInterval [ level + 1 ] ; skipDoc [ level ] = lastDoc ; if ( level > 0 ) { childPointer [ level ] = skipStream [ level ] . readVLong ( ) + skipPointer [ level - 1 ] ; } } void close ( ) throws IOException { for ( int i = 1 ; i < skipStream . length ; i ++ ) { if ( skipStream [ i ] != null ) { skipStream [ i ] . close ( ) ; } } } void init ( long skipPointer , int df ) { this . skipPointer [ 0 ] = skipPointer ; this . docCount = df ; Arrays . fill ( skipDoc , 0 ) ; Arrays . fill ( numSkipped , 0 ) ; Arrays . fill ( childPointer , 0 ) ; haveSkipped = false ; for ( int i = 1 ; i < numberOfSkipLevels ; i ++ ) { skipStream [ i ] = null ; } } private void loadSkipLevels ( ) throws IOException { numberOfSkipLevels = docCount == 0 ? 0 : ( int ) Math . floor ( Math . log ( docCount ) / Math . log ( skipInterval [ 0 ] ) ) ; if ( numberOfSkipLevels > maxNumberOfSkipLevels ) { numberOfSkipLevels = maxNumberOfSkipLevels ; } skipStream [ 0 ] . seek ( skipPointer [ 0 ] ) ; int toBuffer = numberOfLevelsToBuffer ; for ( int i = numberOfSkipLevels - 1 ; i > 0 ; i -- ) { long length = skipStream [ 0 ] . readVLong ( ) ; skipPointer [ i ] = skipStream [ 0 ] . getFilePointer ( ) ; if ( toBuffer > 0 ) { skipStream [ i ] = new SkipBuffer ( skipStream [ 0 ] , ( int ) length ) ; toBuffer -- ; } else { skipStream [ i ] = ( IndexInput ) skipStream [ 0 ] . clone ( ) ; if ( inputIsBuffered && length < BufferedIndexInput . BUFFER_SIZE ) { ( ( BufferedIndexInput ) skipStream [ i ] ) . setBufferSize ( ( int ) length ) ; } skipStream [ 0 ] . seek ( skipStream [ 0 ] . getFilePointer ( ) + length ) ; } } skipPointer [ 0 ] = skipStream [ 0 ] . getFilePointer ( ) ; } protected abstract int readSkipData ( int level , IndexInput skipStream ) throws IOException ; protected void setLastSkipData ( int level ) { lastDoc = skipDoc [ level ] ; lastChildPointer = childPointer [ level ] ; } private final static class SkipBuffer extends IndexInput { private byte [ ] data ; private long pointer ; private int pos ; SkipBuffer ( IndexInput input , int length ) throws IOException { data = new byte [ length ] ; pointer = input . getFilePointer ( ) ; input . readBytes ( data , 0 , length ) ; } public void close ( ) throws IOException { data = null ; } public long getFilePointer ( ) { return pointer + pos ; } public long length ( ) { return data . length ; } public byte readByte ( ) throws IOException { return data [ pos ++ ] ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { System . arraycopy ( data , pos , b , offset , len ) ; pos += len ; } public void seek ( long pos ) throws IOException { this . pos = ( int ) ( pos - pointer ) ; } } } 	0	['10', '1', '1', '4', '22', '0', '1', '3', '1', '0.611111111', '481', '1', '1', '0', '0.5', '0', '0', '45.7', '2', '1', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Map ; abstract class TermsHashConsumer { abstract int bytesPerPosting ( ) ; abstract void createPostings ( RawPostingList [ ] postings , int start , int count ) ; abstract TermsHashConsumerPerThread addThread ( TermsHashPerThread perThread ) ; abstract void flush ( Map threadsAndFields , final DocumentsWriter . FlushState state ) throws IOException ; abstract void abort ( ) ; abstract void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException ; FieldInfos fieldInfos ; void setFieldInfos ( FieldInfos fieldInfos ) { this . fieldInfos = fieldInfos ; } } 	0	['8', '1', '2', '9', '9', '28', '5', '5', '0', '1', '16', '0', '1', '0', '0.267857143', '0', '0', '0.875', '1', '0.875', '0']
package org . apache . lucene . search ; import java . io . IOException ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermEnum ; public abstract class FilteredTermEnum extends TermEnum { private Term currentTerm = null ; private TermEnum actualEnum = null ; public FilteredTermEnum ( ) { } protected abstract boolean termCompare ( Term term ) ; public abstract float difference ( ) ; protected abstract boolean endEnum ( ) ; protected void setEnum ( TermEnum actualEnum ) throws IOException { this . actualEnum = actualEnum ; Term term = actualEnum . term ( ) ; if ( term != null && termCompare ( term ) ) currentTerm = term ; else next ( ) ; } public int docFreq ( ) { if ( actualEnum == null ) return - 1 ; return actualEnum . docFreq ( ) ; } public boolean next ( ) throws IOException { if ( actualEnum == null ) return false ; currentTerm = null ; while ( currentTerm == null ) { if ( endEnum ( ) ) return false ; if ( actualEnum . next ( ) ) { Term term = actualEnum . term ( ) ; if ( termCompare ( term ) ) { currentTerm = term ; return true ; } } else return false ; } currentTerm = null ; return false ; } public Term term ( ) { return currentTerm ; } public void close ( ) throws IOException { actualEnum . close ( ) ; currentTerm = null ; actualEnum = null ; } } 	0	['9', '2', '2', '7', '14', '8', '5', '2', '6', '0.5', '103', '1', '2', '0.384615385', '0.407407407', '1', '2', '10.22222222', '2', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; import java . text . Collator ; import java . util . Locale ; class FieldDocSortedHitQueue extends PriorityQueue { volatile SortField [ ] fields ; volatile Collator [ ] collators ; FieldDocSortedHitQueue ( SortField [ ] fields , int size ) { this . fields = fields ; this . collators = hasCollators ( fields ) ; initialize ( size ) ; } synchronized void setFields ( SortField [ ] fields ) { if ( this . fields == null ) { this . fields = fields ; this . collators = hasCollators ( fields ) ; } } SortField [ ] getFields ( ) { return fields ; } private Collator [ ] hasCollators ( final SortField [ ] fields ) { if ( fields == null ) return null ; Collator [ ] ret = new Collator [ fields . length ] ; for ( int i = 0 ; i < fields . length ; ++ i ) { Locale locale = fields [ i ] . getLocale ( ) ; if ( locale != null ) ret [ i ] = Collator . getInstance ( locale ) ; } return ret ; } protected final boolean lessThan ( final Object a , final Object b ) { final FieldDoc docA = ( FieldDoc ) a ; final FieldDoc docB = ( FieldDoc ) b ; final int n = fields . length ; int c = 0 ; for ( int i = 0 ; i < n && c == 0 ; ++ i ) { final int type = fields [ i ] . getType ( ) ; switch ( type ) { case SortField . SCORE : { float r1 = ( ( Float ) docA . fields [ i ] ) . floatValue ( ) ; float r2 = ( ( Float ) docB . fields [ i ] ) . floatValue ( ) ; if ( r1 > r2 ) c = - 1 ; if ( r1 < r2 ) c = 1 ; break ; } case SortField . DOC : case SortField . INT : { int i1 = ( ( Integer ) docA . fields [ i ] ) . intValue ( ) ; int i2 = ( ( Integer ) docB . fields [ i ] ) . intValue ( ) ; if ( i1 < i2 ) c = - 1 ; if ( i1 > i2 ) c = 1 ; break ; } case SortField . LONG : { long l1 = ( ( Long ) docA . fields [ i ] ) . longValue ( ) ; long l2 = ( ( Long ) docB . fields [ i ] ) . longValue ( ) ; if ( l1 < l2 ) c = - 1 ; if ( l1 > l2 ) c = 1 ; break ; } case SortField . STRING : { String s1 = ( String ) docA . fields [ i ] ; String s2 = ( String ) docB . fields [ i ] ; if ( s1 == null ) c = ( s2 == null ) ? 0 : - 1 ; else if ( s2 == null ) c = 1 ; else if ( fields [ i ] . getLocale ( ) == null ) { c = s1 . compareTo ( s2 ) ; } else { c = collators [ i ] . compare ( s1 , s2 ) ; } break ; } case SortField . FLOAT : { float f1 = ( ( Float ) docA . fields [ i ] ) . floatValue ( ) ; float f2 = ( ( Float ) docB . fields [ i ] ) . floatValue ( ) ; if ( f1 < f2 ) c = - 1 ; if ( f1 > f2 ) c = 1 ; break ; } case SortField . DOUBLE : { double d1 = ( ( Double ) docA . fields [ i ] ) . doubleValue ( ) ; double d2 = ( ( Double ) docB . fields [ i ] ) . doubleValue ( ) ; if ( d1 < d2 ) c = - 1 ; if ( d1 > d2 ) c = 1 ; break ; } case SortField . BYTE : { int i1 = ( ( Byte ) docA . fields [ i ] ) . byteValue ( ) ; int i2 = ( ( Byte ) docB . fields [ i ] ) . byteValue ( ) ; if ( i1 < i2 ) c = - 1 ; if ( i1 > i2 ) c = 1 ; break ; } case SortField . SHORT : { int i1 = ( ( Short ) docA . fields [ i ] ) . shortValue ( ) ; int i2 = ( ( Short ) docB . fields [ i ] ) . shortValue ( ) ; if ( i1 < i2 ) c = - 1 ; if ( i1 > i2 ) c = 1 ; break ; } case SortField . CUSTOM : { c = docA . fields [ i ] . compareTo ( docB . fields [ i ] ) ; break ; } case SortField . AUTO : { throw new RuntimeException ( "FieldDocSortedHitQueue cannot use an AUTO SortField" ) ; } default : { throw new RuntimeException ( "invalid SortField type: " + type ) ; } } if ( fields [ i ] . getReverse ( ) ) { c = - c ; } } if ( c == 0 ) return docA . doc > docB . doc ; return c > 0 ; } } 	0	['5', '2', '0', '6', '25', '0', '3', '3', '0', '0.375', '378', '0', '1', '0.75', '0.5', '1', '3', '74.2', '26', '6.6', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . analysis . Token ; final class FreqProxTermsWriterPerField extends TermsHashConsumerPerField implements Comparable { final FreqProxTermsWriterPerThread perThread ; final TermsHashPerField termsHashPerField ; final FieldInfo fieldInfo ; final DocumentsWriter . DocState docState ; final DocInverter . FieldInvertState fieldState ; boolean omitTf ; public FreqProxTermsWriterPerField ( TermsHashPerField termsHashPerField , FreqProxTermsWriterPerThread perThread , FieldInfo fieldInfo ) { this . termsHashPerField = termsHashPerField ; this . perThread = perThread ; this . fieldInfo = fieldInfo ; docState = termsHashPerField . docState ; fieldState = termsHashPerField . fieldState ; omitTf = fieldInfo . omitTf ; } int getStreamCount ( ) { if ( fieldInfo . omitTf ) return 1 ; else return 2 ; } void finish ( ) { } boolean hasPayloads ; void skippingLongTerm ( Token t ) throws IOException { } public int compareTo ( Object other0 ) { FreqProxTermsWriterPerField other = ( FreqProxTermsWriterPerField ) other0 ; return fieldInfo . name . compareTo ( other . fieldInfo . name ) ; } void reset ( ) { omitTf = fieldInfo . omitTf ; } boolean start ( Fieldable [ ] fields , int count ) { for ( int i = 0 ; i < count ; i ++ ) if ( fields [ i ] . isIndexed ( ) ) return true ; return false ; } final void writeProx ( Token t , FreqProxTermsWriter . PostingList p , int proxCode ) { final Payload payload = t . getPayload ( ) ; if ( payload != null && payload . length > 0 ) { termsHashPerField . writeVInt ( 1 , ( proxCode << 1 ) | 1 ) ; termsHashPerField . writeVInt ( 1 , payload . length ) ; termsHashPerField . writeBytes ( 1 , payload . data , payload . offset , payload . length ) ; hasPayloads = true ; } else termsHashPerField . writeVInt ( 1 , proxCode << 1 ) ; p . lastPosition = fieldState . position ; } final void newTerm ( Token t , RawPostingList p0 ) { assert docState . testPoint ( "FreqProxTermsWriterPerField.newTerm start" ) ; FreqProxTermsWriter . PostingList p = ( FreqProxTermsWriter . PostingList ) p0 ; p . lastDocID = docState . docID ; if ( omitTf ) { p . lastDocCode = docState . docID ; } else { p . lastDocCode = docState . docID << 1 ; p . docFreq = 1 ; writeProx ( t , p , fieldState . position ) ; } } final void addTerm ( Token t , RawPostingList p0 ) { assert docState . testPoint ( "FreqProxTermsWriterPerField.addTerm start" ) ; FreqProxTermsWriter . PostingList p = ( FreqProxTermsWriter . PostingList ) p0 ; assert omitTf || p . docFreq > 0 ; if ( omitTf ) { if ( docState . docID != p . lastDocID ) { assert docState . docID > p . lastDocID ; termsHashPerField . writeVInt ( 0 , p . lastDocCode ) ; p . lastDocCode = docState . docID - p . lastDocID ; p . lastDocID = docState . docID ; } } else { if ( docState . docID != p . lastDocID ) { assert docState . docID > p . lastDocID ; if ( 1 == p . docFreq ) termsHashPerField . writeVInt ( 0 , p . lastDocCode | 1 ) ; else { termsHashPerField . writeVInt ( 0 , p . lastDocCode ) ; termsHashPerField . writeVInt ( 0 , p . docFreq ) ; } p . docFreq = 1 ; p . lastDocCode = ( docState . docID - p . lastDocID ) << 1 ; p . lastDocID = docState . docID ; writeProx ( t , p , fieldState . position ) ; } else { p . docFreq ++ ; writeProx ( t , p , fieldState . position - p . lastPosition ) ; } } } public void abort ( ) { } } 	0	['13', '2', '0', '13', '25', '46', '3', '11', '3', '0.833333333', '364', '0', '5', '0.352941176', '0.196969697', '0', '0', '26.30769231', '14', '2.4615', '0']
package org . apache . lucene . util . cache ; public abstract class Cache { static class SynchronizedCache extends Cache { Object mutex ; Cache cache ; SynchronizedCache ( Cache cache ) { this . cache = cache ; this . mutex = this ; } SynchronizedCache ( Cache cache , Object mutex ) { this . cache = cache ; this . mutex = mutex ; } public void put ( Object key , Object value ) { synchronized ( mutex ) { cache . put ( key , value ) ; } } public Object get ( Object key ) { synchronized ( mutex ) { return cache . get ( key ) ; } } public boolean containsKey ( Object key ) { synchronized ( mutex ) { return cache . containsKey ( key ) ; } } public void close ( ) { synchronized ( mutex ) { cache . close ( ) ; } } Cache getSynchronizedCache ( ) { return this ; } } public static Cache synchronizedCache ( Cache cache ) { return cache . getSynchronizedCache ( ) ; } Cache getSynchronizedCache ( ) { return new SynchronizedCache ( this ) ; } public abstract void put ( Object key , Object value ) ; public abstract Object get ( Object key ) ; public abstract boolean containsKey ( Object key ) ; public abstract void close ( ) ; } 	0	['7', '1', '2', '5', '9', '21', '5', '1', '6', '2', '18', '0', '0', '0', '0.476190476', '0', '0', '1.571428571', '1', '0.8571', '0']
package org . apache . lucene . analysis . standard ; import org . apache . lucene . analysis . TokenFilter ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . analysis . TokenStream ; public final class StandardFilter extends TokenFilter { public StandardFilter ( TokenStream in ) { super ( in ) ; } private static final String APOSTROPHE_TYPE = StandardTokenizerImpl . TOKEN_TYPES [ StandardTokenizerImpl . APOSTROPHE ] ; private static final String ACRONYM_TYPE = StandardTokenizerImpl . TOKEN_TYPES [ StandardTokenizerImpl . ACRONYM ] ; public final Token next ( final Token reusableToken ) throws java . io . IOException { assert reusableToken != null ; Token nextToken = input . next ( reusableToken ) ; if ( nextToken == null ) return null ; char [ ] buffer = nextToken . termBuffer ( ) ; final int bufferLength = nextToken . termLength ( ) ; final String type = nextToken . type ( ) ; if ( type == APOSTROPHE_TYPE && bufferLength >= 2 && buffer [ bufferLength - 2 ] == '\'' && ( buffer [ bufferLength - 1 ] == 's' || buffer [ bufferLength - 1 ] == 'S' ) ) { nextToken . setTermLength ( bufferLength - 2 ) ; } else if ( type == ACRONYM_TYPE ) { int upto = 0 ; for ( int i = 0 ; i < bufferLength ; i ++ ) { char c = buffer [ i ] ; if ( c != '.' ) buffer [ upto ++ ] = c ; } nextToken . setTermLength ( upto ) ; } return nextToken ; } } 	0	['4', '3', '0', '5', '15', '4', '1', '4', '2', '0.75', '133', '0.5', '0', '0.777777778', '0.416666667', '1', '2', '31.25', '1', '0.5', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . util . PriorityQueue ; final class SegmentMergeQueue extends PriorityQueue { SegmentMergeQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object a , Object b ) { SegmentMergeInfo stiA = ( SegmentMergeInfo ) a ; SegmentMergeInfo stiB = ( SegmentMergeInfo ) b ; int comparison = stiA . term . compareTo ( stiB . term ) ; if ( comparison == 0 ) return stiA . base < stiB . base ; else return comparison < 0 ; } final void close ( ) throws IOException { while ( top ( ) != null ) ( ( SegmentMergeInfo ) pop ( ) ) . close ( ) ; } } 	0	['3', '2', '0', '5', '9', '3', '2', '3', '0', '2', '47', '0', '0', '0.857142857', '0.555555556', '1', '3', '14.66666667', '4', '1.6667', '0']
package org . apache . lucene . index ; import java . io . Serializable ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . analysis . TokenStream ; import org . apache . lucene . util . ArrayUtil ; public class Payload implements Serializable , Cloneable { protected byte [ ] data ; protected int offset ; protected int length ; public Payload ( ) { } public Payload ( byte [ ] data ) { this ( data , 0 , data . length ) ; } public Payload ( byte [ ] data , int offset , int length ) { if ( offset < 0 || offset + length > data . length ) { throw new IllegalArgumentException ( ) ; } this . data = data ; this . offset = offset ; this . length = length ; } public void setData ( byte [ ] data ) { setData ( data , 0 , data . length ) ; } public void setData ( byte [ ] data , int offset , int length ) { this . data = data ; this . offset = offset ; this . length = length ; } public byte [ ] getData ( ) { return this . data ; } public int getOffset ( ) { return this . offset ; } public int length ( ) { return this . length ; } public byte byteAt ( int index ) { if ( 0 <= index && index < this . length ) { return this . data [ this . offset + index ] ; } throw new ArrayIndexOutOfBoundsException ( index ) ; } public byte [ ] toByteArray ( ) { byte [ ] retArray = new byte [ this . length ] ; System . arraycopy ( this . data , this . offset , retArray , 0 , this . length ) ; return retArray ; } public void copyTo ( byte [ ] target , int targetOffset ) { if ( this . length > target . length + targetOffset ) { throw new ArrayIndexOutOfBoundsException ( ) ; } System . arraycopy ( this . data , this . offset , target , targetOffset , this . length ) ; } public Object clone ( ) { try { Payload clone = ( Payload ) super . clone ( ) ; if ( offset == 0 && length == data . length ) { clone . data = ( byte [ ] ) data . clone ( ) ; } else { clone . data = this . toByteArray ( ) ; clone . offset = 0 ; } return clone ; } catch ( CloneNotSupportedException e ) { throw new RuntimeException ( e ) ; } } public boolean equals ( Object obj ) { if ( obj == this ) return true ; if ( obj instanceof Payload ) { Payload other = ( Payload ) obj ; if ( length == other . length ) { for ( int i = 0 ; i < length ; i ++ ) if ( data [ offset + i ] != other . data [ other . offset + i ] ) return false ; return true ; } else return false ; } else return false ; } public int hashCode ( ) { return ArrayUtil . hashCode ( data , offset , offset + length ) ; } } 	0	['14', '1', '0', '4', '22', '0', '3', '1', '14', '0.230769231', '227', '1', '0', '0', '0.428571429', '1', '1', '15', '6', '1.5', '0']
package org . apache . lucene . store ; import java . net . Socket ; import java . io . IOException ; import java . io . InputStream ; import java . io . OutputStream ; public class VerifyingLockFactory extends LockFactory { LockFactory lf ; byte id ; String host ; int port ; private class CheckedLock extends Lock { private Lock lock ; public CheckedLock ( Lock lock ) { this . lock = lock ; } private void verify ( byte message ) { try { Socket s = new Socket ( host , port ) ; OutputStream out = s . getOutputStream ( ) ; out . write ( id ) ; out . write ( message ) ; InputStream in = s . getInputStream ( ) ; int result = in . read ( ) ; in . close ( ) ; out . close ( ) ; s . close ( ) ; if ( result != 0 ) throw new RuntimeException ( "lock was double acquired" ) ; } catch ( Exception e ) { throw new RuntimeException ( e ) ; } } public synchronized boolean obtain ( long lockWaitTimeout ) throws LockObtainFailedException , IOException { boolean obtained = lock . obtain ( lockWaitTimeout ) ; if ( obtained ) verify ( ( byte ) 1 ) ; return obtained ; } public synchronized boolean obtain ( ) throws LockObtainFailedException , IOException { return lock . obtain ( ) ; } public synchronized boolean isLocked ( ) { return lock . isLocked ( ) ; } public synchronized void release ( ) throws IOException { if ( isLocked ( ) ) { verify ( ( byte ) 0 ) ; lock . release ( ) ; } } } public VerifyingLockFactory ( byte id , LockFactory lf , String host , int port ) throws IOException { this . id = id ; this . lf = lf ; this . host = host ; this . port = port ; } public synchronized Lock makeLock ( String lockName ) { return new CheckedLock ( lf . makeLock ( lockName ) ) ; } public synchronized void clearLock ( String lockName ) throws IOException { lf . clearLock ( lockName ) ; } } 	0	['3', '2', '0', '4', '7', '0', '2', '3', '3', '0.75', '36', '0', '1', '0.666666667', '0.6', '0', '0', '9.666666667', '1', '0.6667', '0']
package org . apache . lucene . index ; final class FreqProxTermsWriterPerThread extends TermsHashConsumerPerThread { final TermsHashPerThread termsHashPerThread ; final DocumentsWriter . DocState docState ; public FreqProxTermsWriterPerThread ( TermsHashPerThread perThread ) { docState = perThread . docState ; termsHashPerThread = perThread ; } public TermsHashConsumerPerField addField ( TermsHashPerField termsHashPerField , FieldInfo fieldInfo ) { return new FreqProxTermsWriterPerField ( termsHashPerField , this , fieldInfo ) ; } void startDocument ( ) { } DocumentsWriter . DocWriter finishDocument ( ) { return null ; } public void abort ( ) { } } 	0	['5', '2', '0', '10', '7', '10', '3', '8', '3', '1', '28', '0', '2', '0.5', '0.4', '0', '0', '4.2', '1', '0.8', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import org . apache . lucene . document . Fieldable ; import org . apache . lucene . analysis . Token ; import org . apache . lucene . util . UnicodeUtil ; final class TermsHashPerField extends InvertedDocConsumerPerField { final TermsHashConsumerPerField consumer ; final TermsHashPerField nextPerField ; final TermsHashPerThread perThread ; final DocumentsWriter . DocState docState ; final DocInverter . FieldInvertState fieldState ; final CharBlockPool charPool ; final IntBlockPool intPool ; final ByteBlockPool bytePool ; final int streamCount ; final int numPostingInt ; final FieldInfo fieldInfo ; boolean postingsCompacted ; int numPostings ; private int postingsHashSize = 4 ; private int postingsHashHalfSize = postingsHashSize / 2 ; private int postingsHashMask = postingsHashSize - 1 ; private RawPostingList [ ] postingsHash = new RawPostingList [ postingsHashSize ] ; private RawPostingList p ; public TermsHashPerField ( DocInverterPerField docInverterPerField , final TermsHashPerThread perThread , final TermsHashPerThread nextPerThread , final FieldInfo fieldInfo ) { this . perThread = perThread ; intPool = perThread . intPool ; charPool = perThread . charPool ; bytePool = perThread . bytePool ; docState = perThread . docState ; fieldState = docInverterPerField . fieldState ; this . consumer = perThread . consumer . addField ( this , fieldInfo ) ; streamCount = consumer . getStreamCount ( ) ; numPostingInt = 2 * streamCount ; this . fieldInfo = fieldInfo ; if ( nextPerThread != null ) nextPerField = ( TermsHashPerField ) nextPerThread . addField ( docInverterPerField , fieldInfo ) ; else nextPerField = null ; } void shrinkHash ( int targetSize ) { assert postingsCompacted || numPostings == 0 ; int newSize = postingsHash . length ; while ( newSize >= 8 && newSize / 4 > targetSize ) { newSize /= 2 ; } if ( newSize != postingsHash . length ) { postingsHash = new RawPostingList [ newSize ] ; postingsHashSize = newSize ; postingsHashHalfSize = newSize / 2 ; postingsHashMask = newSize - 1 ; } } public void reset ( ) { if ( ! postingsCompacted ) compactPostings ( ) ; assert numPostings <= postingsHash . length ; if ( numPostings > 0 ) { perThread . termsHash . recyclePostings ( postingsHash , numPostings ) ; Arrays . fill ( postingsHash , 0 , numPostings , null ) ; numPostings = 0 ; } postingsCompacted = false ; if ( nextPerField != null ) nextPerField . reset ( ) ; } synchronized public void abort ( ) { reset ( ) ; if ( nextPerField != null ) nextPerField . abort ( ) ; } public void initReader ( ByteSliceReader reader , RawPostingList p , int stream ) { assert stream < streamCount ; final int [ ] ints = intPool . buffers [ p . intStart > > DocumentsWriter . INT_BLOCK_SHIFT ] ; final int upto = p . intStart & DocumentsWriter . INT_BLOCK_MASK ; reader . init ( bytePool , p . byteStart + stream * ByteBlockPool . FIRST_LEVEL_SIZE , ints [ upto + stream ] ) ; } private synchronized void compactPostings ( ) { int upto = 0 ; for ( int i = 0 ; i < postingsHashSize ; i ++ ) { if ( postingsHash [ i ] != null ) { if ( upto < i ) { postingsHash [ upto ] = postingsHash [ i ] ; postingsHash [ i ] = null ; } upto ++ ; } } assert upto == numPostings ; postingsCompacted = true ; } public RawPostingList [ ] sortPostings ( ) { compactPostings ( ) ; quickSort ( postingsHash , 0 , numPostings - 1 ) ; return postingsHash ; } void quickSort ( RawPostingList [ ] postings , int lo , int hi ) { if ( lo >= hi ) return ; else if ( hi == 1 + lo ) { if ( comparePostings ( postings [ lo ] , postings [ hi ] ) > 0 ) { final RawPostingList tmp = postings [ lo ] ; postings [ lo ] = postings [ hi ] ; postings [ hi ] = tmp ; } return ; } int mid = ( lo + hi ) > > > 1 ; if ( comparePostings ( postings [ lo ] , postings [ mid ] ) > 0 ) { RawPostingList tmp = postings [ lo ] ; postings [ lo ] = postings [ mid ] ; postings [ mid ] = tmp ; } if ( comparePostings ( postings [ mid ] , postings [ hi ] ) > 0 ) { RawPostingList tmp = postings [ mid ] ; postings [ mid ] = postings [ hi ] ; postings [ hi ] = tmp ; if ( comparePostings ( postings [ lo ] , postings [ mid ] ) > 0 ) { RawPostingList tmp2 = postings [ lo ] ; postings [ lo ] = postings [ mid ] ; postings [ mid ] = tmp2 ; } } int left = lo + 1 ; int right = hi - 1 ; if ( left >= right ) return ; RawPostingList partition = postings [ mid ] ; for ( ; ; ) { while ( comparePostings ( postings [ right ] , partition ) > 0 ) -- right ; while ( left < right && comparePostings ( postings [ left ] , partition ) <= 0 ) ++ left ; if ( left < right ) { RawPostingList tmp = postings [ left ] ; postings [ left ] = postings [ right ] ; postings [ right ] = tmp ; -- right ; } else { break ; } } quickSort ( postings , lo , left ) ; quickSort ( postings , left + 1 , hi ) ; } int comparePostings ( RawPostingList p1 , RawPostingList p2 ) { if ( p1 == p2 ) return 0 ; final char [ ] text1 = charPool . buffers [ p1 . textStart > > DocumentsWriter . CHAR_BLOCK_SHIFT ] ; int pos1 = p1 . textStart & DocumentsWriter . CHAR_BLOCK_MASK ; final char [ ] text2 = charPool . buffers [ p2 . textStart > > DocumentsWriter . CHAR_BLOCK_SHIFT ] ; int pos2 = p2 . textStart & DocumentsWriter . CHAR_BLOCK_MASK ; assert text1 != text2 || pos1 != pos2 ; while ( true ) { final char c1 = text1 [ pos1 ++ ] ; final char c2 = text2 [ pos2 ++ ] ; if ( c1 != c2 ) { if ( 0xffff == c2 ) return 1 ; else if ( 0xffff == c1 ) return - 1 ; else return c1 - c2 ; } else assert c1 != 0xffff ; } } private boolean postingEquals ( final char [ ] tokenText , final int tokenTextLen ) { final char [ ] text = perThread . charPool . buffers [ p . textStart > > DocumentsWriter . CHAR_BLOCK_SHIFT ] ; assert text != null ; int pos = p . textStart & DocumentsWriter . CHAR_BLOCK_MASK ; int tokenPos = 0 ; for ( ; tokenPos < tokenTextLen ; pos ++ , tokenPos ++ ) if ( tokenText [ tokenPos ] != text [ pos ] ) return false ; return 0xffff == text [ pos ] ; } private boolean doCall ; private boolean doNextCall ; boolean start ( Fieldable [ ] fields , int count ) throws IOException { doCall = consumer . start ( fields , count ) ; if ( nextPerField != null ) doNextCall = nextPerField . start ( fields , count ) ; return doCall || doNextCall ; } public void add ( Token token , int textStart ) throws IOException { int code = textStart ; int hashPos = code & postingsHashMask ; assert ! postingsCompacted ; p = postingsHash [ hashPos ] ; if ( p != null && p . textStart != textStart ) { final int inc = ( ( code > > 8 ) + code ) | 1 ; do { code += inc ; hashPos = code & postingsHashMask ; p = postingsHash [ hashPos ] ; } while ( p != null && p . textStart != textStart ) ; } if ( p == null ) { if ( 0 == perThread . freePostingsCount ) perThread . morePostings ( ) ; p = perThread . freePostings [ -- perThread . freePostingsCount ] ; assert p != null ; p . textStart = textStart ; assert postingsHash [ hashPos ] == null ; postingsHash [ hashPos ] = p ; numPostings ++ ; if ( numPostings == postingsHashHalfSize ) rehashPostings ( 2 * postingsHashSize ) ; if ( numPostingInt + intPool . intUpto > DocumentsWriter . INT_BLOCK_SIZE ) intPool . nextBuffer ( ) ; if ( DocumentsWriter . BYTE_BLOCK_SIZE - bytePool . byteUpto < numPostingInt * ByteBlockPool . FIRST_LEVEL_SIZE ) bytePool . nextBuffer ( ) ; intUptos = intPool . buffer ; intUptoStart = intPool . intUpto ; intPool . intUpto += streamCount ; p . intStart = intUptoStart + intPool . intOffset ; for ( int i = 0 ; i < streamCount ; i ++ ) { final int upto = bytePool . newSlice ( ByteBlockPool . FIRST_LEVEL_SIZE ) ; intUptos [ intUptoStart + i ] = upto + bytePool . byteOffset ; } p . byteStart = intUptos [ intUptoStart ] ; consumer . newTerm ( token , p ) ; } else { intUptos = intPool . buffers [ p . intStart > > DocumentsWriter . INT_BLOCK_SHIFT ] ; intUptoStart = p . intStart & DocumentsWriter . INT_BLOCK_MASK ; consumer . addTerm ( token , p ) ; } } void add ( Token token ) throws IOException { assert ! postingsCompacted ; final char [ ] tokenText = token . termBuffer ( ) ; final int tokenTextLen = token . termLength ( ) ; int downto = tokenTextLen ; int code = 0 ; while ( downto > 0 ) { char ch = tokenText [ -- downto ] ; if ( ch >= UnicodeUtil . UNI_SUR_LOW_START && ch <= UnicodeUtil . UNI_SUR_LOW_END ) { if ( 0 == downto ) { ch = tokenText [ downto ] = UnicodeUtil . UNI_REPLACEMENT_CHAR ; } else { final char ch2 = tokenText [ downto - 1 ] ; if ( ch2 >= UnicodeUtil . UNI_SUR_HIGH_START && ch2 <= UnicodeUtil . UNI_SUR_HIGH_END ) { code = ( ( code * 31 ) + ch ) * 31 + ch2 ; downto -- ; continue ; } else { ch = tokenText [ downto ] = UnicodeUtil . UNI_REPLACEMENT_CHAR ; } } } else if ( ch >= UnicodeUtil . UNI_SUR_HIGH_START && ch <= UnicodeUtil . UNI_SUR_HIGH_END ) ch = tokenText [ downto ] = UnicodeUtil . UNI_REPLACEMENT_CHAR ; code = ( code * 31 ) + ch ; } int hashPos = code & postingsHashMask ; p = postingsHash [ hashPos ] ; if ( p != null && ! postingEquals ( tokenText , tokenTextLen ) ) { final int inc = ( ( code > > 8 ) + code ) | 1 ; do { code += inc ; hashPos = code & postingsHashMask ; p = postingsHash [ hashPos ] ; } while ( p != null && ! postingEquals ( tokenText , tokenTextLen ) ) ; } if ( p == null ) { final int textLen1 = 1 + tokenTextLen ; if ( textLen1 + charPool . charUpto > DocumentsWriter . CHAR_BLOCK_SIZE ) { if ( textLen1 > DocumentsWriter . CHAR_BLOCK_SIZE ) { if ( docState . maxTermPrefix == null ) docState . maxTermPrefix = new String ( tokenText , 0 , 30 ) ; consumer . skippingLongTerm ( token ) ; return ; } charPool . nextBuffer ( ) ; } if ( 0 == perThread . freePostingsCount ) perThread . morePostings ( ) ; p = perThread . freePostings [ -- perThread . freePostingsCount ] ; assert p != null ; final char [ ] text = charPool . buffer ; final int textUpto = charPool . charUpto ; p . textStart = textUpto + charPool . charOffset ; charPool . charUpto += textLen1 ; System . arraycopy ( tokenText , 0 , text , textUpto , tokenTextLen ) ; text [ textUpto + tokenTextLen ] = 0xffff ; assert postingsHash [ hashPos ] == null ; postingsHash [ hashPos ] = p ; numPostings ++ ; if ( numPostings == postingsHashHalfSize ) rehashPostings ( 2 * postingsHashSize ) ; if ( numPostingInt + intPool . intUpto > DocumentsWriter . INT_BLOCK_SIZE ) intPool . nextBuffer ( ) ; if ( DocumentsWriter . BYTE_BLOCK_SIZE - bytePool . byteUpto < numPostingInt * ByteBlockPool . FIRST_LEVEL_SIZE ) bytePool . nextBuffer ( ) ; intUptos = intPool . buffer ; intUptoStart = intPool . intUpto ; intPool . intUpto += streamCount ; p . intStart = intUptoStart + intPool . intOffset ; for ( int i = 0 ; i < streamCount ; i ++ ) { final int upto = bytePool . newSlice ( ByteBlockPool . FIRST_LEVEL_SIZE ) ; intUptos [ intUptoStart + i ] = upto + bytePool . byteOffset ; } p . byteStart = intUptos [ intUptoStart ] ; consumer . newTerm ( token , p ) ; } else { intUptos = intPool . buffers [ p . intStart > > DocumentsWriter . INT_BLOCK_SHIFT ] ; intUptoStart = p . intStart & DocumentsWriter . INT_BLOCK_MASK ; consumer . addTerm ( token , p ) ; } if ( doNextCall ) nextPerField . add ( token , p . textStart ) ; } int [ ] intUptos ; int intUptoStart ; void writeByte ( int stream , byte b ) { int upto = intUptos [ intUptoStart + stream ] ; byte [ ] bytes = bytePool . buffers [ upto > > DocumentsWriter . BYTE_BLOCK_SHIFT ] ; assert bytes != null ; int offset = upto & DocumentsWriter . BYTE_BLOCK_MASK ; if ( bytes [ offset ] != 0 ) { offset = bytePool . allocSlice ( bytes , offset ) ; bytes = bytePool . buffer ; intUptos [ intUptoStart + stream ] = offset + bytePool . byteOffset ; } bytes [ offset ] = b ; ( intUptos [ intUptoStart + stream ] ) ++ ; } public void writeBytes ( int stream , byte [ ] b , int offset , int len ) { final int end = offset + len ; for ( int i = offset ; i < end ; i ++ ) writeByte ( stream , b [ i ] ) ; } void writeVInt ( int stream , int i ) { assert stream < streamCount ; while ( ( i & ~ 0x7F ) != 0 ) { writeByte ( stream , ( byte ) ( ( i & 0x7f ) | 0x80 ) ) ; i >>>= 7 ; } writeByte ( stream , ( byte ) i ) ; } void finish ( ) throws IOException { consumer . finish ( ) ; if ( nextPerField != null ) nextPerField . finish ( ) ; } void rehashPostings ( final int newSize ) { final int newMask = newSize - 1 ; RawPostingList [ ] newHash = new RawPostingList [ newSize ] ; for ( int i = 0 ; i < postingsHashSize ; i ++ ) { RawPostingList p0 = postingsHash [ i ] ; if ( p0 != null ) { int code ; if ( perThread . primary ) { final int start = p0 . textStart & DocumentsWriter . CHAR_BLOCK_MASK ; final char [ ] text = charPool . buffers [ p0 . textStart > > DocumentsWriter . CHAR_BLOCK_SHIFT ] ; int pos = start ; while ( text [ pos ] != 0xffff ) pos ++ ; code = 0 ; while ( pos > start ) code = ( code * 31 ) + text [ -- pos ] ; } else code = p0 . textStart ; int hashPos = code & newMask ; assert hashPos >= 0 ; if ( newHash [ hashPos ] != null ) { final int inc = ( ( code > > 8 ) + code ) | 1 ; do { code += inc ; hashPos = code & newMask ; } while ( newHash [ hashPos ] != null ) ; } newHash [ hashPos ] = p0 ; } } postingsHashMask = newMask ; postingsHash = newHash ; postingsHashSize = newSize ; postingsHashHalfSize = newSize > > 1 ; } } 	0	['20', '2', '0', '23', '47', '0', '10', '16', '7', '0.785087719', '1605', '0.291666667', '11', '0.181818182', '0.157894737', '0', '0', '78.05', '12', '3.75', '0']
package org . apache . lucene . index ; import java . io . IOException ; public class StaleReaderException extends IOException { public StaleReaderException ( String message ) { super ( message ) ; } } 	0	['1', '4', '0', '3', '2', '0', '3', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Collection ; import java . util . Iterator ; import java . util . HashMap ; import java . util . Map ; import java . util . List ; import java . util . ArrayList ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . search . Similarity ; final class NormsWriter extends InvertedDocEndConsumer { private static final byte defaultNorm = Similarity . encodeNorm ( 1.0f ) ; private FieldInfos fieldInfos ; public InvertedDocEndConsumerPerThread addThread ( DocInverterPerThread docInverterPerThread ) { return new NormsWriterPerThread ( docInverterPerThread , this ) ; } public void abort ( ) { } void files ( Collection files ) { } void setFieldInfos ( FieldInfos fieldInfos ) { this . fieldInfos = fieldInfos ; } public void flush ( Map threadsAndFields , DocumentsWriter . FlushState state ) throws IOException { final Map byField = new HashMap ( ) ; final Iterator it = threadsAndFields . entrySet ( ) . iterator ( ) ; while ( it . hasNext ( ) ) { Map . Entry entry = ( Map . Entry ) it . next ( ) ; Collection fields = ( Collection ) entry . getValue ( ) ; Iterator fieldsIt = fields . iterator ( ) ; while ( fieldsIt . hasNext ( ) ) { NormsWriterPerField perField = ( NormsWriterPerField ) fieldsIt . next ( ) ; if ( perField . upto > 0 ) { List l = ( List ) byField . get ( perField . fieldInfo ) ; if ( l == null ) { l = new ArrayList ( ) ; byField . put ( perField . fieldInfo , l ) ; } l . add ( perField ) ; } else fieldsIt . remove ( ) ; } } final String normsFileName = state . segmentName + "." + IndexFileNames . NORMS_EXTENSION ; state . flushedFiles . add ( normsFileName ) ; IndexOutput normsOut = state . directory . createOutput ( normsFileName ) ; try { normsOut . writeBytes ( SegmentMerger . NORMS_HEADER , 0 , SegmentMerger . NORMS_HEADER . length ) ; final int numField = fieldInfos . size ( ) ; int normCount = 0 ; for ( int fieldNumber = 0 ; fieldNumber < numField ; fieldNumber ++ ) { final FieldInfo fieldInfo = fieldInfos . fieldInfo ( fieldNumber ) ; List toMerge = ( List ) byField . get ( fieldInfo ) ; int upto = 0 ; if ( toMerge != null ) { final int numFields = toMerge . size ( ) ; normCount ++ ; final NormsWriterPerField [ ] fields = new NormsWriterPerField [ numFields ] ; int [ ] uptos = new int [ numFields ] ; for ( int j = 0 ; j < numFields ; j ++ ) fields [ j ] = ( NormsWriterPerField ) toMerge . get ( j ) ; int numLeft = numFields ; while ( numLeft > 0 ) { assert uptos [ 0 ] < fields [ 0 ] . docIDs . length : " uptos[0]=" + uptos [ 0 ] + " len=" + ( fields [ 0 ] . docIDs . length ) ; int minLoc = 0 ; int minDocID = fields [ 0 ] . docIDs [ uptos [ 0 ] ] ; for ( int j = 1 ; j < numLeft ; j ++ ) { final int docID = fields [ j ] . docIDs [ uptos [ j ] ] ; if ( docID < minDocID ) { minDocID = docID ; minLoc = j ; } } assert minDocID < state . numDocsInRAM ; for ( ; upto < minDocID ; upto ++ ) normsOut . writeByte ( defaultNorm ) ; normsOut . writeByte ( fields [ minLoc ] . norms [ uptos [ minLoc ] ] ) ; ( uptos [ minLoc ] ) ++ ; upto ++ ; if ( uptos [ minLoc ] == fields [ minLoc ] . upto ) { fields [ minLoc ] . reset ( ) ; if ( minLoc != numLeft - 1 ) { fields [ minLoc ] = fields [ numLeft - 1 ] ; uptos [ minLoc ] = uptos [ numLeft - 1 ] ; } numLeft -- ; } } for ( ; upto < state . numDocsInRAM ; upto ++ ) normsOut . writeByte ( defaultNorm ) ; } else if ( fieldInfo . isIndexed && ! fieldInfo . omitNorms ) { normCount ++ ; for ( ; upto < state . numDocsInRAM ; upto ++ ) normsOut . writeByte ( defaultNorm ) ; } assert 4 + normCount * state . numDocsInRAM == normsOut . getFilePointer ( ) : ".nrm file size mismatch: expected=" + ( 4 + normCount * state . numDocsInRAM ) + " actual=" + normsOut . getFilePointer ( ) ; } } finally { normsOut . close ( ) ; } } void closeDocStore ( DocumentsWriter . FlushState state ) { } } 	0	['9', '2', '0', '13', '46', '32', '2', '12', '3', '0.90625', '409', '0.5', '1', '0.416666667', '0.25', '0', '0', '44', '1', '0.7778', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import org . apache . lucene . store . IndexOutput ; class DefaultSkipListWriter extends MultiLevelSkipListWriter { private int [ ] lastSkipDoc ; private int [ ] lastSkipPayloadLength ; private long [ ] lastSkipFreqPointer ; private long [ ] lastSkipProxPointer ; private IndexOutput freqOutput ; private IndexOutput proxOutput ; private int curDoc ; private boolean curStorePayloads ; private int curPayloadLength ; private long curFreqPointer ; private long curProxPointer ; DefaultSkipListWriter ( int skipInterval , int numberOfSkipLevels , int docCount , IndexOutput freqOutput , IndexOutput proxOutput ) { super ( skipInterval , numberOfSkipLevels , docCount ) ; this . freqOutput = freqOutput ; this . proxOutput = proxOutput ; lastSkipDoc = new int [ numberOfSkipLevels ] ; lastSkipPayloadLength = new int [ numberOfSkipLevels ] ; lastSkipFreqPointer = new long [ numberOfSkipLevels ] ; lastSkipProxPointer = new long [ numberOfSkipLevels ] ; } void setSkipData ( int doc , boolean storePayloads , int payloadLength ) { this . curDoc = doc ; this . curStorePayloads = storePayloads ; this . curPayloadLength = payloadLength ; this . curFreqPointer = freqOutput . getFilePointer ( ) ; if ( proxOutput != null ) this . curProxPointer = proxOutput . getFilePointer ( ) ; } protected void resetSkip ( ) { super . resetSkip ( ) ; Arrays . fill ( lastSkipDoc , 0 ) ; Arrays . fill ( lastSkipPayloadLength , - 1 ) ; Arrays . fill ( lastSkipFreqPointer , freqOutput . getFilePointer ( ) ) ; if ( proxOutput != null ) Arrays . fill ( lastSkipProxPointer , proxOutput . getFilePointer ( ) ) ; } protected void writeSkipData ( int level , IndexOutput skipBuffer ) throws IOException { if ( curStorePayloads ) { int delta = curDoc - lastSkipDoc [ level ] ; if ( curPayloadLength == lastSkipPayloadLength [ level ] ) { skipBuffer . writeVInt ( delta * 2 ) ; } else { skipBuffer . writeVInt ( delta * 2 + 1 ) ; skipBuffer . writeVInt ( curPayloadLength ) ; lastSkipPayloadLength [ level ] = curPayloadLength ; } } else { skipBuffer . writeVInt ( curDoc - lastSkipDoc [ level ] ) ; } skipBuffer . writeVInt ( ( int ) ( curFreqPointer - lastSkipFreqPointer [ level ] ) ) ; skipBuffer . writeVInt ( ( int ) ( curProxPointer - lastSkipProxPointer [ level ] ) ) ; lastSkipDoc [ level ] = curDoc ; lastSkipFreqPointer [ level ] = curFreqPointer ; lastSkipProxPointer [ level ] = curProxPointer ; } } 	0	['4', '2', '0', '4', '10', '0', '2', '2', '0', '0.484848485', '182', '1', '2', '0.625', '0.625', '1', '1', '41.75', '2', '1.25', '0']
package org . apache . lucene . util . cache ; import java . util . HashMap ; import java . util . Map ; import java . util . Set ; public class SimpleMapCache extends Cache { Map map ; public SimpleMapCache ( ) { this ( new HashMap ( ) ) ; } public SimpleMapCache ( Map map ) { this . map = map ; } public Object get ( Object key ) { return map . get ( key ) ; } public void put ( Object key , Object value ) { map . put ( key , value ) ; } public void close ( ) { } public boolean containsKey ( Object key ) { return map . containsKey ( key ) ; } public Set keySet ( ) { return map . keySet ( ) ; } Cache getSynchronizedCache ( ) { return new SynchronizedSimpleMapCache ( this ) ; } private static class SynchronizedSimpleMapCache extends SimpleMapCache { Object mutex ; SimpleMapCache cache ; SynchronizedSimpleMapCache ( SimpleMapCache cache ) { this . cache = cache ; this . mutex = this ; } public void put ( Object key , Object value ) { synchronized ( mutex ) { cache . put ( key , value ) ; } } public Object get ( Object key ) { synchronized ( mutex ) { return cache . get ( key ) ; } } public boolean containsKey ( Object key ) { synchronized ( mutex ) { return cache . containsKey ( key ) ; } } public void close ( ) { synchronized ( mutex ) { cache . close ( ) ; } } public Set keySet ( ) { synchronized ( mutex ) { return cache . keySet ( ) ; } } Cache getSynchronizedCache ( ) { return this ; } } } 	0	['8', '2', '2', '3', '15', '8', '2', '2', '7', '0.285714286', '48', '0', '0', '0.5', '0.5', '1', '1', '4.875', '1', '0.75', '0']
package org . apache . lucene . search ; import org . apache . lucene . search . Filter ; import org . apache . lucene . util . OpenBitSet ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . TermEnum ; import org . apache . lucene . index . TermDocs ; import java . util . BitSet ; import java . io . IOException ; public class PrefixFilter extends Filter { protected final Term prefix ; public PrefixFilter ( Term prefix ) { this . prefix = prefix ; } public Term getPrefix ( ) { return prefix ; } public BitSet bits ( IndexReader reader ) throws IOException { final BitSet bitSet = new BitSet ( reader . maxDoc ( ) ) ; new PrefixGenerator ( prefix ) { public void handleDoc ( int doc ) { bitSet . set ( doc ) ; } } . generate ( reader ) ; return bitSet ; } public DocIdSet getDocIdSet ( IndexReader reader ) throws IOException { final OpenBitSet bitSet = new OpenBitSet ( reader . maxDoc ( ) ) ; new PrefixGenerator ( prefix ) { public void handleDoc ( int doc ) { bitSet . set ( doc ) ; } } . generate ( reader ) ; return bitSet ; } public String toString ( ) { StringBuffer buffer = new StringBuffer ( ) ; buffer . append ( "PrefixFilter(" ) ; buffer . append ( prefix . toString ( ) ) ; buffer . append ( ")" ) ; return buffer . toString ( ) ; } } interface IdGenerator { public void generate ( IndexReader reader ) throws IOException ; public void handleDoc ( int doc ) ; } abstract class PrefixGenerator implements IdGenerator { protected final Term prefix ; PrefixGenerator ( Term prefix ) { this . prefix = prefix ; } public void generate ( IndexReader reader ) throws IOException { TermEnum enumerator = reader . terms ( prefix ) ; TermDocs termDocs = reader . termDocs ( ) ; try { String prefixText = prefix . text ( ) ; String prefixField = prefix . field ( ) ; do { Term term = enumerator . term ( ) ; if ( term != null && term . text ( ) . startsWith ( prefixText ) && term . field ( ) == prefixField ) { termDocs . seek ( term ) ; while ( termDocs . next ( ) ) { handleDoc ( termDocs . doc ( ) ) ; } } else { break ; } } while ( enumerator . next ( ) ) ; } finally { termDocs . close ( ) ; enumerator . close ( ) ; } } } 	0	['5', '2', '0', '7', '17', '0', '2', '7', '5', '0', '71', '1', '1', '0.333333333', '0.533333333', '1', '1', '13', '1', '0.8', '0']
package org . apache . lucene . index ; import java . util . * ; public class SortedTermVectorMapper extends TermVectorMapper { private SortedSet currentSet ; private Map termToTVE = new HashMap ( ) ; private boolean storeOffsets ; private boolean storePositions ; public static final String ALL = "_ALL_" ; public SortedTermVectorMapper ( Comparator comparator ) { this ( false , false , comparator ) ; } public SortedTermVectorMapper ( boolean ignoringPositions , boolean ignoringOffsets , Comparator comparator ) { super ( ignoringPositions , ignoringOffsets ) ; currentSet = new TreeSet ( comparator ) ; } public void map ( String term , int frequency , TermVectorOffsetInfo [ ] offsets , int [ ] positions ) { TermVectorEntry entry = ( TermVectorEntry ) termToTVE . get ( term ) ; if ( entry == null ) { entry = new TermVectorEntry ( ALL , term , frequency , storeOffsets == true ? offsets : null , storePositions == true ? positions : null ) ; termToTVE . put ( term , entry ) ; currentSet . add ( entry ) ; } else { entry . setFrequency ( entry . getFrequency ( ) + frequency ) ; if ( storeOffsets ) { TermVectorOffsetInfo [ ] existingOffsets = entry . getOffsets ( ) ; if ( existingOffsets != null && offsets != null && offsets . length > 0 ) { TermVectorOffsetInfo [ ] newOffsets = new TermVectorOffsetInfo [ existingOffsets . length + offsets . length ] ; System . arraycopy ( existingOffsets , 0 , newOffsets , 0 , existingOffsets . length ) ; System . arraycopy ( offsets , 0 , newOffsets , existingOffsets . length , offsets . length ) ; entry . setOffsets ( newOffsets ) ; } else if ( existingOffsets == null && offsets != null && offsets . length > 0 ) { entry . setOffsets ( offsets ) ; } } if ( storePositions ) { int [ ] existingPositions = entry . getPositions ( ) ; if ( existingPositions != null && positions != null && positions . length > 0 ) { int [ ] newPositions = new int [ existingPositions . length + positions . length ] ; System . arraycopy ( existingPositions , 0 , newPositions , 0 , existingPositions . length ) ; System . arraycopy ( positions , 0 , newPositions , existingPositions . length , positions . length ) ; entry . setPositions ( newPositions ) ; } else if ( existingPositions == null && positions != null && positions . length > 0 ) { entry . setPositions ( positions ) ; } } } } public void setExpectations ( String field , int numTerms , boolean storeOffsets , boolean storePositions ) { this . storeOffsets = storeOffsets ; this . storePositions = storePositions ; } public SortedSet getTermVectorEntrySet ( ) { return currentSet ; } } 	0	['5', '2', '0', '3', '19', '2', '0', '3', '5', '0.7', '188', '0.8', '0', '0.625', '0.428571429', '0', '0', '35.6', '18', '4', '0']
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . FieldCache ; import java . io . IOException ; public class OrdFieldSource extends ValueSource { protected String field ; public OrdFieldSource ( String field ) { this . field = field ; } public String description ( ) { return "ord(" + field + ')' ; } public DocValues getValues ( IndexReader reader ) throws IOException { final int [ ] arr = FieldCache . DEFAULT . getStringIndex ( reader , field ) . order ; return new DocValues ( ) { public float floatVal ( int doc ) { return ( float ) arr [ doc ] ; } public String strVal ( int doc ) { return Integer . toString ( arr [ doc ] ) ; } public String toString ( int doc ) { return description ( ) + '=' + intVal ( doc ) ; } Object getInnerArray ( ) { return arr ; } } ; } public boolean equals ( Object o ) { if ( o . getClass ( ) != OrdFieldSource . class ) return false ; OrdFieldSource other = ( OrdFieldSource ) o ; return this . field . equals ( other . field ) ; } private static final int hcode = OrdFieldSource . class . hashCode ( ) ; public int hashCode ( ) { return hcode + field . hashCode ( ) ; } } 	0	['7', '2', '0', '6', '21', '0', '1', '6', '5', '0.666666667', '90', '0.666666667', '0', '0.5', '0.375', '2', '2', '11.42857143', '3', '1', '0']
package org . apache . lucene . document ; import java . util . HashMap ; import java . util . List ; import java . util . Map ; public class MapFieldSelector implements FieldSelector { Map fieldSelections ; public MapFieldSelector ( Map fieldSelections ) { this . fieldSelections = fieldSelections ; } public MapFieldSelector ( List fields ) { fieldSelections = new HashMap ( fields . size ( ) * 5 / 3 ) ; for ( int i = 0 ; i < fields . size ( ) ; i ++ ) fieldSelections . put ( fields . get ( i ) , FieldSelectorResult . LOAD ) ; } public MapFieldSelector ( String [ ] fields ) { fieldSelections = new HashMap ( fields . length * 5 / 3 ) ; for ( int i = 0 ; i < fields . length ; i ++ ) fieldSelections . put ( fields [ i ] , FieldSelectorResult . LOAD ) ; } public FieldSelectorResult accept ( String field ) { FieldSelectorResult selection = ( FieldSelectorResult ) fieldSelections . get ( field ) ; return selection != null ? selection : FieldSelectorResult . NO_LOAD ; } } 	0	['4', '1', '0', '2', '10', '0', '0', '2', '4', '0', '83', '0', '0', '0', '0.4', '0', '0', '19.5', '2', '0.5', '0']
package org . apache . lucene . analysis ; import java . io . * ; class PorterStemmer { private char [ ] b ; private int i , j , k , k0 ; private boolean dirty = false ; private static final int INC = 50 ; private static final int EXTRA = 1 ; public PorterStemmer ( ) { b = new char [ INC ] ; i = 0 ; } public void reset ( ) { i = 0 ; dirty = false ; } public void add ( char ch ) { if ( b . length <= i + EXTRA ) { char [ ] new_b = new char [ b . length + INC ] ; System . arraycopy ( b , 0 , new_b , 0 , b . length ) ; b = new_b ; } b [ i ++ ] = ch ; } public String toString ( ) { return new String ( b , 0 , i ) ; } public int getResultLength ( ) { return i ; } public char [ ] getResultBuffer ( ) { return b ; } private final boolean cons ( int i ) { switch ( b [ i ] ) { case 'a' : case 'e' : case 'i' : case 'o' : case 'u' : return false ; case 'y' : return ( i == k0 ) ? true : ! cons ( i - 1 ) ; default : return true ; } } private final int m ( ) { int n = 0 ; int i = k0 ; while ( true ) { if ( i > j ) return n ; if ( ! cons ( i ) ) break ; i ++ ; } i ++ ; while ( true ) { while ( true ) { if ( i > j ) return n ; if ( cons ( i ) ) break ; i ++ ; } i ++ ; n ++ ; while ( true ) { if ( i > j ) return n ; if ( ! cons ( i ) ) break ; i ++ ; } i ++ ; } } private final boolean vowelinstem ( ) { int i ; for ( i = k0 ; i <= j ; i ++ ) if ( ! cons ( i ) ) return true ; return false ; } private final boolean doublec ( int j ) { if ( j < k0 + 1 ) return false ; if ( b [ j ] != b [ j - 1 ] ) return false ; return cons ( j ) ; } private final boolean cvc ( int i ) { if ( i < k0 + 2 || ! cons ( i ) || cons ( i - 1 ) || ! cons ( i - 2 ) ) return false ; else { int ch = b [ i ] ; if ( ch == 'w' || ch == 'x' || ch == 'y' ) return false ; } return true ; } private final boolean ends ( String s ) { int l = s . length ( ) ; int o = k - l + 1 ; if ( o < k0 ) return false ; for ( int i = 0 ; i < l ; i ++ ) if ( b [ o + i ] != s . charAt ( i ) ) return false ; j = k - l ; return true ; } void setto ( String s ) { int l = s . length ( ) ; int o = j + 1 ; for ( int i = 0 ; i < l ; i ++ ) b [ o + i ] = s . charAt ( i ) ; k = j + l ; dirty = true ; } void r ( String s ) { if ( m ( ) > 0 ) setto ( s ) ; } private final void step1 ( ) { if ( b [ k ] == 's' ) { if ( ends ( "sses" ) ) k -= 2 ; else if ( ends ( "ies" ) ) setto ( "i" ) ; else if ( b [ k - 1 ] != 's' ) k -- ; } if ( ends ( "eed" ) ) { if ( m ( ) > 0 ) k -- ; } else if ( ( ends ( "ed" ) || ends ( "ing" ) ) && vowelinstem ( ) ) { k = j ; if ( ends ( "at" ) ) setto ( "ate" ) ; else if ( ends ( "bl" ) ) setto ( "ble" ) ; else if ( ends ( "iz" ) ) setto ( "ize" ) ; else if ( doublec ( k ) ) { int ch = b [ k -- ] ; if ( ch == 'l' || ch == 's' || ch == 'z' ) k ++ ; } else if ( m ( ) == 1 && cvc ( k ) ) setto ( "e" ) ; } } private final void step2 ( ) { if ( ends ( "y" ) && vowelinstem ( ) ) { b [ k ] = 'i' ; dirty = true ; } } private final void step3 ( ) { if ( k == k0 ) return ; switch ( b [ k - 1 ] ) { case 'a' : if ( ends ( "ational" ) ) { r ( "ate" ) ; break ; } if ( ends ( "tional" ) ) { r ( "tion" ) ; break ; } break ; case 'c' : if ( ends ( "enci" ) ) { r ( "ence" ) ; break ; } if ( ends ( "anci" ) ) { r ( "ance" ) ; break ; } break ; case 'e' : if ( ends ( "izer" ) ) { r ( "ize" ) ; break ; } break ; case 'l' : if ( ends ( "bli" ) ) { r ( "ble" ) ; break ; } if ( ends ( "alli" ) ) { r ( "al" ) ; break ; } if ( ends ( "entli" ) ) { r ( "ent" ) ; break ; } if ( ends ( "eli" ) ) { r ( "e" ) ; break ; } if ( ends ( "ousli" ) ) { r ( "ous" ) ; break ; } break ; case 'o' : if ( ends ( "ization" ) ) { r ( "ize" ) ; break ; } if ( ends ( "ation" ) ) { r ( "ate" ) ; break ; } if ( ends ( "ator" ) ) { r ( "ate" ) ; break ; } break ; case 's' : if ( ends ( "alism" ) ) { r ( "al" ) ; break ; } if ( ends ( "iveness" ) ) { r ( "ive" ) ; break ; } if ( ends ( "fulness" ) ) { r ( "ful" ) ; break ; } if ( ends ( "ousness" ) ) { r ( "ous" ) ; break ; } break ; case 't' : if ( ends ( "aliti" ) ) { r ( "al" ) ; break ; } if ( ends ( "iviti" ) ) { r ( "ive" ) ; break ; } if ( ends ( "biliti" ) ) { r ( "ble" ) ; break ; } break ; case 'g' : if ( ends ( "logi" ) ) { r ( "log" ) ; break ; } } } private final void step4 ( ) { switch ( b [ k ] ) { case 'e' : if ( ends ( "icate" ) ) { r ( "ic" ) ; break ; } if ( ends ( "ative" ) ) { r ( "" ) ; break ; } if ( ends ( "alize" ) ) { r ( "al" ) ; break ; } break ; case 'i' : if ( ends ( "iciti" ) ) { r ( "ic" ) ; break ; } break ; case 'l' : if ( ends ( "ical" ) ) { r ( "ic" ) ; break ; } if ( ends ( "ful" ) ) { r ( "" ) ; break ; } break ; case 's' : if ( ends ( "ness" ) ) { r ( "" ) ; break ; } break ; } } private final void step5 ( ) { if ( k == k0 ) return ; switch ( b [ k - 1 ] ) { case 'a' : if ( ends ( "al" ) ) break ; return ; case 'c' : if ( ends ( "ance" ) ) break ; if ( ends ( "ence" ) ) break ; return ; case 'e' : if ( ends ( "er" ) ) break ; return ; case 'i' : if ( ends ( "ic" ) ) break ; return ; case 'l' : if ( ends ( "able" ) ) break ; if ( ends ( "ible" ) ) break ; return ; case 'n' : if ( ends ( "ant" ) ) break ; if ( ends ( "ement" ) ) break ; if ( ends ( "ment" ) ) break ; if ( ends ( "ent" ) ) break ; return ; case 'o' : if ( ends ( "ion" ) && j >= 0 && ( b [ j ] == 's' || b [ j ] == 't' ) ) break ; if ( ends ( "ou" ) ) break ; return ; case 's' : if ( ends ( "ism" ) ) break ; return ; case 't' : if ( ends ( "ate" ) ) break ; if ( ends ( "iti" ) ) break ; return ; case 'u' : if ( ends ( "ous" ) ) break ; return ; case 'v' : if ( ends ( "ive" ) ) break ; return ; case 'z' : if ( ends ( "ize" ) ) break ; return ; default : return ; } if ( m ( ) > 1 ) k = j ; } private final void step6 ( ) { j = k ; if ( b [ k ] == 'e' ) { int a = m ( ) ; if ( a > 1 || a == 1 && ! cvc ( k - 1 ) ) k -- ; } if ( b [ k ] == 'l' && doublec ( k ) && m ( ) > 1 ) k -- ; } public String stem ( String s ) { if ( stem ( s . toCharArray ( ) , s . length ( ) ) ) return toString ( ) ; else return s ; } public boolean stem ( char [ ] word ) { return stem ( word , word . length ) ; } public boolean stem ( char [ ] wordBuffer , int offset , int wordLen ) { reset ( ) ; if ( b . length < wordLen ) { char [ ] new_b = new char [ wordLen + EXTRA ] ; b = new_b ; } System . arraycopy ( wordBuffer , offset , b , 0 , wordLen ) ; i = wordLen ; return stem ( 0 ) ; } public boolean stem ( char [ ] word , int wordLen ) { return stem ( word , 0 , wordLen ) ; } public boolean stem ( ) { return stem ( 0 ) ; } public boolean stem ( int i0 ) { k = i - 1 ; k0 = i0 ; if ( k > k0 + 1 ) { step1 ( ) ; step2 ( ) ; step3 ( ) ; step4 ( ) ; step5 ( ) ; step6 ( ) ; } if ( i != k + 1 ) dirty = true ; i = k + 1 ; return dirty ; } public static void main ( String [ ] args ) { PorterStemmer s = new PorterStemmer ( ) ; for ( int i = 0 ; i < args . length ; i ++ ) { try { InputStream in = new FileInputStream ( args [ i ] ) ; byte [ ] buffer = new byte [ 1024 ] ; int bufferLen , offset , ch ; bufferLen = in . read ( buffer ) ; offset = 0 ; s . reset ( ) ; while ( true ) { if ( offset < bufferLen ) ch = buffer [ offset ++ ] ; else { bufferLen = in . read ( buffer ) ; offset = 0 ; if ( bufferLen < 0 ) ch = - 1 ; else ch = buffer [ offset ++ ] ; } if ( Character . isLetter ( ( char ) ch ) ) { s . add ( Character . toLowerCase ( ( char ) ch ) ) ; } else { s . stem ( ) ; System . out . print ( s . toString ( ) ) ; s . reset ( ) ; if ( ch < 0 ) break ; else { System . out . print ( ( char ) ch ) ; } } } in . close ( ) ; } catch ( IOException e ) { System . out . println ( "error reading " + args [ i ] ) ; } } } } 	0	['27', '1', '0', '1', '44', '13', '1', '0', '13', '0.600961538', '1158', '1', '0', '0', '0.25308642', '0', '0', '41.59259259', '26', '5.6667', '0']
package org . apache . lucene . index ; import java . io . IOException ; public interface TermPositions extends TermDocs { int nextPosition ( ) throws IOException ; int getPayloadLength ( ) ; byte [ ] getPayload ( byte [ ] data , int offset ) throws IOException ; public boolean isPayloadAvailable ( ) ; } 	0	['4', '1', '0', '25', '4', '6', '24', '1', '4', '2', '4', '0', '0', '0', '0.5', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . FieldCache ; import java . io . IOException ; public class ReverseOrdFieldSource extends ValueSource { public String field ; public ReverseOrdFieldSource ( String field ) { this . field = field ; } public String description ( ) { return "rord(" + field + ')' ; } public DocValues getValues ( IndexReader reader ) throws IOException { final FieldCache . StringIndex sindex = FieldCache . DEFAULT . getStringIndex ( reader , field ) ; final int arr [ ] = sindex . order ; final int end = sindex . lookup . length ; return new DocValues ( ) { public float floatVal ( int doc ) { return ( float ) ( end - arr [ doc ] ) ; } public int intVal ( int doc ) { return end - arr [ doc ] ; } public String strVal ( int doc ) { return Integer . toString ( intVal ( doc ) ) ; } public String toString ( int doc ) { return description ( ) + '=' + strVal ( doc ) ; } Object getInnerArray ( ) { return arr ; } } ; } public boolean equals ( Object o ) { if ( o . getClass ( ) != ReverseOrdFieldSource . class ) return false ; ReverseOrdFieldSource other = ( ReverseOrdFieldSource ) o ; return this . field . equals ( other . field ) ; } private static final int hcode = ReverseOrdFieldSource . class . hashCode ( ) ; public int hashCode ( ) { return hcode + field . hashCode ( ) ; } } 	0	['7', '2', '0', '6', '21', '0', '1', '6', '5', '0.666666667', '97', '0.333333333', '0', '0.5', '0.375', '2', '2', '12.42857143', '3', '1', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; public final class PorterStemFilter extends TokenFilter { private PorterStemmer stemmer ; public PorterStemFilter ( TokenStream in ) { super ( in ) ; stemmer = new PorterStemmer ( ) ; } public final Token next ( final Token reusableToken ) throws IOException { assert reusableToken != null ; Token nextToken = input . next ( reusableToken ) ; if ( nextToken == null ) return null ; if ( stemmer . stem ( nextToken . termBuffer ( ) , 0 , nextToken . termLength ( ) ) ) nextToken . setTermBuffer ( stemmer . getResultBuffer ( ) , 0 , stemmer . getResultLength ( ) ) ; return nextToken ; } } 	0	['4', '3', '0', '4', '18', '2', '0', '4', '2', '0.777777778', '78', '0.333333333', '1', '0.777777778', '0.416666667', '1', '2', '17.75', '1', '0.5', '0']
package org . apache . lucene . index ; import java . io . IOException ; import java . util . Arrays ; import org . apache . lucene . store . IndexInput ; class DefaultSkipListReader extends MultiLevelSkipListReader { private boolean currentFieldStoresPayloads ; private long freqPointer [ ] ; private long proxPointer [ ] ; private int payloadLength [ ] ; private long lastFreqPointer ; private long lastProxPointer ; private int lastPayloadLength ; DefaultSkipListReader ( IndexInput skipStream , int maxSkipLevels , int skipInterval ) { super ( skipStream , maxSkipLevels , skipInterval ) ; freqPointer = new long [ maxSkipLevels ] ; proxPointer = new long [ maxSkipLevels ] ; payloadLength = new int [ maxSkipLevels ] ; } void init ( long skipPointer , long freqBasePointer , long proxBasePointer , int df , boolean storesPayloads ) { super . init ( skipPointer , df ) ; this . currentFieldStoresPayloads = storesPayloads ; lastFreqPointer = freqBasePointer ; lastProxPointer = proxBasePointer ; Arrays . fill ( freqPointer , freqBasePointer ) ; Arrays . fill ( proxPointer , proxBasePointer ) ; Arrays . fill ( payloadLength , 0 ) ; } long getFreqPointer ( ) { return lastFreqPointer ; } long getProxPointer ( ) { return lastProxPointer ; } int getPayloadLength ( ) { return lastPayloadLength ; } protected void seekChild ( int level ) throws IOException { super . seekChild ( level ) ; freqPointer [ level ] = lastFreqPointer ; proxPointer [ level ] = lastProxPointer ; payloadLength [ level ] = lastPayloadLength ; } protected void setLastSkipData ( int level ) { super . setLastSkipData ( level ) ; lastFreqPointer = freqPointer [ level ] ; lastProxPointer = proxPointer [ level ] ; lastPayloadLength = payloadLength [ level ] ; } protected int readSkipData ( int level , IndexInput skipStream ) throws IOException { int delta ; if ( currentFieldStoresPayloads ) { delta = skipStream . readVInt ( ) ; if ( ( delta & 1 ) != 0 ) { payloadLength [ level ] = skipStream . readVInt ( ) ; } delta >>>= 1 ; } else { delta = skipStream . readVInt ( ) ; } freqPointer [ level ] += skipStream . readVInt ( ) ; proxPointer [ level ] += skipStream . readVInt ( ) ; return delta ; } } 	0	['8', '2', '0', '3', '15', '0', '1', '2', '0', '0.571428571', '158', '1', '0', '0.5625', '0.425', '1', '3', '17.875', '1', '0.875', '0']
package org . apache . lucene . index ; import java . io . IOException ; import org . apache . lucene . store . IndexInput ; final class SegmentTermEnum extends TermEnum implements Cloneable { private IndexInput input ; FieldInfos fieldInfos ; long size ; long position = - 1 ; private TermBuffer termBuffer = new TermBuffer ( ) ; private TermBuffer prevBuffer = new TermBuffer ( ) ; private TermBuffer scanBuffer = new TermBuffer ( ) ; private TermInfo termInfo = new TermInfo ( ) ; private int format ; private boolean isIndex = false ; long indexPointer = 0 ; int indexInterval ; int skipInterval ; int maxSkipLevels ; private int formatM1SkipInterval ; SegmentTermEnum ( IndexInput i , FieldInfos fis , boolean isi ) throws CorruptIndexException , IOException { input = i ; fieldInfos = fis ; isIndex = isi ; maxSkipLevels = 1 ; int firstInt = input . readInt ( ) ; if ( firstInt >= 0 ) { format = 0 ; size = firstInt ; indexInterval = 128 ; skipInterval = Integer . MAX_VALUE ; } else { format = firstInt ; if ( format < TermInfosWriter . FORMAT_CURRENT ) throw new CorruptIndexException ( "Unknown format version:" + format + " expected " + TermInfosWriter . FORMAT_CURRENT + " or higher" ) ; size = input . readLong ( ) ; if ( format == - 1 ) { if ( ! isIndex ) { indexInterval = input . readInt ( ) ; formatM1SkipInterval = input . readInt ( ) ; } skipInterval = Integer . MAX_VALUE ; } else { indexInterval = input . readInt ( ) ; skipInterval = input . readInt ( ) ; if ( format <= TermInfosWriter . FORMAT ) { maxSkipLevels = input . readInt ( ) ; } } } if ( format > TermInfosWriter . FORMAT_VERSION_UTF8_LENGTH_IN_BYTES ) { termBuffer . setPreUTF8Strings ( ) ; scanBuffer . setPreUTF8Strings ( ) ; prevBuffer . setPreUTF8Strings ( ) ; } } protected Object clone ( ) { SegmentTermEnum clone = null ; try { clone = ( SegmentTermEnum ) super . clone ( ) ; } catch ( CloneNotSupportedException e ) { } clone . input = ( IndexInput ) input . clone ( ) ; clone . termInfo = new TermInfo ( termInfo ) ; clone . termBuffer = ( TermBuffer ) termBuffer . clone ( ) ; clone . prevBuffer = ( TermBuffer ) prevBuffer . clone ( ) ; clone . scanBuffer = new TermBuffer ( ) ; return clone ; } final void seek ( long pointer , int p , Term t , TermInfo ti ) throws IOException { input . seek ( pointer ) ; position = p ; termBuffer . set ( t ) ; prevBuffer . reset ( ) ; termInfo . set ( ti ) ; } public final boolean next ( ) throws IOException { if ( position ++ >= size - 1 ) { prevBuffer . set ( termBuffer ) ; termBuffer . reset ( ) ; return false ; } prevBuffer . set ( termBuffer ) ; termBuffer . read ( input , fieldInfos ) ; termInfo . docFreq = input . readVInt ( ) ; termInfo . freqPointer += input . readVLong ( ) ; termInfo . proxPointer += input . readVLong ( ) ; if ( format == - 1 ) { if ( ! isIndex ) { if ( termInfo . docFreq > formatM1SkipInterval ) { termInfo . skipOffset = input . readVInt ( ) ; } } } else { if ( termInfo . docFreq >= skipInterval ) termInfo . skipOffset = input . readVInt ( ) ; } if ( isIndex ) indexPointer += input . readVLong ( ) ; return true ; } final int scanTo ( Term term ) throws IOException { scanBuffer . set ( term ) ; int count = 0 ; while ( scanBuffer . compareTo ( termBuffer ) > 0 && next ( ) ) { count ++ ; } return count ; } public final Term term ( ) { return termBuffer . toTerm ( ) ; } final Term prev ( ) { return prevBuffer . toTerm ( ) ; } final TermInfo termInfo ( ) { return new TermInfo ( termInfo ) ; } final void termInfo ( TermInfo ti ) { ti . set ( termInfo ) ; } public final int docFreq ( ) { return termInfo . docFreq ; } final long freqPointer ( ) { return termInfo . freqPointer ; } final long proxPointer ( ) { return termInfo . proxPointer ; } public final void close ( ) throws IOException { input . close ( ) ; } } 	0	['13', '2', '0', '11', '39', '0', '4', '7', '4', '0.761111111', '394', '0.533333333', '6', '0.294117647', '0.211538462', '1', '2', '28.15384615', '1', '0.9231', '0']
package org . apache . lucene . index ; import java . util . Map ; import java . io . IOException ; abstract class InvertedDocEndConsumer { abstract InvertedDocEndConsumerPerThread addThread ( DocInverterPerThread docInverterPerThread ) ; abstract void flush ( Map threadsAndFields , DocumentsWriter . FlushState state ) throws IOException ; abstract void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException ; abstract void abort ( ) ; abstract void setFieldInfos ( FieldInfos fieldInfos ) ; } 	0	['6', '1', '1', '7', '7', '15', '4', '4', '0', '2', '9', '0', '0', '0', '0.366666667', '0', '0', '0.5', '1', '0.8333', '0']
package org . apache . lucene . queryParser ; public interface QueryParserConstants { int EOF = 0 ; int _NUM_CHAR = 1 ; int _ESCAPED_CHAR = 2 ; int _TERM_START_CHAR = 3 ; int _TERM_CHAR = 4 ; int _WHITESPACE = 5 ; int _QUOTED_CHAR = 6 ; int AND = 8 ; int OR = 9 ; int NOT = 10 ; int PLUS = 11 ; int MINUS = 12 ; int LPAREN = 13 ; int RPAREN = 14 ; int COLON = 15 ; int STAR = 16 ; int CARAT = 17 ; int QUOTED = 18 ; int TERM = 19 ; int FUZZY_SLOP = 20 ; int PREFIXTERM = 21 ; int WILDTERM = 22 ; int RANGEIN_START = 23 ; int RANGEEX_START = 24 ; int NUMBER = 25 ; int RANGEIN_TO = 26 ; int RANGEIN_END = 27 ; int RANGEIN_QUOTED = 28 ; int RANGEIN_GOOP = 29 ; int RANGEEX_TO = 30 ; int RANGEEX_END = 31 ; int RANGEEX_QUOTED = 32 ; int RANGEEX_GOOP = 33 ; int Boost = 0 ; int RangeEx = 1 ; int RangeIn = 2 ; int DEFAULT = 3 ; String [ ] tokenImage = { "<EOF>" , "<_NUM_CHAR>" , "<_ESCAPED_CHAR>" , "<_TERM_START_CHAR>" , "<_TERM_CHAR>" , "<_WHITESPACE>" , "<_QUOTED_CHAR>" , "<token of kind 7>" , "<AND>" , "<OR>" , "<NOT>" , "\"+\"" , "\"-\"" , "\"(\"" , "\")\"" , "\":\"" , "\"*\"" , "\"^\"" , "<QUOTED>" , "<TERM>" , "<FUZZY_SLOP>" , "<PREFIXTERM>" , "<WILDTERM>" , "\"[\"" , "\"{\"" , "<NUMBER>" , "\"TO\"" , "\"]\"" , "<RANGEIN_QUOTED>" , "<RANGEIN_GOOP>" , "\"TO\"" , "\"}\"" , "<RANGEEX_QUOTED>" , "<RANGEEX_GOOP>" , } ; } 	0	['1', '1', '0', '2', '1', '0', '2', '0', '0', '2', '179', '0', '0', '0', '0', '0', '0', '140', '0', '0', '0']
package org . apache . lucene . index ; import java . util . Map ; import java . io . IOException ; abstract class InvertedDocConsumer { abstract InvertedDocConsumerPerThread addThread ( DocInverterPerThread docInverterPerThread ) ; abstract void abort ( ) ; abstract void flush ( Map threadsAndFields , DocumentsWriter . FlushState state ) throws IOException ; abstract void closeDocStore ( DocumentsWriter . FlushState state ) throws IOException ; abstract boolean freeRAM ( ) ; FieldInfos fieldInfos ; void setFieldInfos ( FieldInfos fieldInfos ) { this . fieldInfos = fieldInfos ; } } 	0	['7', '1', '1', '7', '8', '21', '4', '4', '0', '1', '15', '0', '1', '0', '0.342857143', '0', '0', '1', '1', '0.8571', '0']
package org . apache . lucene . search ; import org . apache . lucene . index . IndexReader ; import java . io . IOException ; public abstract class SpanFilter extends Filter { public abstract SpanFilterResult bitSpans ( IndexReader reader ) throws IOException ; } 	0	['2', '2', '2', '5', '3', '1', '2', '3', '2', '2', '5', '0', '0', '0.666666667', '0.75', '0', '0', '1.5', '1', '0.5', '0']
package org . apache . lucene . index ; final class CharBlockPool { public char [ ] [ ] buffers = new char [ 10 ] [ ] ; int numBuffer ; int bufferUpto = - 1 ; public int charUpto = DocumentsWriter . CHAR_BLOCK_SIZE ; public char [ ] buffer ; public int charOffset = - DocumentsWriter . CHAR_BLOCK_SIZE ; final private DocumentsWriter docWriter ; public CharBlockPool ( DocumentsWriter docWriter ) { this . docWriter = docWriter ; } public void reset ( ) { docWriter . recycleCharBlocks ( buffers , 1 + bufferUpto ) ; bufferUpto = - 1 ; charUpto = DocumentsWriter . CHAR_BLOCK_SIZE ; charOffset = - DocumentsWriter . CHAR_BLOCK_SIZE ; } public void nextBuffer ( ) { if ( 1 + bufferUpto == buffers . length ) { char [ ] [ ] newBuffers = new char [ ( int ) ( buffers . length * 1.5 ) ] [ ] ; System . arraycopy ( buffers , 0 , newBuffers , 0 , buffers . length ) ; buffers = newBuffers ; } buffer = buffers [ 1 + bufferUpto ] = docWriter . getCharBlock ( ) ; bufferUpto ++ ; charUpto = 0 ; charOffset += DocumentsWriter . CHAR_BLOCK_SIZE ; } } 	0	['3', '1', '0', '5', '7', '0', '4', '1', '3', '0.357142857', '106', '0.142857143', '1', '0', '0.666666667', '0', '0', '32', '2', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . util . Parameter ; public class BooleanClause implements java . io . Serializable { public static final class Occur extends Parameter implements java . io . Serializable { private Occur ( String name ) { super ( name ) ; } public String toString ( ) { if ( this == MUST ) return "+" ; if ( this == MUST_NOT ) return "-" ; return "" ; } public static final Occur MUST = new Occur ( "MUST" ) ; public static final Occur SHOULD = new Occur ( "SHOULD" ) ; public static final Occur MUST_NOT = new Occur ( "MUST_NOT" ) ; } private Query query ; private Occur occur ; public BooleanClause ( Query query , Occur occur ) { this . query = query ; this . occur = occur ; } public Occur getOccur ( ) { return occur ; } public void setOccur ( Occur occur ) { this . occur = occur ; } public Query getQuery ( ) { return query ; } public void setQuery ( Query query ) { this . query = query ; } public boolean isProhibited ( ) { return Occur . MUST_NOT . equals ( occur ) ; } public boolean isRequired ( ) { return Occur . MUST . equals ( occur ) ; } public boolean equals ( Object o ) { if ( ! ( o instanceof BooleanClause ) ) return false ; BooleanClause other = ( BooleanClause ) o ; return this . query . equals ( other . query ) && this . occur . equals ( other . occur ) ; } public int hashCode ( ) { return query . hashCode ( ) ^ ( Occur . MUST . equals ( occur ) ? 1 : 0 ) ^ ( Occur . MUST_NOT . equals ( occur ) ? 2 : 0 ) ; } public String toString ( ) { return occur . toString ( ) + query . toString ( ) ; } } 	0	['10', '1', '0', '7', '18', '0', '6', '2', '10', '0.333333333', '104', '1', '2', '0', '0.375', '1', '1', '9.2', '4', '1.4', '0']
package org . apache . lucene . index ; import java . util . Collection ; import java . io . IOException ; public interface IndexCommitPoint { public String getSegmentsFileName ( ) ; public Collection getFileNames ( ) throws IOException ; public void delete ( ) ; } 	0	['3', '1', '0', '2', '3', '3', '2', '0', '3', '2', '3', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . store ; import java . io . IOException ; public class NoLockFactory extends LockFactory { private static NoLock singletonLock = new NoLock ( ) ; private static NoLockFactory singleton = new NoLockFactory ( ) ; public static NoLockFactory getNoLockFactory ( ) { return singleton ; } public Lock makeLock ( String lockName ) { return singletonLock ; } public void clearLock ( String lockName ) { } ; } ; class NoLock extends Lock { public boolean obtain ( ) throws IOException { return true ; } public void release ( ) { } public boolean isLocked ( ) { return false ; } public String toString ( ) { return "NoLock" ; } } 	0	['5', '2', '0', '4', '7', '6', '1', '3', '4', '0.75', '24', '1', '2', '0.571428571', '0.625', '0', '0', '3.4', '1', '0.6', '0']
package org . apache . lucene . index ; import java . util . * ; class SegmentTermVector implements TermFreqVector { private String field ; private String terms [ ] ; private int termFreqs [ ] ; SegmentTermVector ( String field , String terms [ ] , int termFreqs [ ] ) { this . field = field ; this . terms = terms ; this . termFreqs = termFreqs ; } public String getField ( ) { return field ; } public String toString ( ) { StringBuffer sb = new StringBuffer ( ) ; sb . append ( '{' ) ; sb . append ( field ) . append ( ": " ) ; if ( terms != null ) { for ( int i = 0 ; i < terms . length ; i ++ ) { if ( i > 0 ) sb . append ( ", " ) ; sb . append ( terms [ i ] ) . append ( '/' ) . append ( termFreqs [ i ] ) ; } } sb . append ( '}' ) ; return sb . toString ( ) ; } public int size ( ) { return terms == null ? 0 : terms . length ; } public String [ ] getTerms ( ) { return terms ; } public int [ ] getTermFrequencies ( ) { return termFreqs ; } public int indexOf ( String termText ) { if ( terms == null ) return - 1 ; int res = Arrays . binarySearch ( terms , termText ) ; return res >= 0 ? res : - 1 ; } public int [ ] indexesOf ( String [ ] termNumbers , int start , int len ) { int res [ ] = new int [ len ] ; for ( int i = 0 ; i < len ; i ++ ) { res [ i ] = indexOf ( termNumbers [ start + i ] ) ; } return res ; } } 	0	['8', '1', '1', '4', '15', '0', '3', '1', '7', '0.571428571', '133', '1', '0', '0', '0.35', '0', '0', '15.25', '4', '1.75', '0']
package org . apache . lucene . search . function ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . search . function . DocValues ; import java . io . IOException ; import java . io . Serializable ; public abstract class ValueSource implements Serializable { public abstract DocValues getValues ( IndexReader reader ) throws IOException ; public abstract String description ( ) ; public String toString ( ) { return description ( ) ; } public abstract boolean equals ( Object o ) ; public abstract int hashCode ( ) ; } 	0	['6', '1', '3', '8', '7', '15', '6', '2', '6', '2', '12', '0', '0', '0', '0.444444444', '1', '1', '1', '1', '0.8333', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; public final class LengthFilter extends TokenFilter { final int min ; final int max ; public LengthFilter ( TokenStream in , int min , int max ) { super ( in ) ; this . min = min ; this . max = max ; } public final Token next ( final Token reusableToken ) throws IOException { assert reusableToken != null ; for ( Token nextToken = input . next ( reusableToken ) ; nextToken != null ; nextToken = input . next ( reusableToken ) ) { int len = nextToken . termLength ( ) ; if ( len >= min && len <= max ) { return nextToken ; } } return null ; } } 	0	['4', '3', '0', '3', '12', '2', '0', '3', '2', '0.75', '79', '0', '0', '0.777777778', '0.4', '1', '2', '17.75', '1', '0.5', '0']
package org . apache . lucene . index ; abstract class InvertedDocEndConsumerPerField { abstract void finish ( ) ; abstract void abort ( ) ; } 	0	['3', '1', '1', '5', '4', '3', '5', '0', '0', '2', '6', '0', '0', '0', '1', '0', '0', '1', '1', '0.6667', '0']
package org . apache . lucene . index ; import java . io . IOException ; final class FreqProxFieldMergeState { final FreqProxTermsWriterPerField field ; final int numPostings ; final CharBlockPool charPool ; final RawPostingList [ ] postings ; private FreqProxTermsWriter . PostingList p ; char [ ] text ; int textOffset ; private int postingUpto = - 1 ; final ByteSliceReader freq = new ByteSliceReader ( ) ; final ByteSliceReader prox = new ByteSliceReader ( ) ; int docID ; int termFreq ; public FreqProxFieldMergeState ( FreqProxTermsWriterPerField field ) { this . field = field ; this . charPool = field . perThread . termsHashPerThread . charPool ; this . numPostings = field . termsHashPerField . numPostings ; this . postings = field . termsHashPerField . sortPostings ( ) ; } boolean nextTerm ( ) throws IOException { postingUpto ++ ; if ( postingUpto == numPostings ) return false ; p = ( FreqProxTermsWriter . PostingList ) postings [ postingUpto ] ; docID = 0 ; text = charPool . buffers [ p . textStart > > DocumentsWriter . CHAR_BLOCK_SHIFT ] ; textOffset = p . textStart & DocumentsWriter . CHAR_BLOCK_MASK ; field . termsHashPerField . initReader ( freq , p , 0 ) ; if ( ! field . fieldInfo . omitTf ) field . termsHashPerField . initReader ( prox , p , 1 ) ; boolean result = nextDoc ( ) ; assert result ; return true ; } public boolean nextDoc ( ) throws IOException { if ( freq . eof ( ) ) { if ( p . lastDocCode != - 1 ) { docID = p . lastDocID ; if ( ! field . omitTf ) termFreq = p . docFreq ; p . lastDocCode = - 1 ; return true ; } else return false ; } final int code = freq . readVInt ( ) ; if ( field . omitTf ) docID += code ; else { docID += code > > > 1 ; if ( ( code & 1 ) != 0 ) termFreq = 1 ; else termFreq = freq . readVInt ( ) ; } assert docID != p . lastDocID ; return true ; } } 	0	['5', '1', '0', '10', '16', '0', '1', '9', '2', '0.75', '238', '0.142857143', '6', '0', '0.416666667', '0', '0', '43.8', '1', '0.6', '0']
package org . apache . lucene . index ; import java . io . File ; import java . io . FilenameFilter ; import java . util . HashSet ; public class IndexFileNameFilter implements FilenameFilter { static IndexFileNameFilter singleton = new IndexFileNameFilter ( ) ; private HashSet extensions ; private HashSet extensionsInCFS ; public IndexFileNameFilter ( ) { extensions = new HashSet ( ) ; for ( int i = 0 ; i < IndexFileNames . INDEX_EXTENSIONS . length ; i ++ ) { extensions . add ( IndexFileNames . INDEX_EXTENSIONS [ i ] ) ; } extensionsInCFS = new HashSet ( ) ; for ( int i = 0 ; i < IndexFileNames . INDEX_EXTENSIONS_IN_COMPOUND_FILE . length ; i ++ ) { extensionsInCFS . add ( IndexFileNames . INDEX_EXTENSIONS_IN_COMPOUND_FILE [ i ] ) ; } } public boolean accept ( File dir , String name ) { int i = name . lastIndexOf ( '.' ) ; if ( i != - 1 ) { String extension = name . substring ( 1 + i ) ; if ( extensions . contains ( extension ) ) { return true ; } else if ( extension . startsWith ( "f" ) && extension . matches ( "f\\d+" ) ) { return true ; } else if ( extension . startsWith ( "s" ) && extension . matches ( "s\\d+" ) ) { return true ; } } else { if ( name . equals ( IndexFileNames . DELETABLE ) ) return true ; else if ( name . startsWith ( IndexFileNames . SEGMENTS ) ) return true ; } return false ; } public boolean isCFSFile ( String name ) { int i = name . lastIndexOf ( '.' ) ; if ( i != - 1 ) { String extension = name . substring ( 1 + i ) ; if ( extensionsInCFS . contains ( extension ) ) { return true ; } if ( extension . startsWith ( "f" ) && extension . matches ( "f\\d+" ) ) { return true ; } } return false ; } public static IndexFileNameFilter getFilter ( ) { return singleton ; } } 	0	['5', '1', '0', '3', '14', '4', '2', '1', '4', '0.583333333', '145', '0.666666667', '1', '0', '0.5', '0', '0', '27.4', '7', '2.6', '0']
package org . apache . lucene . index ; public abstract class TermVectorMapper { private boolean ignoringPositions ; private boolean ignoringOffsets ; protected TermVectorMapper ( ) { } protected TermVectorMapper ( boolean ignoringPositions , boolean ignoringOffsets ) { this . ignoringPositions = ignoringPositions ; this . ignoringOffsets = ignoringOffsets ; } public abstract void setExpectations ( String field , int numTerms , boolean storeOffsets , boolean storePositions ) ; public abstract void map ( String term , int frequency , TermVectorOffsetInfo [ ] offsets , int [ ] positions ) ; public boolean isIgnoringPositions ( ) { return ignoringPositions ; } public boolean isIgnoringOffsets ( ) { return ignoringOffsets ; } public void setDocumentNumber ( int documentNumber ) { } } 	0	['7', '1', '4', '12', '8', '17', '11', '1', '5', '0.833333333', '28', '1', '0', '0', '0.380952381', '0', '0', '2.714285714', '1', '0.7143', '0']
package org . apache . lucene . util ; public abstract class StringHelper { public static final int bytesDifference ( byte [ ] bytes1 , int len1 , byte [ ] bytes2 , int len2 ) { int len = len1 < len2 ? len1 : len2 ; for ( int i = 0 ; i < len ; i ++ ) if ( bytes1 [ i ] != bytes2 [ i ] ) return i ; return len ; } public static final int stringDifference ( String s1 , String s2 ) { int len1 = s1 . length ( ) ; int len2 = s2 . length ( ) ; int len = len1 < len2 ? len1 : len2 ; for ( int i = 0 ; i < len ; i ++ ) { if ( s1 . charAt ( i ) != s2 . charAt ( i ) ) { return i ; } } return len ; } private StringHelper ( ) { } } 	0	['3', '1', '0', '1', '6', '3', '1', '0', '2', '2', '62', '0', '0', '0', '0.333333333', '0', '0', '19.66666667', '4', '2.6667', '0']
package org . apache . lucene . search ; import java . util . ArrayList ; import java . util . BitSet ; import java . util . List ; public class SpanFilterResult { private BitSet bits ; private DocIdSet docIdSet ; private List positions ; public SpanFilterResult ( BitSet bits , List positions ) { this . bits = bits ; this . positions = positions ; } public SpanFilterResult ( DocIdSet docIdSet , List positions ) { this . docIdSet = docIdSet ; this . positions = positions ; } public List getPositions ( ) { return positions ; } public BitSet getBits ( ) { return bits ; } public DocIdSet getDocIdSet ( ) { return docIdSet ; } public static class PositionInfo { private int doc ; private List positions ; public PositionInfo ( int doc ) { this . doc = doc ; positions = new ArrayList ( ) ; } public void addPosition ( int start , int end ) { positions . add ( new StartEnd ( start , end ) ) ; } public int getDoc ( ) { return doc ; } public List getPositions ( ) { return positions ; } } public static class StartEnd { private int start ; private int end ; public StartEnd ( int start , int end ) { this . start = start ; this . end = end ; } public int getEnd ( ) { return end ; } public int getStart ( ) { return start ; } } } 	0	['5', '1', '0', '4', '6', '0', '3', '1', '5', '0.666666667', '35', '1', '1', '0', '0.45', '0', '0', '5.4', '1', '0.6', '0']
package org . apache . lucene . store ; import java . io . IOException ; class RAMInputStream extends IndexInput implements Cloneable { static final int BUFFER_SIZE = RAMOutputStream . BUFFER_SIZE ; private RAMFile file ; private long length ; private byte [ ] currentBuffer ; private int currentBufferIndex ; private int bufferPosition ; private long bufferStart ; private int bufferLength ; RAMInputStream ( RAMFile f ) throws IOException { file = f ; length = file . length ; if ( length / BUFFER_SIZE >= Integer . MAX_VALUE ) { throw new IOException ( "Too large RAMFile! " + length ) ; } currentBufferIndex = - 1 ; currentBuffer = null ; } public void close ( ) { } public long length ( ) { return length ; } public byte readByte ( ) throws IOException { if ( bufferPosition >= bufferLength ) { currentBufferIndex ++ ; switchCurrentBuffer ( true ) ; } return currentBuffer [ bufferPosition ++ ] ; } public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { while ( len > 0 ) { if ( bufferPosition >= bufferLength ) { currentBufferIndex ++ ; switchCurrentBuffer ( true ) ; } int remainInBuffer = bufferLength - bufferPosition ; int bytesToCopy = len < remainInBuffer ? len : remainInBuffer ; System . arraycopy ( currentBuffer , bufferPosition , b , offset , bytesToCopy ) ; offset += bytesToCopy ; len -= bytesToCopy ; bufferPosition += bytesToCopy ; } } private final void switchCurrentBuffer ( boolean enforceEOF ) throws IOException { if ( currentBufferIndex >= file . numBuffers ( ) ) { if ( enforceEOF ) throw new IOException ( "Read past EOF" ) ; else { currentBufferIndex -- ; bufferPosition = BUFFER_SIZE ; } } else { currentBuffer = ( byte [ ] ) file . getBuffer ( currentBufferIndex ) ; bufferPosition = 0 ; bufferStart = ( long ) BUFFER_SIZE * ( long ) currentBufferIndex ; long buflen = length - bufferStart ; bufferLength = buflen > BUFFER_SIZE ? BUFFER_SIZE : ( int ) buflen ; } } public long getFilePointer ( ) { return currentBufferIndex < 0 ? 0 : bufferStart + bufferPosition ; } public void seek ( long pos ) throws IOException { if ( currentBuffer == null || pos < bufferStart || pos >= bufferStart + BUFFER_SIZE ) { currentBufferIndex = ( int ) ( pos / BUFFER_SIZE ) ; switchCurrentBuffer ( false ) ; } bufferPosition = ( int ) ( pos % BUFFER_SIZE ) ; } } 	0	['8', '2', '0', '3', '17', '0', '1', '2', '6', '0.5', '236', '0.875', '1', '0.708333333', '0.270833333', '1', '4', '27.5', '2', '1', '0']
package org . apache . lucene . store ; import java . io . IOException ; public class LockReleaseFailedException extends IOException { public LockReleaseFailedException ( String message ) { super ( message ) ; } } 	0	['1', '4', '0', '2', '2', '0', '2', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . index ; import java . io . IOException ; final class DocFieldConsumersPerThread extends DocFieldConsumerPerThread { final DocFieldConsumerPerThread one ; final DocFieldConsumerPerThread two ; final DocFieldConsumers parent ; final DocumentsWriter . DocState docState ; public DocFieldConsumersPerThread ( DocFieldProcessorPerThread docFieldProcessorPerThread , DocFieldConsumers parent , DocFieldConsumerPerThread one , DocFieldConsumerPerThread two ) { this . parent = parent ; this . one = one ; this . two = two ; docState = docFieldProcessorPerThread . docState ; } public void startDocument ( ) throws IOException { one . startDocument ( ) ; two . startDocument ( ) ; } public void abort ( ) { try { one . abort ( ) ; } finally { two . abort ( ) ; } } public DocumentsWriter . DocWriter finishDocument ( ) throws IOException { final DocumentsWriter . DocWriter oneDoc = one . finishDocument ( ) ; final DocumentsWriter . DocWriter twoDoc = two . finishDocument ( ) ; if ( oneDoc == null ) return twoDoc ; else if ( twoDoc == null ) return oneDoc ; else { DocFieldConsumers . PerDoc both = parent . getPerDoc ( ) ; both . docID = docState . docID ; assert oneDoc . docID == docState . docID ; assert twoDoc . docID == docState . docID ; both . one = oneDoc ; both . two = twoDoc ; return both ; } } public DocFieldConsumerPerField addField ( FieldInfo fi ) { return new DocFieldConsumersPerField ( this , one . addField ( fi ) , two . addField ( fi ) ) ; } } 	0	['7', '2', '0', '9', '19', '0', '2', '9', '5', '0.694444444', '146', '0', '4', '0.444444444', '0.277777778', '0', '0', '19', '3', '1', '0']
package org . apache . lucene . search ; import org . apache . lucene . util . PriorityQueue ; final class PhraseQueue extends PriorityQueue { PhraseQueue ( int size ) { initialize ( size ) ; } protected final boolean lessThan ( Object o1 , Object o2 ) { PhrasePositions pp1 = ( PhrasePositions ) o1 ; PhrasePositions pp2 = ( PhrasePositions ) o2 ; if ( pp1 . doc == pp2 . doc ) if ( pp1 . position == pp2 . position ) return pp1 . offset < pp2 . offset ; else return pp1 . position < pp2 . position ; else return pp1 . doc < pp2 . doc ; } } 	0	['2', '2', '0', '5', '4', '1', '3', '2', '0', '2', '51', '0', '0', '0.923076923', '0.666666667', '1', '3', '24.5', '6', '3', '0']
package org . apache . lucene . store ; import java . io . IOException ; public class RAMOutputStream extends IndexOutput { static final int BUFFER_SIZE = 1024 ; private RAMFile file ; private byte [ ] currentBuffer ; private int currentBufferIndex ; private int bufferPosition ; private long bufferStart ; private int bufferLength ; public RAMOutputStream ( ) { this ( new RAMFile ( ) ) ; } RAMOutputStream ( RAMFile f ) { file = f ; currentBufferIndex = - 1 ; currentBuffer = null ; } public void writeTo ( IndexOutput out ) throws IOException { flush ( ) ; final long end = file . length ; long pos = 0 ; int buffer = 0 ; while ( pos < end ) { int length = BUFFER_SIZE ; long nextPos = pos + length ; if ( nextPos > end ) { length = ( int ) ( end - pos ) ; } out . writeBytes ( ( byte [ ] ) file . getBuffer ( buffer ++ ) , length ) ; pos = nextPos ; } } public void reset ( ) { try { seek ( 0 ) ; } catch ( IOException e ) { throw new RuntimeException ( e . toString ( ) ) ; } file . setLength ( 0 ) ; } public void close ( ) throws IOException { flush ( ) ; } public void seek ( long pos ) throws IOException { setFileLength ( ) ; if ( pos < bufferStart || pos >= bufferStart + bufferLength ) { currentBufferIndex = ( int ) ( pos / BUFFER_SIZE ) ; switchCurrentBuffer ( ) ; } bufferPosition = ( int ) ( pos % BUFFER_SIZE ) ; } public long length ( ) { return file . length ; } public void writeByte ( byte b ) throws IOException { if ( bufferPosition == bufferLength ) { currentBufferIndex ++ ; switchCurrentBuffer ( ) ; } currentBuffer [ bufferPosition ++ ] = b ; } public void writeBytes ( byte [ ] b , int offset , int len ) throws IOException { assert b != null ; while ( len > 0 ) { if ( bufferPosition == bufferLength ) { currentBufferIndex ++ ; switchCurrentBuffer ( ) ; } int remainInBuffer = currentBuffer . length - bufferPosition ; int bytesToCopy = len < remainInBuffer ? len : remainInBuffer ; System . arraycopy ( b , offset , currentBuffer , bufferPosition , bytesToCopy ) ; offset += bytesToCopy ; len -= bytesToCopy ; bufferPosition += bytesToCopy ; } } private final void switchCurrentBuffer ( ) throws IOException { if ( currentBufferIndex == file . numBuffers ( ) ) { currentBuffer = file . addBuffer ( BUFFER_SIZE ) ; } else { currentBuffer = ( byte [ ] ) file . getBuffer ( currentBufferIndex ) ; } bufferPosition = 0 ; bufferStart = ( long ) BUFFER_SIZE * ( long ) currentBufferIndex ; bufferLength = currentBuffer . length ; } private void setFileLength ( ) { long pointer = bufferStart + bufferPosition ; if ( pointer > file . length ) { file . setLength ( pointer ) ; } } public void flush ( ) throws IOException { file . setLastModified ( System . currentTimeMillis ( ) ) ; setFileLength ( ) ; } public long getFilePointer ( ) { return currentBufferIndex < 0 ? 0 : bufferStart + bufferPosition ; } public long sizeInBytes ( ) { return file . numBuffers ( ) * BUFFER_SIZE ; } } 	0	['16', '2', '0', '11', '33', '26', '9', '2', '11', '0.688888889', '332', '0.666666667', '1', '0.566666667', '0.175', '1', '5', '19.1875', '2', '0.9375', '0']
package org . apache . lucene . store ; public class AlreadyClosedException extends IllegalStateException { public AlreadyClosedException ( String message ) { super ( message ) ; } } 	0	['1', '5', '0', '5', '2', '0', '5', '0', '1', '2', '5', '0', '0', '1', '1', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . index ; public class FieldReaderException extends RuntimeException { public FieldReaderException ( ) { } public FieldReaderException ( Throwable cause ) { super ( cause ) ; } public FieldReaderException ( String message ) { super ( message ) ; } public FieldReaderException ( String message , Throwable cause ) { super ( message , cause ) ; } } 	0	['4', '4', '0', '1', '8', '6', '1', '0', '4', '2', '20', '0', '0', '1', '0.666666667', '0', '0', '4', '0', '0', '0']
package org . apache . lucene . search ; import java . util . Iterator ; import java . util . NoSuchElementException ; public class HitIterator implements Iterator { private Hits hits ; private int hitNumber = 0 ; HitIterator ( Hits hits ) { this . hits = hits ; } public boolean hasNext ( ) { return hitNumber < hits . length ( ) ; } public Object next ( ) { if ( hitNumber == hits . length ( ) ) throw new NoSuchElementException ( ) ; Object next = new Hit ( hits , hitNumber ) ; hitNumber ++ ; return next ; } public void remove ( ) { throw new UnsupportedOperationException ( ) ; } public int length ( ) { return hits . length ( ) ; } } 	0	['5', '1', '0', '2', '10', '0', '1', '2', '4', '0.375', '60', '1', '1', '0', '0.6', '0', '0', '10.6', '2', '1.2', '0']
package org . apache . lucene . index ; import java . io . IOException ; abstract class InvertedDocConsumerPerThread { abstract void startDocument ( ) throws IOException ; abstract InvertedDocConsumerPerField addField ( DocInverterPerField docInverterPerField , FieldInfo fieldInfo ) ; abstract DocumentsWriter . DocWriter finishDocument ( ) throws IOException ; abstract void abort ( ) ; } 	0	['5', '1', '1', '9', '6', '10', '6', '4', '0', '2', '8', '0', '0', '0', '0.466666667', '0', '0', '0.6', '1', '0.8', '0']
package org . apache . lucene . analysis ; import java . io . IOException ; public class TeeTokenFilter extends TokenFilter { SinkTokenizer sink ; public TeeTokenFilter ( TokenStream input , SinkTokenizer sink ) { super ( input ) ; this . sink = sink ; } public Token next ( final Token reusableToken ) throws IOException { assert reusableToken != null ; Token nextToken = input . next ( reusableToken ) ; sink . add ( nextToken ) ; return nextToken ; } } 	0	['4', '3', '0', '4', '12', '2', '0', '4', '2', '0.777777778', '58', '0', '1', '0.777777778', '0.4', '1', '2', '12.75', '1', '0.5', '0']
package org . apache . lucene . index ; public interface TermPositionVector extends TermFreqVector { public int [ ] getTermPositions ( int index ) ; public TermVectorOffsetInfo [ ] getOffsets ( int index ) ; } 	0	['2', '1', '0', '4', '2', '1', '2', '2', '2', '2', '2', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0']
package org . apache . lucene . queryParser ; import java . io . * ; public final class FastCharStream implements CharStream { char [ ] buffer = null ; int bufferLength = 0 ; int bufferPosition = 0 ; int tokenStart = 0 ; int bufferStart = 0 ; Reader input ; public FastCharStream ( Reader r ) { input = r ; } public final char readChar ( ) throws IOException { if ( bufferPosition >= bufferLength ) refill ( ) ; return buffer [ bufferPosition ++ ] ; } private final void refill ( ) throws IOException { int newPosition = bufferLength - tokenStart ; if ( tokenStart == 0 ) { if ( buffer == null ) { buffer = new char [ 2048 ] ; } else if ( bufferLength == buffer . length ) { char [ ] newBuffer = new char [ buffer . length * 2 ] ; System . arraycopy ( buffer , 0 , newBuffer , 0 , bufferLength ) ; buffer = newBuffer ; } } else { System . arraycopy ( buffer , tokenStart , buffer , 0 , newPosition ) ; } bufferLength = newPosition ; bufferPosition = newPosition ; bufferStart += tokenStart ; tokenStart = 0 ; int charsRead = input . read ( buffer , newPosition , buffer . length - newPosition ) ; if ( charsRead == - 1 ) throw new IOException ( "read past eof" ) ; else bufferLength += charsRead ; } public final char BeginToken ( ) throws IOException { tokenStart = bufferPosition ; return readChar ( ) ; } public final void backup ( int amount ) { bufferPosition -= amount ; } public final String GetImage ( ) { return new String ( buffer , tokenStart , bufferPosition - tokenStart ) ; } public final char [ ] GetSuffix ( int len ) { char [ ] value = new char [ len ] ; System . arraycopy ( buffer , bufferPosition - len , value , 0 , len ) ; return value ; } public final void Done ( ) { try { input . close ( ) ; } catch ( IOException e ) { System . err . println ( "Caught: " + e + "; ignoring." ) ; } } public final int getColumn ( ) { return bufferStart + bufferPosition ; } public final int getLine ( ) { return 1 ; } public final int getEndColumn ( ) { return bufferStart + bufferPosition ; } public final int getEndLine ( ) { return 1 ; } public final int getBeginColumn ( ) { return bufferStart + tokenStart ; } public final int getBeginLine ( ) { return 1 ; } } 	0	['14', '1', '0', '2', '25', '3', '1', '1', '13', '0.602564103', '237', '0', '0', '0', '0.404761905', '0', '0', '15.5', '1', '0.9286', '0']
package org . apache . lucene . search ; public class DefaultSimilarity extends Similarity { public float lengthNorm ( String fieldName , int numTerms ) { return ( float ) ( 1.0 / Math . sqrt ( numTerms ) ) ; } public float queryNorm ( float sumOfSquaredWeights ) { return ( float ) ( 1.0 / Math . sqrt ( sumOfSquaredWeights ) ) ; } public float tf ( float freq ) { return ( float ) Math . sqrt ( freq ) ; } public float sloppyFreq ( int distance ) { return 1.0f / ( distance + 1 ) ; } public float idf ( int docFreq , int numDocs ) { return ( float ) ( Math . log ( numDocs / ( double ) ( docFreq + 1 ) ) + 1.0 ) ; } public float coord ( int overlap , int maxOverlap ) { return overlap / ( float ) maxOverlap ; } } 	0	['7', '2', '0', '3', '10', '21', '3', '1', '7', '2', '54', '0', '0', '0.714285714', '0.5', '1', '2', '6.714285714', '1', '0.8571', '0']
package org . apache . lucene . index ; import java . io . IOException ; final class TermsHashPerThread extends InvertedDocConsumerPerThread { final TermsHash termsHash ; final TermsHashConsumerPerThread consumer ; final TermsHashPerThread nextPerThread ; final CharBlockPool charPool ; final IntBlockPool intPool ; final ByteBlockPool bytePool ; final boolean primary ; final DocumentsWriter . DocState docState ; final RawPostingList freePostings [ ] = new RawPostingList [ 256 ] ; int freePostingsCount ; public TermsHashPerThread ( DocInverterPerThread docInverterPerThread , final TermsHash termsHash , final TermsHash nextTermsHash , final TermsHashPerThread primaryPerThread ) { docState = docInverterPerThread . docState ; this . termsHash = termsHash ; this . consumer = termsHash . consumer . addThread ( this ) ; if ( nextTermsHash != null ) { charPool = new CharBlockPool ( termsHash . docWriter ) ; primary = true ; } else { charPool = primaryPerThread . charPool ; primary = false ; } intPool = new IntBlockPool ( termsHash . docWriter , termsHash . trackAllocations ) ; bytePool = new ByteBlockPool ( termsHash . docWriter . byteBlockAllocator , termsHash . trackAllocations ) ; if ( nextTermsHash != null ) nextPerThread = nextTermsHash . addThread ( docInverterPerThread , this ) ; else nextPerThread = null ; } InvertedDocConsumerPerField addField ( DocInverterPerField docInverterPerField , final FieldInfo fieldInfo ) { return new TermsHashPerField ( docInverterPerField , this , nextPerThread , fieldInfo ) ; } synchronized public void abort ( ) { reset ( true ) ; consumer . abort ( ) ; if ( nextPerThread != null ) nextPerThread . abort ( ) ; } void morePostings ( ) throws IOException { assert freePostingsCount == 0 ; termsHash . getPostings ( freePostings ) ; freePostingsCount = freePostings . length ; assert noNullPostings ( freePostings , freePostingsCount , "consumer=" + consumer ) ; } private static boolean noNullPostings ( RawPostingList [ ] postings , int count , String details ) { for ( int i = 0 ; i < count ; i ++ ) assert postings [ i ] != null : "postings[" + i + "] of " + count + " is null: " + details ; return true ; } public void startDocument ( ) throws IOException { consumer . startDocument ( ) ; if ( nextPerThread != null ) nextPerThread . consumer . startDocument ( ) ; } public DocumentsWriter . DocWriter finishDocument ( ) throws IOException { final DocumentsWriter . DocWriter doc = consumer . finishDocument ( ) ; final DocumentsWriter . DocWriter doc2 ; if ( nextPerThread != null ) doc2 = nextPerThread . consumer . finishDocument ( ) ; else doc2 = null ; if ( doc == null ) return doc2 ; else { doc . setNext ( doc2 ) ; return doc ; } } void reset ( boolean recyclePostings ) { intPool . reset ( ) ; bytePool . reset ( ) ; if ( primary ) charPool . reset ( ) ; if ( recyclePostings ) { termsHash . recyclePostings ( freePostings , freePostingsCount ) ; freePostingsCount = 0 ; } } } 	0	['10', '2', '0', '24', '37', '7', '9', '18', '4', '0.759259259', '276', '0', '8', '0.333333333', '0.188888889', '0', '0', '25.4', '4', '1.4', '0']
